{"2025-01-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2501.07572v1","updated":"2025-01-13T18:58:07Z","published":"2025-01-13T18:58:07Z","title":"WebWalker: Benchmarking LLMs in Web Traversal","summary":"  Retrieval-augmented generation (RAG) demonstrates remarkable performance\nacross tasks in open-domain question-answering. However, traditional search\nengines may retrieve shallow content, limiting the ability of LLMs to handle\ncomplex, multi-layered information. To address it, we introduce WebWalkerQA, a\nbenchmark designed to assess the ability of LLMs to perform web traversal. It\nevaluates the capacity of LLMs to traverse a website's subpages to extract\nhigh-quality data systematically. We propose WebWalker, which is a multi-agent\nframework that mimics human-like web navigation through an explore-critic\nparadigm. Extensive experimental results show that WebWalkerQA is challenging\nand demonstrates the effectiveness of RAG combined with WebWalker, through the\nhorizontal and vertical integration in real-world scenarios.\n","authors":["Jialong Wu","Wenbiao Yin","Yong Jiang","Zhenglin Wang","Zekun Xi","Runnan Fang","Deyu Zhou","Pengjun Xie","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2501.07572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07554v1","updated":"2025-01-13T18:37:08Z","published":"2025-01-13T18:37:08Z","title":"SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal\n  Aspects in Video Editing","summary":"  Video editing models have advanced significantly, but evaluating their\nperformance remains challenging. Traditional metrics, such as CLIP text and\nimage scores, often fall short: text scores are limited by inadequate training\ndata and hierarchical dependencies, while image scores fail to assess temporal\nconsistency. We present SST-EM (Semantic, Spatial, and Temporal Evaluation\nMetric), a novel evaluation framework that leverages modern Vision-Language\nModels (VLMs), Object Detection, and Temporal Consistency checks. SST-EM\ncomprises four components: (1) semantic extraction from frames using a VLM, (2)\nprimary object tracking with Object Detection, (3) focused object refinement\nvia an LLM agent, and (4) temporal consistency assessment using a Vision\nTransformer (ViT). These components are integrated into a unified metric with\nweights derived from human evaluations and regression analysis. The name SST-EM\nreflects its focus on Semantic, Spatial, and Temporal aspects of video\nevaluation. SST-EM provides a comprehensive evaluation of semantic fidelity and\ntemporal smoothness in video editing. The source code is available in the\n\\textbf{\\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub\nRepository}}.\n","authors":["Varun Biyyala","Bharat Chanderprakash Kathuria","Jialu Li","Youshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.07554v1.pdf","comment":"WACV workshop"},{"id":"http://arxiv.org/abs/2501.07542v1","updated":"2025-01-13T18:23:57Z","published":"2025-01-13T18:23:57Z","title":"Imagine while Reasoning in Space: Multimodal Visualization-of-Thought","summary":"  Chain-of-Thought (CoT) prompting has proven highly effective for enhancing\ncomplex reasoning in Large Language Models (LLMs) and Multimodal Large Language\nModels (MLLMs). Yet, it struggles in complex spatial reasoning tasks.\nNonetheless, human cognition extends beyond language alone, enabling the\nremarkable capability to think in both words and images. Inspired by this\nmechanism, we propose a new reasoning paradigm, Multimodal\nVisualization-of-Thought (MVoT). It enables visual thinking in MLLMs by\ngenerating image visualizations of their reasoning traces. To ensure\nhigh-quality visualization, we introduce token discrepancy loss into\nautoregressive MLLMs. This innovation significantly improves both visual\ncoherence and fidelity. We validate this approach through several dynamic\nspatial reasoning tasks. Experimental results reveal that MVoT demonstrates\ncompetitive performance across tasks. Moreover, it exhibits robust and reliable\nimprovements in the most challenging scenarios where CoT fails. Ultimately,\nMVoT establishes new possibilities for complex reasoning tasks where visual\nthinking can effectively complement verbal reasoning.\n","authors":["Chengzu Li","Wenshan Wu","Huanyu Zhang","Yan Xia","Shaoguang Mao","Li Dong","Ivan Vulić","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2501.07542v1.pdf","comment":"11 pages, 6 figures, 4 tables (27 pages, 10 figures, 16 tables\n  including references and appendices)"},{"id":"http://arxiv.org/abs/2306.11207v4","updated":"2025-01-13T18:16:34Z","published":"2023-06-20T00:14:47Z","title":"Quilt-1M: One Million Image-Text Pairs for Histopathology","summary":"  Recent accelerations in multi-modal applications have been made possible with\nthe plethora of image and text data available online. However, the scarcity of\nanalogous data in the medical field, specifically in histopathology, has slowed\ncomparable progress. To enable similar representation learning for\nhistopathology, we turn to YouTube, an untapped resource of videos, offering\n$1,087$ hours of valuable educational histopathology videos from expert\nclinicians. From YouTube, we curate QUILT: a large-scale vision-language\ndataset consisting of $802, 144$ image and text pairs. QUILT was automatically\ncurated using a mixture of models, including large language models, handcrafted\nalgorithms, human knowledge databases, and automatic speech recognition. In\ncomparison, the most comprehensive datasets curated for histopathology amass\nonly around $200$K samples. We combine QUILT with datasets from other sources,\nincluding Twitter, research papers, and the internet in general, to create an\neven larger dataset: QUILT-1M, with $1$M paired image-text samples, marking it\nas the largest vision-language histopathology dataset to date. We demonstrate\nthe value of QUILT-1M by fine-tuning a pre-trained CLIP model. Our model\noutperforms state-of-the-art models on both zero-shot and linear probing tasks\nfor classifying new histopathology images across $13$ diverse patch-level\ndatasets of $8$ different sub-pathologies and cross-modal retrieval tasks.\n","authors":["Wisdom Oluchi Ikezogwo","Mehmet Saygin Seyfioglu","Fatemeh Ghezloo","Dylan Stefan Chan Geva","Fatwir Sheikh Mohammed","Pavan Kumar Anand","Ranjay Krishna","Linda Shapiro"],"pdf_url":"https://arxiv.org/pdf/2306.11207v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07532v1","updated":"2025-01-13T18:09:58Z","published":"2025-01-13T18:09:58Z","title":"Investigating Large Language Models in Inferring Personality Traits from\n  User Conversations","summary":"  Large Language Models (LLMs) are demonstrating remarkable human like\ncapabilities across diverse domains, including psychological assessment. This\nstudy evaluates whether LLMs, specifically GPT-4o and GPT-4o mini, can infer\nBig Five personality traits and generate Big Five Inventory-10 (BFI-10) item\nscores from user conversations under zero-shot prompting conditions. Our\nfindings reveal that incorporating an intermediate step--prompting for BFI-10\nitem scores before calculating traits--enhances accuracy and aligns more\nclosely with the gold standard than direct trait inference. This structured\napproach underscores the importance of leveraging psychological frameworks in\nimproving predictive precision. Additionally, a group comparison based on\ndepressive symptom presence revealed differential model performance.\nParticipants were categorized into two groups: those experiencing at least one\ndepressive symptom and those without symptoms. GPT-4o mini demonstrated\nheightened sensitivity to depression-related shifts in traits such as\nNeuroticism and Conscientiousness within the symptom-present group, whereas\nGPT-4o exhibited strengths in nuanced interpretation across groups. These\nfindings underscore the potential of LLMs to analyze real-world psychological\ndata effectively, offering a valuable foundation for interdisciplinary research\nat the intersection of artificial intelligence and psychology.\n","authors":["Jianfeng Zhu","Ruoming Jin","Karin G. Coifman"],"pdf_url":"https://arxiv.org/pdf/2501.07532v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.07523v1","updated":"2025-01-13T17:50:30Z","published":"2025-01-13T17:50:30Z","title":"Parallel Key-Value Cache Fusion for Position Invariant RAG","summary":"  Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.\n","authors":["Philhoon Oh","Jinwoo Shin","James Thorne"],"pdf_url":"https://arxiv.org/pdf/2501.07523v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2307.09998v5","updated":"2025-01-13T17:01:23Z","published":"2023-07-19T14:13:02Z","title":"Controlling Equational Reasoning in Large Language Models with Prompt\n  Interventions","summary":"  This paper investigates how hallucination rates in Large Language Models\n(LLMs) may be controlled via a symbolic data generation framework, exploring a\nfundamental relationship between the rate of certain mathematical errors and\ntypes of input intervention. Specifically, we systematically generate data for\na derivation generation task using a symbolic engine, applying targeted\ninterventions to prompts to perturb features of mathematical derivations such\nas the surface forms of symbols, equational tree structures, and mathematical\ncontext. We then evaluate the effect of prompt interventions across a range of\nLLMs including fine-tuned T5 models, GPT, and LLaMa-based models. Our\nexperiments suggest that T5-Large can outperform the few-shot performance of\nGPT-4 on various evaluation sets generated via the framework. However, an\nextensive evaluation based on human analysis, template-based error detection,\nand text generation metrics reveals model weaknesses beyond what the\nreference-based metrics singularly describe. We use these results to tie\ncharacteristic distributional footprints of interventions to the human\nevaluation of LLM derivation quality, potentially leading to significant\ncontrol over fine-grained mathematical capabilities of language models with\nrespect to specific types of errors.\n","authors":["Jordan Meadows","Marco Valentino","Andre Freitas"],"pdf_url":"https://arxiv.org/pdf/2307.09998v5.pdf","comment":"AAAI 2025 (7 pages)"},{"id":"http://arxiv.org/abs/2501.07482v1","updated":"2025-01-13T16:58:32Z","published":"2025-01-13T16:58:32Z","title":"TiEBe: A Benchmark for Assessing the Current Knowledge of Large Language\n  Models","summary":"  In a rapidly evolving knowledge landscape and the increasing adoption of\nlarge language models, a need has emerged to keep these models continuously\nupdated with current events. While existing benchmarks evaluate general factual\nrecall, they often overlook two critical aspects: the ability of models to\nintegrate evolving knowledge through continual learning and the significant\nregional disparities in their performance. To address these gaps, we introduce\nthe Timely Events Benchmark (TiEBe), a dataset containing over 11,000\nquestion-answer pairs focused on globally and regionally significant events.\nTiEBe leverages structured retrospective data from Wikipedia, enabling\ncontinuous updates to assess LLMs' knowledge of evolving global affairs and\ntheir understanding of events across different regions. Our benchmark\ndemonstrates that LLMs exhibit substantial geographic disparities in factual\nrecall, emphasizing the need for more balanced global knowledge representation.\nFurthermore, TiEBe serves as a tool for evaluating continual learning\nstrategies, providing insights into models' ability to acquire new information\nwithout forgetting past knowledge.\n","authors":["Thales Sales Almeida","Giovana Kerche Bonás","João Guilherme Alves Santos","Hugo Abonizio","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2501.07482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11547v2","updated":"2025-01-13T15:37:03Z","published":"2024-09-17T20:40:02Z","title":"Small Language Models can Outperform Humans in Short Creative Writing: A\n  Study Comparing SLMs with Humans and LLMs","summary":"  In this paper, we evaluate the creative fiction writing abilities of a\nfine-tuned small language model (SLM), BART-large, and compare its performance\nto human writers and two large language models (LLMs): GPT-3.5 and GPT-4o. Our\nevaluation consists of two experiments: (i) a human study in which 68\nparticipants rated short stories from humans and the SLM on grammaticality,\nrelevance, creativity, and attractiveness, and (ii) a qualitative linguistic\nanalysis examining the textual characteristics of stories produced by each\nmodel. In the first experiment, BART-large outscored average human writers\noverall (2.11 vs. 1.85), a 14% relative improvement, though the slight human\nadvantage in creativity was not statistically significant. In the second\nexperiment, qualitative analysis showed that while GPT-4o demonstrated\nnear-perfect coherence and used less cliche phrases, it tended to produce more\npredictable language, with only 3% of its synopses featuring surprising\nassociations (compared to 15% for BART). These findings highlight how model\nsize and fine-tuning influence the balance between creativity, fluency, and\ncoherence in creative writing tasks, and demonstrate that smaller models can,\nin certain contexts, rival both humans and larger models.\n","authors":["Guillermo Marco","Luz Rello","Julio Gonzalo"],"pdf_url":"https://arxiv.org/pdf/2409.11547v2.pdf","comment":"Accepted as Main Conference Paper at COLING 2025"},{"id":"http://arxiv.org/abs/2405.18874v2","updated":"2025-01-13T15:23:47Z","published":"2024-05-29T08:32:37Z","title":"Are queries and keys always relevant? A case study on Transformer wave\n  functions","summary":"  The dot product attention mechanism, originally designed for natural language\nprocessing tasks, is a cornerstone of modern Transformers. It adeptly captures\nsemantic relationships between word pairs in sentences by computing a\nsimilarity overlap between queries and keys. In this work, we explore the\nsuitability of Transformers, focusing on their attention mechanisms, in the\nspecific domain of the parametrization of variational wave functions to\napproximate ground states of quantum many-body spin Hamiltonians. Specifically,\nwe perform numerical simulations on the two-dimensional $J_1$-$J_2$ Heisenberg\nmodel, a common benchmark in the field of quantum many-body systems on lattice.\nBy comparing the performance of standard attention mechanisms with a simplified\nversion that excludes queries and keys, relying solely on positions, we achieve\ncompetitive results while reducing computational cost and parameter usage.\nFurthermore, through the analysis of the attention maps generated by standard\nattention mechanisms, we show that the attention weights become effectively\ninput-independent at the end of the optimization. We support the numerical\nresults with analytical calculations, providing physical insights of why\nqueries and keys should be, in principle, omitted from the attention mechanism\nwhen studying large systems.\n","authors":["Riccardo Rende","Luciano Loris Viteritti"],"pdf_url":"https://arxiv.org/pdf/2405.18874v2.pdf","comment":"10 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2501.07391v1","updated":"2025-01-13T15:07:55Z","published":"2025-01-13T15:07:55Z","title":"Enhancing Retrieval-Augmented Generation: A Study of Best Practices","summary":"  Retrieval-Augmented Generation (RAG) systems have recently shown remarkable\nadvancements by integrating retrieval mechanisms into language models,\nenhancing their ability to produce more accurate and contextually relevant\nresponses. However, the influence of various components and configurations\nwithin RAG systems remains underexplored. A comprehensive understanding of\nthese elements is essential for tailoring RAG systems to complex retrieval\ntasks and ensuring optimal performance across diverse applications. In this\npaper, we develop several advanced RAG system designs that incorporate query\nexpansion, various novel retrieval strategies, and a novel Contrastive\nIn-Context Learning RAG. Our study systematically investigates key factors,\nincluding language model size, prompt design, document chunk size, knowledge\nbase size, retrieval stride, query expansion techniques, Contrastive In-Context\nLearning knowledge bases, multilingual knowledge bases, and Focus Mode\nretrieving relevant context at sentence-level. Through extensive\nexperimentation, we provide a detailed analysis of how these factors influence\nresponse quality. Our findings offer actionable insights for developing RAG\nsystems, striking a balance between contextual richness and\nretrieval-generation efficiency, thereby paving the way for more adaptable and\nhigh-performing RAG frameworks in diverse real-world scenarios. Our code and\nimplementation details are publicly available.\n","authors":["Siran Li","Linus Stenzel","Carsten Eickhoff","Seyed Ali Bahrainian"],"pdf_url":"https://arxiv.org/pdf/2501.07391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07359v1","updated":"2025-01-13T14:27:39Z","published":"2025-01-13T14:27:39Z","title":"Emergent effects of scaling on the functional hierarchies within large\n  language models","summary":"  Large language model (LLM) architectures are often described as functionally\nhierarchical: Early layers process syntax, middle layers begin to parse\nsemantics, and late layers integrate information. The present work revisits\nthese ideas. This research submits simple texts to an LLM (e.g., \"A church and\norgan\") and extracts the resulting activations. Then, for each layer, support\nvector machines and ridge regressions are fit to predict a text's label and\nthus examine whether a given layer encodes some information. Analyses using a\nsmall model (Llama-3.2-3b; 28 layers) partly bolster the common hierarchical\nperspective: Item-level semantics are most strongly represented early (layers\n2-7), then two-item relations (layers 8-12), and then four-item analogies\n(layers 10-15). Afterward, the representation of items and simple relations\ngradually decreases in deeper layers that focus on more global information.\nHowever, several findings run counter to a steady hierarchy view: First,\nalthough deep layers can represent document-wide abstractions, deep layers also\ncompress information from early portions of the context window without\nmeaningful abstraction. Second, when examining a larger model\n(Llama-3.3-70b-Instruct), stark fluctuations in abstraction level appear: As\ndepth increases, two-item relations and four-item analogies initially increase\nin their representation, then markedly decrease, and afterward increase again\nmomentarily. This peculiar pattern consistently emerges across several\nexperiments. Third, another emergent effect of scaling is coordination between\nthe attention mechanisms of adjacent layers. Across multiple experiments using\nthe larger model, adjacent layers fluctuate between what information they each\nspecialize in representing. In sum, an abstraction hierarchy often manifests\nacross layers, but large models also deviate from this structure in curious\nways.\n","authors":["Paul C. Bogdan"],"pdf_url":"https://arxiv.org/pdf/2501.07359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12493v3","updated":"2025-01-13T14:21:21Z","published":"2024-11-19T13:23:53Z","title":"Eradicating Social Biases in Sentiment Analysis using Semantic Blinding\n  and Semantic Propagation Graph Neural Networks","summary":"  This paper introduces the Semantic Propagation Graph Neural Network (SProp\nGNN), a machine learning sentiment analysis (SA) architecture that relies\nexclusively on syntactic structures and word-level emotional cues to predict\nemotions in text. By semantically blinding the model to information about\nspecific words, it is robust to social biases such as political or gender bias\nthat have been plaguing previous machine learning-based SA systems. The SProp\nGNN shows performance superior to lexicon-based alternatives such as VADER\n(Valence Aware Dictionary and Sentiment Reasoner) and EmoAtlas on two different\nprediction tasks, and across two languages. Additionally, it approaches the\naccuracy of transformer-based models while significantly reducing bias in\nemotion prediction tasks. By offering improved explainability and reducing\nbias, the SProp GNN bridges the methodological gap between interpretable\nlexicon approaches and powerful, yet often opaque, deep learning models,\noffering a robust tool for fair and effective emotion analysis in understanding\nhuman behavior through text.\n","authors":["Hubert Plisiecki"],"pdf_url":"https://arxiv.org/pdf/2411.12493v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08434v2","updated":"2025-01-13T14:13:38Z","published":"2024-12-11T14:55:48Z","title":"Mitigating Out-of-Entity Errors in Named Entity Recognition: A\n  Sentence-Level Strategy","summary":"  Many previous models of named entity recognition (NER) suffer from the\nproblem of Out-of-Entity (OOE), i.e., the tokens in the entity mentions of the\ntest samples have not appeared in the training samples, which hinders the\nachievement of satisfactory performance. To improve OOE-NER performance, in\nthis paper, we propose a new framework, namely S+NER, which fully leverages\nsentence-level information. Our S+NER achieves better OOE-NER performance\nmainly due to the following two particular designs. 1) It first exploits the\npre-trained language model's capability of understanding the target entity's\nsentence-level context with a template set. 2) Then, it refines the\nsentence-level representation based on the positive and negative templates,\nthrough a contrastive learning strategy and template pooling method, to obtain\nbetter NER results. Our extensive experiments on five benchmark datasets have\ndemonstrated that, our S+NER outperforms some state-of-the-art OOE-NER models.\n","authors":["Guochao Jiang","Ziqin Luo","Chengwei Hu","Zepeng Ding","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2412.08434v2.pdf","comment":"Accepted by COLING 2025"},{"id":"http://arxiv.org/abs/2501.07329v1","updated":"2025-01-13T13:43:46Z","published":"2025-01-13T13:43:46Z","title":"Joint Automatic Speech Recognition And Structure Learning For Better\n  Speech Understanding","summary":"  Spoken language understanding (SLU) is a structure prediction task in the\nfield of speech. Recently, many works on SLU that treat it as a\nsequence-to-sequence task have achieved great success. However, This method is\nnot suitable for simultaneous speech recognition and understanding. In this\npaper, we propose a joint speech recognition and structure learning framework\n(JSRSL), an end-to-end SLU model based on span, which can accurately transcribe\nspeech and extract structured content simultaneously. We conduct experiments on\nname entity recognition and intent classification using the Chinese dataset\nAISHELL-NER and the English dataset SLURP. The results show that our proposed\nmethod not only outperforms the traditional sequence-to-sequence method in both\ntranscription and extraction capabilities but also achieves state-of-the-art\nperformance on the two datasets.\n","authors":["Jiliang Hu","Zuchao Li","Mengjia Shen","Haojun Ai","Sheng Li","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.07329v1.pdf","comment":"5 pages, 2 figures, accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.07314v1","updated":"2025-01-13T13:26:50Z","published":"2025-01-13T13:26:50Z","title":"FinerWeb-10BT: Refining Web Data with LLM-Based Line-Level Filtering","summary":"  Data quality is crucial for training Large Language Models (LLMs).\nTraditional heuristic filters often miss low-quality text or mistakenly remove\nvaluable content. In this paper, we introduce an LLM-based line-level filtering\nmethod to enhance training data quality. We use GPT-4o mini to label a\n20,000-document sample from FineWeb at the line level, allowing the model to\ncreate descriptive labels for low-quality lines. These labels are grouped into\nnine main categories, and we train a DeBERTa-v3 classifier to scale the\nfiltering to a 10B-token subset of FineWeb. To test the impact of our\nfiltering, we train GPT-2 models on both the original and the filtered\ndatasets. The results show that models trained on the filtered data achieve\nhigher accuracy on the HellaSwag benchmark and reach their performance targets\nfaster, even with up to 25\\% less data. This demonstrates that LLM-based\nline-level filtering can significantly improve data quality and training\nefficiency for LLMs. We release our quality-annotated dataset, FinerWeb-10BT,\nand the codebase to support further work in this area.\n","authors":["Erik Henriksson","Otto Tarkka","Filip Ginter"],"pdf_url":"https://arxiv.org/pdf/2501.07314v1.pdf","comment":"11 pages, 4 figures, 4 tables. To be published in NoDaLiDa/Baltic-HLT\n  2025 proceedings"},{"id":"http://arxiv.org/abs/2411.17075v4","updated":"2025-01-13T13:22:38Z","published":"2024-11-26T03:27:43Z","title":"Don't Command, Cultivate: An Exploratory Study of System-2 Alignment","summary":"  The o1 system card identifies the o1 models as the most robust within OpenAI,\nwith their defining characteristic being the progression from rapid, intuitive\nthinking to slower, more deliberate reasoning. This observation motivated us to\ninvestigate the influence of System-2 thinking patterns on model safety. In our\npreliminary research, we conducted safety evaluations of the o1 model,\nincluding complex jailbreak attack scenarios using adversarial natural language\nprompts and mathematical encoding prompts. Our findings indicate that the o1\nmodel demonstrates relatively improved safety performance; however, it still\nexhibits vulnerabilities, particularly against jailbreak attacks employing\nmathematical encoding. Through detailed case analysis, we identified specific\npatterns in the o1 model's responses. We also explored the alignment of\nSystem-2 safety in open-source models using prompt engineering and supervised\nfine-tuning techniques. Experimental results show that some simple methods to\nencourage the model to carefully scrutinize user requests are beneficial for\nmodel safety. Additionally, we proposed a implementation plan for process\nsupervision to enhance safety alignment. The implementation details and\nexperimental results will be provided in future versions.\n","authors":["Yuhang Wang","Yuxiang Zhang","Yanxu Zhu","Xinyan Wen","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2411.17075v4.pdf","comment":"In this version, the DPO and reinforcement learning methods have been\n  added"},{"id":"http://arxiv.org/abs/2501.07301v1","updated":"2025-01-13T13:10:16Z","published":"2025-01-13T13:10:16Z","title":"The Lessons of Developing Process Reward Models in Mathematical\n  Reasoning","summary":"  Process Reward Models (PRMs) emerge as a promising approach for process\nsupervision in mathematical reasoning of Large Language Models (LLMs), which\naim to identify and mitigate intermediate errors in the reasoning processes.\nHowever, the development of effective PRMs faces significant challenges,\nparticularly in data annotation and evaluation methodologies. In this paper,\nthrough extensive experiments, we demonstrate that commonly used Monte Carlo\n(MC) estimation-based data synthesis for PRMs typically yields inferior\nperformance and generalization compared to LLM-as-a-judge and human annotation\nmethods. MC estimation relies on completion models to evaluate current-step\ncorrectness, leading to inaccurate step verification. Furthermore, we identify\npotential biases in conventional Best-of-N (BoN) evaluation strategies for\nPRMs: (1) The unreliable policy models generate responses with correct answers\nbut flawed processes, leading to a misalignment between the evaluation criteria\nof BoN and the PRM objectives of process verification. (2) The tolerance of\nPRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a\nsignificant proportion of minimum scores concentrated on the final answer\nsteps, revealing the shift from process to outcome-based assessment in BoN\nOptimized PRMs. To address these challenges, we develop a consensus filtering\nmechanism that effectively integrates MC estimation with LLM-as-a-judge and\nadvocates a more comprehensive evaluation framework that combines\nresponse-level and step-level metrics. Based on the mechanisms, we\nsignificantly improve both model performance and data efficiency in the BoN\nevaluation and the step-wise error identification task. Finally, we release a\nnew state-of-the-art PRM that outperforms existing open-source alternatives and\nprovides practical guidelines for future research in building process\nsupervision models.\n","authors":["Zhenru Zhang","Chujie Zheng","Yangzhen Wu","Beichen Zhang","Runji Lin","Bowen Yu","Dayiheng Liu","Jingren Zhou","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2501.07301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07300v1","updated":"2025-01-13T13:07:51Z","published":"2025-01-13T13:07:51Z","title":"Comparative analysis of optical character recognition methods for Sámi\n  texts from the National Library of Norway","summary":"  Optical Character Recognition (OCR) is crucial to the National Library of\nNorway's (NLN) digitisation process as it converts scanned documents into\nmachine-readable text. However, for the S\\'ami documents in NLN's collection,\nthe OCR accuracy is insufficient. Given that OCR quality affects downstream\nprocesses, evaluating and improving OCR for text written in S\\'ami languages is\nnecessary to make these resources accessible. To address this need, this work\nfine-tunes and evaluates three established OCR approaches, Transkribus,\nTesseract and TrOCR, for transcribing S\\'ami texts from NLN's collection. Our\nresults show that Transkribus and TrOCR outperform Tesseract on this task,\nwhile Tesseract achieves superior performance on an out-of-domain dataset.\nFurthermore, we show that fine-tuning pre-trained models and supplementing\nmanual annotations with machine annotations and synthetic text images can yield\naccurate OCR for S\\'ami languages, even with a moderate amount of manually\nannotated data.\n","authors":["Tita Enstad","Trond Trosterud","Marie Iversdatter Røsok","Yngvil Beyer","Marie Roald"],"pdf_url":"https://arxiv.org/pdf/2501.07300v1.pdf","comment":"To be published in Proceedings of the 25th Nordic Conference on\n  Computational Linguistics (NoDaLiDa)"},{"id":"http://arxiv.org/abs/2501.07246v1","updated":"2025-01-13T11:54:40Z","published":"2025-01-13T11:54:40Z","title":"Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language\n  Model","summary":"  Large Audio-Language Models (LALMs) have demonstrated remarkable performance\nin tasks involving audio perception and understanding, such as speech\nrecognition and audio captioning. However, their reasoning capabilities -\ncritical for solving complex real-world problems - remain underexplored. In\nthis work, we conduct the first exploration into integrating Chain-of-Thought\n(CoT) reasoning into LALMs to enhance their reasoning ability across auditory\nmodalities. We evaluate representative CoT methods, analyzing their performance\nin both information extraction and reasoning tasks across sound, music, and\nspeech domains. Our findings reveal that CoT methods significantly improve\nperformance on easy and medium tasks but encounter challenges with hard tasks,\nwhere reasoning chains can confuse the model rather than improve accuracy.\nAdditionally, we identify a positive correlation between reasoning path length\nand accuracy, demonstrating the potential of scaling inference for advanced\ninstruction-following and reasoning. This study not only highlights the promise\nof CoT in enhancing LALM reasoning capabilities but also identifies key\nlimitations and provides actionable directions for future research.\n","authors":["Ziyang Ma","Zhuo Chen","Yuping Wang","Eng Siong Chng","Xie Chen"],"pdf_url":"https://arxiv.org/pdf/2501.07246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07244v1","updated":"2025-01-13T11:52:55Z","published":"2025-01-13T11:52:55Z","title":"Can Vision-Language Models Evaluate Handwritten Math?","summary":"  Recent advancements in Vision-Language Models (VLMs) have opened new\npossibilities in automatic grading of handwritten student responses,\nparticularly in mathematics. However, a comprehensive study to test the ability\nof VLMs to evaluate and reason over handwritten content remains absent. To\naddress this gap, we introduce FERMAT, a benchmark designed to assess the\nability of VLMs to detect, localize and correct errors in handwritten\nmathematical content. FERMAT spans four key error dimensions - computational,\nconceptual, notational, and presentation - and comprises over 2,200 handwritten\nmath solutions derived from 609 manually curated problems from grades 7-12 with\nintentionally introduced perturbations. Using FERMAT we benchmark nine VLMs\nacross three tasks: error detection, localization, and correction. Our results\nreveal significant shortcomings in current VLMs in reasoning over handwritten\ntext, with Gemini-1.5-Pro achieving the highest error correction rate (77%). We\nalso observed that some models struggle with processing handwritten content, as\ntheir accuracy improves when handwritten inputs are replaced with printed text\nor images. These findings highlight the limitations of current VLMs and reveal\nnew avenues for improvement. We release FERMAT and all the associated resources\nin the open-source to drive further research.\n","authors":["Oikantik Nath","Hanani Bathina","Mohammed Safi Ur Rahman Khan","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2501.07244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12094v2","updated":"2025-01-13T11:46:59Z","published":"2024-03-15T06:57:08Z","title":"Are LLMs Good Cryptic Crossword Solvers?","summary":"  Cryptic crosswords are puzzles that rely not only on general knowledge but\nalso on the solver's ability to manipulate language on different levels and\ndeal with various types of wordplay. Previous research suggests that solving\nsuch puzzles is a challenge even for modern NLP models. However, the abilities\nof large language models (LLMs) have not yet been tested on this task. In this\npaper, we establish the benchmark results for three popular LLMs -- LLaMA2,\nMistral, and ChatGPT -- showing that their performance on this task is still\nfar from that of humans.\n","authors":["Abdelrahman Sadallah","Daria Kotova","Ekaterina Kochmar"],"pdf_url":"https://arxiv.org/pdf/2403.12094v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07217v1","updated":"2025-01-13T11:16:05Z","published":"2025-01-13T11:16:05Z","title":"When lies are mostly truthful: automated verbal deception detection for\n  embedded lies","summary":"  Background: Verbal deception detection research relies on narratives and\ncommonly assumes statements as truthful or deceptive. A more realistic\nperspective acknowledges that the veracity of statements exists on a continuum\nwith truthful and deceptive parts being embedded within the same statement.\nHowever, research on embedded lies has been lagging behind. Methods: We\ncollected a novel dataset of 2,088 truthful and deceptive statements with\nannotated embedded lies. Using a within-subjects design, participants provided\na truthful account of an autobiographical event. They then rewrote their\nstatement in a deceptive manner by including embedded lies, which they\nhighlighted afterwards and judged on lie centrality, deceptiveness, and source.\nResults: We show that a fined-tuned language model (Llama-3-8B) can classify\ntruthful statements and those containing embedded lies with 64% accuracy.\nIndividual differences, linguistic properties and explainability analysis\nsuggest that the challenge of moving the dial towards embedded lies stems from\ntheir resemblance to truthful statements. Typical deceptive statements\nconsisted of 2/3 truthful information and 1/3 embedded lies, largely derived\nfrom past personal experiences and with minimal linguistic differences with\ntheir truthful counterparts. Conclusion: We present this dataset as a novel\nresource to address this challenge and foster research on embedded lies in\nverbal deception detection.\n","authors":["Riccardo Loconte","Bennett Kleinberg"],"pdf_url":"https://arxiv.org/pdf/2501.07217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13503v2","updated":"2025-01-13T10:43:11Z","published":"2024-12-18T04:55:29Z","title":"VaeDiff-DocRE: End-to-end Data Augmentation Framework for Document-level\n  Relation Extraction","summary":"  Document-level Relation Extraction (DocRE) aims to identify relationships\nbetween entity pairs within a document. However, most existing methods assume a\nuniform label distribution, resulting in suboptimal performance on real-world,\nimbalanced datasets. To tackle this challenge, we propose a novel data\naugmentation approach using generative models to enhance data from the\nembedding space. Our method leverages the Variational Autoencoder (VAE)\narchitecture to capture all relation-wise distributions formed by entity pair\nrepresentations and augment data for underrepresented relations. To better\ncapture the multi-label nature of DocRE, we parameterize the VAE's latent space\nwith a Diffusion Model. Additionally, we introduce a hierarchical training\nframework to integrate the proposed VAE-based augmentation module into DocRE\nsystems. Experiments on two benchmark datasets demonstrate that our method\noutperforms state-of-the-art models, effectively addressing the long-tail\ndistribution problem in DocRE.\n","authors":["Khai Phan Tran","Wen Hua","Xue Li"],"pdf_url":"https://arxiv.org/pdf/2412.13503v2.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2501.05855v2","updated":"2025-01-13T10:39:54Z","published":"2025-01-10T10:53:48Z","title":"ConSim: Measuring Concept-Based Explanations' Effectiveness with\n  Automated Simulatability","summary":"  Concept-based explanations work by mapping complex model computations to\nhuman-understandable concepts. Evaluating such explanations is very difficult,\nas it includes not only the quality of the induced space of possible concepts\nbut also how effectively the chosen concepts are communicated to users.\nExisting evaluation metrics often focus solely on the former, neglecting the\nlatter. We introduce an evaluation framework for measuring concept explanations\nvia automated simulatability: a simulator's ability to predict the explained\nmodel's outputs based on the provided explanations. This approach accounts for\nboth the concept space and its interpretation in an end-to-end evaluation.\nHuman studies for simulatability are notoriously difficult to enact,\nparticularly at the scale of a wide, comprehensive empirical evaluation (which\nis the subject of this work). We propose using large language models (LLMs) as\nsimulators to approximate the evaluation and report various analyses to make\nsuch approximations reliable. Our method allows for scalable and consistent\nevaluation across various models and datasets. We report a comprehensive\nempirical evaluation using this framework and show that LLMs provide consistent\nrankings of explanation methods. Code available at\nhttps://github.com/AnonymousConSim/ConSim.\n","authors":["Antonin Poché","Alon Jacovi","Agustin Martin Picard","Victor Boutin","Fanny Jourdan"],"pdf_url":"https://arxiv.org/pdf/2501.05855v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05767v2","updated":"2025-01-13T10:38:32Z","published":"2025-01-10T07:56:23Z","title":"Migician: Revealing the Magic of Free-Form Multi-Image Grounding in\n  Multimodal Large Language Models","summary":"  The recent advancement of Multimodal Large Language Models (MLLMs) has\nsignificantly improved their fine-grained perception of single images and\ngeneral comprehension across multiple images. However, existing MLLMs still\nface challenges in achieving precise grounding in complex multi-image\nscenarios. To address this, we first explore a Chain-of-Thought (CoT) framework\nthat integrates single-image grounding with multi-image comprehension. While\npartially effective, it remains unstable and struggles to capture abstract\nvisual information due to its non-end-to-end nature. Therefore, we introduce\nMigician, the first multi-image grounding model capable of performing free-form\nand accurate grounding across multiple images. To support this, we present the\nMGrounding-630k dataset, which comprises data for several multi-image grounding\ntasks derived from existing datasets, along with newly generated free-form\ngrounding instruction-following data. Furthermore, we propose MIG-Bench, a\ncomprehensive benchmark specifically designed for evaluating multi-image\ngrounding capabilities. Experimental results demonstrate that our model\nachieves significantly superior multi-image grounding capabilities,\noutperforming the best existing MLLMs by 21.61% and even surpassing much larger\n70B models. Our code, model, dataset, and benchmark are fully open-sourced at\nhttps://migician-vg.github.io/.\n","authors":["You Li","Heyu Huang","Chi Chen","Kaiyu Huang","Chao Huang","Zonghao Guo","Zhiyuan Liu","Jinan Xu","Yuhua Li","Ruixuan Li","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2501.05767v2.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.15911v2","updated":"2025-01-13T10:23:35Z","published":"2024-10-21T11:33:18Z","title":"DefVerify: Do Hate Speech Models Reflect Their Dataset's Definition?","summary":"  When building a predictive model, it is often difficult to ensure that\napplication-specific requirements are encoded by the model that will eventually\nbe deployed. Consider researchers working on hate speech detection. They will\nhave an idea of what is considered hate speech, but building a model that\nreflects their view accurately requires preserving those ideals throughout the\nworkflow of data set construction and model training. Complications such as\nsampling bias, annotation bias, and model misspecification almost always arise,\npossibly resulting in a gap between the application specification and the\nmodel's actual behavior upon deployment. To address this issue for hate speech\ndetection, we propose DefVerify: a 3-step procedure that (i) encodes a\nuser-specified definition of hate speech, (ii) quantifies to what extent the\nmodel reflects the intended definition, and (iii) tries to identify the point\nof failure in the workflow. We use DefVerify to find gaps between definition\nand model behavior when applied to six popular hate speech benchmark datasets.\n","authors":["Urja Khurana","Eric Nalisnick","Antske Fokkens"],"pdf_url":"https://arxiv.org/pdf/2410.15911v2.pdf","comment":"Camera-ready COLING 2025"},{"id":"http://arxiv.org/abs/2409.19655v2","updated":"2025-01-13T10:08:07Z","published":"2024-09-29T11:00:41Z","title":"Assessment and manipulation of latent constructs in pre-trained language\n  models using psychometric scales","summary":"  Human-like personality traits have recently been discovered in large language\nmodels, raising the hypothesis that their (known and as yet undiscovered)\nbiases conform with human latent psychological constructs. While large\nconversational models may be tricked into answering psychometric\nquestionnaires, the latent psychological constructs of thousands of simpler\ntransformers, trained for other tasks, cannot be assessed because appropriate\npsychometric methods are currently lacking. Here, we show how standard\npsychological questionnaires can be reformulated into natural language\ninference prompts, and we provide a code library to support the psychometric\nassessment of arbitrary models. We demonstrate, using a sample of 88 publicly\navailable models, the existence of human-like mental health-related constructs\n(including anxiety, depression, and Sense of Coherence) which conform with\nstandard theories in human psychology and show similar correlations and\nmitigation strategies. The ability to interpret and rectify the performance of\nlanguage models by using psychological tools can boost the development of more\nexplainable, controllable, and trustworthy models.\n","authors":["Maor Reuben","Ortal Slobodin","Aviad Elyshar","Idan-Chaim Cohen","Orna Braun-Lewensohn","Odeya Cohen","Rami Puzis"],"pdf_url":"https://arxiv.org/pdf/2409.19655v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07171v1","updated":"2025-01-13T09:58:03Z","published":"2025-01-13T09:58:03Z","title":"BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and\n  Vision-Language Models Derived from Scientific Literature","summary":"  The development of vision-language models (VLMs) is driven by large-scale and\ndiverse multimodal datasets. However, progress toward generalist biomedical\nVLMs is limited by the lack of annotated, publicly accessible datasets across\nbiology and medicine. Existing efforts are restricted to narrow domains,\nmissing the full diversity of biomedical knowledge encoded in scientific\nliterature. To address this gap, we introduce BIOMEDICA, a scalable,\nopen-source framework to extract, annotate, and serialize the entirety of the\nPubMed Central Open Access subset into an easy-to-use, publicly accessible\ndataset.Our framework produces a comprehensive archive with over 24 million\nunique image-text pairs from over 6 million articles. Metadata and\nexpert-guided annotations are also provided. We demonstrate the utility and\naccessibility of our resource by releasing BMCA-CLIP, a suite of CLIP-style\nmodels continuously pre-trained on the BIOMEDICA dataset via streaming,\neliminating the need to download 27 TB of data locally.On average, our models\nachieve state-of-the-art performance across 40 tasks - spanning pathology,\nradiology, ophthalmology, dermatology, surgery, molecular biology,\nparasitology, and cell biology - excelling in zero-shot classification with a\n6.56% average improvement (as high as 29.8% and 17.5% in dermatology and\nophthalmology, respectively), and stronger image-text retrieval, all while\nusing 10x less compute. To foster reproducibility and collaboration, we release\nour codebase and dataset for the broader research community.\n","authors":["Alejandro Lozano","Min Woo Sun","James Burgess","Liangyu Chen","Jeffrey J Nirschl","Jeffrey Gu","Ivan Lopez","Josiah Aklilu","Austin Wolfgang Katzer","Collin Chiu","Anita Rau","Xiaohan Wang","Yuhui Zhang","Alfred Seunghoon Song","Robert Tibshirani","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2501.07171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05862v3","updated":"2025-01-13T09:33:47Z","published":"2024-06-09T17:25:47Z","title":"II-Bench: An Image Implication Understanding Benchmark for Multimodal\n  Large Language Models","summary":"  The rapid advancements in the development of multimodal large language models\n(MLLMs) have consistently led to new breakthroughs on various benchmarks. In\nresponse, numerous challenging and comprehensive benchmarks have been proposed\nto more accurately assess the capabilities of MLLMs. However, there is a dearth\nof exploration of the higher-order perceptual capabilities of MLLMs. To fill\nthis gap, we propose the Image Implication understanding Benchmark, II-Bench,\nwhich aims to evaluate the model's higher-order perception of images. Through\nextensive experiments on II-Bench across multiple MLLMs, we have made\nsignificant findings. Initially, a substantial gap is observed between the\nperformance of MLLMs and humans on II-Bench. The pinnacle accuracy of MLLMs\nattains 74.8%, whereas human accuracy averages 90%, peaking at an impressive\n98%. Subsequently, MLLMs perform worse on abstract and complex images,\nsuggesting limitations in their ability to understand high-level semantics and\ncapture image details. Finally, it is observed that most models exhibit\nenhanced accuracy when image sentiment polarity hints are incorporated into the\nprompts. This observation underscores a notable deficiency in their inherent\nunderstanding of image sentiment. We believe that II-Bench will inspire the\ncommunity to develop the next generation of MLLMs, advancing the journey\ntowards expert artificial general intelligence (AGI). II-Bench is publicly\navailable at https://huggingface.co/datasets/m-a-p/II-Bench.\n","authors":["Ziqiang Liu","Feiteng Fang","Xi Feng","Xinrun Du","Chenhao Zhang","Zekun Wang","Yuelin Bai","Qixuan Zhao","Liyang Fan","Chengguang Gan","Hongquan Lin","Jiaming Li","Yuansheng Ni","Haihong Wu","Yaswanth Narsupalli","Zhigang Zheng","Chengming Li","Xiping Hu","Ruifeng Xu","Xiaojun Chen","Min Yang","Jiaheng Liu","Ruibo Liu","Wenhao Huang","Ge Zhang","Shiwen Ni"],"pdf_url":"https://arxiv.org/pdf/2406.05862v3.pdf","comment":"100 pages, 82 figures, add citations"},{"id":"http://arxiv.org/abs/2312.04746v3","updated":"2025-01-13T08:08:28Z","published":"2023-12-07T23:16:37Z","title":"Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized\n  Narratives from Open-Source Histopathology Videos","summary":"  Diagnosis in histopathology requires a global whole slide images (WSIs)\nanalysis, requiring pathologists to compound evidence from different WSI\npatches. The gigapixel scale of WSIs poses a challenge for histopathology\nmulti-modal models. Training multi-model models for histopathology requires\ninstruction tuning datasets, which currently contain information for individual\nimage patches, without a spatial grounding of the concepts within each patch\nand without a wider view of the WSI. Therefore, they lack sufficient diagnostic\ncapacity for histopathology. To bridge this gap, we introduce Quilt-Instruct, a\nlarge-scale dataset of 107,131 histopathology-specific instruction\nquestion/answer pairs, grounded within diagnostically relevant image patches\nthat make up the WSI. Our dataset is collected by leveraging educational\nhistopathology videos from YouTube, which provides spatial localization of\nnarrations by automatically extracting the narrators' cursor positions.\nQuilt-Instruct supports contextual reasoning by extracting diagnosis and\nsupporting facts from the entire WSI. Using Quilt-Instruct, we train\nQuilt-LLaVA, which can reason beyond the given single image patch, enabling\ndiagnostic reasoning across patches. To evaluate Quilt-LLaVA, we propose a\ncomprehensive evaluation dataset created from 985 images and 1283\nhuman-generated question-answers. We also thoroughly evaluate Quilt-LLaVA using\npublic histopathology datasets, where Quilt-LLaVA significantly outperforms\nSOTA by over 10% on relative GPT-4 score and 4% and 9% on open and closed set\nVQA. Our code, data, and model are publicly accessible at\nquilt-llava.github.io.\n","authors":["Mehmet Saygin Seyfioglu","Wisdom O. Ikezogwo","Fatemeh Ghezloo","Ranjay Krishna","Linda Shapiro"],"pdf_url":"https://arxiv.org/pdf/2312.04746v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07111v1","updated":"2025-01-13T07:51:46Z","published":"2025-01-13T07:51:46Z","title":"ListConRanker: A Contrastive Text Reranker with Listwise Encoding","summary":"  Reranker models aim to re-rank the passages based on the semantics similarity\nbetween the given query and passages, which have recently received more\nattention due to the wide application of the Retrieval-Augmented Generation.\nMost previous methods apply pointwise encoding, meaning that it can only encode\nthe context of the query for each passage input into the model. However, for\nthe reranker model, given a query, the comparison results between passages are\neven more important, which is called listwise encoding. Besides, previous\nmodels are trained using the cross-entropy loss function, which leads to issues\nof unsmooth gradient changes during training and low training efficiency. To\naddress these issues, we propose a novel Listwise-encoded Contrastive text\nreRanker (ListConRanker). It can help the passage to be compared with other\npassages during the encoding process, and enhance the contrastive information\nbetween positive examples and between positive and negative examples. At the\nsame time, we use the circle loss to train the model to increase the\nflexibility of gradients and solve the problem of training efficiency.\nExperimental results show that ListConRanker achieves state-of-the-art\nperformance on the reranking benchmark of Chinese Massive Text Embedding\nBenchmark, including the cMedQA1.0, cMedQA2.0, MMarcoReranking, and T2Reranking\ndatasets.\n","authors":["Junlong Liu","Yue Ma","Ruihui Zhao","Junhao Zheng","Qianli Ma","Yangyang Kang"],"pdf_url":"https://arxiv.org/pdf/2501.07111v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.17692v3","updated":"2025-01-13T07:41:44Z","published":"2024-09-26T09:57:16Z","title":"MIO: A Foundation Model on Multimodal Tokens","summary":"  In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.\n","authors":["Zekun Wang","King Zhu","Chunpu Xu","Wangchunshu Zhou","Jiaheng Liu","Yibo Zhang","Jiashuo Wang","Ning Shi","Siyu Li","Yizhi Li","Haoran Que","Zhaoxiang Zhang","Yuanxing Zhang","Ge Zhang","Ke Xu","Jie Fu","Wenhao Huang"],"pdf_url":"https://arxiv.org/pdf/2409.17692v3.pdf","comment":"Technical Report. Codes and models are available in\n  https://github.com/MIO-Team/MIO"},{"id":"http://arxiv.org/abs/2501.07102v1","updated":"2025-01-13T07:27:00Z","published":"2025-01-13T07:27:00Z","title":"AdaCS: Adaptive Normalization for Enhanced Code-Switching ASR","summary":"  Intra-sentential code-switching (CS) refers to the alternation between\nlanguages that happens within a single utterance and is a significant challenge\nfor Automatic Speech Recognition (ASR) systems. For example, when a Vietnamese\nspeaker uses foreign proper names or specialized terms within their speech. ASR\nsystems often struggle to accurately transcribe intra-sentential CS due to\ntheir training on monolingual data and the unpredictable nature of CS. This\nissue is even more pronounced for low-resource languages, where limited data\navailability hinders the development of robust models. In this study, we\npropose AdaCS, a normalization model integrates an adaptive bias attention\nmodule (BAM) into encoder-decoder network. This novel approach provides a\nrobust solution to CS ASR in unseen domains, thereby significantly enhancing\nour contribution to the field. By utilizing BAM to both identify and normalize\nCS phrases, AdaCS enhances its adaptive capabilities with a biased list of\nwords provided during inference. Our method demonstrates impressive performance\nand the ability to handle unseen CS phrases across various domains. Experiments\nshow that AdaCS outperforms previous state-of-the-art method on Vietnamese CS\nASR normalization by considerable WER reduction of 56.2% and 36.8% on the two\nproposed test sets.\n","authors":["The Chuong Chu","Vu Tuan Dat Pham","Kien Dao","Hoang Nguyen","Quoc Hung Truong"],"pdf_url":"https://arxiv.org/pdf/2501.07102v1.pdf","comment":"Accepted at ICASSP 2025"},{"id":"http://arxiv.org/abs/2408.01933v4","updated":"2025-01-13T07:13:56Z","published":"2024-08-04T05:15:02Z","title":"DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language\n  Models","summary":"  Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios.\n","authors":["Bowen Wang","Jiuyang Chang","Yiming Qian","Guoxin Chen","Junhao Chen","Zhouqiang Jiang","Jiahao Zhang","Yuta Nakashima","Hajime Nagahara"],"pdf_url":"https://arxiv.org/pdf/2408.01933v4.pdf","comment":"9 pages,6 figures"},{"id":"http://arxiv.org/abs/2411.19943v3","updated":"2025-01-13T06:53:56Z","published":"2024-11-29T18:58:22Z","title":"Critical Tokens Matter: Token-Level Contrastive Estimation Enhances\n  LLM's Reasoning Capability","summary":"  Mathematical reasoning tasks pose significant challenges for large language\nmodels (LLMs) because they require precise logical deduction and sequence\nanalysis. In this work, we introduce the concept of critical tokens -- elements\nwithin reasoning trajectories that significantly influence incorrect outcomes.\nWe present a novel framework for identifying these tokens through rollout\nsampling and demonstrate their substantial divergence from traditional error\ntokens. Through extensive experiments on datasets such as GSM8K and MATH500, we\nshow that identifying and replacing critical tokens significantly improves\nmodel accuracy. We propose an efficient methodology for pinpointing these\ntokens in large-scale datasets using contrastive estimation and extend this\nframework to enhance model training processes with direct preference\noptimization (DPO). Experimental results on GSM8K and MATH500 benchmarks with\nthe widely used models Llama-3 (8B and 70B) and Deepseek-math (7B) demonstrate\nthe effectiveness of the proposed approach, cDPO. Our results underscore the\npotential of leveraging critical tokens to reduce errors in reasoning tasks,\nadvancing the development of AI systems capable of robust logical deduction.\nOur code, annotated datasets, and trained models are available at\nhttps://github.com/chenzhiling9954/Critical-Tokens-Matter to support and\nencourage future research in this promising field.\n","authors":["Zicheng Lin","Tian Liang","Jiahao Xu","Qiuzhi Lin","Xing Wang","Ruilin Luo","Chufan Shi","Siheng Li","Yujiu Yang","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2411.19943v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2501.07086v1","updated":"2025-01-13T06:41:23Z","published":"2025-01-13T06:41:23Z","title":"Boosting Text-To-Image Generation via Multilingual Prompting in Large\n  Multimodal Models","summary":"  Previous work on augmenting large multimodal models (LMMs) for text-to-image\n(T2I) generation has focused on enriching the input space of in-context\nlearning (ICL). This includes providing a few demonstrations and optimizing\nimage descriptions to be more detailed and logical. However, as demand for more\ncomplex and flexible image descriptions grows, enhancing comprehension of input\ntext within the ICL paradigm remains a critical yet underexplored area. In this\nwork, we extend this line of research by constructing parallel multilingual\nprompts aimed at harnessing the multilingual capabilities of LMMs. More\nspecifically, we translate the input text into several languages and provide\nthe models with both the original text and the translations. Experiments on two\nLMMs across 3 benchmarks show that our method, PMT2I, achieves superior\nperformance in general, compositional, and fine-grained assessments, especially\nin human preference alignment. Additionally, with its advantage of generating\nmore diverse images, PMT2I significantly outperforms baseline prompts when\nincorporated with reranking methods. Our code and parallel multilingual data\ncan be found at https://github.com/takagi97/PMT2I.\n","authors":["Yongyu Mu","Hengyu Li","Junxin Wang","Xiaoxuan Zhou","Chenglong Wang","Yingfeng Luo","Qiaozhi He","Tong Xiao","Guocheng Chen","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2501.07086v1.pdf","comment":"Accepted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.07063v1","updated":"2025-01-13T05:16:14Z","published":"2025-01-13T05:16:14Z","title":"Research on the Online Update Method for Retrieval-Augmented Generation\n  (RAG) Model with Incremental Learning","summary":"  In the contemporary context of rapid advancements in information technology\nand the exponential growth of data volume, language models are confronted with\nsignificant challenges in effectively navigating the dynamic and ever-evolving\ninformation landscape to update and adapt to novel knowledge in real time. In\nthis work, an online update method is proposed, which is based on the existing\nRetrieval Enhanced Generation (RAG) model with multiple innovation mechanisms.\nFirstly, the dynamic memory is used to capture the emerging data samples, and\nthen gradually integrate them into the core model through a tunable knowledge\ndistillation strategy. At the same time, hierarchical indexing and multi-layer\ngating mechanism are introduced into the retrieval module to ensure that the\nretrieved content is more targeted and accurate. Finally, a multi-stage network\nstructure is established for different types of inputs in the generation stage,\nand cross-attention matching and screening are carried out on the intermediate\nrepresentations of each stage to ensure the effective integration and iterative\nupdate of new and old knowledge. Experimental results show that the proposed\nmethod is better than the existing mainstream comparison models in terms of\nknowledge retention and inference accuracy.\n","authors":["Yuxin Fan","Yuxiang Wang","Lipeng Liu","Xirui Tang","Na Sun","Zidong Yu"],"pdf_url":"https://arxiv.org/pdf/2501.07063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04945v2","updated":"2025-01-13T05:06:10Z","published":"2025-01-09T03:34:07Z","title":"Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of\n  Large Language Models","summary":"  It is crucial for large language models (LLMs) to follow instructions that\ninvolve multiple constraints. However, soft constraints are semantically\nrelated and difficult to verify through automated methods. These constraints\nremain a significant challenge for LLMs. To enhance the ability of LLMs to\nfollow soft constraints, we initially design a pipeline to obtain high-quality\noutputs automatically. Additionally, to fully utilize the acquired data, we\nintroduce a training paradigm based on curriculum learning. We experimentally\nevaluate the effectiveness of our methods in improving LLMs' soft constraint\nfollowing ability and analyze the factors driving the improvements. The\ndatasets and code are publicly available at\nhttps://github.com/Rainier-rq/FollowSoftConstraints.\n","authors":["Qingyu Ren","Jie Zeng","Qianyu He","Jiaqing Liang","Yanghua Xiao","Weikang Zhou","Zeye Sun","Fei Yu"],"pdf_url":"https://arxiv.org/pdf/2501.04945v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16837v2","updated":"2025-01-13T05:04:59Z","published":"2024-07-23T21:02:38Z","title":"MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs","summary":"  The ability to compare objects, scenes, or situations is crucial for\neffective decision-making and problem-solving in everyday life. For instance,\ncomparing the freshness of apples enables better choices during grocery\nshopping while comparing sofa designs helps optimize the aesthetics of our\nliving space. Despite its significance, the comparative capability is largely\nunexplored in artificial general intelligence (AGI). In this paper, we\nintroduce MLLM-CompBench, a benchmark designed to evaluate the comparative\nreasoning capability of multimodal large language models (MLLMs).\nMLLM-CompBench mines and pairs images through visually oriented questions\ncovering eight dimensions of relative comparison: visual attribute, existence,\nstate, emotion, temporality, spatiality, quantity, and quality. We curate a\ncollection of around 40K image pairs using metadata from diverse vision\ndatasets and CLIP similarity scores. These image pairs span a broad array of\nvisual domains, including animals, fashion, sports, and both outdoor and indoor\nscenes. The questions are carefully crafted to discern relative characteristics\nbetween two images and are labeled by human annotators for accuracy and\nrelevance. We use MLLM-CompBench to evaluate recent MLLMs, including\nGPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our results reveal notable\nshortcomings in their comparative abilities. We believe MLLM-COMPBENCH not only\nsheds light on these limitations but also establishes a solid foundation for\nfuture enhancements in the comparative capability of MLLMs.\n","authors":["Jihyung Kil","Zheda Mai","Justin Lee","Zihe Wang","Kerrie Cheng","Lemeng Wang","Ye Liu","Arpita Chowdhury","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2407.16837v2.pdf","comment":"This paper has been accepted to NeurIPS 2024. The first two authors\n  contributed equally to this work"},{"id":"http://arxiv.org/abs/2412.19925v2","updated":"2025-01-13T04:33:01Z","published":"2024-12-27T21:19:01Z","title":"HADES: Hardware Accelerated Decoding for Efficient Speculation in Large\n  Language Models","summary":"  Large Language Models (LLMs) have revolutionized natural language processing\nby understanding and generating human-like text. However, the increasing demand\nfor more sophisticated LLMs presents significant computational challenges due\nto their scale and complexity. This paper introduces Hardware Accelerated\nDecoding (HADES), a novel approach to enhance the performance and energy\nefficiency of LLMs. We address the design of an LLM accelerator with\nhardware-level speculative decoding support, a concept not previously explored\nin existing literature. Our work demonstrates how speculative decoding can\nsignificantly improve the efficiency of LLM operations, paving the way for more\nadvanced and practical applications of these models.\n","authors":["Ze Yang","Yihong Jin","Xinhe Xu"],"pdf_url":"https://arxiv.org/pdf/2412.19925v2.pdf","comment":"Accepted to ICCEA 2025"},{"id":"http://arxiv.org/abs/2501.07047v1","updated":"2025-01-13T04:08:14Z","published":"2025-01-13T04:08:14Z","title":"Leveraging ASIC AI Chips for Homomorphic Encryption","summary":"  Cloud-based services are making the outsourcing of sensitive client data\nincreasingly common. Although homomorphic encryption (HE) offers strong privacy\nguarantee, it requires substantially more resources than computing on\nplaintext, often leading to unacceptably large latencies in getting the\nresults. HE accelerators have emerged to mitigate this latency issue, but with\nthe high cost of ASICs. In this paper we show that HE primitives can be\nconverted to AI operators and accelerated on existing ASIC AI accelerators,\nlike TPUs, which are already widely deployed in the cloud. Adapting such\naccelerators for HE requires (1) supporting modular multiplication, (2)\nhigh-precision arithmetic in software, and (3) efficient mapping on matrix\nengines. We introduce the CROSS compiler (1) to adopt Barrett reduction to\nprovide modular reduction support using multiplier and adder, (2) Basis Aligned\nTransformation (BAT) to convert high-precision multiplication as low-precision\nmatrix-vector multiplication, (3) Matrix Aligned Transformation (MAT) to covert\nvectorized modular operation with reduction into matrix multiplication that can\nbe efficiently processed on 2D spatial matrix engine. Our evaluation of CROSS\non a Google TPUv4 demonstrates significant performance improvements, with up to\n161x and 5x speedup compared to the previous work on many-core CPUs and V100.\nThe kernel-level codes are open-sourced at\nhttps://github.com/google/jaxite.git.\n","authors":["Jianming Tong","Tianhao Huang","Leo de Castro","Anirudh Itagi","Jingtian Dang","Anupam Golder","Asra Ali","Jevin Jiang"," Arvind","G. Edward Suh","Tushar Krishna"],"pdf_url":"https://arxiv.org/pdf/2501.07047v1.pdf","comment":"16 pages, 10 figures, 4 algorithms, 7 tables. Enabling Google TPUv4\n  for privacy-preserving AI inference"},{"id":"http://arxiv.org/abs/2403.09792v3","updated":"2025-01-13T03:30:37Z","published":"2024-03-14T18:24:55Z","title":"Images are Achilles' Heel of Alignment: Exploiting Visual\n  Vulnerabilities for Jailbreaking Multimodal Large Language Models","summary":"  In this paper, we study the harmlessness alignment problem of multimodal\nlarge language models (MLLMs). We conduct a systematic empirical analysis of\nthe harmlessness performance of representative MLLMs and reveal that the image\ninput poses the alignment vulnerability of MLLMs. Inspired by this, we propose\na novel jailbreak method named HADES, which hides and amplifies the harmfulness\nof the malicious intent within the text input, using meticulously crafted\nimages. Experimental results show that HADES can effectively jailbreak existing\nMLLMs, which achieves an average Attack Success Rate (ASR) of 90.26% for\nLLaVA-1.5 and 71.60% for Gemini Pro Vision. Our code and data are available at\nhttps://github.com/RUCAIBox/HADES.\n","authors":["Yifan Li","Hangyu Guo","Kun Zhou","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.09792v3.pdf","comment":"ECCV 2024 Oral"},{"id":"http://arxiv.org/abs/2501.07020v1","updated":"2025-01-13T02:47:13Z","published":"2025-01-13T02:47:13Z","title":"ViSoLex: An Open-Source Repository for Vietnamese Social Media Lexical\n  Normalization","summary":"  ViSoLex is an open-source system designed to address the unique challenges of\nlexical normalization for Vietnamese social media text. The platform provides\ntwo core services: Non-Standard Word (NSW) Lookup and Lexical Normalization,\nenabling users to retrieve standard forms of informal language and standardize\ntext containing NSWs. ViSoLex's architecture integrates pre-trained language\nmodels and weakly supervised learning techniques to ensure accurate and\nefficient normalization, overcoming the scarcity of labeled data in Vietnamese.\nThis paper details the system's design, functionality, and its applications for\nresearchers and non-technical users. Additionally, ViSoLex offers a flexible,\ncustomizable framework that can be adapted to various datasets and research\nrequirements. By publishing the source code, ViSoLex aims to contribute to the\ndevelopment of more robust Vietnamese natural language processing tools and\nencourage further research in lexical normalization. Future directions include\nexpanding the system's capabilities for additional languages and improving the\nhandling of more complex non-standard linguistic patterns.\n","authors":["Anh Thi-Hoang Nguyen","Dung Ha Nguyen","Kiet Van Nguyen"],"pdf_url":"https://arxiv.org/pdf/2501.07020v1.pdf","comment":"The 31st International Conference on Computational Linguistics\n  (COLING 2025)"},{"id":"http://arxiv.org/abs/2409.00265v2","updated":"2025-01-13T00:29:56Z","published":"2024-08-30T21:42:17Z","title":"Explainable Artificial Intelligence: A Survey of Needs, Techniques,\n  Applications, and Future Direction","summary":"  Artificial intelligence models encounter significant challenges due to their\nblack-box nature, particularly in safety-critical domains such as healthcare,\nfinance, and autonomous vehicles. Explainable Artificial Intelligence (XAI)\naddresses these challenges by providing explanations for how these models make\ndecisions and predictions, ensuring transparency, accountability, and fairness.\nExisting studies have examined the fundamental concepts of XAI, its general\nprinciples, and the scope of XAI techniques. However, there remains a gap in\nthe literature as there are no comprehensive reviews that delve into the\ndetailed mathematical representations, design methodologies of XAI models, and\nother associated aspects. This paper provides a comprehensive literature review\nencompassing common terminologies and definitions, the need for XAI,\nbeneficiaries of XAI, a taxonomy of XAI methods, and the application of XAI\nmethods in different application areas. The survey is aimed at XAI researchers,\nXAI practitioners, AI model developers, and XAI beneficiaries who are\ninterested in enhancing the trustworthiness, transparency, accountability, and\nfairness of their AI models.\n","authors":["Melkamu Mersha","Khang Lam","Joseph Wood","Ali AlShami","Jugal Kalita"],"pdf_url":"https://arxiv.org/pdf/2409.00265v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06986v1","updated":"2025-01-13T00:29:55Z","published":"2025-01-13T00:29:55Z","title":"LEO: Boosting Mixture of Vision Encoders for Multimodal Large Language\n  Models","summary":"  Enhanced visual understanding serves as a cornerstone for multimodal large\nlanguage models (MLLMs). Recent hybrid MLLMs incorporate a mixture of vision\nexperts to address the limitations of using a single vision encoder and\nexcessively long visual tokens. Despite the progress of these MLLMs, a research\ngap remains in effectively integrating diverse vision encoders. This work\nexplores fusion strategies of visual tokens for hybrid MLLMs, leading to the\ndesign of LEO, a novel MLLM with a dual-branch vision encoder framework that\nincorporates a post-adaptation fusion strategy and adaptive tiling: for each\nsegmented tile of the input images, LEO sequentially interleaves the visual\ntokens from its two vision encoders. Extensive evaluation across 13\nvision-language benchmarks reveals that LEO outperforms state-of-the-art\nopen-source MLLMs and hybrid MLLMs on the majority of tasks. Furthermore, we\nshow that LEO can be adapted to the specialized domain of autonomous driving\nwithout altering the model architecture or training recipe, achieving\ncompetitive performance compared to existing baselines. The code and model will\nbe publicly available.\n","authors":["Mozhgan Nasr Azadani","James Riddell","Sean Sedwards","Krzysztof Czarnecki"],"pdf_url":"https://arxiv.org/pdf/2501.06986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00142v2","updated":"2025-01-13T23:45:26Z","published":"2024-11-28T18:55:41Z","title":"Sparse Attention Vectors: Generative Multimodal Model Features Are\n  Discriminative Vision-Language Classifiers","summary":"  Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a\nwide variety of vision-language (VL) tasks such as image captioning or visual\nquestion answering. Despite strong performance, LMMs are not directly suited\nfor foundational discriminative vision-language tasks (i.e., tasks requiring\ndiscrete label predictions) such as image classification and multiple-choice\nVQA. One key challenge in utilizing LMMs for discriminative tasks is the\nextraction of useful features from generative models. To overcome this issue,\nwe propose an approach for finding features in the model's latent space to more\neffectively leverage LMMs for discriminative tasks. Toward this end, we present\nSparse Attention Vectors (SAVs) -- a finetuning-free method that leverages\nsparse attention head activations (fewer than 1\\% of the heads) in LMMs as\nstrong features for VL tasks. With only few-shot examples, SAVs demonstrate\nstate-of-the-art performance compared to a variety of few-shot and finetuned\nbaselines on a collection of discriminative tasks. Our experiments also imply\nthat SAVs can scale in performance with additional examples and generalize to\nsimilar tasks, establishing SAVs as both effective and robust multimodal\nfeature representations.\n","authors":["Chancharik Mitra","Brandon Huang","Tianning Chai","Zhiqiu Lin","Assaf Arbelle","Rogerio Feris","Leonid Karlinsky","Trevor Darrell","Deva Ramanan","Roei Herzig"],"pdf_url":"https://arxiv.org/pdf/2412.00142v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07746v1","updated":"2025-01-13T23:21:33Z","published":"2025-01-13T23:21:33Z","title":"A Heterogeneous Multimodal Graph Learning Framework for Recognizing User\n  Emotions in Social Networks","summary":"  The rapid expansion of social media platforms has provided unprecedented\naccess to massive amounts of multimodal user-generated content. Comprehending\nuser emotions can provide valuable insights for improving communication and\nunderstanding of human behaviors. Despite significant advancements in Affective\nComputing, the diverse factors influencing user emotions in social networks\nremain relatively understudied. Moreover, there is a notable lack of deep\nlearning-based methods for predicting user emotions in social networks, which\ncould be addressed by leveraging the extensive multimodal data available. This\nwork presents a novel formulation of personalized emotion prediction in social\nnetworks based on heterogeneous graph learning. Building upon this formulation,\nwe design HMG-Emo, a Heterogeneous Multimodal Graph Learning Framework that\nutilizes deep learning-based features for user emotion recognition.\nAdditionally, we include a dynamic context fusion module in HMG-Emo that is\ncapable of adaptively integrating the different modalities in social media\ndata. Through extensive experiments, we demonstrate the effectiveness of\nHMG-Emo and verify the superiority of adopting a graph neural network-based\napproach, which outperforms existing baselines that use rich hand-crafted\nfeatures. To the best of our knowledge, HMG-Emo is the first multimodal and\ndeep-learning-based approach to predict personalized emotions within online\nsocial networks. Our work highlights the significance of exploiting advanced\ndeep learning techniques for less-explored problems in Affective Computing.\n","authors":["Sree Bhattacharyya","Shuhua Yang","James Z. Wang"],"pdf_url":"https://arxiv.org/pdf/2501.07746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07740v1","updated":"2025-01-13T23:10:02Z","published":"2025-01-13T23:10:02Z","title":"Advancing Student Writing Through Automated Syntax Feedback","summary":"  This study underscores the pivotal role of syntax feedback in augmenting the\nsyntactic proficiency of students. Recognizing the challenges faced by learners\nin mastering syntactic nuances, we introduce a specialized dataset named\nEssay-Syntax-Instruct designed to enhance the understanding and application of\nEnglish syntax among these students. Leveraging the capabilities of Large\nLanguage Models (LLMs) such as GPT3.5-Turbo, Llama-2-7b-chat-hf,\nLlama-2-13b-chat-hf, and Mistral-7B-Instruct-v0.2, this work embarks on a\ncomprehensive fine-tuning process tailored to the syntax improvement task.\nThrough meticulous evaluation, we demonstrate that the fine-tuned LLMs exhibit\na marked improvement in addressing syntax-related challenges, thereby serving\nas a potent tool for students to identify and rectify their syntactic errors.\nThe findings not only highlight the effectiveness of the proposed dataset in\nelevating the performance of LLMs for syntax enhancement but also illuminate a\npromising path for utilizing advanced language models to support language\nacquisition efforts. This research contributes to the broader field of language\nlearning technology by showcasing the potential of LLMs in facilitating the\nlinguistic development of Students.\n","authors":["Kamyar Zeinalipour","Mehak Mehak","Fatemeh Parsamotamed","Marco Maggini","Marco Gori"],"pdf_url":"https://arxiv.org/pdf/2501.07740v1.pdf","comment":"This paper has been accepted for presentation at AIEER 2024"},{"id":"http://arxiv.org/abs/2501.07726v1","updated":"2025-01-13T22:24:52Z","published":"2025-01-13T22:24:52Z","title":"Exploring the encoding of linguistic representations in the\n  Fully-Connected Layer of generative CNNs for Speech","summary":"  Interpretability work on the convolutional layers of CNNs has primarily\nfocused on computer vision, but some studies also explore correspondences\nbetween the latent space and the output in the audio domain. However, it has\nnot been thoroughly examined how acoustic and linguistic information is\nrepresented in the fully connected (FC) layer that bridges the latent space and\nconvolutional layers. The current study presents the first exploration of how\nthe FC layer of CNNs for speech synthesis encodes linguistically relevant\ninformation. We propose two techniques for exploration of the fully connected\nlayer. In Experiment 1, we use weight matrices as inputs into convolutional\nlayers. In Experiment 2, we manipulate the FC layer to explore how\nsymbolic-like representations are encoded in CNNs. We leverage the fact that\nthe FC layer outputs a feature map and that variable-specific weight matrices\nare temporally structured to (1) demonstrate how the distribution of learned\nweights varies between latent variables in systematic ways and (2) demonstrate\nhow manipulating the FC layer while holding constant subsequent model\nparameters affects the output. We ultimately present an FC manipulation that\ncan output a single segment. Using this technique, we show that lexically\nspecific latent codes in generative CNNs (ciwGAN) have shared lexically\ninvariant sublexical representations in the FC-layer weights, showing that\nciwGAN encodes lexical information in a linguistically principled manner.\n","authors":["Bruno Ferenc Šegedin","Gasper Beguš"],"pdf_url":"https://arxiv.org/pdf/2501.07726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09308v3","updated":"2025-01-13T22:22:06Z","published":"2023-11-15T19:02:40Z","title":"Divergences between Language Models and Human Brains","summary":"  Do machines and humans process language in similar ways? Recent research has\nhinted at the affirmative, showing that human neural activity can be\neffectively predicted using the internal representations of language models\n(LMs). Although such results are thought to reflect shared computational\nprinciples between LMs and human brains, there are also clear differences in\nhow LMs and humans represent and use language. In this work, we systematically\nexplore the divergences between human and machine language processing by\nexamining the differences between LM representations and human brain responses\nto language as measured by Magnetoencephalography (MEG) across two datasets in\nwhich subjects read and listened to narrative stories. Using an LLM-based\ndata-driven approach, we identify two domains that LMs do not capture well:\nsocial/emotional intelligence and physical commonsense. We validate these\nfindings with human behavioral experiments and hypothesize that the gap is due\nto insufficient representations of social/emotional and physical knowledge in\nLMs. Our results show that fine-tuning LMs on these domains can improve their\nalignment with human brain responses.\n","authors":["Yuchen Zhou","Emmy Liu","Graham Neubig","Michael J. Tarr","Leila Wehbe"],"pdf_url":"https://arxiv.org/pdf/2311.09308v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07723v1","updated":"2025-01-13T22:18:52Z","published":"2025-01-13T22:18:52Z","title":"ESURF: Simple and Effective EDU Segmentation","summary":"  Segmenting text into Elemental Discourse Units (EDUs) is a fundamental task\nin discourse parsing. We present a new simple method for identifying EDU\nboundaries, and hence segmenting them, based on lexical and character n-gram\nfeatures, using random forest classification. We show that the method, despite\nits simplicity, outperforms other methods both for segmentation and within a\nstate of the art discourse parser. This indicates the importance of such\nfeatures for identifying basic discourse elements, pointing towards potentially\nmore training-efficient methods for discourse analysis.\n","authors":["Mohammadreza Sediqin","Shlomo Engelson Argamon"],"pdf_url":"https://arxiv.org/pdf/2501.07723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07721v1","updated":"2025-01-13T22:14:45Z","published":"2025-01-13T22:14:45Z","title":"LLMic: Romanian Foundation Language Model","summary":"  Recent advances in Large Language Models (LLMs) have demonstrated remarkable\ncapabilities across various tasks with commercial models leading the way. While\nopen models usually operate at a smaller scale, they maintain competitiveness\nthrough specialization and fine-tuning. However, a significant challenge\npersists: open models often underperform in low-resource languages due to\nlimited representation in the training corpus. In this paper, we present LLMic,\na bilingual foundation language model designed specifically for the Romanian\nLanguage. We document the complete process of pretraining a foundation model\nfor a low-resource language, including corpus construction, architecture\nselection, and hyper-parameter optimization. Our evaluation demonstrates that\nLLMic can be specialized for tasks in the target language, achieving results\ncomparable to other much larger open models. We show that fine-tuning LLMic for\nlanguage translation after the initial pretraining phase outperforms existing\nsolutions in English-to-Romanian translation tasks. This opens the path for\nefficient large-scale processing for the Romanian language community, using the\nmuch smaller LLMic model\n","authors":["Vlad-Andrei Bădoiu","Mihai-Valentin Dumitru","Alexandru M. Gherghescu","Alexandru Agache","Costin Raiciu"],"pdf_url":"https://arxiv.org/pdf/2501.07721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07719v1","updated":"2025-01-13T22:09:44Z","published":"2025-01-13T22:09:44Z","title":"Entailed Between the Lines: Incorporating Implication into NLI","summary":"  Much of human communication depends on implication, conveying meaning beyond\nliteral words to express a wider range of thoughts, intentions, and feelings.\nFor models to better understand and facilitate human communication, they must\nbe responsive to the text's implicit meaning. We focus on Natural Language\nInference (NLI), a core tool for many language tasks, and find that\nstate-of-the-art NLI models and datasets struggle to recognize a range of cases\nwhere entailment is implied, rather than explicit from the text. We formalize\nimplied entailment as an extension of the NLI task and introduce the Implied\nNLI dataset (INLI) to help today's LLMs both recognize a broader variety of\nimplied entailments and to distinguish between implicit and explicit\nentailment. We show how LLMs fine-tuned on INLI understand implied entailment\nand can generalize this understanding across datasets and domains.\n","authors":["Shreya Havaldar","Hamidreza Alvari","Alex Fabrikant","John Palowitch","Mohammad Javad Hosseini","Senaka Buthpitiya"],"pdf_url":"https://arxiv.org/pdf/2501.07719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07718v1","updated":"2025-01-13T22:08:29Z","published":"2025-01-13T22:08:29Z","title":"Benchmarking Abstractive Summarisation: A Dataset of Human-authored\n  Summaries of Norwegian News Articles","summary":"  We introduce a dataset of high-quality human-authored summaries of news\narticles in Norwegian. The dataset is intended for benchmarking the abstractive\nsummarisation capabilities of generative language models. Each document in the\ndataset is provided with three different candidate gold-standard summaries\nwritten by native Norwegian speakers, and all summaries are provided in both of\nthe written variants of Norwegian -- Bokm{\\aa}l and Nynorsk. The paper\ndescribes details on the data creation effort as well as an evaluation of\nexisting open LLMs for Norwegian on the dataset. We also provide insights from\na manual human evaluation, comparing human-authored to model-generated\nsummaries. Our results indicate that the dataset provides a challenging LLM\nbenchmark for Norwegian summarisation capabilities\n","authors":["Samia Touileb","Vladislav Mikhailov","Marie Kroka","Lilja Øvrelid","Erik Velldal"],"pdf_url":"https://arxiv.org/pdf/2501.07718v1.pdf","comment":"Accepted at NoDaLiDa2025"},{"id":"http://arxiv.org/abs/2404.12652v2","updated":"2025-01-13T21:59:56Z","published":"2024-04-19T06:41:32Z","title":"Pre-trained Vision-Language Models Learn Discoverable Visual Concepts","summary":"  Do vision-language models (VLMs) pre-trained to caption an image of a\n\"durian\" learn visual concepts such as \"brown\" (color) and \"spiky\" (texture) at\nthe same time? We aim to answer this question as visual concepts learned \"for\nfree\" would enable wide applications such as neuro-symbolic reasoning or\nhuman-interpretable object classification. We assume that the visual concepts,\nif captured by pre-trained VLMs, can be extracted by their vision-language\ninterface with text-based concept prompts. We observe that recent works\nprompting VLMs with concepts often differ in their strategies to define and\nevaluate the visual concepts, leading to conflicting conclusions. We propose a\nnew concept definition strategy based on two observations: First, certain\nconcept prompts include shortcuts that recognize correct concepts for wrong\nreasons; Second, multimodal information (e.g. visual discriminativeness, and\ntextual knowledge) should be leveraged when selecting the concepts. Our\nproposed concept discovery and learning (CDL) framework is thus designed to\nidentify a diverse list of generic visual concepts (e.g. \"spiky\" as opposed to\n\"spiky durian\"), which are ranked and selected based on visual and language\nmutual information. We carefully design quantitative and human evaluations of\nthe discovered concepts on six diverse visual recognition datasets, which\nconfirm that pre-trained VLMs do learn visual concepts that provide accurate\nand thorough descriptions for the recognized objects. All code and models are\npublicly released.\n","authors":["Yuan Zang","Tian Yun","Hao Tan","Trung Bui","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2404.12652v2.pdf","comment":"Transactions on Machine Learning Research, 2025"},{"id":"http://arxiv.org/abs/2501.07670v1","updated":"2025-01-13T20:08:52Z","published":"2025-01-13T20:08:52Z","title":"A Survey of Early Exit Deep Neural Networks in NLP","summary":"  Deep Neural Networks (DNNs) have grown increasingly large in size to achieve\nstate of the art performance across a wide range of tasks. However, their high\ncomputational requirements make them less suitable for resource-constrained\napplications. Also, real-world datasets often consist of a mixture of easy and\ncomplex samples, necessitating adaptive inference mechanisms that account for\nsample difficulty. Early exit strategies offer a promising solution by enabling\nadaptive inference, where simpler samples are classified using the initial\nlayers of the DNN, thereby accelerating the overall inference process. By\nattaching classifiers at different layers, early exit methods not only reduce\ninference latency but also improve the model robustness against adversarial\nattacks. This paper presents a comprehensive survey of early exit methods and\ntheir applications in NLP.\n","authors":["Divya Jyoti Bajpai","Manjesh Kumar Hanawal"],"pdf_url":"https://arxiv.org/pdf/2501.07670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02052v4","updated":"2025-01-13T19:51:53Z","published":"2024-10-02T21:42:35Z","title":"ExACT: Teaching AI Agents to Explore with Reflective-MCTS and\n  Exploratory Learning","summary":"  Autonomous agents have demonstrated significant potential in automating\ncomplex multistep decision-making tasks. However, even state-of-the-art\nvision-language models (VLMs), such as GPT-4o, still fall short of human-level\nperformance, particularly in intricate web environments and long-horizon tasks.\nTo address these limitations, we present ExACT, an approach to combine\ntest-time search and self-learning to build o1-like models for agentic\napplications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a\nnovel test time algorithm designed to enhance AI agents' ability to explore\ndecision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating\ncontrastive reflection, allowing agents to learn from past interactions and\ndynamically improve their search efficiency; and 2) using multi-agent debate\nfor reliable state evaluation. Next, we introduce Exploratory Learning, a novel\nlearning strategy to teach agents to search at inference time without relying\non any external search algorithms. On the challenging VisualWebArena benchmark,\nour GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across\nvarious tasks compared to the previous state-of-the-art. Additionally, we show\nthat the knowledge and experience gained from test-time search can be\neffectively transferred back to GPT-4o via fine-tuning. After Exploratory\nLearning, GPT-4o 1) demonstrates the ability to explore the environment,\nevaluate a state, and backtrack to viable ones when it detects that the current\nstate cannot lead to success, and 2) matches 87% of R-MCTS's performance while\nusing significantly less compute. Notably, our work demonstrates the compute\nscaling properties in both training - data collection with R-MCTS - and testing\ntime. These results suggest a promising research direction to enhance VLMs'\ncapabilities for agentic applications via test-time search and self-learning.\n","authors":["Xiao Yu","Baolin Peng","Vineeth Vajipey","Hao Cheng","Michel Galley","Jianfeng Gao","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02052v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07663v1","updated":"2025-01-13T19:49:49Z","published":"2025-01-13T19:49:49Z","title":"Enhancing Talent Employment Insights Through Feature Extraction with LLM\n  Finetuning","summary":"  This paper explores the application of large language models (LLMs) to\nextract nuanced and complex job features from unstructured job postings. Using\na dataset of 1.2 million job postings provided by AdeptID, we developed a\nrobust pipeline to identify and classify variables such as remote work\navailability, remuneration structures, educational requirements, and work\nexperience preferences. Our methodology combines semantic chunking,\nretrieval-augmented generation (RAG), and fine-tuning DistilBERT models to\novercome the limitations of traditional parsing tools. By leveraging these\ntechniques, we achieved significant improvements in identifying variables often\nmislabeled or overlooked, such as non-salary-based compensation and inferred\nremote work categories. We present a comprehensive evaluation of our fine-tuned\nmodels and analyze their strengths, limitations, and potential for scaling.\nThis work highlights the promise of LLMs in labor market analytics, providing a\nfoundation for more accurate and actionable insights into job data.\n","authors":["Karishma Thakrar","Nick Young"],"pdf_url":"https://arxiv.org/pdf/2501.07663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07641v1","updated":"2025-01-13T19:04:57Z","published":"2025-01-13T19:04:57Z","title":"GPT as a Monte Carlo Language Tree: A Probabilistic Perspective","summary":"  Large Language Models (LLMs), such as GPT, are considered to learn the latent\ndistributions within large-scale web-crawl datasets and accomplish natural\nlanguage processing (NLP) tasks by predicting the next token. However, this\nmechanism of latent distribution modeling lacks quantitative understanding and\nanalysis. In this paper, we propose a novel perspective that any language\ndataset can be represented by a Monte Carlo Language Tree (abbreviated as\n``Data-Tree''), where each node denotes a token, each edge denotes a token\ntransition probability, and each sequence has a unique path. Any GPT-like\nlanguage model can also be flattened into another Monte Carlo Language Tree\n(abbreviated as ``GPT-Tree''). Our experiments show that different GPT models\ntrained on the same dataset exhibit significant structural similarity in\nGPT-Tree visualization, and larger models converge more closely to the\nData-Tree. More than 87\\% GPT output tokens can be recalled by Data-Tree. These\nfindings may confirm that the reasoning process of LLMs is more likely to be\nprobabilistic pattern-matching rather than formal reasoning, as each model\ninference seems to find a context pattern with maximum probability from the\nData-Tree. Furthermore, we provide deeper insights into issues such as\nhallucination, Chain-of-Thought (CoT) reasoning, and token bias in LLMs.\n","authors":["Kun-Peng Ning","Jia-Yu Yao","Yu-Yang Liu","Mu-Nan Ning","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2501.07641v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2501.07575v1","updated":"2025-01-13T18:59:48Z","published":"2025-01-13T18:59:48Z","title":"Dataset Distillation via Committee Voting","summary":"  Dataset distillation aims to synthesize a smaller, representative dataset\nthat preserves the essential properties of the original data, enabling\nefficient model training with reduced computational resources. Prior work has\nprimarily focused on improving the alignment or matching process between\noriginal and synthetic data, or on enhancing the efficiency of distilling large\ndatasets. In this work, we introduce ${\\bf C}$ommittee ${\\bf V}$oting for ${\\bf\nD}$ataset ${\\bf D}$istillation (CV-DD), a novel and orthogonal approach that\nleverages the collective wisdom of multiple models or experts to create\nhigh-quality distilled datasets. We start by showing how to establish a strong\nbaseline that already achieves state-of-the-art accuracy through leveraging\nrecent advancements and thoughtful adjustments in model design and optimization\nprocesses. By integrating distributions and predictions from a committee of\nmodels while generating high-quality soft labels, our method captures a wider\nspectrum of data features, reduces model-specific biases and the adverse\neffects of distribution shifts, leading to significant improvements in\ngeneralization. This voting-based strategy not only promotes diversity and\nrobustness within the distilled dataset but also significantly reduces\noverfitting, resulting in improved performance on post-eval tasks. Extensive\nexperiments across various datasets and IPCs (images per class) demonstrate\nthat Committee Voting leads to more reliable and adaptable distilled data\ncompared to single/multi-model distillation methods, demonstrating its\npotential for efficient and accurate dataset distillation. Code is available\nat: https://github.com/Jiacheng8/CV-DD.\n","authors":["Jiacheng Cui","Zhaoyi Li","Xiaochen Ma","Xinyue Bi","Yaxin Luo","Zhiqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2501.07575v1.pdf","comment":"Code at: https://github.com/Jiacheng8/CV-DD"},{"id":"http://arxiv.org/abs/2501.07574v1","updated":"2025-01-13T18:59:20Z","published":"2025-01-13T18:59:20Z","title":"UnCommon Objects in 3D","summary":"  We introduce Uncommon Objects in 3D (uCO3D), a new object-centric dataset for\n3D deep learning and 3D generative AI. uCO3D is the largest publicly-available\ncollection of high-resolution videos of objects with 3D annotations that\nensures full-360$^{\\circ}$ coverage. uCO3D is significantly more diverse than\nMVImgNet and CO3Dv2, covering more than 1,000 object categories. It is also of\nhigher quality, due to extensive quality checks of both the collected videos\nand the 3D annotations. Similar to analogous datasets, uCO3D contains\nannotations for 3D camera poses, depth maps and sparse point clouds. In\naddition, each object is equipped with a caption and a 3D Gaussian Splat\nreconstruction. We train several large 3D models on MVImgNet, CO3Dv2, and uCO3D\nand obtain superior results using the latter, showing that uCO3D is better for\nlearning applications.\n","authors":["Xingchen Liu","Piyush Tayal","Jianyuan Wang","Jesus Zarzar","Tom Monnier","Konstantinos Tertikas","Jiali Duan","Antoine Toisoul","Jason Y. Zhang","Natalia Neverova","Andrea Vedaldi","Roman Shapovalov","David Novotny"],"pdf_url":"https://arxiv.org/pdf/2501.07574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07563v1","updated":"2025-01-13T18:53:08Z","published":"2025-01-13T18:53:08Z","title":"Training-Free Motion-Guided Video Generation with Enhanced Temporal\n  Consistency Using Motion Consistency Loss","summary":"  In this paper, we address the challenge of generating temporally consistent\nvideos with motion guidance. While many existing methods depend on additional\ncontrol modules or inference-time fine-tuning, recent studies suggest that\neffective motion guidance is achievable without altering the model architecture\nor requiring extra training. Such approaches offer promising compatibility with\nvarious video generation foundation models. However, existing training-free\nmethods often struggle to maintain consistent temporal coherence across frames\nor to follow guided motion accurately. In this work, we propose a simple yet\neffective solution that combines an initial-noise-based approach with a novel\nmotion consistency loss, the latter being our key innovation. Specifically, we\ncapture the inter-frame feature correlation patterns of intermediate features\nfrom a video diffusion model to represent the motion pattern of the reference\nvideo. We then design a motion consistency loss to maintain similar feature\ncorrelation patterns in the generated video, using the gradient of this loss in\nthe latent space to guide the generation process for precise motion control.\nThis approach improves temporal consistency across various motion control tasks\nwhile preserving the benefits of a training-free setup. Extensive experiments\nshow that our method sets a new standard for efficient, temporally coherent\nvideo generation.\n","authors":["Xinyu Zhang","Zicheng Duan","Dong Gong","Lingqiao Liu"],"pdf_url":"https://arxiv.org/pdf/2501.07563v1.pdf","comment":"Project page:\n  https://zhangxinyu-xyz.github.io/SimulateMotion.github.io/"},{"id":"http://arxiv.org/abs/2501.07556v1","updated":"2025-01-13T18:37:36Z","published":"2025-01-13T18:37:36Z","title":"MatchAnything: Universal Cross-Modality Image Matching with Large-Scale\n  Pre-Training","summary":"  Image matching, which aims to identify corresponding pixel locations between\nimages, is crucial in a wide range of scientific disciplines, aiding in image\nregistration, fusion, and analysis. In recent years, deep learning-based image\nmatching algorithms have dramatically outperformed humans in rapidly and\naccurately finding large amounts of correspondences. However, when dealing with\nimages captured under different imaging modalities that result in significant\nappearance changes, the performance of these algorithms often deteriorates due\nto the scarcity of annotated cross-modal training data. This limitation hinders\napplications in various fields that rely on multiple image modalities to obtain\ncomplementary information. To address this challenge, we propose a large-scale\npre-training framework that utilizes synthetic cross-modal training signals,\nincorporating diverse data from various sources, to train models to recognize\nand match fundamental structures across images. This capability is transferable\nto real-world, unseen cross-modality image matching tasks. Our key finding is\nthat the matching model trained with our framework achieves remarkable\ngeneralizability across more than eight unseen cross-modality registration\ntasks using the same network weight, substantially outperforming existing\nmethods, whether designed for generalization or tailored for specific tasks.\nThis advancement significantly enhances the applicability of image matching\ntechnologies across various scientific disciplines and paves the way for new\napplications in multi-modality human and artificial intelligence analysis and\nbeyond.\n","authors":["Xingyi He","Hao Yu","Sida Peng","Dongli Tan","Zehong Shen","Hujun Bao","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.07556v1.pdf","comment":"Project page: https://zju3dv.github.io/MatchAnything/"},{"id":"http://arxiv.org/abs/2501.07554v1","updated":"2025-01-13T18:37:08Z","published":"2025-01-13T18:37:08Z","title":"SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal\n  Aspects in Video Editing","summary":"  Video editing models have advanced significantly, but evaluating their\nperformance remains challenging. Traditional metrics, such as CLIP text and\nimage scores, often fall short: text scores are limited by inadequate training\ndata and hierarchical dependencies, while image scores fail to assess temporal\nconsistency. We present SST-EM (Semantic, Spatial, and Temporal Evaluation\nMetric), a novel evaluation framework that leverages modern Vision-Language\nModels (VLMs), Object Detection, and Temporal Consistency checks. SST-EM\ncomprises four components: (1) semantic extraction from frames using a VLM, (2)\nprimary object tracking with Object Detection, (3) focused object refinement\nvia an LLM agent, and (4) temporal consistency assessment using a Vision\nTransformer (ViT). These components are integrated into a unified metric with\nweights derived from human evaluations and regression analysis. The name SST-EM\nreflects its focus on Semantic, Spatial, and Temporal aspects of video\nevaluation. SST-EM provides a comprehensive evaluation of semantic fidelity and\ntemporal smoothness in video editing. The source code is available in the\n\\textbf{\\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub\nRepository}}.\n","authors":["Varun Biyyala","Bharat Chanderprakash Kathuria","Jialu Li","Youshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.07554v1.pdf","comment":"WACV workshop"},{"id":"http://arxiv.org/abs/2501.07542v1","updated":"2025-01-13T18:23:57Z","published":"2025-01-13T18:23:57Z","title":"Imagine while Reasoning in Space: Multimodal Visualization-of-Thought","summary":"  Chain-of-Thought (CoT) prompting has proven highly effective for enhancing\ncomplex reasoning in Large Language Models (LLMs) and Multimodal Large Language\nModels (MLLMs). Yet, it struggles in complex spatial reasoning tasks.\nNonetheless, human cognition extends beyond language alone, enabling the\nremarkable capability to think in both words and images. Inspired by this\nmechanism, we propose a new reasoning paradigm, Multimodal\nVisualization-of-Thought (MVoT). It enables visual thinking in MLLMs by\ngenerating image visualizations of their reasoning traces. To ensure\nhigh-quality visualization, we introduce token discrepancy loss into\nautoregressive MLLMs. This innovation significantly improves both visual\ncoherence and fidelity. We validate this approach through several dynamic\nspatial reasoning tasks. Experimental results reveal that MVoT demonstrates\ncompetitive performance across tasks. Moreover, it exhibits robust and reliable\nimprovements in the most challenging scenarios where CoT fails. Ultimately,\nMVoT establishes new possibilities for complex reasoning tasks where visual\nthinking can effectively complement verbal reasoning.\n","authors":["Chengzu Li","Wenshan Wu","Huanyu Zhang","Yan Xia","Shaoguang Mao","Li Dong","Ivan Vulić","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2501.07542v1.pdf","comment":"11 pages, 6 figures, 4 tables (27 pages, 10 figures, 16 tables\n  including references and appendices)"},{"id":"http://arxiv.org/abs/2411.11222v2","updated":"2025-01-13T18:20:35Z","published":"2024-11-18T01:19:37Z","title":"The Sound of Water: Inferring Physical Properties from Pouring Liquids","summary":"  We study the connection between audio-visual observations and the underlying\nphysics of a mundane yet intriguing everyday activity: pouring liquids. Given\nonly the sound of liquid pouring into a container, our objective is to\nautomatically infer physical properties such as the liquid level, the shape and\nsize of the container, the pouring rate and the time to fill. To this end, we:\n(i) show in theory that these properties can be determined from the fundamental\nfrequency (pitch); (ii) train a pitch detection model with supervision from\nsimulated data and visual data with a physics-inspired objective; (iii)\nintroduce a new large dataset of real pouring videos for a systematic study;\n(iv) show that the trained model can indeed infer these physical properties for\nreal data; and finally, (v) we demonstrate strong generalization to various\ncontainer shapes, other datasets, and in-the-wild YouTube videos. Our work\npresents a keen understanding of a narrow yet rich problem at the intersection\nof acoustics, physics, and learning. It opens up applications to enhance\nmultisensory perception in robotic pouring.\n","authors":["Piyush Bagad","Makarand Tapaswi","Cees G. M. Snoek","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2411.11222v2.pdf","comment":"Project page at https://bpiyush.github.io/pouring-water-website.\n  Short version accepted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2302.04850v3","updated":"2025-01-13T18:18:24Z","published":"2023-02-09T18:53:44Z","title":"Robot Synesthesia: A Sound and Emotion Guided AI Painter","summary":"  If a picture paints a thousand words, sound may voice a million. While recent\nrobotic painting and image synthesis methods have achieved progress in\ngenerating visuals from text inputs, the translation of sound into images is\nvastly unexplored. Generally, sound-based interfaces and sonic interactions\nhave the potential to expand accessibility and control for the user and provide\na means to convey complex emotions and the dynamic aspects of the real world.\nIn this paper, we propose an approach for using sound and speech to guide a\nrobotic painting process, known here as robot synesthesia. For general sound,\nwe encode the simulated paintings and input sounds into the same latent space.\nFor speech, we decouple speech into its transcribed text and the tone of the\nspeech. Whereas we use the text to control the content, we estimate the\nemotions from the tone to guide the mood of the painting. Our approach has been\nfully integrated with FRIDA, a robotic painting framework, adding sound and\nspeech to FRIDA's existing input modalities, such as text and style. In two\nsurveys, participants were able to correctly guess the emotion or natural sound\nused to generate a given painting more than twice as likely as random chance.\nOn our sound-guided image manipulation and music-guided paintings, we discuss\nthe results qualitatively.\n","authors":["Vihaan Misra","Peter Schaldenbrand","Jean Oh"],"pdf_url":"https://arxiv.org/pdf/2302.04850v3.pdf","comment":"9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2306.11207v4","updated":"2025-01-13T18:16:34Z","published":"2023-06-20T00:14:47Z","title":"Quilt-1M: One Million Image-Text Pairs for Histopathology","summary":"  Recent accelerations in multi-modal applications have been made possible with\nthe plethora of image and text data available online. However, the scarcity of\nanalogous data in the medical field, specifically in histopathology, has slowed\ncomparable progress. To enable similar representation learning for\nhistopathology, we turn to YouTube, an untapped resource of videos, offering\n$1,087$ hours of valuable educational histopathology videos from expert\nclinicians. From YouTube, we curate QUILT: a large-scale vision-language\ndataset consisting of $802, 144$ image and text pairs. QUILT was automatically\ncurated using a mixture of models, including large language models, handcrafted\nalgorithms, human knowledge databases, and automatic speech recognition. In\ncomparison, the most comprehensive datasets curated for histopathology amass\nonly around $200$K samples. We combine QUILT with datasets from other sources,\nincluding Twitter, research papers, and the internet in general, to create an\neven larger dataset: QUILT-1M, with $1$M paired image-text samples, marking it\nas the largest vision-language histopathology dataset to date. We demonstrate\nthe value of QUILT-1M by fine-tuning a pre-trained CLIP model. Our model\noutperforms state-of-the-art models on both zero-shot and linear probing tasks\nfor classifying new histopathology images across $13$ diverse patch-level\ndatasets of $8$ different sub-pathologies and cross-modal retrieval tasks.\n","authors":["Wisdom Oluchi Ikezogwo","Mehmet Saygin Seyfioglu","Fatemeh Ghezloo","Dylan Stefan Chan Geva","Fatwir Sheikh Mohammed","Pavan Kumar Anand","Ranjay Krishna","Linda Shapiro"],"pdf_url":"https://arxiv.org/pdf/2306.11207v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07533v1","updated":"2025-01-13T18:10:19Z","published":"2025-01-13T18:10:19Z","title":"Confident Pseudo-labeled Diffusion Augmentation for Canine Cardiomegaly\n  Detection","summary":"  Canine cardiomegaly, marked by an enlarged heart, poses serious health risks\nif undetected, requiring accurate diagnostic methods. Current detection models\noften rely on small, poorly annotated datasets and struggle to generalize\nacross diverse imaging conditions, limiting their real-world applicability. To\naddress these issues, we propose a Confident Pseudo-labeled Diffusion\nAugmentation (CDA) model for identifying canine cardiomegaly. Our approach\naddresses the challenge of limited high-quality training data by employing\ndiffusion models to generate synthetic X-ray images and annotate Vertebral\nHeart Score key points, thereby expanding the dataset. We also employ a\npseudo-labeling strategy with Monte Carlo Dropout to select high-confidence\nlabels, refine the synthetic dataset, and improve accuracy. Iteratively\nincorporating these labels enhances the model's performance, overcoming the\nlimitations of existing approaches. Experimental results show that the CDA\nmodel outperforms traditional methods, achieving state-of-the-art accuracy in\ncanine cardiomegaly detection. The code implementation is available at\nhttps://github.com/Shira7z/CDA.\n","authors":["Shiman Zhang","Lakshmikar Reddy Polamreddy","Youshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.07533v1.pdf","comment":"WACV workshop"},{"id":"http://arxiv.org/abs/2501.07530v1","updated":"2025-01-13T18:08:27Z","published":"2025-01-13T18:08:27Z","title":"IP-FaceDiff: Identity-Preserving Facial Video Editing with Diffusion","summary":"  Facial video editing has become increasingly important for content creators,\nenabling the manipulation of facial expressions and attributes. However,\nexisting models encounter challenges such as poor editing quality, high\ncomputational costs and difficulties in preserving facial identity across\ndiverse edits. Additionally, these models are often constrained to editing\npredefined facial attributes, limiting their flexibility to diverse editing\nprompts. To address these challenges, we propose a novel facial video editing\nframework that leverages the rich latent space of pre-trained text-to-image\n(T2I) diffusion models and fine-tune them specifically for facial video editing\ntasks. Our approach introduces a targeted fine-tuning scheme that enables high\nquality, localized, text-driven edits while ensuring identity preservation\nacross video frames. Additionally, by using pre-trained T2I models during\ninference, our approach significantly reduces editing time by 80%, while\nmaintaining temporal consistency throughout the video sequence. We evaluate the\neffectiveness of our approach through extensive testing across a wide range of\nchallenging scenarios, including varying head poses, complex action sequences,\nand diverse facial expressions. Our method consistently outperforms existing\ntechniques, demonstrating superior performance across a broad set of metrics\nand benchmarks.\n","authors":["Tharun Anand","Aryan Garg","Kaushik Mitra"],"pdf_url":"https://arxiv.org/pdf/2501.07530v1.pdf","comment":"WACV-25 Workshop"},{"id":"http://arxiv.org/abs/2402.16865v3","updated":"2025-01-13T18:06:23Z","published":"2024-01-21T04:14:54Z","title":"Enhance Eye Disease Detection using Learnable Probabilistic Discrete\n  Latents in Machine Learning Architectures","summary":"  Ocular diseases, including diabetic retinopathy and glaucoma, present a\nsignificant public health challenge due to their high prevalence and potential\nfor causing vision impairment. Early and accurate diagnosis is crucial for\neffective treatment and management. In recent years, deep learning models have\nemerged as powerful tools for analysing medical images, such as retina imaging.\nHowever, challenges persist in model relibability and uncertainty estimation,\nwhich are critical for clinical decision-making. This study leverages the\nprobabilistic framework of Generative Flow Networks (GFlowNets) to learn the\nposterior distribution over latent discrete dropout masks for the\nclassification and analysis of ocular diseases using fundus images. We develop\na robust and generalizable method that utilizes GFlowOut integrated with\nResNet18 and ViT models as the backbone in identifying various ocular\nconditions. This study employs a unique set of dropout masks - none, random,\nbottomup, and topdown - to enhance model performance in analyzing these fundus\nimages. Our results demonstrate that our learnable probablistic latents\nsignificantly improves accuracy, outperforming the traditional dropout\napproach. We utilize a gradient map calculation method, Grad-CAM, to assess\nmodel explainability, observing that the model accurately focuses on critical\nimage regions for predictions. The integration of GFlowOut in neural networks\npresents a promising advancement in the automated diagnosis of ocular diseases,\nwith implications for improving clinical workflows and patient outcomes.\n","authors":["Anirudh Prabhakaran","YeKun Xiao","Ching-Yu Cheng","Dianbo Liu"],"pdf_url":"https://arxiv.org/pdf/2402.16865v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07525v1","updated":"2025-01-13T17:55:32Z","published":"2025-01-13T17:55:32Z","title":"RadAlign: Advancing Radiology Report Generation with Vision-Language\n  Concept Alignment","summary":"  Automated chest radiographs interpretation requires both accurate disease\nclassification and detailed radiology report generation, presenting a\nsignificant challenge in the clinical workflow. Current approaches either focus\non classification accuracy at the expense of interpretability or generate\ndetailed but potentially unreliable reports through image captioning\ntechniques. In this study, we present RadAlign, a novel framework that combines\nthe predictive accuracy of vision-language models (VLMs) with the reasoning\ncapabilities of large language models (LLMs). Inspired by the radiologist's\nworkflow, RadAlign first employs a specialized VLM to align visual features\nwith key medical concepts, achieving superior disease classification with an\naverage AUC of 0.885 across multiple diseases. These recognized medical\nconditions, represented as text-based concepts in the aligned visual-language\nspace, are then used to prompt LLM-based report generation. Enhanced by a\nretrieval-augmented generation mechanism that grounds outputs in similar\nhistorical cases, RadAlign delivers superior report quality with a GREEN score\nof 0.678, outperforming state-of-the-art methods' 0.634. Our framework\nmaintains strong clinical interpretability while reducing hallucinations,\nadvancing automated medical imaging and report analysis through integrated\npredictive and generative AI. Code is available at\nhttps://github.com/difeigu/RadAlign.\n","authors":["Difei Gu","Yunhe Gao","Yang Zhou","Mu Zhou","Dimitris Metaxas"],"pdf_url":"https://arxiv.org/pdf/2501.07525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.11677v2","updated":"2025-01-13T17:45:59Z","published":"2023-04-23T15:09:02Z","title":"RGB-D Indiscernible Object Counting in Underwater Scenes","summary":"  Recently, indiscernible/camouflaged scene understanding has attracted lots of\nresearch attention in the vision community. We further advance the frontier of\nthis field by systematically studying a new challenge named indiscernible\nobject counting (IOC), the goal of which is to count objects that are blended\nwith respect to their surroundings. Due to a lack of appropriate IOC datasets,\nwe present a large-scale dataset IOCfish5K which contains a total of 5,637\nhigh-resolution images and 659,024 annotated center points. Our dataset\nconsists of a large number of indiscernible objects (mainly fish) in underwater\nscenes, making the annotation process all the more challenging. IOCfish5K is\nsuperior to existing datasets with indiscernible scenes because of its larger\nscale, higher image resolutions, more annotations, and denser scenes. All these\naspects make it the most challenging dataset for IOC so far, supporting\nprogress in this area. Benefiting from the recent advancements of depth\nestimation foundation models, we construct high-quality depth maps for\nIOCfish5K by generating pseudo labels using the Depth Anything V2 model. The\nRGB-D version of IOCfish5K is named IOCfish5K-D. For benchmarking purposes on\nIOCfish5K, we select 14 mainstream methods for object counting and carefully\nevaluate them. For multimodal IOCfish5K-D, we evaluate other 4 popular\nmultimodal counting methods. Furthermore, we propose IOCFormer, a new strong\nbaseline that combines density and regression branches in a unified framework\nand can effectively tackle object counting under concealed scenes. We also\npropose IOCFormer-D to enable the effective usage of depth modality in helping\ndetect and count objects hidden in their environments. Experiments show that\nIOCFormer and IOCFormer-D achieve state-of-the-art scores on IOCfish5K and\nIOCfish5K-D, respectively.\n","authors":["Guolei Sun","Xiaogang Cheng","Zhaochong An","Xiaokang Wang","Yun Liu","Deng-Ping Fan","Ming-Ming Cheng","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2304.11677v2.pdf","comment":"Journal version. The resources are available at\n  https://github.com/GuoleiSun/Indiscernible-Object-Counting"},{"id":"http://arxiv.org/abs/2406.04158v3","updated":"2025-01-13T17:44:43Z","published":"2024-06-06T15:18:59Z","title":"CMAR-Net: Accurate Cross-Modal 3D SAR Reconstruction of Vehicle Targets\n  with Sparse Multi-Baseline Data","summary":"  Multi-baseline Synthetic Aperture Radar (SAR) three-dimensional (3D)\ntomography is a crucial remote sensing technique that provides 3D resolution\nunavailable in conventional SAR imaging. However, achieving high-quality\nimaging typically requires multi-angle or full-aperture data, resulting in\nsignificant imaging costs. Recent advancements in sparse 3D SAR, which rely on\ndata from limited apertures, have gained attention as a cost-effective\nalternative. Notably, deep learning techniques have markedly enhanced the\nimaging quality of sparse 3D SAR. Despite these advancements, existing methods\nprimarily depend on high-resolution radar images for supervising the training\nof deep neural networks (DNNs). This exclusive dependence on single-modal data\nprevents the introduction of complementary information from other data sources,\nlimiting further improvements in imaging performance. In this paper, we\nintroduce a Cross-Modal 3D-SAR Reconstruction Network (CMAR-Net) to enhance 3D\nSAR imaging by integrating heterogeneous information. Leveraging cross-modal\nsupervision from 2D optical images and error transfer guaranteed by\ndifferentiable rendering, CMAR-Net achieves efficient training and reconstructs\nhighly sparse multi-baseline SAR data into visually structured and accurate 3D\nimages, particularly for vehicle targets. Extensive experiments on simulated\nand real-world datasets demonstrate that CMAR-Net significantly outperforms\nSOTA sparse reconstruction algorithms based on compressed sensing (CS) and deep\nlearning (DL). Furthermore, our method eliminates the need for time-consuming\nfull-aperture data preprocessing and relies solely on computer-rendered optical\nimages, significantly reducing dataset construction costs. This work highlights\nthe potential of deep learning for multi-baseline SAR 3D imaging and introduces\na novel framework for radar imaging research through cross-modal learning.\n","authors":["Da Li","Guoqiang Zhao","Houjun Sun","Jiacheng Bao"],"pdf_url":"https://arxiv.org/pdf/2406.04158v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05379v2","updated":"2025-01-13T17:22:30Z","published":"2025-01-09T17:04:33Z","title":"Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID\n  Guidance","summary":"  Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in\nreconstructing detailed 3D scenes within multi-view setups and the emergence of\nlarge 2D human foundation models, we introduce Arc2Avatar, the first SDS-based\nmethod utilizing a human face foundation model as guidance with just a single\nimage as input. To achieve that, we extend such a model for diverse-view human\nhead generation by fine-tuning on synthetic data and modifying its\nconditioning. Our avatars maintain a dense correspondence with a human face\nmesh template, allowing blendshape-based expression generation. This is\nachieved through a modified 3DGS approach, connectivity regularizers, and a\nstrategic initialization tailored for our task. Additionally, we propose an\noptional efficient SDS-based correction step to refine the blendshape\nexpressions, enhancing realism and diversity. Experiments demonstrate that\nArc2Avatar achieves state-of-the-art realism and identity preservation,\neffectively addressing color issues by allowing the use of very low guidance,\nenabled by our strong identity prior and initialization strategy, without\ncompromising detail. Please visit https://arc2avatar.github.io for more\nresources.\n","authors":["Dimitrios Gerogiannis","Foivos Paraperas Papantoniou","Rolandos Alexandros Potamias","Alexandros Lattas","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2501.05379v2.pdf","comment":"Project Page https://arc2avatar.github.io"},{"id":"http://arxiv.org/abs/2501.07499v1","updated":"2025-01-13T17:17:17Z","published":"2025-01-13T17:17:17Z","title":"Three-view Focal Length Recovery From Homographies","summary":"  In this paper, we propose a novel approach for recovering focal lengths from\nthree-view homographies. By examining the consistency of normal vectors between\ntwo homographies, we derive new explicit constraints between the focal lengths\nand homographies using an elimination technique. We demonstrate that three-view\nhomographies provide two additional constraints, enabling the recovery of one\nor two focal lengths. We discuss four possible cases, including three cameras\nhaving an unknown equal focal length, three cameras having two different\nunknown focal lengths, three cameras where one focal length is known, and the\nother two cameras have equal or different unknown focal lengths. All the\nproblems can be converted into solving polynomials in one or two unknowns,\nwhich can be efficiently solved using Sturm sequence or hidden variable\ntechnique. Evaluation using both synthetic and real data shows that the\nproposed solvers are both faster and more accurate than methods relying on\nexisting two-view solvers. The code and data are available on\nhttps://github.com/kocurvik/hf\n","authors":["Yaqing Ding","Viktor Kocur","Zuzana Berger Haladová","Qianliang Wu","Shen Cai","Jian Yang","Zuzana Kukelova"],"pdf_url":"https://arxiv.org/pdf/2501.07499v1.pdf","comment":"Code available at https://github.com/kocurvik/hf Dataset available\n  at: https://doi.org/10.5281/zenodo.14638904"},{"id":"http://arxiv.org/abs/2401.10815v2","updated":"2025-01-13T17:14:52Z","published":"2024-01-19T17:02:17Z","title":"RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text\n  Supervision","summary":"  Language-supervised pre-training has proven to be a valuable method for\nextracting semantically meaningful features from images, serving as a\nfoundational element in multimodal systems within the computer vision and\nmedical imaging domains. However, the computed features are limited by the\ninformation contained in the text, which is particularly problematic in medical\nimaging, where the findings described by radiologists focus on specific\nobservations. This challenge is compounded by the scarcity of paired\nimaging-text data due to concerns over leakage of personal health information.\nIn this work, we fundamentally challenge the prevailing reliance on language\nsupervision for learning general-purpose biomedical imaging encoders. We\nintroduce RAD-DINO, a biomedical image encoder pre-trained solely on unimodal\nbiomedical imaging data that obtains similar or greater performance than\nstate-of-the-art biomedical language-supervised models on a diverse range of\nbenchmarks. Specifically, the quality of learned representations is evaluated\non standard imaging tasks (classification and semantic segmentation), and a\nvision-language alignment task (text report generation from images). To further\ndemonstrate the drawback of language supervision, we show that features from\nRAD-DINO correlate with other medical records (e.g., sex or age) better than\nlanguage-supervised models, which are generally not mentioned in radiology\nreports. Finally, we conduct a series of ablations determining the factors in\nRAD-DINO's performance; notably, we observe that RAD-DINO's downstream\nperformance scales well with the quantity and diversity of training data,\ndemonstrating that image-only supervision is a scalable approach for training a\nfoundational biomedical image encoder. Model weights of RAD-DINO trained on\npublicly available datasets are available at\nhttps://huggingface.co/microsoft/rad-dino.\n","authors":["Fernando Pérez-García","Harshita Sharma","Sam Bond-Taylor","Kenza Bouzid","Valentina Salvatelli","Maximilian Ilse","Shruthi Bannur","Daniel C. Castro","Anton Schwaighofer","Matthew P. Lungren","Maria Wetscherek","Noel Codella","Stephanie L. Hyland","Javier Alvarez-Valle","Ozan Oktay"],"pdf_url":"https://arxiv.org/pdf/2401.10815v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07496v1","updated":"2025-01-13T17:14:25Z","published":"2025-01-13T17:14:25Z","title":"Aligning First, Then Fusing: A Novel Weakly Supervised Multimodal\n  Violence Detection Method","summary":"  Weakly supervised violence detection refers to the technique of training\nmodels to identify violent segments in videos using only video-level labels.\nAmong these approaches, multimodal violence detection, which integrates\nmodalities such as audio and optical flow, holds great potential. Existing\nmethods in this domain primarily focus on designing multimodal fusion models to\naddress modality discrepancies. In contrast, we take a different approach;\nleveraging the inherent discrepancies across modalities in violence event\nrepresentation to propose a novel multimodal semantic feature alignment method.\nThis method sparsely maps the semantic features of local, transient, and less\ninformative modalities ( such as audio and optical flow ) into the more\ninformative RGB semantic feature space. Through an iterative process, the\nmethod identifies the suitable no-zero feature matching subspace and aligns the\nmodality-specific event representations based on this subspace, enabling the\nfull exploitation of information from all modalities during the subsequent\nmodality fusion stage. Building on this, we design a new weakly supervised\nviolence detection framework that consists of unimodal multiple-instance\nlearning for extracting unimodal semantic features, multimodal alignment,\nmultimodal fusion, and final detection. Experimental results on benchmark\ndatasets demonstrate the effectiveness of our method, achieving an average\nprecision (AP) of 86.07% on the XD-Violence dataset. Our code is available at\nhttps://github.com/xjpp2016/MAVD.\n","authors":["Wenping Jin","Li Zhu","Jing Sun"],"pdf_url":"https://arxiv.org/pdf/2501.07496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01541v2","updated":"2025-01-13T16:55:29Z","published":"2024-09-03T02:18:45Z","title":"Agentic Copyright Watermarking against Adversarial Evidence Forgery with\n  Purification-Agnostic Curriculum Proxy Learning","summary":"  With the proliferation of AI agents in various domains, protecting the\nownership of AI models has become crucial due to the significant investment in\ntheir development. Unauthorized use and illegal distribution of these models\npose serious threats to intellectual property, necessitating effective\ncopyright protection measures. Model watermarking has emerged as a key\ntechnique to address this issue, embedding ownership information within models\nto assert rightful ownership during copyright disputes. This paper presents\nseveral contributions to model watermarking: a self-authenticating black-box\nwatermarking protocol using hash techniques, a study on evidence forgery\nattacks using adversarial perturbations, a proposed defense involving a\npurification step to counter adversarial attacks, and a purification-agnostic\ncurriculum proxy learning method to enhance watermark robustness and model\nperformance. Experimental results demonstrate the effectiveness of these\napproaches in improving the security, reliability, and performance of\nwatermarked models.\n","authors":["Erjin Bao","Ching-Chun Chang","Hanrui Wang","Isao Echizen"],"pdf_url":"https://arxiv.org/pdf/2409.01541v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07478v1","updated":"2025-01-13T16:52:28Z","published":"2025-01-13T16:52:28Z","title":"3DGS-to-PC: Convert a 3D Gaussian Splatting Scene into a Dense Point\n  Cloud or Mesh","summary":"  3D Gaussian Splatting (3DGS) excels at producing highly detailed 3D\nreconstructions, but these scenes often require specialised renderers for\neffective visualisation. In contrast, point clouds are a widely used 3D\nrepresentation and are compatible with most popular 3D processing software, yet\nconverting 3DGS scenes into point clouds is a complex challenge. In this work\nwe introduce 3DGS-to-PC, a flexible and highly customisable framework that is\ncapable of transforming 3DGS scenes into dense, high-accuracy point clouds. We\nsample points probabilistically from each Gaussian as a 3D density function. We\nadditionally threshold new points using the Mahalanobis distance to the\nGaussian centre, preventing extreme outliers. The result is a point cloud that\nclosely represents the shape encoded into the 3D Gaussian scene. Individual\nGaussians use spherical harmonics to adapt colours depending on view, and each\npoint may contribute only subtle colour hints to the resulting rendered scene.\nTo avoid spurious or incorrect colours that do not fit with the final point\ncloud, we recalculate Gaussian colours via a customised image rendering\napproach, assigning each Gaussian the colour of the pixel to which it\ncontributes most across all views. 3DGS-to-PC also supports mesh generation\nthrough Poisson Surface Reconstruction, applied to points sampled from\npredicted surface Gaussians. This allows coloured meshes to be generated from\n3DGS scenes without the need for re-training. This package is highly\ncustomisable and capability of simple integration into existing 3DGS pipelines.\n3DGS-to-PC provides a powerful tool for converting 3DGS data into point cloud\nand surface-based formats.\n","authors":["Lewis A G Stuart","Michael P Pound"],"pdf_url":"https://arxiv.org/pdf/2501.07478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03033v2","updated":"2025-01-13T16:42:03Z","published":"2024-11-05T12:10:02Z","title":"Rethinking Decoders for Transformer-based Semantic Segmentation: A\n  Compression Perspective","summary":"  State-of-the-art methods for Transformer-based semantic segmentation\ntypically adopt Transformer decoders that are used to extract additional\nembeddings from image embeddings via cross-attention, refine either or both\ntypes of embeddings via self-attention, and project image embeddings onto the\nadditional embeddings via dot-product. Despite their remarkable success, these\nempirical designs still lack theoretical justifications or interpretations,\nthus hindering potentially principled improvements. In this paper, we argue\nthat there are fundamental connections between semantic segmentation and\ncompression, especially between the Transformer decoders and Principal\nComponent Analysis (PCA). From such a perspective, we derive a white-box, fully\nattentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the\ninterpretations as follows: 1) the self-attention operator refines image\nembeddings to construct an ideal principal subspace that aligns with the\nsupervision and retains most information; 2) the cross-attention operator seeks\nto find a low-rank approximation of the refined image embeddings, which is\nexpected to be a set of orthonormal bases of the principal subspace and\ncorresponds to the predefined classes; 3) the dot-product operation yields\ncompact representation for image embeddings as segmentation masks. Experiments\nconducted on dataset ADE20K find that DEPICT consistently outperforms its\nblack-box counterpart, Segmenter, and it is light weight and more robust.\n","authors":["Qishuai Wen","Chun-Guang Li"],"pdf_url":"https://arxiv.org/pdf/2411.03033v2.pdf","comment":"NeurIPS2024. Code:https://github.com/QishuaiWen/DEPICT/"},{"id":"http://arxiv.org/abs/2410.00982v2","updated":"2025-01-13T16:27:06Z","published":"2024-10-01T18:10:23Z","title":"ScVLM: Enhancing Vision-Language Model for Safety-Critical Event\n  Understanding","summary":"  Accurately identifying, understanding and describing traffic safety-critical\nevents (SCEs), including crashes, tire strikes, and near-crashes, is crucial\nfor advanced driver assistance systems, automated driving systems, and traffic\nsafety. As SCEs are rare events, most general vision-language models (VLMs)\nhave not been trained sufficiently to link SCE videos and narratives, which\ncould lead to hallucinations and missing key safety characteristics. Here, we\nintroduce ScVLM, a novel hybrid methodology that integrates supervised and\ncontrastive learning techniques to classify the severity and types of SCEs, as\nwell as to generate narrative descriptions of SCEs. This approach utilizes\nclassification to enhance VLMs' comprehension of driving videos and improve the\nrationality of event descriptions. The proposed approach is trained on and\nevaluated by more than 8,600 SCEs from the Second Strategic Highway Research\nProgram Naturalistic Driving Study dataset, the largest publicly accessible\ndriving dataset with videos and SCE annotations. The results demonstrate the\nsuperiority of the proposed approach in generating contextually accurate event\ndescriptions and mitigating VLM hallucinations. The code will be available at\nhttps://github.com/datadrivenwheels/ScVLM.\n","authors":["Liang Shi","Boyu Jiang","Tong Zeng","Feng Guo"],"pdf_url":"https://arxiv.org/pdf/2410.00982v2.pdf","comment":"To appear in Proceedings of the IEEE/CVF Winter Conference on\n  Applications of Computer Vision (WACV) 2025"},{"id":"http://arxiv.org/abs/2501.07451v1","updated":"2025-01-13T16:24:49Z","published":"2025-01-13T16:24:49Z","title":"A Survey on Dynamic Neural Networks: from Computer Vision to Multi-modal\n  Sensor Fusion","summary":"  Model compression is essential in the deployment of large Computer Vision\nmodels on embedded devices. However, static optimization techniques (e.g.\npruning, quantization, etc.) neglect the fact that different inputs have\ndifferent complexities, thus requiring different amount of computations.\nDynamic Neural Networks allow to condition the number of computations to the\nspecific input. The current literature on the topic is very extensive and\nfragmented. We present a comprehensive survey that synthesizes and unifies\nexisting Dynamic Neural Networks research in the context of Computer Vision.\nAdditionally, we provide a logical taxonomy based on which component of the\nnetwork is adaptive: the output, the computation graph or the input.\nFurthermore, we argue that Dynamic Neural Networks are particularly beneficial\nin the context of Sensor Fusion for better adaptivity, noise reduction and\ninformation prioritization. We present preliminary works in this direction.\n","authors":["Fabio Montello","Ronja Güldenring","Simone Scardapane","Lazaros Nalpantidis"],"pdf_url":"https://arxiv.org/pdf/2501.07451v1.pdf","comment":"Under review at International Journal of Computer Vision"},{"id":"http://arxiv.org/abs/2402.13699v5","updated":"2025-01-13T16:21:58Z","published":"2024-02-21T11:00:23Z","title":"Automation of Quantum Dot Measurement Analysis via Explainable Machine\n  Learning","summary":"  The rapid development of quantum dot (QD) devices for quantum computing has\nnecessitated more efficient and automated methods for device characterization\nand tuning. This work demonstrates the feasibility and advantages of applying\nexplainable machine learning techniques to the analysis of quantum dot\nmeasurements, paving the way for further advances in automated and transparent\nQD device tuning. Many of the measurements acquired during the tuning process\ncome in the form of images that need to be properly analyzed to guide the\nsubsequent tuning steps. By design, features present in such images capture\ncertain behaviors or states of the measured QD devices. When considered\ncarefully, such features can aid the control and calibration of QD devices. An\nimportant example of such images are so-called $\\textit{triangle plots}$, which\nvisually represent current flow and reveal characteristics important for QD\ndevice calibration. While image-based classification tools, such as\nconvolutional neural networks (CNNs), can be used to verify whether a given\nmeasurement is $\\textit{good}$ and thus warrants the initiation of the next\nphase of tuning, they do not provide any insights into how the device should be\nadjusted in the case of $\\textit{bad}$ images. This is because CNNs sacrifice\nprediction and model intelligibility for high accuracy. To ameliorate this\ntrade-off, a recent study introduced an image vectorization approach that\nrelies on the Gabor wavelet transform (Schug $\\textit{et al.}$ 2024\n$\\textit{Proc. XAI4Sci: Explainable Machine Learning for Sciences Workshop\n(AAAI 2024) (Vancouver, Canada)}$ pp 1-6). Here we propose an alternative\nvectorization method that involves mathematical modeling of synthetic triangles\nto mimic the experimental data. Using explainable boosting machines, we show\nthat this new method offers superior explainability of model prediction without\nsacrificing accuracy.\n","authors":["Daniel Schug","Tyler J. Kovach","M. A. Wolfe","Jared Benson","Sanghyeok Park","J. P. Dodson","J. Corrigan","M. A. Eriksson","Justyna P. Zwolak"],"pdf_url":"https://arxiv.org/pdf/2402.13699v5.pdf","comment":"20 pages, 5 figures, abbreviated version published in Proceedings of\n  the XAI4Sci: Explainable machine learning for sciences workshop at AAAI 2024,\n  (Vancouver, Canada)"},{"id":"http://arxiv.org/abs/2501.07447v1","updated":"2025-01-13T16:18:31Z","published":"2025-01-13T16:18:31Z","title":"PrecipDiff: Leveraging image diffusion models to enhance satellite-based\n  precipitation observations","summary":"  A recent report from the World Meteorological Organization (WMO) highlights\nthat water-related disasters have caused the highest human losses among natural\ndisasters over the past 50 years, with over 91\\% of deaths occurring in\nlow-income countries. This disparity is largely due to the lack of adequate\nground monitoring stations, such as weather surveillance radars (WSR), which\nare expensive to install. For example, while the US and Europe combined possess\nover 600 WSRs, Africa, despite having almost one and half times their landmass,\nhas fewer than 40. To address this issue, satellite-based observations offer a\nglobal, near-real-time monitoring solution. However, they face several\nchallenges like accuracy, bias, and low spatial resolution. This study\nleverages the power of diffusion models and residual learning to address these\nlimitations in a unified framework. We introduce the first diffusion model for\ncorrecting the inconsistency between different precipitation products. Our\nmethod demonstrates the effectiveness in downscaling satellite precipitation\nestimates from 10 km to 1 km resolution. Extensive experiments conducted in the\nSeattle region demonstrate significant improvements in accuracy, bias\nreduction, and spatial detail. Importantly, our approach achieves these results\nusing only precipitation data, showcasing the potential of a purely computer\nvision-based approach for enhancing satellite precipitation products and paving\nthe way for further advancements in this domain.\n","authors":["Ting-Yu Dai","Hayato Ushijima-Mwesigwa"],"pdf_url":"https://arxiv.org/pdf/2501.07447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01246v3","updated":"2025-01-13T16:07:46Z","published":"2024-12-02T08:06:14Z","title":"Class Distance Weighted Cross Entropy Loss for Classification of Disease\n  Severity","summary":"  Assessing disease severity with ordinal classes, where each class reflects\nincreasing severity levels, benefits from loss functions designed for this\nordinal structure. Traditional categorical loss functions, like Cross-Entropy\n(CE), often perform suboptimally in these scenarios. To address this, we\npropose a novel loss function, Class Distance Weighted Cross-Entropy (CDW-CE),\nwhich penalizes misclassifications more severely when the predicted and actual\nclasses are farther apart. We evaluated CDW-CE using various deep\narchitectures, comparing its performance against several categorical and\nordinal loss functions. To assess the quality of latent representations, we\nused t-distributed stochastic neighbor embedding (t-SNE) and uniform manifold\napproximation and projection (UMAP) visualizations, quantified the clustering\nquality using the Silhouette Score, and compared Class Activation Maps (CAM)\ngenerated by models trained with CDW-CE and CE loss. Feedback from domain\nexperts was incorporated to evaluate how well model attention aligns with\nexpert opinion. Our results show that CDW-CE consistently improves performance\nin ordinal image classification tasks. It achieves higher Silhouette Scores,\nindicating better class discrimination capability, and its CAM visualizations\nshow a stronger focus on clinically significant regions, as validated by domain\nexperts. Receiver operator characteristics (ROC) curves and the area under the\ncurve (AUC) scores highlight that CDW-CE outperforms other loss functions,\nincluding prominent ordinal loss functions from the literature.\n","authors":["Gorkem Polat","Ümit Mert Çağlar","Alptekin Temizel"],"pdf_url":"https://arxiv.org/pdf/2412.01246v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07434v1","updated":"2025-01-13T16:02:33Z","published":"2025-01-13T16:02:33Z","title":"Guided SAM: Label-Efficient Part Segmentation","summary":"  Localizing object parts precisely is essential for tasks such as object\nrecognition and robotic manipulation. Recent part segmentation methods require\nextensive training data and labor-intensive annotations. Segment-Anything Model\n(SAM) has demonstrated good performance on a wide range of segmentation\nproblems, but requires (manual) positional prompts to guide it where to\nsegment. Furthermore, since it has been trained on full objects instead of\nobject parts, it is prone to over-segmentation of parts. To address this, we\npropose a novel approach that guides SAM towards the relevant object parts. Our\nmethod learns positional prompts from coarse patch annotations that are easier\nand cheaper to acquire. We train classifiers on image patches to identify part\nclasses and aggregate patches into regions of interest (ROIs) with positional\nprompts. SAM is conditioned on these ROIs and prompts. This approach, termed\n`Guided SAM', enhances efficiency and reduces manual effort, allowing effective\npart segmentation with minimal labeled data. We demonstrate the efficacy of\nGuided SAM on a dataset of car parts, improving the average IoU on state of the\nart models from 0.37 to 0.49 with annotations that are on average five times\nmore efficient to acquire.\n","authors":["S. B. van Rooij","G. J. Burghouts"],"pdf_url":"https://arxiv.org/pdf/2501.07434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07430v1","updated":"2025-01-13T15:54:21Z","published":"2025-01-13T15:54:21Z","title":"Diff-Ensembler: Learning to Ensemble 2D Diffusion Models for\n  Volume-to-Volume Medical Image Translation","summary":"  Despite success in volume-to-volume translations in medical images, most\nexisting models struggle to effectively capture the inherent volumetric\ndistribution using 3D representations. The current state-of-the-art approach\ncombines multiple 2D-based networks through weighted averaging, thereby\nneglecting the 3D spatial structures. Directly training 3D models in medical\nimaging presents significant challenges due to high computational demands and\nthe need for large-scale datasets. To address these challenges, we introduce\nDiff-Ensembler, a novel hybrid 2D-3D model for efficient and effective\nvolumetric translations by ensembling perpendicularly trained 2D diffusion\nmodels with a 3D network in each diffusion step. Moreover, our model can\nnaturally be used to ensemble diffusion models conditioned on different\nmodalities, allowing flexible and accurate fusion of input conditions.\nExtensive experiments demonstrate that Diff-Ensembler attains superior accuracy\nand volumetric realism in 3D medical image super-resolution and modality\ntranslation. We further demonstrate the strength of our model's volumetric\nrealism using tumor segmentation as a downstream task.\n","authors":["Xiyue Zhu","Dou Hoon Kwark","Ruike Zhu","Kaiwen Hong","Yiqi Tao","Shirui Luo","Yudu Li","Zhi-Pei Liang","Volodymyr Kindratenko"],"pdf_url":"https://arxiv.org/pdf/2501.07430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00843v2","updated":"2025-01-13T15:48:06Z","published":"2025-01-01T13:51:03Z","title":"FusionSORT: Fusion Methods for Online Multi-object Visual Tracking","summary":"  In this work, we investigate four different fusion methods for associating\ndetections to tracklets in multi-object visual tracking. In addition to\nconsidering strong cues such as motion and appearance information, we also\nconsider weak cues such as height intersection-over-union (height-IoU) and\ntracklet confidence information in the data association using different fusion\nmethods. These fusion methods include minimum, weighted sum based on IoU,\nKalman filter (KF) gating, and hadamard product of costs due to the different\ncues. We conduct extensive evaluations on validation sets of MOT17, MOT20 and\nDanceTrack datasets, and find out that the choice of a fusion method is key for\ndata association in multi-object visual tracking. We hope that this\ninvestigative work helps the computer vision research community to use the\nright fusion method for data association in multi-object visual tracking.\n","authors":["Nathanael L. Baisa"],"pdf_url":"https://arxiv.org/pdf/2501.00843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05226v2","updated":"2025-01-13T15:30:39Z","published":"2025-01-09T13:29:54Z","title":"Light Transport-aware Diffusion Posterior Sampling for Single-View\n  Reconstruction of 3D Volumes","summary":"  We introduce a single-view reconstruction technique of volumetric fields in\nwhich multiple light scattering effects are omnipresent, such as in clouds. We\nmodel the unknown distribution of volumetric fields using an unconditional\ndiffusion model trained on a novel benchmark dataset comprising 1,000\nsynthetically simulated volumetric density fields. The neural diffusion model\nis trained on the latent codes of a novel, diffusion-friendly, monoplanar\nrepresentation. The generative model is used to incorporate a tailored\nparametric diffusion posterior sampling technique into different reconstruction\ntasks. A physically-based differentiable volume renderer is employed to provide\ngradients with respect to light transport in the latent space. This stands in\ncontrast to classic NeRF approaches and makes the reconstructions better\naligned with observed data. Through various experiments, we demonstrate\nsingle-view reconstruction of volumetric clouds at a previously unattainable\nquality.\n","authors":["Ludwic Leonard","Nils Thuerey","Ruediger Westermann"],"pdf_url":"https://arxiv.org/pdf/2501.05226v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08926v3","updated":"2025-01-13T15:19:14Z","published":"2024-10-11T15:50:53Z","title":"Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million\n  Images","summary":"  We explore the transformative potential of SAM 2, a vision foundation model,\nin advancing gaze estimation and eye tracking technologies. By significantly\nreducing annotation time, lowering technical barriers through its ease of\ndeployment, and enhancing segmentation accuracy, SAM 2 addresses critical\nchallenges faced by researchers and practitioners. Utilizing its zero-shot\nsegmentation capabilities with minimal user input-a single click per video-we\ntested SAM 2 on over 14 million eye images from diverse datasets, including\nvirtual reality setups and the world's largest unified dataset recorded using\nwearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matches\nthe performance of domain-specific models trained solely on eye images,\nachieving competitive mean Intersection over Union (mIoU) scores of up to 93%\nwithout fine-tuning. Additionally, we provide our code and segmentation masks\nfor these widely used datasets to promote further research.\n","authors":["Virmarie Maquiling","Sean Anthony Byrne","Diederick C. Niehorster","Marco Carminati","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2410.08926v3.pdf","comment":"Virmarie Maquiling and Sean Anthony Byrne contributed equally to this\n  paper, 8 pages, 3 figures, ETRA 2025, pre-print"},{"id":"http://arxiv.org/abs/2501.07397v1","updated":"2025-01-13T15:12:40Z","published":"2025-01-13T15:12:40Z","title":"OCORD: Open-Campus Object Removal Dataset","summary":"  The rapid advancements in generative models, particularly diffusion-based\ntechniques, have revolutionized image inpainting tasks by enabling the\ngeneration of high-fidelity and diverse content. However, object removal\nremains under-explored as a specific subset of inpainting, facing challenges\nsuch as inadequate semantic understanding and the unintended generation of\nartifacts. Existing datasets for object removal often rely on synthetic data,\nwhich fails to align with real-world scenarios, limiting model performance.\nAlthough some real-world datasets address these issues partially, they suffer\nfrom scalability, annotation inefficiencies, and limited realism in physical\nphenomena such as lighting and shadows. To address these limitations, this\npaper introduces a novel approach to object removal by constructing a\nhigh-resolution real-world dataset through long-duration video capture with\nfixed camera settings. Leveraging advanced tools such as Grounding-DINO,\nSegment-Anything-Model, and MASA for automated annotation, we provides image,\nbackground, and mask pairs while significantly reducing annotation time and\nlabor. With our efficient annotation pipeline, we release the first fully open,\nhigh-resolution real-world dataset for object removal, and improved performance\nin object removal tasks through fine-tuning of pre-trained diffusion models.\n","authors":["Shuo Zhang","Runpu Wei","Kongming Liang"],"pdf_url":"https://arxiv.org/pdf/2501.07397v1.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2501.07396v1","updated":"2025-01-13T15:11:27Z","published":"2025-01-13T15:11:27Z","title":"Zero-Shot Scene Understanding for Automatic Target Recognition Using\n  Large Vision-Language Models","summary":"  Automatic target recognition (ATR) plays a critical role in tasks such as\nnavigation and surveillance, where safety and accuracy are paramount. In\nextreme use cases, such as military applications, these factors are often\nchallenged due to the presence of unknown terrains, environmental conditions,\nand novel object categories. Current object detectors, including open-world\ndetectors, lack the ability to confidently recognize novel objects or operate\nin unknown environments, as they have not been exposed to these new conditions.\nHowever, Large Vision-Language Models (LVLMs) exhibit emergent properties that\nenable them to recognize objects in varying conditions in a zero-shot manner.\nDespite this, LVLMs struggle to localize objects effectively within a scene. To\naddress these limitations, we propose a novel pipeline that combines the\ndetection capabilities of open-world detectors with the recognition confidence\nof LVLMs, creating a robust system for zero-shot ATR of novel classes and\nunknown domains. In this study, we compare the performance of various LVLMs for\nrecognizing military vehicles, which are often underrepresented in training\ndatasets. Additionally, we examine the impact of factors such as distance\nrange, modality, and prompting methods on the recognition performance,\nproviding insights into the development of more reliable ATR systems for novel\nconditions and classes.\n","authors":["Yasiru Ranasinghe","Vibashan VS","James Uplinger","Celso De Melo","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2501.07396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07390v1","updated":"2025-01-13T15:06:51Z","published":"2025-01-13T15:06:51Z","title":"Kolmogorov-Arnold Network for Remote Sensing Image Semantic Segmentation","summary":"  Semantic segmentation plays a crucial role in remote sensing applications,\nwhere the accurate extraction and representation of features are essential for\nhigh-quality results. Despite the widespread use of encoder-decoder\narchitectures, existing methods often struggle with fully utilizing the\nhigh-dimensional features extracted by the encoder and efficiently recovering\ndetailed information during decoding. To address these problems, we propose a\nnovel semantic segmentation network, namely DeepKANSeg, including two key\ninnovations based on the emerging Kolmogorov Arnold Network (KAN). Notably, the\nadvantage of KAN lies in its ability to decompose high-dimensional complex\nfunctions into univariate transformations, enabling efficient and flexible\nrepresentation of intricate relationships in data. First, we introduce a\nKAN-based deep feature refinement module, namely DeepKAN to effectively capture\ncomplex spatial and rich semantic relationships from high-dimensional features.\nSecond, we replace the traditional multi-layer perceptron (MLP) layers in the\nglobal-local combined decoder with KAN-based linear layers, namely GLKAN. This\nmodule enhances the decoder's ability to capture fine-grained details during\ndecoding. To evaluate the effectiveness of the proposed method, experiments are\nconducted on two well-known fine-resolution remote sensing benchmark datasets,\nnamely ISPRS Vaihingen and ISPRS Potsdam. The results demonstrate that the\nKAN-enhanced segmentation model achieves superior performance in terms of\naccuracy compared to state-of-the-art methods. They highlight the potential of\nKANs as a powerful alternative to traditional architectures in semantic\nsegmentation tasks. Moreover, the explicit univariate decomposition provides\nimproved interpretability, which is particularly beneficial for applications\nrequiring explainable learning in remote sensing.\n","authors":["Xianping Ma","Ziyao Wang","Yin Hu","Xiaokang Zhang","Man-On Pun"],"pdf_url":"https://arxiv.org/pdf/2501.07390v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2501.07378v1","updated":"2025-01-13T14:54:49Z","published":"2025-01-13T14:54:49Z","title":"FedSemiDG: Domain Generalized Federated Semi-supervised Medical Image\n  Segmentation","summary":"  Medical image segmentation is challenging due to the diversity of medical\nimages and the lack of labeled data, which motivates recent developments in\nfederated semi-supervised learning (FSSL) to leverage a large amount of\nunlabeled data from multiple centers for model training without sharing raw\ndata. However, what remains under-explored in FSSL is the domain shift problem\nwhich may cause suboptimal model aggregation and low effectivity of the\nutilization of unlabeled data, eventually leading to unsatisfactory performance\nin unseen domains. In this paper, we explore this previously ignored scenario,\nnamely domain generalized federated semi-supervised learning (FedSemiDG), which\naims to learn a model in a distributed manner from multiple domains with\nlimited labeled data and abundant unlabeled data such that the model can\ngeneralize well to unseen domains. We present a novel framework, Federated\nGeneralization-Aware SemiSupervised Learning (FGASL), to address the challenges\nin FedSemiDG by effectively tackling critical issues at both global and local\nlevels. Globally, we introduce Generalization-Aware Aggregation (GAA),\nassigning adaptive weights to local models based on their generalization\nperformance. Locally, we use a Dual-Teacher Adaptive Pseudo Label Refinement\n(DR) strategy to combine global and domain-specific knowledge, generating more\nreliable pseudo labels. Additionally, Perturbation-Invariant Alignment (PIA)\nenforces feature consistency under perturbations, promoting domain-invariant\nlearning. Extensive experiments on three medical segmentation tasks (cardiac\nMRI, spine MRI and bladder cancer MRI) demonstrate that our method\nsignificantly outperforms state-of-the-art FSSL and domain generalization\napproaches, achieving robust generalization on unseen domains.\n","authors":["Zhipeng Deng","Zhe Xu","Tsuyoshi Isshiki","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2501.07378v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2412.05271v4","updated":"2025-01-13T14:42:20Z","published":"2024-12-06T18:57:08Z","title":"Expanding Performance Boundaries of Open-Source Multimodal Models with\n  Model, Data, and Test-Time Scaling","summary":"  We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)\nseries that builds upon InternVL 2.0, maintaining its core model architecture\nwhile introducing significant enhancements in training and testing strategies\nas well as data quality. In this work, we delve into the relationship between\nmodel scaling and performance, systematically exploring the performance trends\nin vision encoders, language models, dataset sizes, and test-time\nconfigurations. Through extensive evaluations on a wide range of benchmarks,\nincluding multi-discipline reasoning, document understanding, multi-image /\nvideo understanding, real-world comprehension, multimodal hallucination\ndetection, visual grounding, multilingual capabilities, and pure language\nprocessing, InternVL 2.5 exhibits competitive performance, rivaling leading\ncommercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is\nthe first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a\n3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing\nstrong potential for test-time scaling. We hope this model contributes to the\nopen-source community by setting new standards for developing and applying\nmultimodal AI systems. HuggingFace demo see\nhttps://huggingface.co/spaces/OpenGVLab/InternVL\n","authors":["Zhe Chen","Weiyun Wang","Yue Cao","Yangzhou Liu","Zhangwei Gao","Erfei Cui","Jinguo Zhu","Shenglong Ye","Hao Tian","Zhaoyang Liu","Lixin Gu","Xuehui Wang","Qingyun Li","Yimin Ren","Zixuan Chen","Jiapeng Luo","Jiahao Wang","Tan Jiang","Bo Wang","Conghui He","Botian Shi","Xingcheng Zhang","Han Lv","Yi Wang","Wenqi Shao","Pei Chu","Zhongying Tu","Tong He","Zhiyong Wu","Huipeng Deng","Jiaye Ge","Kai Chen","Kaipeng Zhang","Limin Wang","Min Dou","Lewei Lu","Xizhou Zhu","Tong Lu","Dahua Lin","Yu Qiao","Jifeng Dai","Wenhai Wang"],"pdf_url":"https://arxiv.org/pdf/2412.05271v4.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2412.09718v2","updated":"2025-01-13T14:37:52Z","published":"2024-12-12T20:48:06Z","title":"BayesAdapter: enhanced uncertainty estimation in CLIP few-shot\n  adaptation","summary":"  The emergence of large pre-trained vision-language models (VLMs) represents a\nparadigm shift in machine learning, with unprecedented results in a broad span\nof visual recognition tasks. CLIP, one of the most popular VLMs, has exhibited\nremarkable zero-shot and transfer learning capabilities in classification. To\ntransfer CLIP to downstream tasks, adapters constitute a parameter-efficient\napproach that avoids backpropagation through the large model (unlike related\nprompt learning methods). However, CLIP adapters have been developed to target\ndiscriminative performance, and the quality of their uncertainty estimates has\nbeen overlooked. In this work we show that the discriminative performance of\nstate-of-the-art CLIP adapters does not always correlate with their uncertainty\nestimation capabilities, which are essential for a safe deployment in\nreal-world scenarios. We also demonstrate that one of such adapters is obtained\nthrough MAP inference from a more general probabilistic framework. Based on\nthis observation we introduce BayesAdapter, which leverages Bayesian inference\nto estimate a full probability distribution instead of a single point, better\ncapturing the variability inherent in the parameter space. In a comprehensive\nempirical evaluation we show that our approach obtains high quality uncertainty\nestimates in the predictions, standing out in calibration and selective\nclassification. Our code will be publicly available upon acceptance of the\npaper.\n","authors":["Pablo Morales-Álvarez","Stergios Christodoulidis","Maria Vakalopoulou","Pablo Piantanida","Jose Dolz"],"pdf_url":"https://arxiv.org/pdf/2412.09718v2.pdf","comment":"30 pages, 5 figures, 23 tables"},{"id":"http://arxiv.org/abs/2406.16531v2","updated":"2025-01-13T14:34:40Z","published":"2024-06-24T11:10:41Z","title":"GIM: A Million-scale Benchmark for Generative Image Manipulation\n  Detection and Localization","summary":"  The extraordinary ability of generative models emerges as a new trend in\nimage editing and generating realistic images, posing a serious threat to the\ntrustworthiness of multimedia data and driving the research of image\nmanipulation detection and location (IMDL). However, the lack of a large-scale\ndata foundation makes the IMDL task unattainable. In this paper, we build a\nlocal manipulation data generation pipeline that integrates the powerful\ncapabilities of SAM, LLM, and generative models. Upon this basis, we propose\nthe GIM dataset, which has the following advantages: 1) Large scale, GIM\nincludes over one million pairs of AI-manipulated images and real images. 2)\nRich image content, GIM encompasses a broad range of image classes. 3) Diverse\ngenerative manipulation, the images are manipulated images with\nstate-of-the-art generators and various manipulation tasks. The aforementioned\nadvantages allow for a more comprehensive evaluation of IMDL methods, extending\ntheir applicability to diverse images. We introduce the GIM benchmark with two\nsettings to evaluate existing IMDL methods. In addition, we propose a novel\nIMDL framework, termed GIMFormer, which consists of a ShadowTracer,\nFrequency-Spatial block (FSB), and a Multi-Window Anomalous Modeling (MWAM)\nmodule. Extensive experiments on the GIM demonstrate that GIMFormer surpasses\nthe previous state-of-the-art approach on two different benchmarks.\n","authors":["Yirui Chen","Xudong Huang","Quan Zhang","Wei Li","Mingjian Zhu","Qiangyu Yan","Simiao Li","Hanting Chen","Hailin Hu","Jie Yang","Wei Liu","Jie Hu"],"pdf_url":"https://arxiv.org/pdf/2406.16531v2.pdf","comment":"Code page: https://github.com/chenyirui/GIM"},{"id":"http://arxiv.org/abs/2404.16432v5","updated":"2025-01-13T14:34:18Z","published":"2024-04-25T09:07:19Z","title":"Point-JEPA: A Joint Embedding Predictive Architecture for\n  Self-Supervised Learning on Point Cloud","summary":"  Recent advancements in self-supervised learning in the point cloud domain\nhave demonstrated significant potential. However, these methods often suffer\nfrom drawbacks, including lengthy pre-training time, the necessity of\nreconstruction in the input space, or the necessity of additional modalities.\nIn order to address these issues, we introduce Point-JEPA, a joint embedding\npredictive architecture designed specifically for point cloud data. To this\nend, we introduce a sequencer that orders point cloud patch embeddings to\nefficiently compute and utilize their proximity based on the indices during\ntarget and context selection. The sequencer also allows shared computations of\nthe patch embeddings' proximity between context and target selection, further\nimproving the efficiency. Experimentally, our method achieves competitive\nresults with state-of-the-art methods while avoiding the reconstruction in the\ninput space or additional modality.\n","authors":["Ayumu Saito","Prachi Kudeshia","Jiju Poovvancheri"],"pdf_url":"https://arxiv.org/pdf/2404.16432v5.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2501.07360v1","updated":"2025-01-13T14:30:01Z","published":"2025-01-13T14:30:01Z","title":"TimberVision: A Multi-Task Dataset and Framework for Log-Component\n  Segmentation and Tracking in Autonomous Forestry Operations","summary":"  Timber represents an increasingly valuable and versatile resource. However,\nforestry operations such as harvesting, handling and measuring logs still\nrequire substantial human labor in remote environments posing significant\nsafety risks. Progressively automating these tasks has the potential of\nincreasing their efficiency as well as safety, but requires an accurate\ndetection of individual logs as well as live trees and their context. Although\ninitial approaches have been proposed for this challenging application domain,\nspecialized data and algorithms are still too scarce to develop robust\nsolutions. To mitigate this gap, we introduce the TimberVision dataset,\nconsisting of more than 2k annotated RGB images containing a total of 51k trunk\ncomponents including cut and lateral surfaces, thereby surpassing any existing\ndataset in this domain in terms of both quantity and detail by a large margin.\nBased on this data, we conduct a series of ablation experiments for oriented\nobject detection and instance segmentation and evaluate the influence of\nmultiple scene parameters on model performance. We introduce a generic\nframework to fuse the components detected by our models for both tasks into\nunified trunk representations. Furthermore, we automatically derive geometric\nproperties and apply multi-object tracking to further enhance robustness. Our\ndetection and tracking approach provides highly descriptive and accurate trunk\nrepresentations solely from RGB image data, even under challenging\nenvironmental conditions. Our solution is suitable for a wide range of\napplication scenarios and can be readily combined with other sensor modalities.\n","authors":["Daniel Steininger","Julia Simon","Andreas Trondl","Markus Murschitz"],"pdf_url":"https://arxiv.org/pdf/2501.07360v1.pdf","comment":"Accepted at Winter Conference on Applications of Computer Vision\n  (WACV) 2025. Code and dataset available at\n  https://github.com/timbervision/timbervision"},{"id":"http://arxiv.org/abs/2501.03836v2","updated":"2025-01-13T14:10:16Z","published":"2025-01-07T14:45:39Z","title":"SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor\n  Diagnosis","summary":"  Brain tumors can result in neurological dysfunction, alterations in cognitive\nand psychological states, increased intracranial pressure, and the occurrence\nof seizures, thereby presenting a substantial risk to human life and health.\nThe You Only Look Once(YOLO) series models have demonstrated superior accuracy\nin object detection for medical imaging. In this paper, we develop a novel\nSCC-YOLO architecture by integrating the SCConv attention mechanism into\nYOLOv9. The SCConv module reconstructs an efficient convolutional module by\nreducing spatial and channel redundancy among features, thereby enhancing the\nlearning of image features. We investigate the impact of intergrating different\nattention mechanisms with the YOLOv9 model on brain tumor image detection using\nboth the Br35H dataset and our self-made dataset(Brain_Tumor_Dataset).\nExperimental results show that on the Br35H dataset, SCC-YOLO achieved a 0.3%\nimprovement in mAp50 compared to YOLOv9, while on our self-made dataset,\nSCC-YOLO exhibited a 0.5% improvement over YOLOv9. SCC-YOLO has reached\nstate-of-the-art performance in brain tumor detection. Source code is available\nat : https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master\n","authors":["Runci Bai"],"pdf_url":"https://arxiv.org/pdf/2501.03836v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15770v2","updated":"2025-01-13T14:00:18Z","published":"2024-11-24T09:48:03Z","title":"Text-Guided Coarse-to-Fine Fusion Network for Robust Remote Sensing\n  Visual Question Answering","summary":"  Remote Sensing Visual Question Answering (RSVQA) has gained significant\nresearch interest. However, current RSVQA methods are limited by the imaging\nmechanisms of optical sensors, particularly under challenging conditions such\nas cloud-covered and low-light scenarios. Given the all-time and all-weather\nimaging capabilities of Synthetic Aperture Radar (SAR), it is crucial to\ninvestigate the integration of optical-SAR images to improve RSVQA performance.\nIn this work, we propose a Text-guided Coarse-to-Fine Fusion Network (TGFNet),\nwhich leverages the semantic relationships between question text and\nmulti-source images to guide the network toward complementary fusion at the\nfeature level. Specifically, we develop a Text-guided Coarse-to-Fine Attention\nRefinement (CFAR) module to focus on key areas related to the question in\ncomplex remote sensing images. This module progressively directs attention from\nbroad areas to finer details through key region routing, enhancing the model's\nability to focus on relevant regions. Furthermore, we propose an Adaptive\nMulti-Expert Fusion (AMEF) module that dynamically integrates different\nexperts, enabling the adaptive fusion of optical and SAR features. In addition,\nwe create the first large-scale benchmark dataset for evaluating optical-SAR\nRSVQA methods, comprising 6,008 well-aligned optical-SAR image pairs and\n1,036,694 well-labeled question-answer pairs across 16 diverse question types,\nincluding complex relational reasoning questions. Extensive experiments on the\nproposed dataset demonstrate that our TGFNet effectively integrates\ncomplementary information between optical and SAR images, significantly\nimproving the model's performance in challenging scenarios. The dataset is\navailable at: https://github.com/mmic-lcl/.\n  Index Terms: Remote Sensing Visual Question Answering, Multi-source Data\nFusion, Multimodal, Remote Sensing, OPT-SAR.\n","authors":["Zhicheng Zhao","Changfu Zhou","Yu Zhang","Chenglong Li","Xiaoliang Ma","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2411.15770v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07342v1","updated":"2025-01-13T13:56:31Z","published":"2025-01-13T13:56:31Z","title":"A method for estimating roadway billboard salience","summary":"  Roadside billboards and other forms of outdoor advertising play a crucial\nrole in marketing initiatives; however, they can also distract drivers,\npotentially contributing to accidents. This study delves into the significance\nof roadside advertising in images captured from a driver's perspective.\nFirstly, it evaluates the effectiveness of neural networks in detecting\nadvertising along roads, focusing on the YOLOv5 and Faster R-CNN models.\nSecondly, the study addresses the determination of billboard significance using\nmethods for saliency extraction. The UniSal and SpectralResidual methods were\nemployed to create saliency maps for each image. The study establishes a\ndatabase of eye tracking sessions captured during city highway driving to\nassess the saliency models.\n","authors":["Zuzana Berger Haladova","Michal Zrubec","Zuzana Cernekova"],"pdf_url":"https://arxiv.org/pdf/2501.07342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05450v2","updated":"2025-01-13T13:54:31Z","published":"2024-10-07T19:34:25Z","title":"AI-Driven Early Mental Health Screening: Analyzing Selfies of Pregnant\n  Women","summary":"  Major Depressive Disorder and anxiety disorders affect millions globally,\ncontributing significantly to the burden of mental health issues. Early\nscreening is crucial for effective intervention, as timely identification of\nmental health issues can significantly improve treatment outcomes. Artificial\nintelligence (AI) can be valuable for improving the screening of mental\ndisorders, enabling early intervention and better treatment outcomes. AI-driven\nscreening can leverage the analysis of multiple data sources, including facial\nfeatures in digital images. However, existing methods often rely on controlled\nenvironments or specialized equipment, limiting their broad applicability. This\nstudy explores the potential of AI models for ubiquitous depression-anxiety\nscreening given face-centric selfies. The investigation focuses on high-risk\npregnant patients, a population that is particularly vulnerable to mental\nhealth issues. To cope with limited training data resulting from our clinical\nsetup, pre-trained models were utilized in two different approaches:\nfine-tuning convolutional neural networks (CNNs) originally designed for facial\nexpression recognition and employing vision-language models (VLMs) for\nzero-shot analysis of facial expressions. Experimental results indicate that\nthe proposed VLM-based method significantly outperforms CNNs, achieving an\naccuracy of 77.6%. Although there is significant room for improvement, the\nresults suggest that VLMs can be a promising approach for mental health\nscreening.\n","authors":["Gustavo A. Basílio","Thiago B. Pereira","Alessandro L. Koerich","Hermano Tavares","Ludmila Dias","Maria das Graças da S. Teixeira","Rafael T. Sousa","Wilian H. Hisatugu","Amanda S. Mota","Anilton S. Garcia","Marco Aurélio K. Galletta","Thiago M. Paixão"],"pdf_url":"https://arxiv.org/pdf/2410.05450v2.pdf","comment":"This article has been accepted for publication in HEALTHINF25 at the\n  18th International Joint Conference on Biomedical Engineering Systems and\n  Technologies (BIOSTEC 2025)"},{"id":"http://arxiv.org/abs/2501.07334v1","updated":"2025-01-13T13:47:00Z","published":"2025-01-13T13:47:00Z","title":"Anonymization of Documents for Law Enforcement with Machine Learning","summary":"  The steadily increasing utilization of data-driven methods and approaches in\nareas that handle sensitive personal information such as in law enforcement\nmandates an ever increasing effort in these institutions to comply with data\nprotection guidelines. In this work, we present a system for automatically\nanonymizing images of scanned documents, reducing manual effort while ensuring\ndata protection compliance. Our method considers the viability of further\nforensic processing after anonymization by minimizing automatically redacted\nareas by combining automatic detection of sensitive regions with knowledge from\na manually anonymized reference document. Using a self-supervised image model\nfor instance retrieval of the reference document, our approach requires only\none anonymized example to efficiently redact all documents of the same type,\nsignificantly reducing processing time. We show that our approach outperforms\nboth a purely automatic redaction system and also a naive copy-paste scheme of\nthe reference anonymization to other documents on a hand-crafted dataset of\nground truth redactions.\n","authors":["Manuel Eberhardinger","Patrick Takenaka","Daniel Grießhaber","Johannes Maucher"],"pdf_url":"https://arxiv.org/pdf/2501.07334v1.pdf","comment":"Accepted at IEEE Symposium on CI in Security, Defence and Biometrics\n  2025 (IEEE CISDB)"},{"id":"http://arxiv.org/abs/2403.15517v2","updated":"2025-01-13T13:32:48Z","published":"2024-03-22T11:14:30Z","title":"Improving Forward Compatibility in Class Incremental Learning by\n  Increasing Representation Rank and Feature Richness","summary":"  Class Incremental Learning (CIL) constitutes a pivotal subfield within\ncontinual learning, aimed at enabling models to progressively learn new\nclassification tasks while retaining knowledge obtained from prior tasks.\nAlthough previous studies have predominantly focused on backward compatible\napproaches to mitigate catastrophic forgetting, recent investigations have\nintroduced forward compatible methods to enhance performance on novel tasks and\ncomplement existing backward compatible methods. In this study, we introduce an\neffective-Rank based Feature Richness enhancement (RFR) method, designed for\nimproving forward compatibility. Specifically, this method increases the\neffective rank of representations during the base session, thereby facilitating\nthe incorporation of more informative features pertinent to unseen novel tasks.\nConsequently, RFR achieves dual objectives in backward and forward\ncompatibility: minimizing feature extractor modifications and enhancing novel\ntask performance, respectively. To validate the efficacy of our approach, we\nestablish a theoretical connection between effective rank and the Shannon\nentropy of representations. Subsequently, we conduct comprehensive experiments\nby integrating RFR into eleven well-known CIL methods. Our results demonstrate\nthe effectiveness of our approach in enhancing novel-task performance while\nmitigating catastrophic forgetting. Furthermore, our method notably improves\nthe average incremental accuracy across all eleven cases examined.\n","authors":["Jaeill Kim","Wonseok Lee","Moonjung Eo","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.15517v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07312v1","updated":"2025-01-13T13:24:41Z","published":"2025-01-13T13:24:41Z","title":"Localization-Aware Multi-Scale Representation Learning for Repetitive\n  Action Counting","summary":"  Repetitive action counting (RAC) aims to estimate the number of\nclass-agnostic action occurrences in a video without exemplars. Most current\nRAC methods rely on a raw frame-to-frame similarity representation for period\nprediction. However, this approach can be significantly disrupted by common\nnoise such as action interruptions and inconsistencies, leading to sub-optimal\ncounting performance in realistic scenarios. In this paper, we introduce a\nforeground localization optimization objective into similarity representation\nlearning to obtain more robust and efficient video features. We propose a\nLocalization-Aware Multi-Scale Representation Learning (LMRL) framework.\nSpecifically, we apply a Multi-Scale Period-Aware Representation (MPR) with a\nscale-specific design to accommodate various action frequencies and learn more\nflexible temporal correlations. Furthermore, we introduce the Repetition\nForeground Localization (RFL) method, which enhances the representation by\ncoarsely identifying periodic actions and incorporating global semantic\ninformation. These two modules can be jointly optimized, resulting in a more\ndiscerning periodic action representation. Our approach significantly reduces\nthe impact of noise, thereby improving counting accuracy. Additionally, the\nframework is designed to be scalable and adaptable to different types of video\ncontent. Experimental results on the RepCountA and UCFRep datasets demonstrate\nthat our proposed method effectively handles repetitive action counting.\n","authors":["Sujia Wang","Xiangwei Shen","Yansong Tang","Xin Dong","Wenjia Geng","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2501.07312v1.pdf","comment":"Accepted by IEEE VCIP2024"},{"id":"http://arxiv.org/abs/2501.07305v1","updated":"2025-01-13T13:13:06Z","published":"2025-01-13T13:13:06Z","title":"The Devil is in the Spurious Correlation: Boosting Moment Retrieval via\n  Temporal Dynamic Learning","summary":"  Given a textual query along with a corresponding video, the objective of\nmoment retrieval aims to localize the moments relevant to the query within the\nvideo. While commendable results have been demonstrated by existing\ntransformer-based approaches, predicting the accurate temporal span of the\ntarget moment is currently still a major challenge. In this paper, we reveal\nthat a crucial reason stems from the spurious correlation between the text\nqueries and the moment context. Namely, the model may associate the textual\nquery with the background frames rather than the target moment. To address this\nissue, we propose a temporal dynamic learning approach for moment retrieval,\nwhere two strategies are designed to mitigate the spurious correlation. First,\nwe introduce a novel video synthesis approach to construct a dynamic context\nfor the relevant moment. With separate yet similar videos mixed up, the\nsynthesis approach empowers our model to attend to the target moment of the\ncorresponding query under various dynamic contexts. Second, we enhance the\nrepresentation by learning temporal dynamics. Besides the visual\nrepresentation, text queries are aligned with temporal dynamic representations,\nwhich enables our model to establish a non-spurious correlation between the\nquery-related moment and context. With the aforementioned proposed method, the\nspurious correlation issue in moment retrieval can be largely alleviated. Our\nmethod establishes a new state-of-the-art performance on two popular benchmarks\nof moment retrieval, \\ie, QVHighlights and Charades-STA. In addition, the\ndetailed ablation analyses demonstrate the effectiveness of the proposed\nstrategies. Our code will be publicly available.\n","authors":["Xinyang Zhou","Fanyue Wei","Lixin Duan","Wen Li"],"pdf_url":"https://arxiv.org/pdf/2501.07305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07304v1","updated":"2025-01-13T13:12:18Z","published":"2025-01-13T13:12:18Z","title":"Code and Pixels: Multi-Modal Contrastive Pre-training for Enhanced\n  Tabular Data Analysis","summary":"  Learning from tabular data is of paramount importance, as it complements the\nconventional analysis of image and video data by providing a rich source of\nstructured information that is often critical for comprehensive understanding\nand decision-making processes. We present Multi-task Contrastive Masked Tabular\nModeling (MT-CMTM), a novel method aiming to enhance tabular models by\nleveraging the correlation between tabular data and corresponding images.\nMT-CMTM employs a dual strategy combining contrastive learning with masked\ntabular modeling, optimizing the synergy between these data modalities.\n  Central to our approach is a 1D Convolutional Neural Network with residual\nconnections and an attention mechanism (1D-ResNet-CBAM), designed to\nefficiently process tabular data without relying on images. This enables\nMT-CMTM to handle purely tabular data for downstream tasks, eliminating the\nneed for potentially costly image acquisition and processing.\n  We evaluated MT-CMTM on the DVM car dataset, which is uniquely suited for\nthis particular scenario, and the newly developed HIPMP dataset, which connects\nmembrane fabrication parameters with image data. Our MT-CMTM model outperforms\nthe proposed tabular 1D-ResNet-CBAM, which is trained from scratch, achieving a\nrelative 1.48% improvement in relative MSE on HIPMP and a 2.38% increase in\nabsolute accuracy on DVM. These results demonstrate MT-CMTM's robustness and\nits potential to advance the field of multi-modal learning.\n","authors":["Kankana Roy","Lars Krämer","Sebastian Domaschke","Malik Haris","Roland Aydin","Fabian Isensee","Martin Held"],"pdf_url":"https://arxiv.org/pdf/2501.07304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20287v5","updated":"2025-01-13T13:12:17Z","published":"2024-03-29T16:58:13Z","title":"Benchmarking Counterfactual Image Generation","summary":"  Generative AI has revolutionised visual content editing, empowering users to\neffortlessly modify images and videos. However, not all edits are equal. To\nperform realistic edits in domains such as natural image or medical imaging,\nmodifications must respect causal relationships inherent to the data generation\nprocess. Such image editing falls into the counterfactual image generation\nregime. Evaluating counterfactual image generation is substantially complex:\nnot only it lacks observable ground truths, but also requires adherence to\ncausal constraints. Although several counterfactual image generation methods\nand evaluation metrics exist, a comprehensive comparison within a unified\nsetting is lacking. We present a comparison framework to thoroughly benchmark\ncounterfactual image generation methods. We integrate all models that have been\nused for the task at hand and expand them to novel datasets and causal graphs,\ndemonstrating the superiority of Hierarchical VAEs across most datasets and\nmetrics. Our framework is implemented in a user-friendly Python package that\ncan be extended to incorporate additional SCMs, causal methods, generative\nmodels, and datasets for the community to build on. Code:\nhttps://github.com/gulnazaki/counterfactual-benchmark.\n","authors":["Thomas Melistas","Nikos Spyrou","Nefeli Gkouti","Pedro Sanchez","Athanasios Vlontzos","Yannis Panagakis","Giorgos Papanastasiou","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2403.20287v5.pdf","comment":"Published as a conference paper at NeurIPS 2024 Datasets and\n  Benchmarks Track https://openreview.net/forum?id=0T8xRFrScB Project page:\n  https://gulnazaki.github.io/counterfactual-benchmark"},{"id":"http://arxiv.org/abs/2501.07300v1","updated":"2025-01-13T13:07:51Z","published":"2025-01-13T13:07:51Z","title":"Comparative analysis of optical character recognition methods for Sámi\n  texts from the National Library of Norway","summary":"  Optical Character Recognition (OCR) is crucial to the National Library of\nNorway's (NLN) digitisation process as it converts scanned documents into\nmachine-readable text. However, for the S\\'ami documents in NLN's collection,\nthe OCR accuracy is insufficient. Given that OCR quality affects downstream\nprocesses, evaluating and improving OCR for text written in S\\'ami languages is\nnecessary to make these resources accessible. To address this need, this work\nfine-tunes and evaluates three established OCR approaches, Transkribus,\nTesseract and TrOCR, for transcribing S\\'ami texts from NLN's collection. Our\nresults show that Transkribus and TrOCR outperform Tesseract on this task,\nwhile Tesseract achieves superior performance on an out-of-domain dataset.\nFurthermore, we show that fine-tuning pre-trained models and supplementing\nmanual annotations with machine annotations and synthetic text images can yield\naccurate OCR for S\\'ami languages, even with a moderate amount of manually\nannotated data.\n","authors":["Tita Enstad","Trond Trosterud","Marie Iversdatter Røsok","Yngvil Beyer","Marie Roald"],"pdf_url":"https://arxiv.org/pdf/2501.07300v1.pdf","comment":"To be published in Proceedings of the 25th Nordic Conference on\n  Computational Linguistics (NoDaLiDa)"},{"id":"http://arxiv.org/abs/2410.22829v2","updated":"2025-01-13T13:07:25Z","published":"2024-10-30T09:11:25Z","title":"Situational Scene Graph for Structured Human-centric Situation\n  Understanding","summary":"  Graph based representation has been widely used in modelling spatio-temporal\nrelationships in video understanding. Although effective, existing graph-based\napproaches focus on capturing the human-object relationships while ignoring\nfine-grained semantic properties of the action components. These semantic\nproperties are crucial for understanding the current situation, such as where\ndoes the action takes place, what tools are used and functional properties of\nthe objects. In this work, we propose a graph-based representation called\nSituational Scene Graph (SSG) to encode both human-object relationships and the\ncorresponding semantic properties. The semantic details are represented as\npredefined roles and values inspired by situation frame, which is originally\ndesigned to represent a single action. Based on our proposed representation, we\nintroduce the task of situational scene graph generation and propose a\nmulti-stage pipeline Interactive and Complementary Network (InComNet) to\naddress the task. Given that the existing datasets are not applicable to the\ntask, we further introduce a SSG dataset whose annotations consist of semantic\nrole-value frames for human, objects and verb predicates of human-object\nrelations. Finally, we demonstrate the effectiveness of our proposed SSG\nrepresentation by testing on different downstream tasks. Experimental results\nshow that the unified representation can not only benefit predicate\nclassification and semantic role-value classification, but also benefit\nreasoning tasks on human-centric situation understanding. We will release the\ncode and the dataset soon.\n","authors":["Chinthani Sugandhika","Chen Li","Deepu Rajan","Basura Fernando"],"pdf_url":"https://arxiv.org/pdf/2410.22829v2.pdf","comment":"Accepted for WACV 2025"},{"id":"http://arxiv.org/abs/2501.07297v1","updated":"2025-01-13T13:04:00Z","published":"2025-01-13T13:04:00Z","title":"Toward Realistic Camouflaged Object Detection: Benchmarks and Method","summary":"  Camouflaged object detection (COD) primarily relies on semantic or instance\nsegmentation methods. While these methods have made significant advancements in\nidentifying the contours of camouflaged objects, they may be inefficient or\ncost-effective for tasks that only require the specific location of the object.\nObject detection algorithms offer an optimized solution for Realistic\nCamouflaged Object Detection (RCOD) in such cases. However, detecting\ncamouflaged objects remains a formidable challenge due to the high degree of\nsimilarity between the features of the objects and their backgrounds. Unlike\nsegmentation methods that perform pixel-wise comparisons to differentiate\nbetween foreground and background, object detectors omit this analysis, further\naggravating the challenge. To solve this problem, we propose a camouflage-aware\nfeature refinement (CAFR) strategy. Since camouflaged objects are not rare\ncategories, CAFR fully utilizes a clear perception of the current object within\nthe prior knowledge of large models to assist detectors in deeply understanding\nthe distinctions between background and foreground. Specifically, in CAFR, we\nintroduce the Adaptive Gradient Propagation (AGP) module that fine-tunes all\nfeature extractor layers in large detection models to fully refine\nclass-specific features from camouflaged contexts. We then design the Sparse\nFeature Refinement (SFR) module that optimizes the transformer-based feature\nextractor to focus primarily on capturing class-specific features in\ncamouflaged scenarios. To facilitate the assessment of RCOD tasks, we manually\nannotate the labels required for detection on three existing segmentation COD\ndatasets, creating a new benchmark for RCOD tasks. Code and datasets are\navailable at: https://github.com/zhimengXin/RCOD.\n","authors":["Zhimeng Xin","Tianxu Wu","Shiming Chen","Shuo Ye","Zijing Xie","Yixiong Zou","Xinge You","Yufei Guo"],"pdf_url":"https://arxiv.org/pdf/2501.07297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07296v1","updated":"2025-01-13T13:03:28Z","published":"2025-01-13T13:03:28Z","title":"Event-based Video Person Re-identification via Cross-Modality and\n  Temporal Collaboration","summary":"  Video-based person re-identification (ReID) has become increasingly important\ndue to its applications in video surveillance applications. By employing events\nin video-based person ReID, more motion information can be provided between\ncontinuous frames to improve recognition accuracy. Previous approaches have\nassisted by introducing event data into the video person ReID task, but they\nstill cannot avoid the privacy leakage problem caused by RGB images. In order\nto avoid privacy attacks and to take advantage of the benefits of event data,\nwe consider using only event data. To make full use of the information in the\nevent stream, we propose a Cross-Modality and Temporal Collaboration (CMTC)\nnetwork for event-based video person ReID. First, we design an event transform\nnetwork to obtain corresponding auxiliary information from the input of raw\nevents. Additionally, we propose a differential modality collaboration module\nto balance the roles of events and auxiliaries to achieve complementary\neffects. Furthermore, we introduce a temporal collaboration module to exploit\nmotion information and appearance cues. Experimental results demonstrate that\nour method outperforms others in the task of event-based video person ReID.\n","authors":["Renkai Li","Xin Yuan","Wei Liu","Xin Xu"],"pdf_url":"https://arxiv.org/pdf/2501.07296v1.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.01311v2","updated":"2025-01-13T12:42:14Z","published":"2025-01-02T15:47:56Z","title":"Multi-Head Explainer: A General Framework to Improve Explainability in\n  CNNs and Transformers","summary":"  In this study, we introduce the Multi-Head Explainer (MHEX), a versatile and\nmodular framework that enhances both the explainability and accuracy of\nConvolutional Neural Networks (CNNs) and Transformer-based models. MHEX\nconsists of three core components: an Attention Gate that dynamically\nhighlights task-relevant features, Deep Supervision that guides early layers to\ncapture fine-grained details pertinent to the target class, and an Equivalent\nMatrix that unifies refined local and global representations to generate\ncomprehensive saliency maps. Our approach demonstrates superior compatibility,\nenabling effortless integration into existing residual networks like ResNet and\nTransformer architectures such as BERT with minimal modifications. Extensive\nexperiments on benchmark datasets in medical imaging and text classification\nshow that MHEX not only improves classification accuracy but also produces\nhighly interpretable and detailed saliency scores.\n","authors":["Bohang Sun","Pietro Liò"],"pdf_url":"https://arxiv.org/pdf/2501.01311v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14128v2","updated":"2025-01-13T12:23:55Z","published":"2024-07-19T08:56:12Z","title":"OCTolyzer: Fully automatic toolkit for segmentation and feature\n  extracting in optical coherence tomography and scanning laser ophthalmoscopy\n  data","summary":"  Optical coherence tomography (OCT) and scanning laser ophthalmoscopy (SLO) of\nthe eye has become essential to ophthalmology and the emerging field of\noculomics, thus requiring a need for transparent, reproducible, and rapid\nanalysis of this data for clinical research and the wider research community.\nHere, we introduce OCTolyzer, the first open-source toolkit for retinochoroidal\nanalysis in OCT/SLO data. It features two analysis suites for OCT and SLO data,\nfacilitating deep learning-based anatomical segmentation and feature extraction\nof the cross-sectional retinal and choroidal layers and en face retinal\nvessels. We describe OCTolyzer and evaluate the reproducibility of its OCT\nchoroid analysis. At the population level, metrics for choroid region thickness\nwere highly reproducible, with a mean absolute error (MAE)/Pearson correlation\nfor macular volume choroid thickness (CT) of 6.7$\\mu$m/0.99, macular B-scan CT\nof 11.6$\\mu$m/0.99, and peripapillary CT of 5.0$\\mu$m/0.99. Macular choroid\nvascular index (CVI) also showed strong reproducibility, with MAE/Pearson for\nvolume CVI yielding 0.0271/0.97 and B-scan CVI 0.0130/0.91. At the eye level,\nmeasurement noise for regional and vessel metrics was below 5% and 20% of the\npopulation's variability, respectively. Outliers were caused by poor-quality\nB-scans with thick choroids and invisible choroid-sclera boundary. Processing\ntimes on a laptop CPU were under three seconds for macular/peripapillary\nB-scans and 85 seconds for volume scans. OCTolyzer can convert OCT/SLO data\ninto reproducible and clinically meaningful retinochoroidal features and will\nimprove the standardisation of ocular measurements in OCT/SLO image analysis,\nrequiring no specialised training or proprietary software to be used. OCTolyzer\nis freely available here: https://github.com/jaburke166/OCTolyzer.\n","authors":["Jamie Burke","Justin Engelmann","Samuel Gibbon","Charlene Hamid","Diana Moukaddem","Dan Pugh","Tariq Farrah","Niall Strang","Neeraj Dhaun","Tom MacGillivray","Stuart King","Ian J. C. MacCormick"],"pdf_url":"https://arxiv.org/pdf/2407.14128v2.pdf","comment":"Main paper: 15 pages, 9 figures, 3 tables. Supplementary material: 9\n  pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2412.10351v2","updated":"2025-01-13T12:22:52Z","published":"2024-12-13T18:47:11Z","title":"VibrantVS: A high-resolution multi-task transformer for forest canopy\n  height estimation","summary":"  This paper explores the application of a novel multi-task vision transformer\n(ViT) model for the estimation of canopy height models (CHMs) using 4-band\nNational Agriculture Imagery Program (NAIP) imagery across the western United\nStates. We compare the effectiveness of this model in terms of accuracy and\nprecision aggregated across ecoregions and class heights versus three other\nbenchmark peer-reviewed models. Key findings suggest that, while other\nbenchmark models can provide high precision in localized areas, the VibrantVS\nmodel has substantial advantages across a broad reach of ecoregions in the\nwestern United States with higher accuracy, higher precision, the ability to\ngenerate updated inference at a cadence of three years or less, and high\nspatial resolution. The VibrantVS model provides significant value for\necological monitoring and land management decisions for wildfire mitigation.\n","authors":["Tony Chang","Kiarie Ndegwa","Andreas Gros","Vincent A. Landau","Luke J. Zachmann","Bogdan State","Mitchell A. Gritts","Colton W. Miller","Nathan E. Rutenbeck","Scott Conway","Guy Bayes"],"pdf_url":"https://arxiv.org/pdf/2412.10351v2.pdf","comment":"15 pages, 12 figures"},{"id":"http://arxiv.org/abs/2501.07260v1","updated":"2025-01-13T12:18:58Z","published":"2025-01-13T12:18:58Z","title":"Skip Mamba Diffusion for Monocular 3D Semantic Scene Completion","summary":"  3D semantic scene completion is critical for multiple downstream tasks in\nautonomous systems. It estimates missing geometric and semantic information in\nthe acquired scene data. Due to the challenging real-world conditions, this\ntask usually demands complex models that process multi-modal data to achieve\nacceptable performance. We propose a unique neural model, leveraging advances\nfrom the state space and diffusion generative modeling to achieve remarkable 3D\nsemantic scene completion performance with monocular image input. Our technique\nprocesses the data in the conditioned latent space of a variational autoencoder\nwhere diffusion modeling is carried out with an innovative state space\ntechnique. A key component of our neural network is the proposed Skimba (Skip\nMamba) denoiser, which is adept at efficiently processing long-sequence data.\nThe Skimba diffusion model is integral to our 3D scene completion network,\nincorporating a triple Mamba structure, dimensional decomposition residuals and\nvarying dilations along three directions. We also adopt a variant of this\nnetwork for the subsequent semantic segmentation stage of our method. Extensive\nevaluation on the standard SemanticKITTI and SSCBench-KITTI360 datasets show\nthat our approach not only outperforms other monocular techniques by a large\nmargin, it also achieves competitive performance against stereo methods. The\ncode is available at https://github.com/xrkong/skimba\n","authors":["Li Liang","Naveed Akhtar","Jordan Vice","Xiangrui Kong","Ajmal Saeed Mian"],"pdf_url":"https://arxiv.org/pdf/2501.07260v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2501.07256v1","updated":"2025-01-13T12:11:07Z","published":"2025-01-13T12:11:07Z","title":"EdgeTAM: On-Device Track Anything Model","summary":"  On top of Segment Anything Model (SAM), SAM 2 further extends its capability\nfrom image to video inputs through a memory bank mechanism and obtains a\nremarkable performance compared with previous methods, making it a foundation\nmodel for video segmentation task. In this paper, we aim at making SAM 2 much\nmore efficient so that it even runs on mobile devices while maintaining a\ncomparable performance. Despite several works optimizing SAM for better\nefficiency, we find they are not sufficient for SAM 2 because they all focus on\ncompressing the image encoder, while our benchmark shows that the newly\nintroduced memory attention blocks are also the latency bottleneck. Given this\nobservation, we propose EdgeTAM, which leverages a novel 2D Spatial Perceiver\nto reduce the computational cost. In particular, the proposed 2D Spatial\nPerceiver encodes the densely stored frame-level memories with a lightweight\nTransformer that contains a fixed set of learnable queries. Given that video\nsegmentation is a dense prediction task, we find preserving the spatial\nstructure of the memories is essential so that the queries are split into\nglobal-level and patch-level groups. We also propose a distillation pipeline\nthat further improves the performance without inference overhead. As a result,\nEdgeTAM achieves 87.7, 70.0, 72.3, and 71.7 J&F on DAVIS 2017, MOSE, SA-V val,\nand SA-V test, while running at 16 FPS on iPhone 15 Pro Max.\n","authors":["Chong Zhou","Chenchen Zhu","Yunyang Xiong","Saksham Suri","Fanyi Xiao","Lemeng Wu","Raghuraman Krishnamoorthi","Bo Dai","Chen Change Loy","Vikas Chandra","Bilge Soran"],"pdf_url":"https://arxiv.org/pdf/2501.07256v1.pdf","comment":"Code will be released at https://github.com/facebookresearch/EdgeTAM"},{"id":"http://arxiv.org/abs/2501.07251v1","updated":"2025-01-13T12:00:34Z","published":"2025-01-13T12:00:34Z","title":"MOS-Attack: A Scalable Multi-objective Adversarial Attack Framework","summary":"  Crafting adversarial examples is crucial for evaluating and enhancing the\nrobustness of Deep Neural Networks (DNNs), presenting a challenge equivalent to\nmaximizing a non-differentiable 0-1 loss function.\n  However, existing single objective methods, namely adversarial attacks focus\non a surrogate loss function, do not fully harness the benefits of engaging\nmultiple loss functions, as a result of insufficient understanding of their\nsynergistic and conflicting nature.\n  To overcome these limitations, we propose the Multi-Objective Set-based\nAttack (MOS Attack), a novel adversarial attack framework leveraging multiple\nloss functions and automatically uncovering their interrelations.\n  The MOS Attack adopts a set-based multi-objective optimization strategy,\nenabling the incorporation of numerous loss functions without additional\nparameters.\n  It also automatically mines synergistic patterns among various losses,\nfacilitating the generation of potent adversarial attacks with fewer\nobjectives.\n  Extensive experiments have shown that our MOS Attack outperforms\nsingle-objective attacks. Furthermore, by harnessing the identified synergistic\npatterns, MOS Attack continues to show superior results with a reduced number\nof loss functions.\n","authors":["Ping Guo","Cheng Gong","Xi Lin","Fei Liu","Zhichao Lu","Qingfu Zhang","Zhenkun Wang"],"pdf_url":"https://arxiv.org/pdf/2501.07251v1.pdf","comment":"Under Review of CVPR 2025"},{"id":"http://arxiv.org/abs/2501.07248v1","updated":"2025-01-13T11:58:02Z","published":"2025-01-13T11:58:02Z","title":"Implicit Neural Representations for Registration of Left Ventricle\n  Myocardium During a Cardiac Cycle","summary":"  Understanding the movement of the left ventricle myocardium (LVmyo) during\nthe cardiac cycle is essential for assessing cardiac function. One way to model\nthis movement is through a series of deformable image registrations (DIRs) of\nthe LVmyo. Traditional deep learning methods for DIRs, such as those based on\nconvolutional neural networks, often require substantial memory and\ncomputational resources. In contrast, implicit neural representations (INRs)\noffer an efficient approach by operating on any number of continuous points.\nThis study extends the use of INRs for DIR to cardiac computed tomography (CT),\nfocusing on LVmyo registration. To enhance the precision of the registration\naround the LVmyo, we incorporate the signed distance field of the LVmyo with\nthe Hounsfield Unit values from the CT frames. This guides the registration of\nthe LVmyo, while keeping the tissue information from the CT frames. Our\nframework demonstrates high registration accuracy and provides a robust method\nfor temporal registration that facilitates further analysis of LVmyo motion.\n","authors":["Mathias Micheelsen Lowes","Jonas Jalili Pedersen","Bjørn S. Hansen","Klaus Fuglsang Kofoed","Maxime Sermesant","Rasmus R. Paulsen"],"pdf_url":"https://arxiv.org/pdf/2501.07248v1.pdf","comment":"9 pages, 5 figures, STACOM 2024"},{"id":"http://arxiv.org/abs/2501.07245v1","updated":"2025-01-13T11:54:26Z","published":"2025-01-13T11:54:26Z","title":"Depth and Image Fusion for Road Obstacle Detection Using Stereo Camera","summary":"  This paper is devoted to the detection of objects on a road, performed with a\ncombination of two methods based on both the use of depth information and video\nanalysis of data from a stereo camera. Since neither the time of the appearance\nof an object on the road, nor its size and shape is known in advance,\nML/DL-based approaches are not applicable. The task becomes more complicated\ndue to variations in artificial illumination, inhomogeneous road surface\ntexture, and unknown character and features of the object. To solve this\nproblem we developed the depth and image fusion method that complements a\nsearch of small contrast objects by RGB-based method, and obstacle detection by\nstereo image-based approach with SLIC superpixel segmentation. We conducted\nexperiments with static and low speed obstacles in an underground parking lot\nand demonstrated the successful work of the developed technique for detecting\nand even tracking small objects, which can be parking infrastructure objects,\nthings left on the road, wheels, dropped boxes, etc.\n","authors":["Oleg Perezyabov","Mikhail Gavrilenkov","Ilya Afanasyev"],"pdf_url":"https://arxiv.org/pdf/2501.07245v1.pdf","comment":"8 pages, 15 figures"},{"id":"http://arxiv.org/abs/2501.07244v1","updated":"2025-01-13T11:52:55Z","published":"2025-01-13T11:52:55Z","title":"Can Vision-Language Models Evaluate Handwritten Math?","summary":"  Recent advancements in Vision-Language Models (VLMs) have opened new\npossibilities in automatic grading of handwritten student responses,\nparticularly in mathematics. However, a comprehensive study to test the ability\nof VLMs to evaluate and reason over handwritten content remains absent. To\naddress this gap, we introduce FERMAT, a benchmark designed to assess the\nability of VLMs to detect, localize and correct errors in handwritten\nmathematical content. FERMAT spans four key error dimensions - computational,\nconceptual, notational, and presentation - and comprises over 2,200 handwritten\nmath solutions derived from 609 manually curated problems from grades 7-12 with\nintentionally introduced perturbations. Using FERMAT we benchmark nine VLMs\nacross three tasks: error detection, localization, and correction. Our results\nreveal significant shortcomings in current VLMs in reasoning over handwritten\ntext, with Gemini-1.5-Pro achieving the highest error correction rate (77%). We\nalso observed that some models struggle with processing handwritten content, as\ntheir accuracy improves when handwritten inputs are replaced with printed text\nor images. These findings highlight the limitations of current VLMs and reveal\nnew avenues for improvement. We release FERMAT and all the associated resources\nin the open-source to drive further research.\n","authors":["Oikantik Nath","Hanani Bathina","Mohammed Safi Ur Rahman Khan","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2501.07244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20104v2","updated":"2025-01-13T11:46:06Z","published":"2024-12-28T10:12:12Z","title":"SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object\n  Interaction Synthesis","summary":"  Synthesizing realistic human-object interaction motions is a critical problem\nin VR/AR and human animation. Unlike the commonly studied scenarios involving a\nsingle human or hand interacting with one object, we address a more generic\nmulti-body setting with arbitrary numbers of humans, hands, and objects. This\ncomplexity introduces significant challenges in synchronizing motions due to\nthe high correlations and mutual influences among bodies. To address these\nchallenges, we introduce SyncDiff, a novel method for multi-body interaction\nsynthesis using a synchronized motion diffusion strategy. SyncDiff employs a\nsingle diffusion model to capture the joint distribution of multi-body motions.\nTo enhance motion fidelity, we propose a frequency-domain motion decomposition\nscheme. Additionally, we introduce a new set of alignment scores to emphasize\nthe synchronization of different body motions. SyncDiff jointly optimizes both\ndata sample likelihood and alignment likelihood through an explicit\nsynchronization strategy. Extensive experiments across four datasets with\nvarious multi-body configurations demonstrate the superiority of SyncDiff over\nexisting state-of-the-art motion synthesis methods.\n","authors":["Wenkun He","Yun Liu","Ruitao Liu","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2412.20104v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07236v1","updated":"2025-01-13T11:34:55Z","published":"2025-01-13T11:34:55Z","title":"CSTA: Spatial-Temporal Causal Adaptive Learning for Exemplar-Free Video\n  Class-Incremental Learning","summary":"  Continual learning aims to acquire new knowledge while retaining past\ninformation. Class-incremental learning (CIL) presents a challenging scenario\nwhere classes are introduced sequentially. For video data, the task becomes\nmore complex than image data because it requires learning and preserving both\nspatial appearance and temporal action involvement. To address this challenge,\nwe propose a novel exemplar-free framework that equips separate spatiotemporal\nadapters to learn new class patterns, accommodating the incremental information\nrepresentation requirements unique to each class. While separate adapters are\nproven to mitigate forgetting and fit unique requirements, naively applying\nthem hinders the intrinsic connection between spatial and temporal information\nincrements, affecting the efficiency of representing newly learned class\ninformation. Motivated by this, we introduce two key innovations from a causal\nperspective. First, a causal distillation module is devised to maintain the\nrelation between spatial-temporal knowledge for a more efficient\nrepresentation. Second, a causal compensation mechanism is proposed to reduce\nthe conflicts during increment and memorization between different types of\ninformation. Extensive experiments conducted on benchmark datasets demonstrate\nthat our framework can achieve new state-of-the-art results, surpassing current\nexample-based methods by 4.2% in accuracy on average.\n","authors":["Tieyuan Chen","Huabin Liu","Chern Hong Lim","John See","Xing Gao","Junhui Hou","Weiyao Lin"],"pdf_url":"https://arxiv.org/pdf/2501.07236v1.pdf","comment":"IEEE TCSVT Submission"},{"id":"http://arxiv.org/abs/2501.07227v1","updated":"2025-01-13T11:28:49Z","published":"2025-01-13T11:28:49Z","title":"MECD+: Unlocking Event-Level Causal Graph Discovery for Video Reasoning","summary":"  Video causal reasoning aims to achieve a high-level understanding of videos\nfrom a causal perspective. However, it exhibits limitations in its scope,\nprimarily executed in a question-answering paradigm and focusing on brief video\nsegments containing isolated events and basic causal relations, lacking\ncomprehensive and structured causality analysis for videos with multiple\ninterconnected events. To fill this gap, we introduce a new task and dataset,\nMulti-Event Causal Discovery (MECD). It aims to uncover the causal relations\nbetween events distributed chronologically across long videos. Given visual\nsegments and textual descriptions of events, MECD identifies the causal\nassociations between these events to derive a comprehensive and structured\nevent-level video causal graph explaining why and how the result event\noccurred. To address the challenges of MECD, we devise a novel framework\ninspired by the Granger Causality method, incorporating an efficient mask-based\nevent prediction model to perform an Event Granger Test. It estimates causality\nby comparing the predicted result event when premise events are masked versus\nunmasked. Furthermore, we integrate causal inference techniques such as\nfront-door adjustment and counterfactual inference to mitigate challenges in\nMECD like causality confounding and illusory causality. Additionally, context\nchain reasoning is introduced to conduct more robust and generalized reasoning.\nExperiments validate the effectiveness of our framework in reasoning complete\ncausal relations, outperforming GPT-4o and VideoChat2 by 5.77% and 2.70%,\nrespectively. Further experiments demonstrate that causal relation graphs can\nalso contribute to downstream video understanding tasks such as video question\nanswering and video event prediction.\n","authors":["Tieyuan Chen","Huabin Liu","Yi Wang","Yihang Chen","Tianyao He","Chaofan Gan","Huanyu He","Weiyao Lin"],"pdf_url":"https://arxiv.org/pdf/2501.07227v1.pdf","comment":"IEEE TPAMI Submission. arXiv admin note: substantial text overlap\n  with arXiv:2409.17647"},{"id":"http://arxiv.org/abs/2501.07221v1","updated":"2025-01-13T11:20:44Z","published":"2025-01-13T11:20:44Z","title":"Exploring the Use of Contrastive Language-Image Pre-Training for Human\n  Posture Classification: Insights from Yoga Pose Analysis","summary":"  Accurate human posture classification in images and videos is crucial for\nautomated applications across various fields, including work safety, physical\nrehabilitation, sports training, or daily assisted living. Recently, multimodal\nlearning methods, such as Contrastive Language-Image Pretraining (CLIP), have\nadvanced significantly in jointly understanding images and text. This study\naims to assess the effectiveness of CLIP in classifying human postures,\nfocusing on its application in yoga. Despite the initial limitations of the\nzero-shot approach, applying transfer learning on 15,301 images (real and\nsynthetic) with 82 classes has shown promising results. The article describes\nthe full procedure for fine-tuning, including the choice for image description\nsyntax, models and hyperparameters adjustment. The fine-tuned CLIP model,\ntested on 3826 images, achieves an accuracy of over 85%, surpassing the current\nstate-of-the-art of previous works on the same dataset by approximately 6%, its\ntraining time being 3.5 times lower than what is needed to fine-tune a\nYOLOv8-based model. For more application-oriented scenarios, with smaller\ndatasets of six postures each, containing 1301 and 401 training images, the\nfine-tuned models attain an accuracy of 98.8% and 99.1%, respectively.\nFurthermore, our experiments indicate that training with as few as 20 images\nper pose can yield around 90% accuracy in a six-class dataset. This study\ndemonstrates that this multimodal technique can be effectively used for yoga\npose classification, and possibly for human posture classification, in general.\nAdditionally, CLIP inference time (around 7 ms) supports that the model can be\nintegrated into automated systems for posture evaluation, e.g., for developing\na real-time personal yoga assistant for performance assessment.\n","authors":["Andrzej D. Dobrzycki","Ana M. Bernardos","Luca Bergesio","Andrzej Pomirski","Daniel Sáez-Trigueros"],"pdf_url":"https://arxiv.org/pdf/2501.07221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07214v1","updated":"2025-01-13T11:12:59Z","published":"2025-01-13T11:12:59Z","title":"TimeLogic: A Temporal Logic Benchmark for Video QA","summary":"  Temporal logical understanding, a core facet of human cognition, plays a\npivotal role in capturing complex sequential events and their temporal\nrelationships within videos. This capability is particularly crucial in tasks\nlike Video Question Answering (VideoQA), where the goal is to process visual\ndata over time together with textual data to provide coherent answers. However,\ncurrent VideoQA benchmarks devote little focus to evaluating this critical\nskill due to the challenge of annotating temporal logic. Despite the\nadvancement of vision-language models, assessing their temporal logical\nreasoning powers remains a challenge, primarily due to the lack QA pairs that\ndemand formal, complex temporal reasoning. To bridge this gap, we introduce the\nTimeLogic QA (TLQA) framework to automatically generate the QA pairs,\nspecifically designed to evaluate the temporal logical understanding. To this\nend, TLQA leverages temporal annotations from existing video datasets together\nwith temporal operators derived from logic theory to construct questions that\ntest understanding of event sequences and their temporal relationships. TLQA\nframework is generic and scalable, capable of leveraging both, existing video\naction datasets with temporal action segmentation annotations, or video\ndatasets with temporal scene graph annotations, to automatically generate\ntemporal logical questions. We leverage 4 datasets, STAR, Breakfast, AGQA, and\nCrossTask, and generate two VideoQA dataset variants - small (TLQA-S) and large\n(TLQA-L) - containing 2k and 10k QA pairs for each category, resulting in 32k\nand 160k total pairs per dataset. We undertake a comprehensive evaluation of\nleading-edge VideoQA models, employing the TLQA to benchmark their temporal\nlogical understanding capabilities. We assess the VideoQA model's temporal\nreasoning performance on 16 categories of temporal logic with varying temporal\ncomplexity.\n","authors":["Sirnam Swetha","Hilde Kuehne","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2501.07214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07213v1","updated":"2025-01-13T11:12:47Z","published":"2025-01-13T11:12:47Z","title":"Multi-face emotion detection for effective Human-Robot Interaction","summary":"  The integration of dialogue interfaces in mobile devices has become\nubiquitous, providing a wide array of services. As technology progresses,\nhumanoid robots designed with human-like features to interact effectively with\npeople are gaining prominence, and the use of advanced human-robot dialogue\ninterfaces is continually expanding. In this context, emotion recognition plays\na crucial role in enhancing human-robot interaction by enabling robots to\nunderstand human intentions. This research proposes a facial emotion detection\ninterface integrated into a mobile humanoid robot, capable of displaying\nreal-time emotions from multiple individuals on a user interface. To this end,\nvarious deep neural network models for facial expression recognition were\ndeveloped and evaluated under consistent computer-based conditions, yielding\npromising results. Afterwards, a trade-off between accuracy and memory\nfootprint was carefully considered to effectively implement this application on\na mobile humanoid robot.\n","authors":["Mohamed Ala Yahyaoui","Mouaad Oujabour","Leila Ben Letaifa","Amine Bohi"],"pdf_url":"https://arxiv.org/pdf/2501.07213v1.pdf","comment":"9 pages, 8 figures and 1 table. Accepted at the 17th International\n  Conference on Agents and Artificial Intelligence (ICAART 2025), Porto,\n  Portugal"},{"id":"http://arxiv.org/abs/2501.07202v1","updated":"2025-01-13T10:53:48Z","published":"2025-01-13T10:53:48Z","title":"FaceOracle: Chat with a Face Image Oracle","summary":"  A face image is a mandatory part of ID and travel documents. Obtaining\nhigh-quality face images when issuing such documents is crucial for both human\nexaminers and automated face recognition systems. In several international\nstandards, face image quality requirements are intricate and defined in detail.\nIdentifying and understanding non-compliance or defects in the submitted face\nimages is crucial for both issuing authorities and applicants. In this work, we\nintroduce FaceOracle, an LLM-powered AI assistant that helps its users analyze\na face image in a natural conversational manner using standard compliant\nalgorithms. Leveraging the power of LLMs, users can get explanations of various\nface image quality concepts as well as interpret the outcome of face image\nquality assessment (FIQA) algorithms. We implement a proof-of-concept that\ndemonstrates how experts at an issuing authority could integrate FaceOracle\ninto their workflow to analyze, understand, and communicate their decisions\nmore efficiently, resulting in enhanced productivity.\n","authors":["Wassim Kabbani","Kiran Raja","Raghavendra Ramachandra","Christoph Busch"],"pdf_url":"https://arxiv.org/pdf/2501.07202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07197v1","updated":"2025-01-13T10:44:08Z","published":"2025-01-13T10:44:08Z","title":"Lung Cancer detection using Deep Learning","summary":"  In this paper we discuss lung cancer detection using hybrid model of\nConvolutional-Neural-Networks (CNNs) and Support-Vector-Machines-(SVMs) in\norder to gain early detection of tumors, benign or malignant. The work uses\nthis hybrid model by training upon the Computed Tomography scans (CT scans) as\ndataset. Using deep learning for detecting lung cancer early is a cutting-edge\nmethod.\n","authors":["Aryan Chaudhari","Ankush Singh","Sanchi Gajbhiye","Pratham Agrawal"],"pdf_url":"https://arxiv.org/pdf/2501.07197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07194v1","updated":"2025-01-13T10:42:18Z","published":"2025-01-13T10:42:18Z","title":"VAGeo: View-specific Attention for Cross-View Object Geo-Localization","summary":"  Cross-view object geo-localization (CVOGL) aims to locate an object of\ninterest in a captured ground- or drone-view image within the satellite image.\nHowever, existing works treat ground-view and drone-view query images\nequivalently, overlooking their inherent viewpoint discrepancies and the\nspatial correlation between the query image and the satellite-view reference\nimage. To this end, this paper proposes a novel View-specific Attention\nGeo-localization method (VAGeo) for accurate CVOGL. Specifically, VAGeo\ncontains two key modules: view-specific positional encoding (VSPE) module and\nchannel-spatial hybrid attention (CSHA) module. In object-level, according to\nthe characteristics of different viewpoints of ground and drone query images,\nviewpoint-specific positional codings are designed to more accurately identify\nthe click-point object of the query image in the VSPE module. In feature-level,\na hybrid attention in the CSHA module is introduced by combining channel\nattention and spatial attention mechanisms simultaneously for learning\ndiscriminative features. Extensive experimental results demonstrate that the\nproposed VAGeo gains a significant performance improvement, i.e., improving\nacc@0.25/acc@0.5 on the CVOGL dataset from 45.43%/42.24% to 48.21%/45.22% for\nground-view, and from 61.97%/57.66% to 66.19%/61.87% for drone-view.\n","authors":["Zhongyang Li","Xin Yuan","Wei Liu","Xin Xu"],"pdf_url":"https://arxiv.org/pdf/2501.07194v1.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2411.11543v4","updated":"2025-01-13T10:39:04Z","published":"2024-11-18T13:01:57Z","title":"PSA-VLM: Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment","summary":"  Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark.\n","authors":["Zhendong Liu","Yuanbi Nie","Yingshui Tan","Jiaheng Liu","Xiangyu Yue","Qiushi Cui","Chongjun Wang","Xiaoyong Zhu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.11543v4.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2405.13581"},{"id":"http://arxiv.org/abs/2501.07192v1","updated":"2025-01-13T10:38:58Z","published":"2025-01-13T10:38:58Z","title":"A4O: All Trigger for One sample","summary":"  Backdoor attacks have become a critical threat to deep neural networks\n(DNNs), drawing many research interests. However, most of the studied attacks\nemploy a single type of trigger. Consequently, proposed backdoor defenders\noften rely on the assumption that triggers would appear in a unified way. In\nthis paper, we show that this naive assumption can create a loophole, allowing\nmore sophisticated backdoor attacks to bypass. We design a novel backdoor\nattack mechanism that incorporates multiple types of backdoor triggers,\nfocusing on stealthiness and effectiveness. Our journey begins with the\nintriguing observation that the performance of a backdoor attack in deep\nlearning models, as well as its detectability and removability, are all\nproportional to the magnitude of the trigger. Based on this correlation, we\npropose reducing the magnitude of each trigger type and combining them to\nachieve a strong backdoor relying on the combined trigger while still staying\nsafely under the radar of defenders. Extensive experiments on three standard\ndatasets demonstrate that our method can achieve high attack success rates\n(ASRs) while consistently bypassing state-of-the-art defenses.\n","authors":["Duc Anh Vu","Anh Tuan Tran","Cong Tran","Cuong Pham"],"pdf_url":"https://arxiv.org/pdf/2501.07192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05767v2","updated":"2025-01-13T10:38:32Z","published":"2025-01-10T07:56:23Z","title":"Migician: Revealing the Magic of Free-Form Multi-Image Grounding in\n  Multimodal Large Language Models","summary":"  The recent advancement of Multimodal Large Language Models (MLLMs) has\nsignificantly improved their fine-grained perception of single images and\ngeneral comprehension across multiple images. However, existing MLLMs still\nface challenges in achieving precise grounding in complex multi-image\nscenarios. To address this, we first explore a Chain-of-Thought (CoT) framework\nthat integrates single-image grounding with multi-image comprehension. While\npartially effective, it remains unstable and struggles to capture abstract\nvisual information due to its non-end-to-end nature. Therefore, we introduce\nMigician, the first multi-image grounding model capable of performing free-form\nand accurate grounding across multiple images. To support this, we present the\nMGrounding-630k dataset, which comprises data for several multi-image grounding\ntasks derived from existing datasets, along with newly generated free-form\ngrounding instruction-following data. Furthermore, we propose MIG-Bench, a\ncomprehensive benchmark specifically designed for evaluating multi-image\ngrounding capabilities. Experimental results demonstrate that our model\nachieves significantly superior multi-image grounding capabilities,\noutperforming the best existing MLLMs by 21.61% and even surpassing much larger\n70B models. Our code, model, dataset, and benchmark are fully open-sourced at\nhttps://migician-vg.github.io/.\n","authors":["You Li","Heyu Huang","Chi Chen","Kaiyu Huang","Chao Huang","Zonghao Guo","Zhiyuan Liu","Jinan Xu","Yuhua Li","Ruixuan Li","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2501.05767v2.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2501.07185v1","updated":"2025-01-13T10:30:10Z","published":"2025-01-13T10:30:10Z","title":"Uncertainty Guarantees on Automated Precision Weeding using Conformal\n  Prediction","summary":"  Precision agriculture in general, and precision weeding in particular, have\ngreatly benefited from the major advancements in deep learning and computer\nvision. A large variety of commercial robotic solutions are already available\nand deployed. However, the adoption by farmers of such solutions is still low\nfor many reasons, an important one being the lack of trust in these systems.\nThis is in great part due to the opaqueness and complexity of deep neural\nnetworks and the manufacturers' inability to provide valid guarantees on their\nperformance. Conformal prediction, a well-established methodology in the\nmachine learning community, is an efficient and reliable strategy for providing\ntrustworthy guarantees on the predictions of any black-box model under very\nminimal constraints. Bridging the gap between the safe machine learning and\nprecision agriculture communities, this article showcases conformal prediction\nin action on the task of precision weeding through deep learning-based image\nclassification. After a detailed presentation of the conformal prediction\nmethodology and the development of a precision spraying pipeline based on a\n''conformalized'' neural network and well-defined spraying decision rules, the\narticle evaluates this pipeline on two real-world scenarios: one under\nin-distribution conditions, the other reflecting a near out-of-distribution\nsetting. The results show that we are able to provide formal, i.e. certifiable,\nguarantees on spraying at least 90% of the weeds.\n","authors":["Paul Melki","Lionel Bombrun","Boubacar Diallo","Jérôme Dias","Jean-Pierre da Costa"],"pdf_url":"https://arxiv.org/pdf/2501.07185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07179v1","updated":"2025-01-13T10:19:16Z","published":"2025-01-13T10:19:16Z","title":"Radial Distortion in Face Images: Detection and Impact","summary":"  Acquiring face images of sufficiently high quality is important for online ID\nand travel document issuance applications using face recognition systems (FRS).\nLow-quality, manipulated (intentionally or unintentionally), or distorted\nimages degrade the FRS performance and facilitate documents' misuse. Securing\nquality for enrolment images, especially in the unsupervised self-enrolment\nscenario via a smartphone, becomes important to assure FRS performance. In this\nwork, we focus on the less studied area of radial distortion (a.k.a., the\nfish-eye effect) in face images and its impact on FRS performance. We introduce\nan effective radial distortion detection model that can detect and flag radial\ndistortion in the enrolment scenario. We formalize the detection model as a\nface image quality assessment (FIQA) algorithm and provide a careful inspection\nof the effect of radial distortion on FRS performance. Evaluation results show\nexcellent detection results for the proposed models, and the study on the\nimpact on FRS uncovers valuable insights into how to best use these models in\noperational systems.\n","authors":["Wassim Kabbani","Tristan Le Pessot","Kiran Raja","Raghavendra Ramachandra","Christoph Busch"],"pdf_url":"https://arxiv.org/pdf/2501.07179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20971v2","updated":"2025-01-13T10:14:27Z","published":"2024-05-31T16:18:46Z","title":"Amortizing intractable inference in diffusion models for vision,\n  language, and control","summary":"  Diffusion models have emerged as effective distribution estimators in vision,\nlanguage, and reinforcement learning, but their use as priors in downstream\ntasks poses an intractable posterior inference problem. This paper studies\namortized sampling of the posterior over data, $\\mathbf{x}\\sim p^{\\rm\npost}(\\mathbf{x})\\propto p(\\mathbf{x})r(\\mathbf{x})$, in a model that consists\nof a diffusion generative model prior $p(\\mathbf{x})$ and a black-box\nconstraint or likelihood function $r(\\mathbf{x})$. We state and prove the\nasymptotic correctness of a data-free learning objective, relative trajectory\nbalance, for training a diffusion model that samples from this posterior, a\nproblem that existing methods solve only approximately or in restricted cases.\nRelative trajectory balance arises from the generative flow network perspective\non diffusion models, which allows the use of deep reinforcement learning\ntechniques to improve mode coverage. Experiments illustrate the broad potential\nof unbiased inference of arbitrary posteriors under diffusion priors: in vision\n(classifier guidance), language (infilling under a discrete diffusion LLM), and\nmultimodal data (text-to-image generation). Beyond generative modeling, we\napply relative trajectory balance to the problem of continuous control with a\nscore-based behavior prior, achieving state-of-the-art results on benchmarks in\noffline reinforcement learning.\n","authors":["Siddarth Venkatraman","Moksh Jain","Luca Scimeca","Minsu Kim","Marcin Sendera","Mohsin Hasan","Luke Rowe","Sarthak Mittal","Pablo Lemos","Emmanuel Bengio","Alexandre Adam","Jarrid Rector-Brooks","Yoshua Bengio","Glen Berseth","Nikolay Malkin"],"pdf_url":"https://arxiv.org/pdf/2405.20971v2.pdf","comment":"NeurIPS 2024; code: https://github.com/GFNOrg/diffusion-finetuning"},{"id":"http://arxiv.org/abs/2412.15523v2","updated":"2025-01-13T10:01:56Z","published":"2024-12-20T03:23:26Z","title":"InstructOCR: Instruction Boosting Scene Text Spotting","summary":"  In the field of scene text spotting, previous OCR methods primarily relied on\nimage encoders and pre-trained text information, but they often overlooked the\nadvantages of incorporating human language instructions. To address this gap,\nwe propose InstructOCR, an innovative instruction-based scene text spotting\nmodel that leverages human language instructions to enhance the understanding\nof text within images. Our framework employs both text and image encoders\nduring training and inference, along with instructions meticulously designed\nbased on text attributes. This approach enables the model to interpret text\nmore accurately and flexibly. Extensive experiments demonstrate the\neffectiveness of our model and we achieve state-of-the-art results on widely\nused benchmarks. Furthermore, the proposed framework can be seamlessly applied\nto scene text VQA tasks. By leveraging instruction strategies during\npre-training, the performance on downstream VQA tasks can be significantly\nimproved, with a 2.6% increase on the TextVQA dataset and a 2.1% increase on\nthe ST-VQA dataset. These experimental results provide insights into the\nbenefits of incorporating human language instructions for OCR-related tasks.\n","authors":["Chen Duan","Qianyi Jiang","Pei Fu","Jiamin Chen","Shengxi Li","Zining Wang","Shan Guo","Junfeng Luo"],"pdf_url":"https://arxiv.org/pdf/2412.15523v2.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2501.07171v1","updated":"2025-01-13T09:58:03Z","published":"2025-01-13T09:58:03Z","title":"BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and\n  Vision-Language Models Derived from Scientific Literature","summary":"  The development of vision-language models (VLMs) is driven by large-scale and\ndiverse multimodal datasets. However, progress toward generalist biomedical\nVLMs is limited by the lack of annotated, publicly accessible datasets across\nbiology and medicine. Existing efforts are restricted to narrow domains,\nmissing the full diversity of biomedical knowledge encoded in scientific\nliterature. To address this gap, we introduce BIOMEDICA, a scalable,\nopen-source framework to extract, annotate, and serialize the entirety of the\nPubMed Central Open Access subset into an easy-to-use, publicly accessible\ndataset.Our framework produces a comprehensive archive with over 24 million\nunique image-text pairs from over 6 million articles. Metadata and\nexpert-guided annotations are also provided. We demonstrate the utility and\naccessibility of our resource by releasing BMCA-CLIP, a suite of CLIP-style\nmodels continuously pre-trained on the BIOMEDICA dataset via streaming,\neliminating the need to download 27 TB of data locally.On average, our models\nachieve state-of-the-art performance across 40 tasks - spanning pathology,\nradiology, ophthalmology, dermatology, surgery, molecular biology,\nparasitology, and cell biology - excelling in zero-shot classification with a\n6.56% average improvement (as high as 29.8% and 17.5% in dermatology and\nophthalmology, respectively), and stronger image-text retrieval, all while\nusing 10x less compute. To foster reproducibility and collaboration, we release\nour codebase and dataset for the broader research community.\n","authors":["Alejandro Lozano","Min Woo Sun","James Burgess","Liangyu Chen","Jeffrey J Nirschl","Jeffrey Gu","Ivan Lopez","Josiah Aklilu","Austin Wolfgang Katzer","Collin Chiu","Anita Rau","Xiaohan Wang","Yuhui Zhang","Alfred Seunghoon Song","Robert Tibshirani","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2501.07171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07163v1","updated":"2025-01-13T09:49:34Z","published":"2025-01-13T09:49:34Z","title":"Adaptive Noise-Tolerant Network for Image Segmentation","summary":"  Unlike image classification and annotation, for which deep network models\nhave achieved dominating superior performances compared to traditional computer\nvision algorithms, deep learning for automatic image segmentation still faces\ncritical challenges. One of such hurdles is to obtain ground-truth\nsegmentations as the training labels for deep network training. Especially when\nwe study biomedical images, such as histopathological images (histo-images), it\nis unrealistic to ask for manual segmentation labels as the ground truth for\ntraining due to the fine image resolution as well as the large image size and\ncomplexity. In this paper, instead of relying on clean segmentation labels, we\nstudy whether and how integrating imperfect or noisy segmentation results from\noff-the-shelf segmentation algorithms may help achieve better segmentation\nresults through a new Adaptive Noise-Tolerant Network (ANTN) model. We extend\nthe noisy label deep learning to image segmentation with two novel aspects: (1)\nmultiple noisy labels can be integrated into one deep learning model; (2) noisy\nsegmentation modeling, including probabilistic parameters, is adaptive,\ndepending on the given testing image appearance. Implementation of the new ANTN\nmodel on both the synthetic data and real-world histo-images demonstrates its\neffectiveness and superiority over off-the-shelf and other existing\ndeep-learning-based image segmentation algorithms.\n","authors":["Weizhi Li"],"pdf_url":"https://arxiv.org/pdf/2501.07163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05862v3","updated":"2025-01-13T09:33:47Z","published":"2024-06-09T17:25:47Z","title":"II-Bench: An Image Implication Understanding Benchmark for Multimodal\n  Large Language Models","summary":"  The rapid advancements in the development of multimodal large language models\n(MLLMs) have consistently led to new breakthroughs on various benchmarks. In\nresponse, numerous challenging and comprehensive benchmarks have been proposed\nto more accurately assess the capabilities of MLLMs. However, there is a dearth\nof exploration of the higher-order perceptual capabilities of MLLMs. To fill\nthis gap, we propose the Image Implication understanding Benchmark, II-Bench,\nwhich aims to evaluate the model's higher-order perception of images. Through\nextensive experiments on II-Bench across multiple MLLMs, we have made\nsignificant findings. Initially, a substantial gap is observed between the\nperformance of MLLMs and humans on II-Bench. The pinnacle accuracy of MLLMs\nattains 74.8%, whereas human accuracy averages 90%, peaking at an impressive\n98%. Subsequently, MLLMs perform worse on abstract and complex images,\nsuggesting limitations in their ability to understand high-level semantics and\ncapture image details. Finally, it is observed that most models exhibit\nenhanced accuracy when image sentiment polarity hints are incorporated into the\nprompts. This observation underscores a notable deficiency in their inherent\nunderstanding of image sentiment. We believe that II-Bench will inspire the\ncommunity to develop the next generation of MLLMs, advancing the journey\ntowards expert artificial general intelligence (AGI). II-Bench is publicly\navailable at https://huggingface.co/datasets/m-a-p/II-Bench.\n","authors":["Ziqiang Liu","Feiteng Fang","Xi Feng","Xinrun Du","Chenhao Zhang","Zekun Wang","Yuelin Bai","Qixuan Zhao","Liyang Fan","Chengguang Gan","Hongquan Lin","Jiaming Li","Yuansheng Ni","Haihong Wu","Yaswanth Narsupalli","Zhigang Zheng","Chengming Li","Xiping Hu","Ruifeng Xu","Xiaojun Chen","Min Yang","Jiaheng Liu","Ruibo Liu","Wenhao Huang","Ge Zhang","Shiwen Ni"],"pdf_url":"https://arxiv.org/pdf/2406.05862v3.pdf","comment":"100 pages, 82 figures, add citations"},{"id":"http://arxiv.org/abs/2501.07158v1","updated":"2025-01-13T09:33:03Z","published":"2025-01-13T09:33:03Z","title":"Eye Sclera for Fair Face Image Quality Assessment","summary":"  Fair operational systems are crucial in gaining and maintaining society's\ntrust in face recognition systems (FRS). FRS start with capturing an image and\nassessing its quality before using it further for enrollment or verification.\nFair Face Image Quality Assessment (FIQA) schemes therefore become equally\nimportant in the context of fair FRS. This work examines the sclera as a\nquality assessment region for obtaining a fair FIQA. The sclera region is\nagnostic to demographic variations and skin colour for assessing the quality of\na face image. We analyze three skin tone related ISO/IEC face image quality\nassessment measures and assess the sclera region as an alternative area for\nassessing FIQ. Our analysis of the face dataset of individuals from different\ndemographic groups representing different skin tones indicates sclera as an\nalternative to measure dynamic range, over- and under-exposure of face using\nsclera region alone. The sclera region being agnostic to skin tone, i.e.,\ndemographic factors, provides equal utility as a fair FIQA as shown by our\nError-vs-Discard Characteristic (EDC) curve analysis.\n","authors":["Wassim Kabbani","Kiran Raja","Raghavendra Ramachandra","Christoph Busch"],"pdf_url":"https://arxiv.org/pdf/2501.07158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14432v2","updated":"2025-01-13T09:26:17Z","published":"2024-09-22T13:11:08Z","title":"EM-DARTS: Hierarchical Differentiable Architecture Search for Eye\n  Movement Recognition","summary":"  Eye movement biometrics has received increasing attention thanks to its\nhighly secure identification. Although deep learning (DL) models have shown\nsuccess in eye movement recognition, their architectures largely rely on human\nprior knowledge. Differentiable Neural Architecture Search (DARTS) automates\nthe manual process of architecture design with high search efficiency. However,\nDARTS typically stacks multiple cells to form a convolutional network, which\nlimits the diversity of architecture. Furthermore, DARTS generally searches for\narchitectures using shallower networks than those used in the evaluation,\ncreating a significant disparity in architecture depth between the search and\nevaluation phases. To address this issue, we propose EM-DARTS, a hierarchical\ndifferentiable architecture search algorithm to automatically design the DL\narchitecture for eye movement recognition. First, we define a supernet and\npropose a global and local alternate Neural Architecture Search method to\nsearch the optimal architecture alternately with a differentiable neural\narchitecture search. The local search strategy aims to find an optimal\narchitecture for different cells while the global search strategy is\nresponsible for optimizing the architecture of the target network. To minimize\nredundancy, transfer entropy is proposed to compute the information amount of\neach layer, thereby further simplifying the network search process.\nExperimental results on three public datasets demonstrate that the proposed\nEM-DARTS is capable of producing an optimal architecture that leads to\nstate-of-the-art recognition performance, {Specifically, the recognition models\ndeveloped using EM-DARTS achieved the lowest EERs of 0.0453 on the GazeBase\ndataset, 0.0377 on the JuDo1000 dataset, and 0.1385 on the EMglasses dataset.\n","authors":["Huafeng Qin","Hongyu Zhu","Xin Jin","Xin Yu","Mounim A. El-Yacoubi","Shuqiang Yang"],"pdf_url":"https://arxiv.org/pdf/2409.14432v2.pdf","comment":"Submited to IEEE Transactions on Instrumentation and Measurement"},{"id":"http://arxiv.org/abs/2407.19507v2","updated":"2025-01-13T08:58:40Z","published":"2024-07-28T14:58:07Z","title":"WeCromCL: Weakly Supervised Cross-Modality Contrastive Learning for\n  Transcription-only Supervised Text Spotting","summary":"  Transcription-only Supervised Text Spotting aims to learn text spotters\nrelying only on transcriptions but no text boundaries for supervision, thus\neliminating expensive boundary annotation. The crux of this task lies in\nlocating each transcription in scene text images without location annotations.\nIn this work, we formulate this challenging problem as a Weakly Supervised\nCross-modality Contrastive Learning problem, and design a simple yet effective\nmodel dubbed WeCromCL that is able to detect each transcription in a scene\nimage in a weakly supervised manner. Unlike typical methods for cross-modality\ncontrastive learning that focus on modeling the holistic semantic correlation\nbetween an entire image and a text description, our WeCromCL conducts atomistic\ncontrastive learning to model the character-wise appearance consistency between\na text transcription and its correlated region in a scene image to detect an\nanchor point for the transcription in a weakly supervised manner. The detected\nanchor points by WeCromCL are further used as pseudo location labels to guide\nthe learning of text spotting. Extensive experiments on four challenging\nbenchmarks demonstrate the superior performance of our model over other\nmethods. Code will be released.\n","authors":["Jingjing Wu","Zhengyao Fang","Pengyuan Lyu","Chengquan Zhang","Fanglin Chen","Guangming Lu","Wenjie Pei"],"pdf_url":"https://arxiv.org/pdf/2407.19507v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2501.05826v2","updated":"2025-01-13T08:56:05Z","published":"2025-01-10T10:03:56Z","title":"AI-Driven Diabetic Retinopathy Screening: Multicentric Validation of\n  AIDRSS in India","summary":"  Purpose: Diabetic retinopathy (DR) is a major cause of vision loss,\nparticularly in India, where access to retina specialists is limited in rural\nareas. This study aims to evaluate the Artificial Intelligence-based Diabetic\nRetinopathy Screening System (AIDRSS) for DR detection and prevalence\nassessment, addressing the growing need for scalable, automated screening\nsolutions in resource-limited settings.\n  Approach: A multicentric, cross-sectional study was conducted in Kolkata,\nIndia, involving 5,029 participants and 10,058 macula-centric retinal fundus\nimages. The AIDRSS employed a deep learning algorithm with 50 million trainable\nparameters, integrated with Contrast Limited Adaptive Histogram Equalization\n(CLAHE) preprocessing for enhanced image quality. DR was graded using the\nInternational Clinical Diabetic Retinopathy (ICDR) Scale, categorizing disease\ninto five stages (DR0 to DR4). Statistical metrics including sensitivity,\nspecificity, and prevalence rates were evaluated against expert retina\nspecialist assessments.\n  Results: The prevalence of DR in the general population was 13.7%, rising to\n38.2% among individuals with elevated random blood glucose levels. The AIDRSS\nachieved an overall sensitivity of 92%, specificity of 88%, and 100%\nsensitivity for detecting referable DR (DR3 and DR4). These results demonstrate\nthe system's robust performance in accurately identifying and grading DR in a\ndiverse population.\n  Conclusions: AIDRSS provides a reliable, scalable solution for early DR\ndetection in resource-constrained environments. Its integration of advanced AI\ntechniques ensures high diagnostic accuracy, with potential to significantly\nreduce the burden of diabetes-related vision loss in underserved regions.\n","authors":["Amit Kr Dey","Pradeep Walia","Girish Somvanshi","Abrar Ali","Sagarnil Das","Pallabi Paul","Minakhi Ghosh"],"pdf_url":"https://arxiv.org/pdf/2501.05826v2.pdf","comment":"22 pages, 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:1812.07105 by other authors without attribution"},{"id":"http://arxiv.org/abs/2501.07133v1","updated":"2025-01-13T08:44:35Z","published":"2025-01-13T08:44:35Z","title":"Robust Single Object Tracking in LiDAR Point Clouds under Adverse\n  Weather Conditions","summary":"  3D single object tracking (3DSOT) in LiDAR point clouds is a critical task\nfor outdoor perception, enabling real-time perception of object location,\norientation, and motion. Despite the impressive performance of current 3DSOT\nmethods, evaluating them on clean datasets inadequately reflects their\ncomprehensive performance, as the adverse weather conditions in real-world\nsurroundings has not been considered. One of the main obstacles is the lack of\nadverse weather benchmarks for the evaluation of 3DSOT. To this end, this work\nproposes a challenging benchmark for LiDAR-based 3DSOT in adverse weather,\nwhich comprises two synthetic datasets (KITTI-A and nuScenes-A) and one\nreal-world dataset (CADC-SOT) spanning three weather types: rain, fog, and\nsnow. Based on this benchmark, five representative 3D trackers from different\ntracking frameworks conducted robustness evaluation, resulting in significant\nperformance degradations. This prompts the question: What are the factors that\ncause current advanced methods to fail on such adverse weather samples?\nConsequently, we explore the impacts of adverse weather and answer the above\nquestion from three perspectives: 1) target distance; 2) template shape\ncorruption; and 3) target shape corruption. Finally, based on domain\nrandomization and contrastive learning, we designed a dual-branch tracking\nframework for adverse weather, named DRCT, achieving excellent performance in\nbenchmarks.\n","authors":["Xiantong Zhao","Xiuping Liu","Shengjing Tian","Yinan Han"],"pdf_url":"https://arxiv.org/pdf/2501.07133v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2408.06019v2","updated":"2025-01-13T08:42:11Z","published":"2024-08-12T09:19:38Z","title":"HeadGAP: Few-Shot 3D Head Avatar via Generalizable Gaussian Priors","summary":"  In this paper, we present a novel 3D head avatar creation approach capable of\ngeneralizing from few-shot in-the-wild data with high-fidelity and animatable\nrobustness. Given the underconstrained nature of this problem, incorporating\nprior knowledge is essential. Therefore, we propose a framework comprising\nprior learning and avatar creation phases. The prior learning phase leverages\n3D head priors derived from a large-scale multi-view dynamic dataset, and the\navatar creation phase applies these priors for few-shot personalization. Our\napproach effectively captures these priors by utilizing a Gaussian\nSplatting-based auto-decoder network with part-based dynamic modeling. Our\nmethod employs identity-shared encoding with personalized latent codes for\nindividual identities to learn the attributes of Gaussian primitives. During\nthe avatar creation phase, we achieve fast head avatar personalization by\nleveraging inversion and fine-tuning strategies. Extensive experiments\ndemonstrate that our model effectively exploits head priors and successfully\ngeneralizes them to few-shot personalization, achieving photo-realistic\nrendering quality, multi-view consistency, and stable animation.\n","authors":["Xiaozheng Zheng","Chao Wen","Zhaohu Li","Weiyi Zhang","Zhuo Su","Xu Chang","Yang Zhao","Zheng Lv","Xiaoyuan Zhang","Yongjie Zhang","Guidong Wang","Lan Xu"],"pdf_url":"https://arxiv.org/pdf/2408.06019v2.pdf","comment":"Accepted to 3DV 2025. Project page: https://headgap.github.io/"},{"id":"http://arxiv.org/abs/2501.07120v1","updated":"2025-01-13T08:22:10Z","published":"2025-01-13T08:22:10Z","title":"MSV-Mamba: A Multiscale Vision Mamba Network for Echocardiography\n  Segmentation","summary":"  Ultrasound imaging frequently encounters challenges, such as those related to\nelevated noise levels, diminished spatiotemporal resolution, and the complexity\nof anatomical structures. These factors significantly hinder the model's\nability to accurately capture and analyze structural relationships and dynamic\npatterns across various regions of the heart. Mamba, an emerging model, is one\nof the most cutting-edge approaches that is widely applied to diverse vision\nand language tasks. To this end, this paper introduces a U-shaped deep learning\nmodel incorporating a large-window Mamba scale (LMS) module and a hierarchical\nfeature fusion approach for echocardiographic segmentation. First, a cascaded\nresidual block serves as an encoder and is employed to incrementally extract\nmultiscale detailed features. Second, a large-window multiscale mamba module is\nintegrated into the decoder to capture global dependencies across regions and\nenhance the segmentation capability for complex anatomical structures.\nFurthermore, our model introduces auxiliary losses at each decoder layer and\nemploys a dual attention mechanism to fuse multilayer features both spatially\nand across channels. This approach enhances segmentation performance and\naccuracy in delineating complex anatomical structures. Finally, the\nexperimental results using the EchoNet-Dynamic and CAMUS datasets demonstrate\nthat the model outperforms other methods in terms of both accuracy and\nrobustness. For the segmentation of the left ventricular endocardium\n(${LV}_{endo}$), the model achieved optimal values of 95.01 and 93.36,\nrespectively, while for the left ventricular epicardium (${LV}_{epi}$), values\nof 87.35 and 87.80, respectively, were achieved. This represents an improvement\nranging between 0.54 and 1.11 compared with the best-performing model.\n","authors":["Xiaoxian Yang","Qi Wang","Kaiqi Zhang","Ke Wei","Jun Lyu","Lingchao Chen"],"pdf_url":"https://arxiv.org/pdf/2501.07120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04746v3","updated":"2025-01-13T08:08:28Z","published":"2023-12-07T23:16:37Z","title":"Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized\n  Narratives from Open-Source Histopathology Videos","summary":"  Diagnosis in histopathology requires a global whole slide images (WSIs)\nanalysis, requiring pathologists to compound evidence from different WSI\npatches. The gigapixel scale of WSIs poses a challenge for histopathology\nmulti-modal models. Training multi-model models for histopathology requires\ninstruction tuning datasets, which currently contain information for individual\nimage patches, without a spatial grounding of the concepts within each patch\nand without a wider view of the WSI. Therefore, they lack sufficient diagnostic\ncapacity for histopathology. To bridge this gap, we introduce Quilt-Instruct, a\nlarge-scale dataset of 107,131 histopathology-specific instruction\nquestion/answer pairs, grounded within diagnostically relevant image patches\nthat make up the WSI. Our dataset is collected by leveraging educational\nhistopathology videos from YouTube, which provides spatial localization of\nnarrations by automatically extracting the narrators' cursor positions.\nQuilt-Instruct supports contextual reasoning by extracting diagnosis and\nsupporting facts from the entire WSI. Using Quilt-Instruct, we train\nQuilt-LLaVA, which can reason beyond the given single image patch, enabling\ndiagnostic reasoning across patches. To evaluate Quilt-LLaVA, we propose a\ncomprehensive evaluation dataset created from 985 images and 1283\nhuman-generated question-answers. We also thoroughly evaluate Quilt-LLaVA using\npublic histopathology datasets, where Quilt-LLaVA significantly outperforms\nSOTA by over 10% on relative GPT-4 score and 4% and 9% on open and closed set\nVQA. Our code, data, and model are publicly accessible at\nquilt-llava.github.io.\n","authors":["Mehmet Saygin Seyfioglu","Wisdom O. Ikezogwo","Fatemeh Ghezloo","Ranjay Krishna","Linda Shapiro"],"pdf_url":"https://arxiv.org/pdf/2312.04746v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07114v1","updated":"2025-01-13T08:04:32Z","published":"2025-01-13T08:04:32Z","title":"Duplex: Dual Prototype Learning for Compositional Zero-Shot Learning","summary":"  Compositional Zero-Shot Learning (CZSL) aims to enable models to recognize\nnovel compositions of visual states and objects that were absent during\ntraining. Existing methods predominantly focus on learning semantic\nrepresentations of seen compositions but often fail to disentangle the\nindependent features of states and objects in images, thereby limiting their\nability to generalize to unseen compositions. To address this challenge, we\npropose Duplex, a novel dual-prototype learning method that integrates semantic\nand visual prototypes through a carefully designed dual-branch architecture,\nenabling effective representation learning for compositional tasks. Duplex\nutilizes a Graph Neural Network (GNN) to adaptively update visual prototypes,\ncapturing complex interactions between states and objects. Additionally, it\nleverages the strong visual-semantic alignment of pre-trained Vision-Language\nModels (VLMs) and employs a multi-path architecture combined with prompt\nengineering to align image and text representations, ensuring robust\ngeneralization. Extensive experiments on three benchmark datasets demonstrate\nthat Duplex outperforms state-of-the-art methods in both closed-world and\nopen-world settings.\n","authors":["Zhong Peng","Yishi Xu","Gerong Wang","Wenchao Chen","Bo Chen","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.07114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07113v1","updated":"2025-01-13T08:03:49Z","published":"2025-01-13T08:03:49Z","title":"Matching Free Depth Recovery from Structured Light","summary":"  We present a novel approach for depth estimation from images captured by\nstructured light systems. Unlike many previous methods that rely on image\nmatching process, our approach uses a density voxel grid to represent scene\ngeometry, which is trained via self-supervised differentiable volume rendering.\nOur method leverages color fields derived from projected patterns in structured\nlight systems during the rendering process, enabling the isolated optimization\nof the geometry field. This contributes to faster convergence and high-quality\noutput. Additionally, we incorporate normalized device coordinates (NDC), a\ndistortion loss, and a novel surface-based color loss to enhance geometric\nfidelity. Experimental results demonstrate that our method outperforms existing\nmatching-based techniques in geometric performance for few-shot scenarios,\nachieving approximately a 60% reduction in average estimated depth errors on\nsynthetic scenes and about 30% on real-world captured scenes. Furthermore, our\napproach delivers fast training, with a speed roughly three times faster than\nprevious matching-free methods that employ implicit representations.\n","authors":["Zhuohang Yu","Kai Wang","Juyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.07113v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2501.07110v1","updated":"2025-01-13T07:51:43Z","published":"2025-01-13T07:51:43Z","title":"Dynamic Multimodal Fusion via Meta-Learning Towards Micro-Video\n  Recommendation","summary":"  Multimodal information (e.g., visual, acoustic, and textual) has been widely\nused to enhance representation learning for micro-video recommendation. For\nintegrating multimodal information into a joint representation of micro-video,\nmultimodal fusion plays a vital role in the existing micro-video recommendation\napproaches. However, the static multimodal fusion used in previous studies is\ninsufficient to model the various relationships among multimodal information of\ndifferent micro-videos. In this paper, we develop a novel meta-learning-based\nmultimodal fusion framework called Meta Multimodal Fusion (MetaMMF), which\ndynamically assigns parameters to the multimodal fusion function for each\nmicro-video during its representation learning. Specifically, MetaMMF regards\nthe multimodal fusion of each micro-video as an independent task. Based on the\nmeta information extracted from the multimodal features of the input task,\nMetaMMF parameterizes a neural network as the item-specific fusion function via\na meta learner. We perform extensive experiments on three benchmark datasets,\ndemonstrating the significant improvements over several state-of-the-art\nmultimodal recommendation models, like MMGCN, LATTICE, and InvRL. Furthermore,\nwe lighten our model by adopting canonical polyadic decomposition to improve\nthe training efficiency, and validate its effectiveness through experimental\nresults. Codes are available at https://github.com/hanliu95/MetaMMF.\n","authors":["Han Liu","Yinwei Wei","Fan Liu","Wenjie Wang","Liqiang Nie","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2501.07110v1.pdf","comment":"This paper has been accepted by ACM Transactions on Information\n  Systems"},{"id":"http://arxiv.org/abs/2501.07109v1","updated":"2025-01-13T07:43:33Z","published":"2025-01-13T07:43:33Z","title":"The Quest for Visual Understanding: A Journey Through the Evolution of\n  Visual Question Answering","summary":"  Visual Question Answering (VQA) is an interdisciplinary field that bridges\nthe gap between computer vision (CV) and natural language processing(NLP),\nenabling Artificial Intelligence(AI) systems to answer questions about images.\nSince its inception in 2015, VQA has rapidly evolved, driven by advances in\ndeep learning, attention mechanisms, and transformer-based models. This survey\ntraces the journey of VQA from its early days, through major breakthroughs,\nsuch as attention mechanisms, compositional reasoning, and the rise of\nvision-language pre-training methods. We highlight key models, datasets, and\ntechniques that shaped the development of VQA systems, emphasizing the pivotal\nrole of transformer architectures and multimodal pre-training in driving recent\nprogress. Additionally, we explore specialized applications of VQA in domains\nlike healthcare and discuss ongoing challenges, such as dataset bias, model\ninterpretability, and the need for common-sense reasoning. Lastly, we discuss\nthe emerging trends in large multimodal language models and the integration of\nexternal knowledge, offering insights into the future directions of VQA. This\npaper aims to provide a comprehensive overview of the evolution of VQA,\nhighlighting both its current state and potential advancements.\n","authors":["Anupam Pandey","Deepjyoti Bodo","Arpan Phukan","Asif Ekbal"],"pdf_url":"https://arxiv.org/pdf/2501.07109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07104v1","updated":"2025-01-13T07:32:44Z","published":"2025-01-13T07:32:44Z","title":"RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular\n  Video Based on Rectified Mesh-embedded Gaussians","summary":"  We introduce RMAvatar, a novel human avatar representation with Gaussian\nsplatting embedded on mesh to learn clothed avatar from a monocular video. We\nutilize the explicit mesh geometry to represent motion and shape of a virtual\nhuman and implicit appearance rendering with Gaussian Splatting. Our method\nconsists of two main modules: Gaussian initialization module and Gaussian\nrectification module. We embed Gaussians into triangular faces and control\ntheir motion through the mesh, which ensures low-frequency motion and surface\ndeformation of the avatar. Due to the limitations of LBS formula, the human\nskeleton is hard to control complex non-rigid transformations. We then design a\npose-related Gaussian rectification module to learn fine-detailed non-rigid\ndeformations, further improving the realism and expressiveness of the avatar.\nWe conduct extensive experiments on public datasets, RMAvatar shows\nstate-of-the-art performance on both rendering quality and quantitative\nevaluations. Please see our project page at https://rm-avatar.github.io.\n","authors":["Sen Peng","Weixing Xie","Zilong Wang","Xiaohu Guo","Zhonggui Chen","Baorong Yang","Xiao Dong"],"pdf_url":"https://arxiv.org/pdf/2501.07104v1.pdf","comment":"CVM2025"},{"id":"http://arxiv.org/abs/2411.14789v2","updated":"2025-01-13T07:29:53Z","published":"2024-11-22T08:17:46Z","title":"Simplifying CLIP: Unleashing the Power of Large-Scale Models on\n  Consumer-level Computers","summary":"  Contrastive Language-Image Pre-training (CLIP) has attracted a surge of\nattention for its superior zero-shot performance and excellent transferability\nto downstream tasks. However, training such large-scale models usually requires\nsubstantial computation and storage, which poses barriers for general users\nwith consumer-level computers. Motivated by this observation, in this paper we\ninvestigate how to achieve competitive performance on only one Nvidia RTX3090\nGPU and with one terabyte for storing dataset. On one hand, we simplify the\ntransformer block structure and combine Weight Inheritance with multi-stage\nKnowledge Distillation (WIKD), thereby reducing the parameters and improving\nthe inference speed during training along with deployment. On the other hand,\nconfronted with the convergence challenge posed by small dataset, we generate\nsynthetic captions for each sample as data augmentation, and devise a novel\nPair Matching (PM) loss to fully exploit the distinguishment among positive and\nnegative image-text pairs. Extensive experiments demonstrate that our model can\nachieve a new state-of-the-art datascale-parameter-accuracy tradeoff, which\ncould further popularize the CLIP model in the related research community.\n","authors":["Hongbo Liu"],"pdf_url":"https://arxiv.org/pdf/2411.14789v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07101v1","updated":"2025-01-13T07:26:37Z","published":"2025-01-13T07:26:37Z","title":"Dual Scale-aware Adaptive Masked Knowledge Distillation for Object\n  Detection","summary":"  Recent feature masking knowledge distillation methods make use of attention\nmechanisms to identify either important spatial regions or channel clues for\ndiscriminative feature reconstruction. However, most of existing strategies\nperform global attention-guided feature masking distillation without delving\ninto fine-grained visual clues in feature maps. In particular, uncovering\nlocality-aware clues across different scales are conducive to reconstructing\nregion-aware features, thereby significantly benefiting distillation\nperformance. In this study, we propose a fine-grained adaptive feature masking\ndistillation framework for accurate object detection. Different from previous\nmethods in which global masking is performed on single-scale feature maps, we\nexplore the scale-aware feature masking by performing feature distillation\nacross various scales, such that the object-aware locality is encoded for\nimproved feature reconstruction. In addition, our fine-grained feature\ndistillation strategy is combined with a masking logits distillation scheme in\nwhich logits difference between teacher and student networks is utilized to\nguide the distillation process. Thus, it can help the student model to better\nlearn from the teacher counterpart with improved knowledge transfer. Extensive\nexperiments for detection task demonstrate the superiority of our method. For\nexample, when RetinaNet, RepPoints and Cascade Mask RCNN are used as teacher\ndetectors, the student network achieves mAP scores of 41.5\\%, 42.9\\%, and\n42.6\\%, respectively, outperforming state-of-the-art methods such as DMKD and\nFreeKD.\n","authors":["ZhouRui Zhang","Jun Li","JiaYan Li","ZhiJian Wu","JianHua Xu"],"pdf_url":"https://arxiv.org/pdf/2501.07101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07100v1","updated":"2025-01-13T07:26:05Z","published":"2025-01-13T07:26:05Z","title":"Collaborative Learning for 3D Hand-Object Reconstruction and\n  Compositional Action Recognition from Egocentric RGB Videos Using\n  Superquadrics","summary":"  With the availability of egocentric 3D hand-object interaction datasets,\nthere is increasing interest in developing unified models for hand-object pose\nestimation and action recognition. However, existing methods still struggle to\nrecognise seen actions on unseen objects due to the limitations in representing\nobject shape and movement using 3D bounding boxes. Additionally, the reliance\non object templates at test time limits their generalisability to unseen\nobjects. To address these challenges, we propose to leverage superquadrics as\nan alternative 3D object representation to bounding boxes and demonstrate their\neffectiveness on both template-free object reconstruction and action\nrecognition tasks. Moreover, as we find that pure appearance-based methods can\noutperform the unified methods, the potential benefits from 3D geometric\ninformation remain unclear. Therefore, we study the compositionality of actions\nby considering a more challenging task where the training combinations of verbs\nand nouns do not overlap with the testing split. We extend H2O and FPHA\ndatasets with compositional splits and design a novel collaborative learning\nframework that can explicitly reason about the geometric relations between\nhands and the manipulated object. Through extensive quantitative and\nqualitative evaluations, we demonstrate significant improvements over the\nstate-of-the-arts in (compositional) action recognition.\n","authors":["Tze Ho Elden Tse","Runyang Feng","Linfang Zheng","Jiho Park","Yixing Gao","Jihie Kim","Ales Leonardis","Hyung Jin Chang"],"pdf_url":"https://arxiv.org/pdf/2501.07100v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.07249v2","updated":"2025-01-13T07:22:02Z","published":"2024-12-10T07:18:51Z","title":"Buster: Implanting Semantic Backdoor into Text Encoder to Mitigate NSFW\n  Content Generation","summary":"  The rise of deep learning models in the digital era has raised substantial\nconcerns regarding the generation of Not-Safe-for-Work (NSFW) content. Existing\ndefense methods primarily involve model fine-tuning and post-hoc content\nmoderation. Nevertheless, these approaches largely lack scalability in\neliminating harmful content, degrade the quality of benign image generation, or\nincur high inference costs. To address these challenges, we propose an\ninnovative framework named \\textit{Buster}, which injects backdoors into the\ntext encoder to prevent NSFW content generation. Buster leverages deep semantic\ninformation rather than explicit prompts as triggers, redirecting NSFW prompts\ntowards targeted benign prompts. Additionally, Buster employs energy-based\ntraining data generation through Langevin dynamics for adversarial knowledge\naugmentation, thereby ensuring robustness in harmful concept definition. This\napproach demonstrates exceptional resilience and scalability in mitigating NSFW\ncontent. Particularly, Buster fine-tunes the text encoder of Text-to-Image\nmodels within merely five minutes, showcasing its efficiency. Our extensive\nexperiments denote that Buster outperforms nine state-of-the-art baselines,\nachieving a superior NSFW content removal rate of at least 91.2\\% while\npreserving the quality of harmless images.\n","authors":["Xin Zhao","Xiaojun Chen","Yuexin Xuan","Zhendong Zhao","Xiaojun Jia","Xinfeng Li","Xiaofeng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.07249v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15378v2","updated":"2025-01-13T06:49:22Z","published":"2023-08-29T15:16:51Z","title":"On the Robustness of Object Detection Models on Aerial Images","summary":"  The robustness of object detection models is a major concern when applied to\nreal-world scenarios. The performance of most models tends to degrade when\nconfronted with images affected by corruptions, since they are usually trained\nand evaluated on clean datasets. While numerous studies have explored the\nrobustness of object detection models on natural images, there is a paucity of\nresearch focused on models applied to aerial images, which feature complex\nbackgrounds, substantial variations in scales, and orientations of objects.\nThis paper addresses the challenge of assessing the robustness of object\ndetection models on aerial images, with a specific emphasis on scenarios where\nimages are affected by clouds. In this study, we introduce two novel benchmarks\nbased on DOTA-v1.0. The first benchmark encompasses 19 prevalent corruptions,\nwhile the second focuses on the cloud-corrupted condition-a phenomenon uncommon\nin natural images yet frequent in aerial photography. We systematically\nevaluate the robustness of mainstream object detection models and perform\nnecessary ablation experiments. Through our investigations, we find that\nrotation-invariant modeling and enhanced backbone architectures can improve the\nrobustness of models. Furthermore, increasing the capacity of Transformer-based\nbackbones can strengthen their robustness. The benchmarks we propose and our\ncomprehensive experimental analyses can facilitate research on robust object\ndetection on aerial images. The codes and datasets are available at:\nhttps://github.com/hehaodong530/DOTA-C.\n","authors":["Haodong He","Jian Ding","Bowen Xu","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2308.15378v2.pdf","comment":"accepted by IEEE TGRS"},{"id":"http://arxiv.org/abs/2501.07087v1","updated":"2025-01-13T06:45:32Z","published":"2025-01-13T06:45:32Z","title":"Video Quality Assessment for Online Processing: From Spatial to Temporal\n  Sampling","summary":"  With the rapid development of multimedia processing and deep learning\ntechnologies, especially in the field of video understanding, video quality\nassessment (VQA) has achieved significant progress. Although researchers have\nmoved from designing efficient video quality mapping models to various research\ndirections, in-depth exploration of the effectiveness-efficiency trade-offs of\nspatio-temporal modeling in VQA models is still less sufficient. Considering\nthe fact that videos have highly redundant information, this paper investigates\nthis problem from the perspective of joint spatial and temporal sampling,\naiming to seek the answer to how little information we should keep at least\nwhen feeding videos into the VQA models while with acceptable performance\nsacrifice. To this end, we drastically sample the video's information from both\nspatial and temporal dimensions, and the heavily squeezed video is then fed\ninto a stable VQA model. Comprehensive experiments regarding joint spatial and\ntemporal sampling are conducted on six public video quality databases, and the\nresults demonstrate the acceptable performance of the VQA model when throwing\naway most of the video information. Furthermore, with the proposed joint\nspatial and temporal sampling strategy, we make an initial attempt to design an\nonline VQA model, which is instantiated by as simple as possible a spatial\nfeature extractor, a temporal feature fusion module, and a global quality\nregression module. Through quantitative and qualitative experiments, we verify\nthe feasibility of online VQA model by simplifying itself and reducing input.\n","authors":["Jiebin Yan","Lei Wu","Yuming Fang","Xuelin Liu","Xue Xia","Weide Liu"],"pdf_url":"https://arxiv.org/pdf/2501.07087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07076v1","updated":"2025-01-13T06:13:25Z","published":"2025-01-13T06:13:25Z","title":"Representation Learning of Point Cloud Upsampling in Global and Local\n  Inputs","summary":"  In recent years, point cloud upsampling has been widely applied in fields\nsuch as 3D reconstruction. Our study investigates the factors influencing point\ncloud upsampling on both global and local levels through representation\nlearning. Specifically, the paper inputs global and local information of the\nsame point cloud model object into two encoders to extract these features,\nfuses them, and then feeds the combined features into an upsampling decoder.\nThe goal is to address issues of sparsity and noise in point clouds by\nleveraging prior knowledge from both global and local inputs. And the proposed\nframework can be applied to any state-of-the-art point cloud upsampling neural\nnetwork. Experiments were conducted on a series of autoencoder-based models\nutilizing deep learning, yielding interpretability for both global and local\ninputs, and it has been proven in the results that our proposed framework can\nfurther improve the upsampling effect in previous SOTA works. At the same time,\nthe Saliency Map reflects the differences between global and local feature\ninputs, as well as the effectiveness of training with both inputs in parallel.\n","authors":["Tongxu Zhang","Bei Wang"],"pdf_url":"https://arxiv.org/pdf/2501.07076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07072v1","updated":"2025-01-13T05:57:09Z","published":"2025-01-13T05:57:09Z","title":"Label Calibration in Source Free Domain Adaptation","summary":"  Source-free domain adaptation (SFDA) utilizes a pre-trained source model with\nunlabeled target data. Self-supervised SFDA techniques generate pseudolabels\nfrom the pre-trained source model, but these pseudolabels often contain noise\ndue to domain discrepancies between the source and target domains. Traditional\nself-supervised SFDA techniques rely on deterministic model predictions using\nthe softmax function, leading to unreliable pseudolabels. In this work, we\npropose to introduce predictive uncertainty and softmax calibration for\npseudolabel refinement using evidential deep learning. The Dirichlet prior is\nplaced over the output of the target network to capture uncertainty using\nevidence with a single forward pass. Furthermore, softmax calibration solves\nthe translation invariance problem to assist in learning with noisy labels. We\nincorporate a combination of evidential deep learning loss and information\nmaximization loss with calibrated softmax in both prior and non-prior target\nknowledge SFDA settings. Extensive experimental analysis shows that our method\noutperforms other state-of-the-art methods on benchmark datasets.\n","authors":["Shivangi Rai","Rini Smita Thakur","Kunal Jangid","Vinod K Kurmi"],"pdf_url":"https://arxiv.org/pdf/2501.07072v1.pdf","comment":"Accepted in IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV) 2025"},{"id":"http://arxiv.org/abs/2501.07070v1","updated":"2025-01-13T05:48:32Z","published":"2025-01-13T05:48:32Z","title":"Enhancing Image Generation Fidelity via Progressive Prompts","summary":"  The diffusion transformer (DiT) architecture has attracted significant\nattention in image generation, achieving better fidelity, performance, and\ndiversity. However, most existing DiT - based image generation methods focus on\nglobal - aware synthesis, and regional prompt control has been less explored.\nIn this paper, we propose a coarse - to - fine generation pipeline for regional\nprompt - following generation. Specifically, we first utilize the powerful\nlarge language model (LLM) to generate both high - level descriptions of the\nimage (such as content, topic, and objects) and low - level descriptions (such\nas details and style). Then, we explore the influence of cross - attention\nlayers at different depths. We find that deeper layers are always responsible\nfor high - level content control, while shallow layers handle low - level\ncontent control. Various prompts are injected into the proposed regional cross\n- attention control for coarse - to - fine generation. By using the proposed\npipeline, we enhance the controllability of DiT - based image generation.\nExtensive quantitative and qualitative results show that our pipeline can\nimprove the performance of the generated images.\n","authors":["Zhen Xiong","Yuqi Li","Chuanguang Yang","Tiao Tan","Zhihong Zhu","Siyuan Li","Yue Ma"],"pdf_url":"https://arxiv.org/pdf/2501.07070v1.pdf","comment":"Accepted by ICASSP 2025, Github:\n  https://github.com/ZhenXiong-dl/ICASSP2025-RCAC"},{"id":"http://arxiv.org/abs/2501.07069v1","updated":"2025-01-13T05:39:43Z","published":"2025-01-13T05:39:43Z","title":"Hierarchical Superpixel Segmentation via Structural Information Theory","summary":"  Superpixel segmentation is a foundation for many higher-level computer vision\ntasks, such as image segmentation, object recognition, and scene understanding.\nExisting graph-based superpixel segmentation methods typically concentrate on\nthe relationships between a given pixel and its directly adjacent pixels while\noverlooking the influence of non-adjacent pixels. These approaches do not fully\nleverage the global information in the graph, leading to suboptimal\nsegmentation quality. To address this limitation, we present SIT-HSS, a\nhierarchical superpixel segmentation method based on structural information\ntheory. Specifically, we first design a novel graph construction strategy that\nincrementally explores the pixel neighborhood to add edges based on\n1-dimensional structural entropy (1D SE). This strategy maximizes the retention\nof graph information while avoiding an overly complex graph structure. Then, we\ndesign a new 2D SE-guided hierarchical graph partitioning method, which\niteratively merges pixel clusters layer by layer to reduce the graph's 2D SE\nuntil a predefined segmentation scale is achieved. Experimental results on\nthree benchmark datasets demonstrate that the SIT-HSS performs better than\nstate-of-the-art unsupervised superpixel segmentation algorithms. The source\ncode is available at \\url{https://github.com/SELGroup/SIT-HSS}.\n","authors":["Minhui Xie","Hao Peng","Pu Li","Guangjie Zeng","Shuhai Wang","Jia Wu","Peng Li","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2501.07069v1.pdf","comment":"Accepted by SDM 2025"},{"id":"http://arxiv.org/abs/2406.17442v3","updated":"2025-01-13T05:36:58Z","published":"2024-06-25T10:23:53Z","title":"Pamba: Enhancing Global Interaction in Point Clouds via State Space\n  Model","summary":"  Transformers have demonstrated impressive results for 3D point cloud semantic\nsegmentation. However, the quadratic complexity of transformer makes\ncomputation costs high, limiting the number of points that can be processed\nsimultaneously and impeding the modeling of long-range dependencies between\nobjects in a single scene. Drawing inspiration from the great potential of\nrecent state space models (SSM) for long sequence modeling, we introduce Mamba,\nan SSM-based architecture, to the point cloud domain and propose Pamba, a novel\narchitecture with strong global modeling capability under linear complexity.\nSpecifically, to make the disorderness of point clouds fit in with the causal\nnature of Mamba, we propose a multi-path serialization strategy applicable to\npoint clouds. Besides, we propose the ConvMamba block to compensate for the\nshortcomings of Mamba in modeling local geometries and in unidirectional\nmodeling. Pamba obtains state-of-the-art results on several 3D point cloud\nsegmentation tasks, including ScanNet v2, ScanNet200, S3DIS and nuScenes, while\nits effectiveness is validated by extensive experiments.\n","authors":["Zhuoyuan Li","Yubo Ai","Jiahao Lu","ChuXin Wang","Jiacheng Deng","Hanzhi Chang","Yanzhe Liang","Wenfei Yang","Shifeng Zhang","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.17442v3.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2410.20974v2","updated":"2025-01-13T05:06:17Z","published":"2024-10-28T12:46:05Z","title":"MovieCharacter: A Tuning-Free Framework for Controllable Character Video\n  Synthesis","summary":"  Recent advancements in character video synthesis still depend on extensive\nfine-tuning or complex 3D modeling processes, which can restrict accessibility\nand hinder real-time applicability. To address these challenges, we propose a\nsimple yet effective tuning-free framework for character video synthesis, named\nMovieCharacter, designed to streamline the synthesis process while ensuring\nhigh-quality outcomes. Our framework decomposes the synthesis task into\ndistinct, manageable modules: character segmentation and tracking, video object\nremoval, character motion imitation, and video composition. This modular design\nnot only facilitates flexible customization but also ensures that each\ncomponent operates collaboratively to effectively meet user needs. By\nleveraging existing open-source models and integrating well-established\ntechniques, MovieCharacter achieves impressive synthesis results without\nnecessitating substantial resources or proprietary datasets. Experimental\nresults demonstrate that our framework enhances the efficiency, accessibility,\nand adaptability of character video synthesis, paving the way for broader\ncreative and interactive applications.\n","authors":["Di Qiu","Zheng Chen","Rui Wang","Mingyuan Fan","Changqian Yu","Junshi Huang","Xiang Wen"],"pdf_url":"https://arxiv.org/pdf/2410.20974v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16837v2","updated":"2025-01-13T05:04:59Z","published":"2024-07-23T21:02:38Z","title":"MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs","summary":"  The ability to compare objects, scenes, or situations is crucial for\neffective decision-making and problem-solving in everyday life. For instance,\ncomparing the freshness of apples enables better choices during grocery\nshopping while comparing sofa designs helps optimize the aesthetics of our\nliving space. Despite its significance, the comparative capability is largely\nunexplored in artificial general intelligence (AGI). In this paper, we\nintroduce MLLM-CompBench, a benchmark designed to evaluate the comparative\nreasoning capability of multimodal large language models (MLLMs).\nMLLM-CompBench mines and pairs images through visually oriented questions\ncovering eight dimensions of relative comparison: visual attribute, existence,\nstate, emotion, temporality, spatiality, quantity, and quality. We curate a\ncollection of around 40K image pairs using metadata from diverse vision\ndatasets and CLIP similarity scores. These image pairs span a broad array of\nvisual domains, including animals, fashion, sports, and both outdoor and indoor\nscenes. The questions are carefully crafted to discern relative characteristics\nbetween two images and are labeled by human annotators for accuracy and\nrelevance. We use MLLM-CompBench to evaluate recent MLLMs, including\nGPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our results reveal notable\nshortcomings in their comparative abilities. We believe MLLM-COMPBENCH not only\nsheds light on these limitations but also establishes a solid foundation for\nfuture enhancements in the comparative capability of MLLMs.\n","authors":["Jihyung Kil","Zheda Mai","Justin Lee","Zihe Wang","Kerrie Cheng","Lemeng Wang","Ye Liu","Arpita Chowdhury","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2407.16837v2.pdf","comment":"This paper has been accepted to NeurIPS 2024. The first two authors\n  contributed equally to this work"},{"id":"http://arxiv.org/abs/2501.07055v1","updated":"2025-01-13T04:30:41Z","published":"2025-01-13T04:30:41Z","title":"SFC-GAN: A Generative Adversarial Network for Brain Functional and\n  Structural Connectome Translation","summary":"  Modern brain imaging technologies have enabled the detailed reconstruction of\nhuman brain connectomes, capturing structural connectivity (SC) from diffusion\nMRI and functional connectivity (FC) from functional MRI. Understanding the\nintricate relationships between SC and FC is vital for gaining deeper insights\ninto the brain's functional and organizational mechanisms. However, obtaining\nboth SC and FC modalities simultaneously remains challenging, hindering\ncomprehensive analyses. Existing deep generative models typically focus on\nsynthesizing a single modality or unidirectional translation between FC and SC,\nthereby missing the potential benefits of bi-directional translation,\nespecially in scenarios where only one connectome is available. Therefore, we\npropose Structural-Functional Connectivity GAN (SFC-GAN), a novel framework for\nbidirectional translation between SC and FC. This approach leverages the\nCycleGAN architecture, incorporating convolutional layers to effectively\ncapture the spatial structures of brain connectomes. To preserve the\ntopological integrity of these connectomes, we employ a structure-preserving\nloss that guides the model in capturing both global and local connectome\npatterns while maintaining symmetry. Our framework demonstrates superior\nperformance in translating between SC and FC, outperforming baseline models in\nsimilarity and graph property evaluations compared to ground truth data, each\ntranslated modality can be effectively utilized for downstream classification.\n","authors":["Yee-Fan Tan","Jun Lin Liow","Pei-Sze Tan","Fuad Noman","Raphael C. -W. Phan","Hernando Ombao","Chee-Ming Ting"],"pdf_url":"https://arxiv.org/pdf/2501.07055v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2411.11477v3","updated":"2025-01-13T04:22:25Z","published":"2024-11-18T11:26:11Z","title":"SL-YOLO: A Stronger and Lighter Drone Target Detection Model","summary":"  Detecting small objects in complex scenes, such as those captured by drones,\nis a daunting challenge due to the difficulty in capturing the complex features\nof small targets. While the YOLO family has achieved great success in large\ntarget detection, its performance is less than satisfactory when faced with\nsmall targets. Because of this, this paper proposes a revolutionary model\nSL-YOLO (Stronger and Lighter YOLO) that aims to break the bottleneck of small\ntarget detection. We propose the Hierarchical Extended Path Aggregation Network\n(HEPAN), a pioneering cross-scale feature fusion method that can ensure\nunparalleled detection accuracy even in the most challenging environments. At\nthe same time, without sacrificing detection capabilities, we design the C2fDCB\nlightweight module and add the SCDown downsampling module to greatly reduce the\nmodel's parameters and computational complexity. Our experimental results on\nthe VisDrone2019 dataset reveal a significant improvement in performance, with\nmAP@0.5 jumping from 43.0% to 46.9% and mAP@0.5:0.95 increasing from 26.0% to\n28.9%. At the same time, the model parameters are reduced from 11.1M to 9.6M,\nand the FPS can reach 132, making it an ideal solution for real-time small\nobject detection in resource-constrained environments.\n","authors":["Defan Chen","Luchan Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.11477v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20870v2","updated":"2025-01-13T04:11:06Z","published":"2024-12-30T11:16:49Z","title":"SoftPatch+: Fully Unsupervised Anomaly Classification and Segmentation","summary":"  Although mainstream unsupervised anomaly detection (AD) (including\nimage-level classification and pixel-level segmentation)algorithms perform well\nin academic datasets, their performance is limited in practical application due\nto the ideal experimental setting of clean training data. Training with noisy\ndata is an inevitable problem in real-world anomaly detection but is seldom\ndiscussed. This paper is the first to consider fully unsupervised industrial\nanomaly detection (i.e., unsupervised AD with noisy data). To solve this\nproblem, we proposed memory-based unsupervised AD methods, SoftPatch and\nSoftPatch+, which efficiently denoise the data at the patch level. Noise\ndiscriminators are utilized to generate outlier scores for patch-level noise\nelimination before coreset construction. The scores are then stored in the\nmemory bank to soften the anomaly detection boundary. Compared with existing\nmethods, SoftPatch maintains a strong modeling ability of normal data and\nalleviates the overconfidence problem in coreset, and SoftPatch+ has more\nrobust performance which is articularly useful in real-world industrial\ninspection scenarios with high levels of noise (from 10% to 40%). Comprehensive\nexperiments conducted in diverse noise scenarios demonstrate that both\nSoftPatch and SoftPatch+ outperform the state-of-the-art AD methods on the\nMVTecAD, ViSA, and BTAD benchmarks. Furthermore, the performance of SoftPatch\nand SoftPatch+ is comparable to that of the noise-free methods in conventional\nunsupervised AD setting. The code of the proposed methods can be found at\nhttps://github.com/TencentYoutuResearch/AnomalyDetection-SoftPatch.\n","authors":["Chengjie Wang","Xi Jiang","Bin-Bin Gao","Zhenye Gan","Yong Liu","Feng Zheng","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2412.20870v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2403.14233\n  paper has been accepted by Pattern Recognition"},{"id":"http://arxiv.org/abs/2501.07044v1","updated":"2025-01-13T03:54:19Z","published":"2025-01-13T03:54:19Z","title":"Protego: Detecting Adversarial Examples for Vision Transformers via\n  Intrinsic Capabilities","summary":"  Transformer models have excelled in natural language tasks, prompting the\nvision community to explore their implementation in computer vision problems.\nHowever, these models are still influenced by adversarial examples. In this\npaper, we investigate the attack capabilities of six common adversarial attacks\non three pretrained ViT models to reveal the vulnerability of ViT models. To\nunderstand and analyse the bias in neural network decisions when the input is\nadversarial, we use two visualisation techniques that are attention rollout and\ngrad attention rollout. To prevent ViT models from adversarial attack, we\npropose Protego, a detection framework that leverages the transformer intrinsic\ncapabilities to detection adversarial examples of ViT models. Nonetheless, this\nis challenging due to a diversity of attack strategies that may be adopted by\nadversaries. Inspired by the attention mechanism, we know that the token of\nprediction contains all the information from the input sample. Additionally,\nthe attention region for adversarial examples differs from that of normal\nexamples. Given these points, we can train a detector that achieves superior\nperformance than existing detection methods to identify adversarial examples.\nOur experiments have demonstrated the high effectiveness of our detection\nmethod. For these six adversarial attack methods, our detector's AUC scores all\nexceed 0.95. Protego may advance investigations in metaverse security.\n","authors":["Jialin Wu","Kaikai Pan","Yanjiao Chen","Jiangyi Deng","Shengyuan Pang","Wenyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2501.07044v1.pdf","comment":"Accepted by IEEE MetaCom 2024"},{"id":"http://arxiv.org/abs/2501.07040v1","updated":"2025-01-13T03:43:21Z","published":"2025-01-13T03:43:21Z","title":"Rethinking Knowledge in Distillation: An In-context Sample Retrieval\n  Perspective","summary":"  Conventional knowledge distillation (KD) approaches are designed for the\nstudent model to predict similar output as the teacher model for each sample.\nUnfortunately, the relationship across samples with same class is often\nneglected. In this paper, we explore to redefine the knowledge in distillation,\ncapturing the relationship between each sample and its corresponding in-context\nsamples (a group of similar samples with the same or different classes), and\nperform KD from an in-context sample retrieval perspective. As KD is a type of\nlearned label smoothing regularization (LSR), we first conduct a theoretical\nanalysis showing that the teacher's knowledge from the in-context samples is a\ncrucial contributor to regularize the student training with the corresponding\nsamples. Buttressed by the analysis, we propose a novel in-context knowledge\ndistillation (IC-KD) framework that shows its superiority across diverse KD\nparadigms (offline, online, and teacher-free KD). Firstly, we construct a\nfeature memory bank from the teacher model and retrieve in-context samples for\neach corresponding sample through retrieval-based learning. We then introduce\nPositive In-Context Distillation (PICD) to reduce the discrepancy between a\nsample from the student and the aggregated in-context samples with the same\nclass from the teacher in the logit space. Moreover, Negative In-Context\nDistillation (NICD) is introduced to separate a sample from the student and the\nin-context samples with different classes from the teacher in the logit space.\nExtensive experiments demonstrate that IC-KD is effective across various types\nof KD, and consistently achieves state-of-the-art performance on CIFAR-100 and\nImageNet datasets.\n","authors":["Jinjing Zhu","Songze Li","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2501.07040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07039v1","updated":"2025-01-13T03:41:57Z","published":"2025-01-13T03:41:57Z","title":"IoT-Based Real-Time Medical-Related Human Activity Recognition Using\n  Skeletons and Multi-Stage Deep Learning for Healthcare","summary":"  The Internet of Things (IoT) and mobile technology have significantly\ntransformed healthcare by enabling real-time monitoring and diagnosis of\npatients. Recognizing medical-related human activities (MRHA) is pivotal for\nhealthcare systems, particularly for identifying actions that are critical to\npatient well-being. However, challenges such as high computational demands, low\naccuracy, and limited adaptability persist in Human Motion Recognition (HMR).\nWhile some studies have integrated HMR with IoT for real-time healthcare\napplications, limited research has focused on recognizing MRHA as essential for\neffective patient monitoring. This study proposes a novel HMR method for MRHA\ndetection, leveraging multi-stage deep learning techniques integrated with IoT.\nThe approach employs EfficientNet to extract optimized spatial features from\nskeleton frame sequences using seven Mobile Inverted Bottleneck Convolutions\n(MBConv) blocks, followed by ConvLSTM to capture spatio-temporal patterns. A\nclassification module with global average pooling, a fully connected layer, and\na dropout layer generates the final predictions. The model is evaluated on the\nNTU RGB+D 120 and HMDB51 datasets, focusing on MRHA, such as sneezing, falling,\nwalking, sitting, etc. It achieves 94.85% accuracy for cross-subject\nevaluations and 96.45% for cross-view evaluations on NTU RGB+D 120, along with\n89.00% accuracy on HMDB51. Additionally, the system integrates IoT capabilities\nusing a Raspberry Pi and GSM module, delivering real-time alerts via Twilios\nSMS service to caregivers and patients. This scalable and efficient solution\nbridges the gap between HMR and IoT, advancing patient monitoring, improving\nhealthcare outcomes, and reducing costs.\n","authors":["Subrata Kumer Paul","Abu Saleh Musa Miah","Rakhi Rani Paul","Md. Ekramul Hamid","Jungpil Shin","Md Abdur Rahim"],"pdf_url":"https://arxiv.org/pdf/2501.07039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04184v2","updated":"2025-01-13T03:33:36Z","published":"2025-01-07T23:32:05Z","title":"MedicalNarratives: Connecting Medical Vision and Language with Localized\n  Narratives","summary":"  We propose MedicalNarratives, a dataset curated from medical pedagogical\nvideos similar in nature to data collected in Think-Aloud studies and inspired\nby Localized Narratives, which collects grounded image-text data by curating\ninstructors' speech and mouse cursor movements synchronized in time.\nMedicalNarratives enables pretraining of both semantic and dense objectives,\nalleviating the need to train medical semantic and dense tasks disparately due\nto the lack of reasonably sized datasets. Our dataset contains 4.7M image-text\npairs from videos and articles, with 1M samples containing dense annotations in\nthe form of traces and bounding boxes. To evaluate the utility of\nMedicalNarratives, we train GenMedClip based on the CLIP architecture using our\ndataset spanning 12 medical domains and demonstrate that it outperforms\nprevious state-of-the-art models on a newly constructed medical imaging\nbenchmark that comprehensively evaluates performance across all modalities.\nData, demo, code and models available at https://medical-narratives.github.io\n","authors":["Wisdom O. Ikezogwo","Kevin Zhang","Mehmet Saygin Seyfioglu","Fatemeh Ghezloo","Linda Shapiro","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2501.04184v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09792v3","updated":"2025-01-13T03:30:37Z","published":"2024-03-14T18:24:55Z","title":"Images are Achilles' Heel of Alignment: Exploiting Visual\n  Vulnerabilities for Jailbreaking Multimodal Large Language Models","summary":"  In this paper, we study the harmlessness alignment problem of multimodal\nlarge language models (MLLMs). We conduct a systematic empirical analysis of\nthe harmlessness performance of representative MLLMs and reveal that the image\ninput poses the alignment vulnerability of MLLMs. Inspired by this, we propose\na novel jailbreak method named HADES, which hides and amplifies the harmfulness\nof the malicious intent within the text input, using meticulously crafted\nimages. Experimental results show that HADES can effectively jailbreak existing\nMLLMs, which achieves an average Attack Success Rate (ASR) of 90.26% for\nLLaVA-1.5 and 71.60% for Gemini Pro Vision. Our code and data are available at\nhttps://github.com/RUCAIBox/HADES.\n","authors":["Yifan Li","Hangyu Guo","Kun Zhou","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.09792v3.pdf","comment":"ECCV 2024 Oral"},{"id":"http://arxiv.org/abs/2501.07033v1","updated":"2025-01-13T03:10:54Z","published":"2025-01-13T03:10:54Z","title":"Detection of AI Deepfake and Fraud in Online Payments Using GAN-Based\n  Models","summary":"  This study explores the use of Generative Adversarial Networks (GANs) to\ndetect AI deepfakes and fraudulent activities in online payment systems. With\nthe growing prevalence of deepfake technology, which can manipulate facial\nfeatures in images and videos, the potential for fraud in online transactions\nhas escalated. Traditional security systems struggle to identify these\nsophisticated forms of fraud. This research proposes a novel GAN-based model\nthat enhances online payment security by identifying subtle manipulations in\npayment images. The model is trained on a dataset consisting of real-world\nonline payment images and deepfake images generated using advanced GAN\narchitectures, such as StyleGAN and DeepFake. The results demonstrate that the\nproposed model can accurately distinguish between legitimate transactions and\ndeepfakes, achieving a high detection rate above 95%. This approach\nsignificantly improves the robustness of payment systems against AI-driven\nfraud. The paper contributes to the growing field of digital security, offering\ninsights into the application of GANs for fraud detection in financial\nservices. Keywords- Payment Security, Image Recognition, Generative Adversarial\nNetworks, AI Deepfake, Fraudulent Activities\n","authors":["Zong Ke","Shicheng Zhou","Yining Zhou","Chia Hong Chang","Rong Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.07033v1.pdf","comment":"The paper will be published and indexed by IEEE at 2025 8th\n  International Conference on Advanced Algorithms and Control Engineering\n  (ICAACE 2025)"},{"id":"http://arxiv.org/abs/2411.19714v2","updated":"2025-01-13T02:43:47Z","published":"2024-11-29T14:02:00Z","title":"The Streetscape Application Services Stack (SASS): Towards a Distributed\n  Sensing Architecture for Urban Applications","summary":"  As urban populations grow, cities are becoming more complex, driving the\ndeployment of interconnected sensing systems to realize the vision of smart\ncities. These systems aim to improve safety, mobility, and quality of life\nthrough applications that integrate diverse sensors with real-time\ndecision-making. Streetscape applications-focusing on challenges like\npedestrian safety and adaptive traffic management-depend on managing\ndistributed, heterogeneous sensor data, aligning information across time and\nspace, and enabling real-time processing. These tasks are inherently complex\nand often difficult to scale. The Streetscape Application Services Stack (SASS)\naddresses these challenges with three core services: multimodal data\nsynchronization, spatiotemporal data fusion, and distributed edge computing. By\nstructuring these capabilities as clear, composable abstractions with clear\nsemantics, SASS allows developers to scale streetscape applications efficiently\nwhile minimizing the complexity of multimodal integration.\n  We evaluated SASS in two real-world testbed environments: a controlled\nparking lot and an urban intersection in a major U.S. city. These testbeds\nallowed us to test SASS under diverse conditions, demonstrating its practical\napplicability. The Multimodal Data Synchronization service reduced temporal\nmisalignment errors by 88%, achieving synchronization accuracy within 50\nmilliseconds. Spatiotemporal Data Fusion service improved detection accuracy\nfor pedestrians and vehicles by over 10%, leveraging multicamera integration.\nThe Distributed Edge Computing service increased system throughput by more than\nan order of magnitude. Together, these results show how SASS provides the\nabstractions and performance needed to support real-time, scalable urban\napplications, bridging the gap between sensing infrastructure and actionable\nstreetscape intelligence.\n","authors":["Navid Salami Pargoo","Mahshid Ghasemi","Shuren Xia","Mehmet Kerem Turkcan","Taqiya Ehsan","Chengbo Zang","Yuan Sun","Javad Ghaderi","Gil Zussman","Zoran Kostic","Jorge Ortiz"],"pdf_url":"https://arxiv.org/pdf/2411.19714v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05901v2","updated":"2025-01-13T02:34:19Z","published":"2025-01-10T11:53:46Z","title":"Valley2: Exploring Multimodal Models with Scalable Vision-Language\n  Design","summary":"  Recently, vision-language models have made remarkable progress, demonstrating\noutstanding capabilities in various tasks such as image captioning and video\nunderstanding. We introduce Valley2, a novel multimodal large language model\ndesigned to enhance performance across all domains and extend the boundaries of\npractical applications in e-commerce and short video scenarios. Notably,\nValley2 achieves state-of-the-art (SOTA) performance on e-commerce benchmarks,\nsurpassing open-source models of similar size by a large margin (79.66 vs.\n72.76). Additionally, Valley2 ranks second on the OpenCompass leaderboard among\nmodels with fewer than 10B parameters, with an impressive average score of\n67.4. The code and model weights are open-sourced at\nhttps://github.com/bytedance/Valley.\n","authors":["Ziheng Wu","Zhenghao Chen","Ruipu Luo","Can Zhang","Yuan Gao","Zhentao He","Xian Wang","Haoran Lin","Minghui Qiu"],"pdf_url":"https://arxiv.org/pdf/2501.05901v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07017v1","updated":"2025-01-13T02:33:28Z","published":"2025-01-13T02:33:28Z","title":"UNetVL: Enhancing 3D Medical Image Segmentation with Chebyshev KAN\n  Powered Vision-LSTM","summary":"  3D medical image segmentation has progressed considerably due to\nConvolutional Neural Networks (CNNs) and Vision Transformers (ViTs), yet these\nmethods struggle to balance long-range dependency acquisition with\ncomputational efficiency. To address this challenge, we propose UNETVL (U-Net\nVision-LSTM), a novel architecture that leverages recent advancements in\ntemporal information processing. UNETVL incorporates Vision-LSTM (ViL) for\nimproved scalability and memory functions, alongside an efficient Chebyshev\nKolmogorov-Arnold Networks (KAN) to handle complex and long-range dependency\npatterns more effectively. We validated our method on the ACDC and AMOS2022\n(post challenge Task 2) benchmark datasets, showing a significant improvement\nin mean Dice score compared to recent state-of-the-art approaches, especially\nover its predecessor, UNETR, with increases of 7.3% on ACDC and 15.6% on AMOS,\nrespectively. Extensive ablation studies were conducted to demonstrate the\nimpact of each component in UNETVL, providing a comprehensive understanding of\nits architecture. Our code is available at https://github.com/tgrex6/UNETVL,\nfacilitating further research and applications in this domain.\n","authors":["Xuhui Guo","Tanmoy Dam","Rohan Dhamdhere","Gourav Modanwal","Anant Madabhushi"],"pdf_url":"https://arxiv.org/pdf/2501.07017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07016v1","updated":"2025-01-13T02:29:42Z","published":"2025-01-13T02:29:42Z","title":"A Multi-Modal Deep Learning Framework for Pan-Cancer Prognosis","summary":"  Prognostic task is of great importance as it closely related to the survival\nanalysis of patients, the optimization of treatment plans and the allocation of\nresources. The existing prognostic models have shown promising results on\nspecific datasets, but there are limitations in two aspects. On the one hand,\nthey merely explore certain types of modal data, such as patient histopathology\nWSI and gene expression analysis. On the other hand, they adopt the\nper-cancer-per-model paradigm, which means the trained models can only predict\nthe prognostic effect of a single type of cancer, resulting in weak\ngeneralization ability. In this paper, a deep-learning based model, named\nUMPSNet, is proposed. Specifically, to comprehensively understand the condition\nof patients, in addition to constructing encoders for histopathology images and\ngenomic expression profiles respectively, UMPSNet further integrates four types\nof important meta data (demographic information, cancer type information,\ntreatment protocols, and diagnosis results) into text templates, and then\nintroduces a text encoder to extract textual features. In addition, the optimal\ntransport OT-based attention mechanism is utilized to align and fuse features\nof different modalities. Furthermore, a guided soft mixture of experts (GMoE)\nmechanism is introduced to effectively address the issue of distribution\ndifferences among multiple cancer datasets. By incorporating the multi-modality\nof patient data and joint training, UMPSNet outperforms all SOTA approaches,\nand moreover, it demonstrates the effectiveness and generalization ability of\nthe proposed learning paradigm of a single model for multiple cancer types. The\ncode of UMPSNet is available at https://github.com/binging512/UMPSNet.\n","authors":["Binyu Zhang","Shichao Li","Junpeng Jian","Zhu Meng","Limei Guo","Zhicheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.07016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07015v1","updated":"2025-01-13T02:28:13Z","published":"2025-01-13T02:28:13Z","title":"SplatMAP: Online Dense Monocular SLAM with 3D Gaussian Splatting","summary":"  Achieving high-fidelity 3D reconstruction from monocular video remains\nchallenging due to the inherent limitations of traditional methods like\nStructure-from-Motion (SfM) and monocular SLAM in accurately capturing scene\ndetails. While differentiable rendering techniques such as Neural Radiance\nFields (NeRF) address some of these challenges, their high computational costs\nmake them unsuitable for real-time applications. Additionally, existing 3D\nGaussian Splatting (3DGS) methods often focus on photometric consistency,\nneglecting geometric accuracy and failing to exploit SLAM's dynamic depth and\npose updates for scene refinement. We propose a framework integrating dense\nSLAM with 3DGS for real-time, high-fidelity dense reconstruction. Our approach\nintroduces SLAM-Informed Adaptive Densification, which dynamically updates and\ndensifies the Gaussian model by leveraging dense point clouds from SLAM.\nAdditionally, we incorporate Geometry-Guided Optimization, which combines\nedge-aware geometric constraints and photometric consistency to jointly\noptimize the appearance and geometry of the 3DGS scene representation, enabling\ndetailed and accurate SLAM mapping reconstruction. Experiments on the Replica\nand TUM-RGBD datasets demonstrate the effectiveness of our approach, achieving\nstate-of-the-art results among monocular systems. Specifically, our method\nachieves a PSNR of 36.864, SSIM of 0.985, and LPIPS of 0.040 on Replica,\nrepresenting improvements of 10.7%, 6.4%, and 49.4%, respectively, over the\nprevious SOTA. On TUM-RGBD, our method outperforms the closest baseline by\n10.2%, 6.6%, and 34.7% in the same metrics. These results highlight the\npotential of our framework in bridging the gap between photometric and\ngeometric dense 3D scene representations, paving the way for practical and\nefficient monocular dense reconstruction.\n","authors":["Yue Hu","Rong Liu","Meida Chen","Andrew Feng","Peter Beerel"],"pdf_url":"https://arxiv.org/pdf/2501.07015v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12185v4","updated":"2025-01-13T02:14:51Z","published":"2024-02-19T14:48:23Z","title":"ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for\n  Complicated Chart Reasoning","summary":"  Recently, many versatile Multi-modal Large Language Models (MLLMs) have\nemerged continuously. However, their capacity to query information depicted in\nvisual charts and engage in reasoning based on the queried contents remains\nunder-explored. In this paper, to comprehensively and rigorously benchmark the\nability of the off-the-shelf MLLMs in the chart domain, we construct ChartX, a\nmulti-modal evaluation set covering 18 chart types, 7 chart tasks, 22\ndisciplinary topics, and high-quality chart data. Besides, we develop ChartVLM\nto offer a new perspective on handling multi-modal tasks that strongly depend\non interpretable patterns, such as reasoning tasks in the field of charts or\ngeometric images. We evaluate the chart-related ability of mainstream MLLMs and\nour ChartVLM on the proposed ChartX evaluation set. Extensive experiments\ndemonstrate that ChartVLM surpasses both versatile and chart-related large\nmodels, achieving results comparable to GPT-4V. We believe that our study can\npave the way for further exploration in creating a more comprehensive chart\nevaluation set and developing more interpretable multi-modal models. Both\nChartX and ChartVLM are available at:\nhttps://github.com/Alpha-Innovator/ChartVLM\n","authors":["Renqiu Xia","Bo Zhang","Hancheng Ye","Xiangchao Yan","Qi Liu","Hongbin Zhou","Zijun Chen","Min Dou","Botian Shi","Junchi Yan","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2402.12185v4.pdf","comment":"Code and dataset are available for downloading at:\n  https://github.com/Alpha-Innovator/ChartVLM 25 pages, 15 figures"},{"id":"http://arxiv.org/abs/2501.02763v2","updated":"2025-01-13T01:21:29Z","published":"2025-01-06T05:14:40Z","title":"LDMapNet-U: An End-to-End System for City-Scale Lane-Level Map Updating","summary":"  An up-to-date city-scale lane-level map is an indispensable infrastructure\nand a key enabling technology for ensuring the safety and user experience of\nautonomous driving systems. In industrial scenarios, reliance on manual\nannotation for map updates creates a critical bottleneck. Lane-level updates\nrequire precise change information and must ensure consistency with adjacent\ndata while adhering to strict standards. Traditional methods utilize a\nthree-stage approach-construction, change detection, and updating-which often\nnecessitates manual verification due to accuracy limitations. This results in\nlabor-intensive processes and hampers timely updates. To address these\nchallenges, we propose LDMapNet-U, which implements a new end-to-end paradigm\nfor city-scale lane-level map updating. By reconceptualizing the update task as\nan end-to-end map generation process grounded in historical map data, we\nintroduce a paradigm shift in map updating that simultaneously generates\nvectorized maps and change information. To achieve this, a Prior-Map Encoding\n(PME) module is introduced to effectively encode historical maps, serving as a\ncritical reference for detecting changes. Additionally, we incorporate a novel\nInstance Change Prediction (ICP) module that learns to predict associations\nwith historical maps. Consequently, LDMapNet-U simultaneously achieves\nvectorized map element generation and change detection. To demonstrate the\nsuperiority and effectiveness of LDMapNet-U, extensive experiments are\nconducted using large-scale real-world datasets. In addition, LDMapNet-U has\nbeen successfully deployed in production at Baidu Maps since April 2024,\nsupporting map updating for over 360 cities and significantly shortening the\nupdate cycle from quarterly to weekly. The updated maps serve hundreds of\nmillions of users and are integrated into the autonomous driving systems of\nseveral leading vehicle companies.\n","authors":["Deguo Xia","Weiming Zhang","Xiyan Liu","Wei Zhang","Chenting Gong","Xiao Tan","Jizhou Huang","Mengmeng Yang","Diange Yang"],"pdf_url":"https://arxiv.org/pdf/2501.02763v2.pdf","comment":"Accepted by KDD 2025, camera-ready version"},{"id":"http://arxiv.org/abs/2501.06986v1","updated":"2025-01-13T00:29:55Z","published":"2025-01-13T00:29:55Z","title":"LEO: Boosting Mixture of Vision Encoders for Multimodal Large Language\n  Models","summary":"  Enhanced visual understanding serves as a cornerstone for multimodal large\nlanguage models (MLLMs). Recent hybrid MLLMs incorporate a mixture of vision\nexperts to address the limitations of using a single vision encoder and\nexcessively long visual tokens. Despite the progress of these MLLMs, a research\ngap remains in effectively integrating diverse vision encoders. This work\nexplores fusion strategies of visual tokens for hybrid MLLMs, leading to the\ndesign of LEO, a novel MLLM with a dual-branch vision encoder framework that\nincorporates a post-adaptation fusion strategy and adaptive tiling: for each\nsegmented tile of the input images, LEO sequentially interleaves the visual\ntokens from its two vision encoders. Extensive evaluation across 13\nvision-language benchmarks reveals that LEO outperforms state-of-the-art\nopen-source MLLMs and hybrid MLLMs on the majority of tasks. Furthermore, we\nshow that LEO can be adapted to the specialized domain of autonomous driving\nwithout altering the model architecture or training recipe, achieving\ncompetitive performance compared to existing baselines. The code and model will\nbe publicly available.\n","authors":["Mozhgan Nasr Azadani","James Riddell","Sean Sedwards","Krzysztof Czarnecki"],"pdf_url":"https://arxiv.org/pdf/2501.06986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07754v1","updated":"2025-01-13T23:55:11Z","published":"2025-01-13T23:55:11Z","title":"Universal Training of Neural Networks to Achieve Bayes Optimal\n  Classification Accuracy","summary":"  This work invokes the notion of $f$-divergence to introduce a novel upper\nbound on the Bayes error rate of a general classification task. We show that\nthe proposed bound can be computed by sampling from the output of a\nparameterized model. Using this practical interpretation, we introduce the\nBayes optimal learning threshold (BOLT) loss whose minimization enforces a\nclassification model to achieve the Bayes error rate. We validate the proposed\nloss for image and text classification tasks, considering MNIST, Fashion-MNIST,\nCIFAR-10, and IMDb datasets. Numerical experiments demonstrate that models\ntrained with BOLT achieve performance on par with or exceeding that of\ncross-entropy, particularly on challenging datasets. This highlights the\npotential of BOLT in improving generalization.\n","authors":["Mohammadreza Tavasoli Naeini","Ali Bereyhi","Morteza Noshad","Ben Liang","Alfred O. Hero III"],"pdf_url":"https://arxiv.org/pdf/2501.07754v1.pdf","comment":"Accepted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2410.20631v2","updated":"2025-01-13T23:45:51Z","published":"2024-10-27T23:29:46Z","title":"PViT: Prior-augmented Vision Transformer for Out-of-distribution\n  Detection","summary":"  Vision Transformers (ViTs) have achieved remarkable success over various\nvision tasks, yet their robustness against data distribution shifts and\ninherent inductive biases remain underexplored. To enhance the robustness of\nViT models for image Out-of-Distribution (OOD) detection, we introduce a novel\nand generic framework named Prior-augmented Vision Transformer (PViT). Taking\nas input the prior class logits from a pretrained model, we train PViT to\npredict the class logits. During inference, PViT identifies OOD samples by\nquantifying the divergence between the predicted class logits and the prior\nlogits obtained from pre-trained models. Unlike existing state-of-the-art(SOTA)\nOOD detection methods, PViT shapes the decision boundary between ID and OOD by\nutilizing the proposed prior guided confidence, without requiring additional\ndata modeling, generation methods, or structural modifications. Extensive\nexperiments on the large-scale ImageNet benchmark, evaluated against over seven\nOOD datasets, demonstrate that PViT significantly outperforms existing SOTA OOD\ndetection methods in terms of FPR95 and AUROC. The codebase is publicly\navailable at https://github.com/RanchoGoose/PViT.\n","authors":["Tianhao Zhang","Zhixiang Chen","Lyudmila S. Mihaylova"],"pdf_url":"https://arxiv.org/pdf/2410.20631v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00142v2","updated":"2025-01-13T23:45:26Z","published":"2024-11-28T18:55:41Z","title":"Sparse Attention Vectors: Generative Multimodal Model Features Are\n  Discriminative Vision-Language Classifiers","summary":"  Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a\nwide variety of vision-language (VL) tasks such as image captioning or visual\nquestion answering. Despite strong performance, LMMs are not directly suited\nfor foundational discriminative vision-language tasks (i.e., tasks requiring\ndiscrete label predictions) such as image classification and multiple-choice\nVQA. One key challenge in utilizing LMMs for discriminative tasks is the\nextraction of useful features from generative models. To overcome this issue,\nwe propose an approach for finding features in the model's latent space to more\neffectively leverage LMMs for discriminative tasks. Toward this end, we present\nSparse Attention Vectors (SAVs) -- a finetuning-free method that leverages\nsparse attention head activations (fewer than 1\\% of the heads) in LMMs as\nstrong features for VL tasks. With only few-shot examples, SAVs demonstrate\nstate-of-the-art performance compared to a variety of few-shot and finetuned\nbaselines on a collection of discriminative tasks. Our experiments also imply\nthat SAVs can scale in performance with additional examples and generalize to\nsimilar tasks, establishing SAVs as both effective and robust multimodal\nfeature representations.\n","authors":["Chancharik Mitra","Brandon Huang","Tianning Chai","Zhiqiu Lin","Assaf Arbelle","Rogerio Feris","Leonid Karlinsky","Trevor Darrell","Deva Ramanan","Roei Herzig"],"pdf_url":"https://arxiv.org/pdf/2412.00142v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07750v1","updated":"2025-01-13T23:38:49Z","published":"2025-01-13T23:38:49Z","title":"Boosting Sclera Segmentation through Semi-supervised Learning with Fewer\n  Labels","summary":"  Sclera segmentation is crucial for developing automatic eye-related medical\ncomputer-aided diagnostic systems, as well as for personal identification and\nverification, because the sclera contains distinct personal features. Deep\nlearning-based sclera segmentation has achieved significant success compared to\ntraditional methods that rely on hand-crafted features, primarily because it\ncan autonomously extract critical output-related features without the need to\nconsider potential physical constraints. However, achieving accurate sclera\nsegmentation using these methods is challenging due to the scarcity of\nhigh-quality, fully labeled datasets, which depend on costly, labor-intensive\nmedical acquisition and expertise. To address this challenge, this paper\nintroduces a novel sclera segmentation framework that excels with limited\nlabeled samples. Specifically, we employ a semi-supervised learning method that\nintegrates domain-specific improvements and image-based spatial transformations\nto enhance segmentation performance. Additionally, we have developed a\nreal-world eye diagnosis dataset to enrich the evaluation process. Extensive\nexperiments on our dataset and two additional public datasets demonstrate the\neffectiveness and superiority of our proposed method, especially with\nsignificantly fewer labeled samples.\n","authors":["Guanjun Wang","Lu Wang","Ning Niu","Qiaoyi Yao","Yixuan Wang","Sufen Ren","Shengchao Chen"],"pdf_url":"https://arxiv.org/pdf/2501.07750v1.pdf","comment":"Under review, 19 pages, 9 figures, 4 tables"},{"id":"http://arxiv.org/abs/2501.07746v1","updated":"2025-01-13T23:21:33Z","published":"2025-01-13T23:21:33Z","title":"A Heterogeneous Multimodal Graph Learning Framework for Recognizing User\n  Emotions in Social Networks","summary":"  The rapid expansion of social media platforms has provided unprecedented\naccess to massive amounts of multimodal user-generated content. Comprehending\nuser emotions can provide valuable insights for improving communication and\nunderstanding of human behaviors. Despite significant advancements in Affective\nComputing, the diverse factors influencing user emotions in social networks\nremain relatively understudied. Moreover, there is a notable lack of deep\nlearning-based methods for predicting user emotions in social networks, which\ncould be addressed by leveraging the extensive multimodal data available. This\nwork presents a novel formulation of personalized emotion prediction in social\nnetworks based on heterogeneous graph learning. Building upon this formulation,\nwe design HMG-Emo, a Heterogeneous Multimodal Graph Learning Framework that\nutilizes deep learning-based features for user emotion recognition.\nAdditionally, we include a dynamic context fusion module in HMG-Emo that is\ncapable of adaptively integrating the different modalities in social media\ndata. Through extensive experiments, we demonstrate the effectiveness of\nHMG-Emo and verify the superiority of adopting a graph neural network-based\napproach, which outperforms existing baselines that use rich hand-crafted\nfeatures. To the best of our knowledge, HMG-Emo is the first multimodal and\ndeep-learning-based approach to predict personalized emotions within online\nsocial networks. Our work highlights the significance of exploiting advanced\ndeep learning techniques for less-explored problems in Affective Computing.\n","authors":["Sree Bhattacharyya","Shuhua Yang","James Z. Wang"],"pdf_url":"https://arxiv.org/pdf/2501.07746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07742v1","updated":"2025-01-13T23:13:33Z","published":"2025-01-13T23:13:33Z","title":"Fixing the Scale and Shift in Monocular Depth For Camera Pose Estimation","summary":"  Recent advances in monocular depth prediction have led to significantly\nimproved depth prediction accuracy. In turn, this enables various applications\nto use such depth predictions. In this paper, we propose a novel framework for\nestimating the relative pose between two cameras from point correspondences\nwith associated monocular depths. Since depth predictions are typically defined\nup to an unknown scale and shift parameter, our solvers jointly estimate both\nscale and shift parameters together with the camera pose. We derive efficient\nsolvers for three cases: (1) two calibrated cameras, (2) two uncalibrated\ncameras with an unknown but shared focal length, and (3) two uncalibrated\ncameras with unknown and different focal lengths. Experiments on synthetic and\nreal data, including experiments with depth maps estimated by 11 different\ndepth predictors, show the practical viability of our solvers. Compared to\nprior work, our solvers achieve state-of-the-art results on two large-scale,\nreal-world datasets. The source code is available at\nhttps://github.com/yaqding/pose_monodepth\n","authors":["Yaqing Ding","Václav Vávra","Viktor Kocur","Jian Yang","Torsten Sattler","Zuzana Kukelova"],"pdf_url":"https://arxiv.org/pdf/2501.07742v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2501.07730v1","updated":"2025-01-13T22:37:17Z","published":"2025-01-13T22:37:17Z","title":"Democratizing Text-to-Image Masked Generative Models with Compact\n  Text-Aware One-Dimensional Tokens","summary":"  Image tokenizers form the foundation of modern text-to-image generative\nmodels but are notoriously difficult to train. Furthermore, most existing\ntext-to-image models rely on large-scale, high-quality private datasets, making\nthem challenging to replicate. In this work, we introduce Text-Aware\nTransformer-based 1-Dimensional Tokenizer (TA-TiTok), an efficient and powerful\nimage tokenizer that can utilize either discrete or continuous 1-dimensional\ntokens. TA-TiTok uniquely integrates textual information during the tokenizer\ndecoding stage (i.e., de-tokenization), accelerating convergence and enhancing\nperformance. TA-TiTok also benefits from a simplified, yet effective, one-stage\ntraining process, eliminating the need for the complex two-stage distillation\nused in previous 1-dimensional tokenizers. This design allows for seamless\nscalability to large datasets. Building on this, we introduce a family of\ntext-to-image Masked Generative Models (MaskGen), trained exclusively on open\ndata while achieving comparable performance to models trained on private data.\nWe aim to release both the efficient, strong TA-TiTok tokenizers and the\nopen-data, open-weight MaskGen models to promote broader access and democratize\nthe field of text-to-image masked generative models.\n","authors":["Dongwon Kim","Ju He","Qihang Yu","Chenglin Yang","Xiaohui Shen","Suha Kwak","Liang-Chieh Chen"],"pdf_url":"https://arxiv.org/pdf/2501.07730v1.pdf","comment":"Project page at https://tacju.github.io/projects/maskgen.html"},{"id":"http://arxiv.org/abs/2404.12652v2","updated":"2025-01-13T21:59:56Z","published":"2024-04-19T06:41:32Z","title":"Pre-trained Vision-Language Models Learn Discoverable Visual Concepts","summary":"  Do vision-language models (VLMs) pre-trained to caption an image of a\n\"durian\" learn visual concepts such as \"brown\" (color) and \"spiky\" (texture) at\nthe same time? We aim to answer this question as visual concepts learned \"for\nfree\" would enable wide applications such as neuro-symbolic reasoning or\nhuman-interpretable object classification. We assume that the visual concepts,\nif captured by pre-trained VLMs, can be extracted by their vision-language\ninterface with text-based concept prompts. We observe that recent works\nprompting VLMs with concepts often differ in their strategies to define and\nevaluate the visual concepts, leading to conflicting conclusions. We propose a\nnew concept definition strategy based on two observations: First, certain\nconcept prompts include shortcuts that recognize correct concepts for wrong\nreasons; Second, multimodal information (e.g. visual discriminativeness, and\ntextual knowledge) should be leveraged when selecting the concepts. Our\nproposed concept discovery and learning (CDL) framework is thus designed to\nidentify a diverse list of generic visual concepts (e.g. \"spiky\" as opposed to\n\"spiky durian\"), which are ranked and selected based on visual and language\nmutual information. We carefully design quantitative and human evaluations of\nthe discovered concepts on six diverse visual recognition datasets, which\nconfirm that pre-trained VLMs do learn visual concepts that provide accurate\nand thorough descriptions for the recognized objects. All code and models are\npublicly released.\n","authors":["Yuan Zang","Tian Yun","Hao Tan","Trung Bui","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2404.12652v2.pdf","comment":"Transactions on Machine Learning Research, 2025"},{"id":"http://arxiv.org/abs/2501.07713v1","updated":"2025-01-13T21:52:46Z","published":"2025-01-13T21:52:46Z","title":"Testing Human-Hand Segmentation on In-Distribution and\n  Out-of-Distribution Data in Human-Robot Interactions Using a Deep Ensemble\n  Model","summary":"  Reliable detection and segmentation of human hands are critical for enhancing\nsafety and facilitating advanced interactions in human-robot collaboration.\nCurrent research predominantly evaluates hand segmentation under\nin-distribution (ID) data, which reflects the training data of deep learning\n(DL) models. However, this approach fails to address out-of-distribution (OOD)\nscenarios that often arise in real-world human-robot interactions. In this\nstudy, we present a novel approach by evaluating the performance of pre-trained\nDL models under both ID data and more challenging OOD scenarios. To mimic\nrealistic industrial scenarios, we designed a diverse dataset featuring simple\nand cluttered backgrounds with industrial tools, varying numbers of hands (0 to\n4), and hands with and without gloves. For OOD scenarios, we incorporated\nunique and rare conditions such as finger-crossing gestures and motion blur\nfrom fast-moving hands, addressing both epistemic and aleatoric uncertainties.\nTo ensure multiple point of views (PoVs), we utilized both egocentric cameras,\nmounted on the operator's head, and static cameras to capture RGB images of\nhuman-robot interactions. This approach allowed us to account for multiple\ncamera perspectives while also evaluating the performance of models trained on\nexisting egocentric datasets as well as static-camera datasets. For\nsegmentation, we used a deep ensemble model composed of UNet and RefineNet as\nbase learners. Performance evaluation was conducted using segmentation metrics\nand uncertainty quantification via predictive entropy. Results revealed that\nmodels trained on industrial datasets outperformed those trained on\nnon-industrial datasets, highlighting the importance of context-specific\ntraining. Although all models struggled with OOD scenarios, those trained on\nindustrial datasets demonstrated significantly better generalization.\n","authors":["Reza Jalayer","Yuxin Chen","Masoud Jalayer","Carlotta Orsenigo","Masayoshi Tomizuka"],"pdf_url":"https://arxiv.org/pdf/2501.07713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07711v1","updated":"2025-01-13T21:45:01Z","published":"2025-01-13T21:45:01Z","title":"Pedestrian Trajectory Prediction Based on Social Interactions Learning\n  With Random Weights","summary":"  Pedestrian trajectory prediction is a critical technology in the evolution of\nself-driving cars toward complete artificial intelligence. Over recent years,\nfocusing on the trajectories of pedestrians to model their social interactions\nhas surged with great interest in more accurate trajectory predictions.\nHowever, existing methods for modeling pedestrian social interactions rely on\npre-defined rules, struggling to capture non-explicit social interactions. In\nthis work, we propose a novel framework named DTGAN, which extends the\napplication of Generative Adversarial Networks (GANs) to graph sequence data,\nwith the primary objective of automatically capturing implicit social\ninteractions and achieving precise predictions of pedestrian trajectory. DTGAN\ninnovatively incorporates random weights within each graph to eliminate the\nneed for pre-defined interaction rules. We further enhance the performance of\nDTGAN by exploring diverse task loss functions during adversarial training,\nwhich yields improvements of 16.7\\% and 39.3\\% on metrics ADE and FDE,\nrespectively. The effectiveness and accuracy of our framework are verified on\ntwo public datasets. The experimental results show that our proposed DTGAN\nachieves superior performance and is well able to understand pedestrians'\nintentions.\n","authors":["Jiajia Xie","Sheng Zhang","Beihao Xia","Zhu Xiao","Hongbo Jiang","Siwang Zhou","Zheng Qin","Hongyang Chen"],"pdf_url":"https://arxiv.org/pdf/2501.07711v1.pdf","comment":"13 pages,7 figures,Accepted to IEEE Transactions on Multimedia (TMM)"},{"id":"http://arxiv.org/abs/2404.00427v2","updated":"2025-01-13T21:20:02Z","published":"2024-03-30T17:21:07Z","title":"Extracting Manifold Information from Point Clouds","summary":"  A kernel based method is proposed for the construction of signature\n(defining) functions of subsets of $\\mathbb{R}^d$. The subsets can range from\nfull dimensional manifolds (open subsets) to point clouds (a finite number of\npoints) and include bounded smooth manifolds of any codimension. The\ninterpolation and analysis of point clouds are the main application. Two\nextreme cases in terms of regularity are considered, where the data set is\ninterpolated by an analytic surface, at the one extreme, and by a H\\\"older\ncontinuous surface, at the other. The signature function can be computed as a\nlinear combination of translated kernels, the coefficients of which are the\nsolution of a finite dimensional linear problem. Once it is obtained, it can be\nused to estimate the dimension as well as the normal and the curvatures of the\ninterpolated surface. The method is global and does not require explicit\nknowledge of local neighborhoods or any other structure present in the data\nset. It admits a variational formulation with a natural ``regularized''\ncounterpart, that proves to be useful in dealing with data sets corrupted by\nnumerical error or noise. The underlying analytical structure of the approach\nis presented in general before it is applied to the case of point clouds.\n","authors":["Patrick Guidotti"],"pdf_url":"https://arxiv.org/pdf/2404.00427v2.pdf","comment":"27 pages, 16 figures, 5 tables"},{"id":"http://arxiv.org/abs/2501.07688v1","updated":"2025-01-13T21:04:37Z","published":"2025-01-13T21:04:37Z","title":"C2PD: Continuity-Constrained Pixelwise Deformation for Guided Depth\n  Super-Resolution","summary":"  Guided depth super-resolution (GDSR) has demonstrated impressive performance\nacross a wide range of domains, with numerous methods being proposed. However,\nexisting methods often treat depth maps as images, where shading values are\ncomputed discretely, making them struggle to effectively restore the continuity\ninherent in the depth map. In this paper, we propose a novel approach that\nmaximizes the utilization of spatial characteristics in depth, coupled with\nhuman abstract perception of real-world substance, by transforming the GDSR\nissue into deformation of a roughcast with ideal plasticity, which can be\ndeformed by force like a continuous object. Specifically, we firstly designed a\ncross-modal operation, Continuity-constrained Asymmetrical Pixelwise Operation\n(CAPO), which can mimic the process of deforming an isovolumetrically flexible\nobject through external forces. Utilizing CAPO as the fundamental component, we\ndevelop the Pixelwise Cross Gradient Deformation (PCGD), which is capable of\nemulating operations on ideal plastic objects (without volume constraint).\nNotably, our approach demonstrates state-of-the-art performance across four\nwidely adopted benchmarks for GDSR, with significant advantages in large-scale\ntasks and generalizability.\n","authors":["Jiahui Kang","Qing Cai","Runqing Tan","Yimei Liu","Zhi Liu"],"pdf_url":"https://arxiv.org/pdf/2501.07688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07681v1","updated":"2025-01-13T20:41:52Z","published":"2025-01-13T20:41:52Z","title":"Dataset Distillation as Pushforward Optimal Quantization","summary":"  Dataset distillation aims to find a synthetic training set such that training\non the synthetic data achieves similar performance to training on real data,\nwith orders of magnitude less computational requirements. Existing methods can\nbe broadly categorized as either bi-level optimization problems that have\nneural network training heuristics as the lower level problem, or disentangled\nmethods that bypass the bi-level optimization by matching distributions of\ndata. The latter method has the major advantages of speed and scalability in\nterms of size of both training and distilled datasets. We demonstrate that when\nequipped with an encoder-decoder structure, the empirically successful\ndisentangled methods can be reformulated as an optimal quantization problem,\nwhere a finite set of points is found to approximate the underlying probability\nmeasure by minimizing the expected projection distance. In particular, we link\nexisting disentangled dataset distillation methods to the classical optimal\nquantization and Wasserstein barycenter problems, demonstrating consistency of\ndistilled datasets for diffusion-based generative priors. We propose a simple\nextension of the state-of-the-art data distillation method D4M, achieving\nbetter performance on the ImageNet-1K dataset with trivial additional\ncomputation, and state-of-the-art performance in higher image-per-class\nsettings.\n","authors":["Hong Ye Tan","Emma Slade"],"pdf_url":"https://arxiv.org/pdf/2501.07681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02052v4","updated":"2025-01-13T19:51:53Z","published":"2024-10-02T21:42:35Z","title":"ExACT: Teaching AI Agents to Explore with Reflective-MCTS and\n  Exploratory Learning","summary":"  Autonomous agents have demonstrated significant potential in automating\ncomplex multistep decision-making tasks. However, even state-of-the-art\nvision-language models (VLMs), such as GPT-4o, still fall short of human-level\nperformance, particularly in intricate web environments and long-horizon tasks.\nTo address these limitations, we present ExACT, an approach to combine\ntest-time search and self-learning to build o1-like models for agentic\napplications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a\nnovel test time algorithm designed to enhance AI agents' ability to explore\ndecision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating\ncontrastive reflection, allowing agents to learn from past interactions and\ndynamically improve their search efficiency; and 2) using multi-agent debate\nfor reliable state evaluation. Next, we introduce Exploratory Learning, a novel\nlearning strategy to teach agents to search at inference time without relying\non any external search algorithms. On the challenging VisualWebArena benchmark,\nour GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across\nvarious tasks compared to the previous state-of-the-art. Additionally, we show\nthat the knowledge and experience gained from test-time search can be\neffectively transferred back to GPT-4o via fine-tuning. After Exploratory\nLearning, GPT-4o 1) demonstrates the ability to explore the environment,\nevaluate a state, and backtrack to viable ones when it detects that the current\nstate cannot lead to success, and 2) matches 87% of R-MCTS's performance while\nusing significantly less compute. Notably, our work demonstrates the compute\nscaling properties in both training - data collection with R-MCTS - and testing\ntime. These results suggest a promising research direction to enhance VLMs'\ncapabilities for agentic applications via test-time search and self-learning.\n","authors":["Xiao Yu","Baolin Peng","Vineeth Vajipey","Hao Cheng","Michel Galley","Jianfeng Gao","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02052v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07647v1","updated":"2025-01-13T19:17:06Z","published":"2025-01-13T19:17:06Z","title":"BlobGEN-Vid: Compositional Text-to-Video Generation with Blob Video\n  Representations","summary":"  Existing video generation models struggle to follow complex text prompts and\nsynthesize multiple objects, raising the need for additional grounding input\nfor improved controllability. In this work, we propose to decompose videos into\nvisual primitives - blob video representation, a general representation for\ncontrollable video generation. Based on blob conditions, we develop a\nblob-grounded video diffusion model named BlobGEN-Vid that allows users to\ncontrol object motions and fine-grained object appearance. In particular, we\nintroduce a masked 3D attention module that effectively improves regional\nconsistency across frames. In addition, we introduce a learnable module to\ninterpolate text embeddings so that users can control semantics in specific\nframes and obtain smooth object transitions. We show that our framework is\nmodel-agnostic and build BlobGEN-Vid based on both U-Net and DiT-based video\ndiffusion models. Extensive experimental results show that BlobGEN-Vid achieves\nsuperior zero-shot video generation ability and state-of-the-art layout\ncontrollability on multiple benchmarks. When combined with an LLM for layout\nplanning, our framework even outperforms proprietary text-to-video generators\nin terms of compositional accuracy.\n","authors":["Weixi Feng","Chao Liu","Sifei Liu","William Yang Wang","Arash Vahdat","Weili Nie"],"pdf_url":"https://arxiv.org/pdf/2501.07647v1.pdf","comment":"Project page: https://blobgen-vid2.github.io/"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.09698v5","updated":"2025-01-13T17:48:09Z","published":"2024-08-19T04:44:32Z","title":"Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation","summary":"  Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences.\n","authors":["Yuyang Ye","Zhi Zheng","Yishan Shen","Tianshu Wang","Hengruo Zhang","Peijun Zhu","Runlong Yu","Kai Zhang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2408.09698v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07365v1","updated":"2025-01-13T14:34:26Z","published":"2025-01-13T14:34:26Z","title":"Multimodal semantic retrieval for product search","summary":"  Semantic retrieval (also known as dense retrieval) based on textual data has\nbeen extensively studied for both web search and product search application\nfields, where the relevance of a query and a potential target document is\ncomputed by their dense vector representation comparison. Product image is\ncrucial for e-commence search interactions and is a key factor for customers at\nproduct explorations. But its impact for semantic retrieval has not been well\nstudied yet. In this research, we build a multimodal representation for product\nitems in e-commerece search in contrast to pure-text representation of\nproducts, and investigate the impact of such representations. The models are\ndeveloped and evaluated on e-commerce datasets. We demonstrate that a\nmultimodal representation scheme for a product can show improvement either on\npurchase recall or relevance accuracy in semantic retrieval. Additionally, we\nprovide numerical analysis for exclusive matches retrieved by a multimodal\nsemantic retrieval model versus a text-only semantic retrieval model, to\ndemonstrate the validation of multimodal solutions.\n","authors":["Dong Liu","Esther Lopez Ramos"],"pdf_url":"https://arxiv.org/pdf/2501.07365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07294v1","updated":"2025-01-13T13:01:00Z","published":"2025-01-13T13:01:00Z","title":"Dataset-Agnostic Recommender Systems","summary":"  [This is a position paper and does not contain any empirical or theoretical\nresults] Recommender systems have become a cornerstone of personalized user\nexperiences, yet their development typically involves significant manual\nintervention, including dataset-specific feature engineering, hyperparameter\ntuning, and configuration. To this end, we introduce a novel paradigm:\nDataset-Agnostic Recommender Systems (DAReS) that aims to enable a single\ncodebase to autonomously adapt to various datasets without the need for\nfine-tuning, for a given recommender system task. Central to this approach is\nthe Dataset Description Language (DsDL), a structured format that provides\nmetadata about the dataset's features and labels, and allow the system to\nunderstand dataset's characteristics, allowing it to autonomously manage\nprocesses like feature selection, missing values imputation, noise removal, and\nhyperparameter optimization. By reducing the need for domain-specific expertise\nand manual adjustments, DAReS offers a more efficient and scalable solution for\nbuilding recommender systems across diverse application domains. It addresses\ncritical challenges in the field, such as reusability, reproducibility, and\naccessibility for non-expert users or entry-level researchers.\n","authors":["Tri Kurniawan Wijaya","Edoardo D'Amico","Xinyang Shao"],"pdf_url":"https://arxiv.org/pdf/2501.07294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07212v1","updated":"2025-01-13T11:12:43Z","published":"2025-01-13T11:12:43Z","title":"Future-Conditioned Recommendations with Multi-Objective Controllable\n  Decision Transformer","summary":"  Securing long-term success is the ultimate aim of recommender systems,\ndemanding strategies capable of foreseeing and shaping the impact of decisions\non future user satisfaction. Current recommendation strategies grapple with two\nsignificant hurdles. Firstly, the future impacts of recommendation decisions\nremain obscured, rendering it impractical to evaluate them through direct\noptimization of immediate metrics. Secondly, conflicts often emerge between\nmultiple objectives, like enhancing accuracy versus exploring diverse\nrecommendations. Existing strategies, trapped in a \"training, evaluation, and\nretraining\" loop, grow more labor-intensive as objectives evolve. To address\nthese challenges, we introduce a future-conditioned strategy for\nmulti-objective controllable recommendations, allowing for the direct\nspecification of future objectives and empowering the model to generate item\nsequences that align with these goals autoregressively. We present the\nMulti-Objective Controllable Decision Transformer (MocDT), an offline\nReinforcement Learning (RL) model capable of autonomously learning the mapping\nfrom multiple objectives to item sequences, leveraging extensive offline data.\nConsequently, it can produce recommendations tailored to any specified\nobjectives during the inference stage. Our empirical findings emphasize the\ncontrollable recommendation strategy's ability to produce item sequences\naccording to different objectives while maintaining performance that is\ncompetitive with current recommendation strategies across various objectives.\n","authors":["Chongming Gao","Kexin Huang","Ziang Fei","Jiaju Chen","Jiawei Chen","Jianshan Sun","Shuchang Liu","Qingpeng Cai","Peng Jiang"],"pdf_url":"https://arxiv.org/pdf/2501.07212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04549v2","updated":"2025-01-13T09:19:53Z","published":"2023-11-08T09:31:48Z","title":"Preference-Consistent Knowledge Distillation for Recommender System","summary":"  Feature-based knowledge distillation has been applied to compress modern\nrecommendation models, usually with projectors that align student (small)\nrecommendation models' dimensions with teacher dimensions. However, existing\nstudies have only focused on making the projected features (i.e., student\nfeatures after projectors) similar to teacher features, overlooking\ninvestigating whether the user preference can be transferred to student\nfeatures (i.e., student features before projectors) in this manner. In this\npaper, we find that due to the lack of restrictions on projectors, the process\nof transferring user preferences will likely be interfered with. We refer to\nthis phenomenon as preference inconsistency. It greatly wastes the power of\nfeature-based knowledge distillation. To mitigate preference inconsistency, we\npropose PCKD, which consists of two regularization terms for projectors. We\nalso propose a hybrid method that combines the two regularization terms. We\nfocus on items with high preference scores and significantly mitigate\npreference inconsistency, improving the performance of feature-based knowledge\ndistillation. Extensive experiments on three public datasets and three\nbackbones demonstrate the effectiveness of PCKD. The code of our method is\nprovided in https://github.com/woriazzc/KDs.\n","authors":["Zhangchi Zhu","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.04549v2.pdf","comment":"TKDE 2024 Accepted"},{"id":"http://arxiv.org/abs/2411.10676v2","updated":"2025-01-13T09:10:18Z","published":"2024-11-16T02:41:12Z","title":"Exploring Feature-based Knowledge Distillation for Recommender System: A\n  Frequency Perspective","summary":"  In this paper, we analyze the feature-based knowledge distillation for\nrecommendation from the frequency perspective. By defining knowledge as\ndifferent frequency components of the features, we theoretically demonstrate\nthat regular feature-based knowledge distillation is equivalent to equally\nminimizing losses on all knowledge and further analyze how this equal loss\nweight allocation method leads to important knowledge being overlooked. In\nlight of this, we propose to emphasize important knowledge by redistributing\nknowledge weights. Furthermore, we propose FreqD, a lightweight knowledge\nreweighting method, to avoid the computational cost of calculating losses on\neach knowledge. Extensive experiments demonstrate that FreqD consistently and\nsignificantly outperforms state-of-the-art knowledge distillation methods for\nrecommender systems. Our code is available at https://github.com/woriazzc/KDs.\n","authors":["Zhangchi Zhu","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.10676v2.pdf","comment":"ACM KDD 2025 Accepted"},{"id":"http://arxiv.org/abs/2501.07111v1","updated":"2025-01-13T07:51:46Z","published":"2025-01-13T07:51:46Z","title":"ListConRanker: A Contrastive Text Reranker with Listwise Encoding","summary":"  Reranker models aim to re-rank the passages based on the semantics similarity\nbetween the given query and passages, which have recently received more\nattention due to the wide application of the Retrieval-Augmented Generation.\nMost previous methods apply pointwise encoding, meaning that it can only encode\nthe context of the query for each passage input into the model. However, for\nthe reranker model, given a query, the comparison results between passages are\neven more important, which is called listwise encoding. Besides, previous\nmodels are trained using the cross-entropy loss function, which leads to issues\nof unsmooth gradient changes during training and low training efficiency. To\naddress these issues, we propose a novel Listwise-encoded Contrastive text\nreRanker (ListConRanker). It can help the passage to be compared with other\npassages during the encoding process, and enhance the contrastive information\nbetween positive examples and between positive and negative examples. At the\nsame time, we use the circle loss to train the model to increase the\nflexibility of gradients and solve the problem of training efficiency.\nExperimental results show that ListConRanker achieves state-of-the-art\nperformance on the reranking benchmark of Chinese Massive Text Embedding\nBenchmark, including the cMedQA1.0, cMedQA2.0, MMarcoReranking, and T2Reranking\ndatasets.\n","authors":["Junlong Liu","Yue Ma","Ruihui Zhao","Junhao Zheng","Qianli Ma","Yangyang Kang"],"pdf_url":"https://arxiv.org/pdf/2501.07111v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2501.07110v1","updated":"2025-01-13T07:51:43Z","published":"2025-01-13T07:51:43Z","title":"Dynamic Multimodal Fusion via Meta-Learning Towards Micro-Video\n  Recommendation","summary":"  Multimodal information (e.g., visual, acoustic, and textual) has been widely\nused to enhance representation learning for micro-video recommendation. For\nintegrating multimodal information into a joint representation of micro-video,\nmultimodal fusion plays a vital role in the existing micro-video recommendation\napproaches. However, the static multimodal fusion used in previous studies is\ninsufficient to model the various relationships among multimodal information of\ndifferent micro-videos. In this paper, we develop a novel meta-learning-based\nmultimodal fusion framework called Meta Multimodal Fusion (MetaMMF), which\ndynamically assigns parameters to the multimodal fusion function for each\nmicro-video during its representation learning. Specifically, MetaMMF regards\nthe multimodal fusion of each micro-video as an independent task. Based on the\nmeta information extracted from the multimodal features of the input task,\nMetaMMF parameterizes a neural network as the item-specific fusion function via\na meta learner. We perform extensive experiments on three benchmark datasets,\ndemonstrating the significant improvements over several state-of-the-art\nmultimodal recommendation models, like MMGCN, LATTICE, and InvRL. Furthermore,\nwe lighten our model by adopting canonical polyadic decomposition to improve\nthe training efficiency, and validate its effectiveness through experimental\nresults. Codes are available at https://github.com/hanliu95/MetaMMF.\n","authors":["Han Liu","Yinwei Wei","Fan Liu","Wenjie Wang","Liqiang Nie","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2501.07110v1.pdf","comment":"This paper has been accepted by ACM Transactions on Information\n  Systems"},{"id":"http://arxiv.org/abs/2411.15005v4","updated":"2025-01-13T07:39:30Z","published":"2024-11-22T15:29:05Z","title":"Multi-granularity Interest Retrieval and Refinement Network for\n  Long-Term User Behavior Modeling in CTR Prediction","summary":"  Click-through Rate (CTR) prediction is crucial for online personalization\nplatforms. Recent advancements have shown that modeling rich user behaviors can\nsignificantly improve the performance of CTR prediction. Current long-term user\nbehavior modeling algorithms predominantly follow two cascading stages. The\nfirst stage retrieves subsequence related to the target item from the long-term\nbehavior sequence, while the second stage models the relationship between the\nsubsequence and the target item. Despite significant progress, these methods\nhave two critical flaws. First, the retrieval query typically includes only\ntarget item information, limiting the ability to capture the user's diverse\ninterests. Second, relational information, such as sequential and interactive\ninformation within the subsequence, is frequently overlooked. Therefore, it\nrequires to be further mined to more accurately model user interests.\n  To this end, we propose Multi-granularity Interest Retrieval and Refinement\nNetwork (MIRRN). Specifically, we first construct queries based on behaviors\nobserved at different time scales to obtain subsequences, each capturing users'\ninterest at various granularities. We then introduce an noval multi-head\nFourier transformer to efficiently learn sequential and interactive information\nwithin the subsequences, leading to more accurate modeling of user interests.\nFinally, we employ multi-head target attention to adaptively assess the impact\nof these multi-granularity interests on the target item. Extensive experiments\nhave demonstrated that MIRRN significantly outperforms state-of-the-art\nbaselines. Furthermore, an A/B test shows that MIRRN increases the average\nnumber of listening songs by 1.32% and the average time of listening songs by\n0.55% on the Huawei Music App. The implementation code is publicly available at\nhttps://github.com/USTC-StarTeam/MIRRN.\n","authors":["Xiang Xu","Hao Wang","Wei Guo","Luankang Zhang","Wanshan Yang","Runlong Yu","Yong Liu","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2411.15005v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07096v1","updated":"2025-01-13T07:09:01Z","published":"2025-01-13T07:09:01Z","title":"Intent-Interest Disentanglement and Item-Aware Intent Contrastive\n  Learning for Sequential Recommendation","summary":"  Recommender systems aim to provide personalized item recommendations by\ncapturing user behaviors derived from their interaction history. Considering\nthat user interactions naturally occur sequentially based on users' intents in\nmind, user behaviors can be interpreted as user intents. Therefore,\nintent-based sequential recommendations are actively studied recently to model\nuser intents from historical interactions for a more precise user understanding\nbeyond traditional studies that often overlook the underlying semantics behind\nuser interactions. However, existing studies face three challenges: 1) the\nlimited understanding of user behaviors by focusing solely on intents, 2) the\nlack of robustness in categorizing intents due to arbitrary fixed numbers of\nintent categories, and 3) the neglect of interacted items in modeling of user\nintents. To address these challenges, we propose Intent-Interest\nDisentanglement and Item-Aware Intent Contrastive Learning for Sequential\nRecommendation (IDCLRec). IDCLRec disentangles user behaviors into intents\nwhich are dynamic motivations and interests which are stable tastes of users\nfor a comprehensive understanding of user behaviors. A causal cross-attention\nmechanism is used to identify consistent interests across interactions, while\nresidual behaviors are modeled as intents by modeling their temporal dynamics\nthrough a similarity adjustment loss. In addition, without predefining the\nnumber of intent categories, an importance-weighted attention mechanism\ncaptures user-specific categorical intent considering the importance of intent\nfor each interaction. Furthermore, we introduce item-aware contrastive learning\nwhich aligns intents that occurred the same interaction and aligns intent with\nitem combinations occurred by the corresponding intent. Extensive experiments\nconducted on real-world datasets demonstrate the effectiveness of IDCLRec.\n","authors":["Yijin Choi","Chiehyeon Lim"],"pdf_url":"https://arxiv.org/pdf/2501.07096v1.pdf","comment":"14 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2412.20163v2","updated":"2025-01-13T06:17:38Z","published":"2024-12-28T14:27:45Z","title":"Topic-Aware Knowledge Graph with Large Language Models for\n  Interoperability in Recommender Systems","summary":"  The use of knowledge graphs in recommender systems has become one of the\ncommon approaches to addressing data sparsity and cold start problems. Recent\nadvances in large language models (LLMs) offer new possibilities for processing\nside and context information within knowledge graphs. However, consistent\nintegration across various systems remains challenging due to the need for\ndomain expert intervention and differences in system characteristics. To\naddress these issues, we propose a consistent approach that extracts both\ngeneral and specific topics from both side and context information using LLMs.\nFirst, general topics are iteratively extracted and updated from side\ninformation. Then, specific topics are extracted using context information.\nFinally, to address synonymous topics generated during the specific topic\nextraction process, a refining algorithm processes and resolves these issues\neffectively. This approach allows general topics to capture broad knowledge\nacross diverse item characteristics, while specific topics emphasize detailed\nattributes, providing a more comprehensive understanding of the semantic\nfeatures of items and the preferences of users. Experimental results\ndemonstrate significant improvements in recommendation performance across\ndiverse knowledge graphs.\n","authors":["Minhye Jeon","Seokho Ahn","Young-Duk Seo"],"pdf_url":"https://arxiv.org/pdf/2412.20163v2.pdf","comment":"Accepted in The 40th ACM/SIGAPP Symposium On Applied Computing(SAC)\n  2025"},{"id":"http://arxiv.org/abs/2406.00323v2","updated":"2025-01-13T05:39:04Z","published":"2024-06-01T06:53:03Z","title":"BeFA: A General Behavior-driven Feature Adapter for Multimedia\n  Recommendation","summary":"  Multimedia recommender systems focus on utilizing behavioral information and\ncontent information to model user preferences. Typically, it employs\npre-trained feature encoders to extract content features, then fuses them with\nbehavioral features. However, pre-trained feature encoders often extract\nfeatures from the entire content simultaneously, including excessive\npreference-irrelevant details. We speculate that it may result in the extracted\nfeatures not containing sufficient features to accurately reflect user\npreferences. To verify our hypothesis, we introduce an attribution analysis\nmethod for visually and intuitively analyzing the content features. The results\nindicate that certain products' content features exhibit the issues of\ninformation drift}and information omission,reducing the expressive ability of\nfeatures. Building upon this finding, we propose an effective and efficient\ngeneral Behavior-driven Feature Adapter (BeFA) to tackle these issues. This\nadapter reconstructs the content feature with the guidance of behavioral\ninformation, enabling content features accurately reflecting user preferences.\nExtensive experiments demonstrate the effectiveness of the adapter across all\nmultimedia recommendation methods. Our code is made publicly available on\nhttps://github.com/fqldom/BeFA.\n","authors":["Qile Fan","Penghang Yu","Zhiyi Tan","Bing-Kun Bao","Guanming Lu"],"pdf_url":"https://arxiv.org/pdf/2406.00323v2.pdf","comment":"This paper is accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2501.07063v1","updated":"2025-01-13T05:16:14Z","published":"2025-01-13T05:16:14Z","title":"Research on the Online Update Method for Retrieval-Augmented Generation\n  (RAG) Model with Incremental Learning","summary":"  In the contemporary context of rapid advancements in information technology\nand the exponential growth of data volume, language models are confronted with\nsignificant challenges in effectively navigating the dynamic and ever-evolving\ninformation landscape to update and adapt to novel knowledge in real time. In\nthis work, an online update method is proposed, which is based on the existing\nRetrieval Enhanced Generation (RAG) model with multiple innovation mechanisms.\nFirstly, the dynamic memory is used to capture the emerging data samples, and\nthen gradually integrate them into the core model through a tunable knowledge\ndistillation strategy. At the same time, hierarchical indexing and multi-layer\ngating mechanism are introduced into the retrieval module to ensure that the\nretrieved content is more targeted and accurate. Finally, a multi-stage network\nstructure is established for different types of inputs in the generation stage,\nand cross-attention matching and screening are carried out on the intermediate\nrepresentations of each stage to ensure the effective integration and iterative\nupdate of new and old knowledge. Experimental results show that the proposed\nmethod is better than the existing mainstream comparison models in terms of\nknowledge retention and inference accuracy.\n","authors":["Yuxin Fan","Yuxiang Wang","Lipeng Liu","Xirui Tang","Na Sun","Zidong Yu"],"pdf_url":"https://arxiv.org/pdf/2501.07063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07024v1","updated":"2025-01-13T02:53:07Z","published":"2025-01-13T02:53:07Z","title":"A Proposed Large Language Model-Based Smart Search for Archive System","summary":"  This study presents a novel framework for smart search in digital archival\nsystems, leveraging the capabilities of Large Language Models (LLMs) to enhance\ninformation retrieval. By employing a Retrieval-Augmented Generation (RAG)\napproach, the framework enables the processing of natural language queries and\ntransforming non-textual data into meaningful textual representations. The\nsystem integrates advanced metadata generation techniques, a hybrid retrieval\nmechanism, a router query engine, and robust response synthesis, the results\nproved search precision and relevance. We present the architecture and\nimplementation of the system and evaluate its performance in four experiments\nconcerning LLM efficiency, hybrid retrieval optimizations, multilingual query\nhandling, and the impacts of individual components. Obtained results show\nsignificant improvements over conventional approaches and have demonstrated the\npotential of AI-powered systems to transform modern archival practices.\n","authors":["Ha Dung Nguyen","Thi-Hoang Anh Nguyen","Thanh Binh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2501.07024v1.pdf","comment":"The 13th International Symposium on Information and Communication\n  Technology (SOICT 2024)"},{"id":"http://arxiv.org/abs/2501.06985v1","updated":"2025-01-13T00:29:29Z","published":"2025-01-13T00:29:29Z","title":"Graph Contrastive Learning on Multi-label Classification for\n  Recommendations","summary":"  In business analysis, providing effective recommendations is essential for\nenhancing company profits. The utilization of graph-based structures, such as\nbipartite graphs, has gained popularity for their ability to analyze complex\ndata relationships. Link prediction is crucial for recommending specific items\nto users. Traditional methods in this area often involve identifying patterns\nin the graph structure or using representational techniques like graph neural\nnetworks (GNNs). However, these approaches encounter difficulties as the volume\nof data increases. To address these challenges, we propose a model called Graph\nContrastive Learning for Multi-label Classification (MCGCL). MCGCL leverages\ncontrastive learning to enhance recommendation effectiveness. The model\nincorporates two training stages: a main task and a subtask. The main task is\nholistic user-item graph learning to capture user-item relationships. The\nhomogeneous user-user (item-item) subgraph is constructed to capture user-user\nand item-item relationships in the subtask. We assessed the performance using\nreal-world datasets from Amazon Reviews in multi-label classification tasks.\nComparative experiments with state-of-the-art methods confirm the effectiveness\nof MCGCL, highlighting its potential for improving recommendation systems.\n","authors":["Jiayang Wu","Wensheng Gan","Huashen Lu","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2501.06985v1.pdf","comment":"Preprint. 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2501.07679v1","updated":"2025-01-13T20:32:38Z","published":"2025-01-13T20:32:38Z","title":"Constructing Set-Compositional and Negated Representations for\n  First-Stage Ranking","summary":"  Set compositional and negated queries are crucial for expressing complex\ninformation needs and enable the discovery of niche items like Books about\nnon-European monarchs. Despite the recent advances in LLMs, first-stage ranking\nremains challenging due to the requirement of encoding documents and queries\nindependently from each other. This limitation calls for constructing\ncompositional query representations that encapsulate logical operations or\nnegations, and can be used to match relevant documents effectively. In the\nfirst part of this work, we explore constructing such representations in a\nzero-shot setting using vector operations between lexically grounded Learned\nSparse Retrieval (LSR) representations. Specifically, we introduce Disentangled\nNegation that penalizes only the negated parts of a query, and a Combined\nPseudo-Term approach that enhances LSRs ability to handle intersections. We\nfind that our zero-shot approach is competitive and often outperforms\nretrievers fine-tuned on compositional data, highlighting certain limitations\nof LSR and Dense Retrievers. Finally, we address some of these limitations and\nimprove LSRs representation power for negation, by allowing them to attribute\nnegative term scores and effectively penalize documents containing the negated\nterms.\n","authors":["Antonios Minas Krasakis","Andrew Yates","Evangelos Kanoulas"],"pdf_url":"https://arxiv.org/pdf/2501.07679v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2501.07250v1","updated":"2025-01-13T11:58:27Z","published":"2025-01-13T11:58:27Z","title":"Large Language Models: New Opportunities for Access to Science","summary":"  The adaptation of Large Language Models like ChatGPT for information\nretrieval from scientific data, software and publications is offering new\nopportunities to simplify access to and understanding of science for persons\nfrom all levels of expertise. They can become tools to both enhance the\nusability of the open science environment we are building as well as help to\nprovide systematic insight to a long-built corpus of scientific publications.\nThe uptake of Retrieval Augmented Generation-enhanced chat applications in the\nconstruction of the open science environment of the KM3NeT neutrino detectors\nserves as a focus point to explore and exemplify prospects for the wider\napplication of Large Language Models for our science.\n","authors":["Jutta Schnabel"],"pdf_url":"https://arxiv.org/pdf/2501.07250v1.pdf","comment":"conference proceeding to ADASS XXXIV 2024"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2501.07564v1","updated":"2025-01-13T18:53:23Z","published":"2025-01-13T18:53:23Z","title":"E2ESlack: An End-to-End Graph-Based Framework for Pre-Routing Slack\n  Prediction","summary":"  Pre-routing slack prediction remains a critical area of research in\nElectronic Design Automation (EDA). Despite numerous machine learning-based\napproaches targeting this task, there is still a lack of a truly end-to-end\nframework that engineers can use to obtain TNS/WNS metrics from raw circuit\ndata at the placement stage. Existing works have demonstrated effectiveness in\nArrival Time (AT) prediction but lack a mechanism for Required Arrival Time\n(RAT) prediction, which is essential for slack prediction and obtaining TNS/WNS\nmetrics. In this work, we propose E2ESlack, an end-to-end graph-based framework\nfor pre-routing slack prediction. The framework includes a TimingParser that\nsupports DEF, SDF and LIB files for feature extraction and graph construction,\nan arrival time prediction model and a fast RAT estimation module. To the best\nof our knowledge, this is the first work capable of predicting path-level\nslacks at the pre-routing stage. We perform extensive experiments and\ndemonstrate that our proposed RAT estimation method outperforms the SOTA\nML-based prediction method and also pre-routing STA tool. Additionally, the\nproposed E2ESlack framework achieves TNS/WNS values comparable to post-routing\nSTA results while saving up to 23x runtime.\n","authors":["Saurabh Bodhe","Zhanguang Zhang","Atia Hamidizadeh","Shixiong Kai","Yingxue Zhang","Mingxuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2501.07564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05451v2","updated":"2025-01-13T18:45:57Z","published":"2024-10-07T19:34:35Z","title":"SecAlign: Defending Against Prompt Injection with Preference\n  Optimization","summary":"  Large language models (LLMs) are becoming increasingly prevalent in modern\nsoftware systems, interfacing between the user and the Internet to assist with\ntasks that require advanced language understanding. To accomplish these tasks,\nthe LLM often uses external data sources such as user documents, web retrieval,\nresults from API calls, etc. This opens up new avenues for attackers to\nmanipulate the LLM via prompt injection. Adversarial prompts can be injected\ninto external data sources to override the system's intended instruction and\ninstead execute a malicious instruction.\n  To mitigate this vulnerability, we propose a new defense called SecAlign\nbased on the technique of preference optimization. Our defense first constructs\na preference dataset with prompt-injected inputs, secure outputs (ones that\nrespond to the legitimate instruction), and insecure outputs (ones that respond\nto the injection). We then perform preference optimization on this dataset to\nteach the LLM to prefer the secure output over the insecure one. This provides\nthe first known method that reduces the success rates of various prompt\ninjections to around 0%, even against attacks much more sophisticated than ones\nseen during training. This indicates our defense generalizes well against\nunknown and yet-to-come attacks. Also, our defended models are still practical\nwith similar utility to the one before our defensive training. Our code is at\nhttps://github.com/facebookresearch/SecAlign\n","authors":["Sizhe Chen","Arman Zharmagambetov","Saeed Mahloujifar","Kamalika Chaudhuri","David Wagner","Chuan Guo"],"pdf_url":"https://arxiv.org/pdf/2410.05451v2.pdf","comment":"Key words: prompt injection defense, LLM security, LLM-integrated\n  applications"},{"id":"http://arxiv.org/abs/2501.07555v1","updated":"2025-01-13T18:37:10Z","published":"2025-01-13T18:37:10Z","title":"Dynamic Prototype Rehearsal for Continual Learning in ECG Arrhythmia\n  Detection","summary":"  Continual Learning (CL) methods aim to learn from a sequence of tasks while\navoiding the challenge of forgetting previous knowledge. We present DREAM-CL, a\nnovel CL method for ECG arrhythmia detection that introduces dynamic prototype\nrehearsal memory. DREAM-CL selects representative prototypes by clustering data\nbased on learning behavior during each training session. Within each cluster,\nwe apply a smooth sorting operation that ranks samples by training difficulty,\ncompressing extreme values and removing outliers. The more challenging samples\nare then chosen as prototypes for the rehearsal memory, ensuring effective\nknowledge retention across sessions. We evaluate our method on\ntime-incremental, class-incremental, and lead-incremental scenarios using two\nwidely used ECG arrhythmia datasets, Chapman and PTB-XL. The results\ndemonstrate that DREAM-CL outperforms the state-of-the-art in CL for ECG\narrhythmia detection. Detailed ablation and sensitivity studies are performed\nto validate the different design choices of our method.\n","authors":["Sana Rahmani","Reetam Chatterjee","Ali Etemad","Javad Hashemi"],"pdf_url":"https://arxiv.org/pdf/2501.07555v1.pdf","comment":"Accepted to 2025 International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP 2025)"},{"id":"http://arxiv.org/abs/2411.04987v2","updated":"2025-01-13T18:24:22Z","published":"2024-11-07T18:55:10Z","title":"Few-Shot Task Learning through Inverse Generative Modeling","summary":"  Learning the intents of an agent, defined by its goals or motion style, is\noften extremely challenging from just a few examples. We refer to this problem\nas task concept learning and present our approach, Few-Shot Task Learning\nthrough Inverse Generative Modeling (FTL-IGM), which learns new task concepts\nby leveraging invertible neural generative models. The core idea is to pretrain\na generative model on a set of basic concepts and their demonstrations. Then,\ngiven a few demonstrations of a new concept (such as a new goal or a new\naction), our method learns the underlying concepts through backpropagation\nwithout updating the model weights, thanks to the invertibility of the\ngenerative model. We evaluate our method in five domains -- object\nrearrangement, goal-oriented navigation, motion caption of human actions,\nautonomous driving, and real-world table-top manipulation. Our experimental\nresults demonstrate that via the pretrained generative model, we successfully\nlearn novel concepts and generate agent plans or motion corresponding to these\nconcepts in (1) unseen environments and (2) in composition with training\nconcepts.\n","authors":["Aviv Netanyahu","Yilun Du","Antonia Bronars","Jyothish Pari","Joshua Tenenbaum","Tianmin Shu","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2411.04987v2.pdf","comment":"Added acknowledgment"},{"id":"http://arxiv.org/abs/2501.07542v1","updated":"2025-01-13T18:23:57Z","published":"2025-01-13T18:23:57Z","title":"Imagine while Reasoning in Space: Multimodal Visualization-of-Thought","summary":"  Chain-of-Thought (CoT) prompting has proven highly effective for enhancing\ncomplex reasoning in Large Language Models (LLMs) and Multimodal Large Language\nModels (MLLMs). Yet, it struggles in complex spatial reasoning tasks.\nNonetheless, human cognition extends beyond language alone, enabling the\nremarkable capability to think in both words and images. Inspired by this\nmechanism, we propose a new reasoning paradigm, Multimodal\nVisualization-of-Thought (MVoT). It enables visual thinking in MLLMs by\ngenerating image visualizations of their reasoning traces. To ensure\nhigh-quality visualization, we introduce token discrepancy loss into\nautoregressive MLLMs. This innovation significantly improves both visual\ncoherence and fidelity. We validate this approach through several dynamic\nspatial reasoning tasks. Experimental results reveal that MVoT demonstrates\ncompetitive performance across tasks. Moreover, it exhibits robust and reliable\nimprovements in the most challenging scenarios where CoT fails. Ultimately,\nMVoT establishes new possibilities for complex reasoning tasks where visual\nthinking can effectively complement verbal reasoning.\n","authors":["Chengzu Li","Wenshan Wu","Huanyu Zhang","Yan Xia","Shaoguang Mao","Li Dong","Ivan Vulić","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2501.07542v1.pdf","comment":"11 pages, 6 figures, 4 tables (27 pages, 10 figures, 16 tables\n  including references and appendices)"},{"id":"http://arxiv.org/abs/2312.15141v2","updated":"2025-01-13T18:21:03Z","published":"2023-12-23T02:34:50Z","title":"Improving the Performance of Echo State Networks Through State Feedback","summary":"  Reservoir computing, using nonlinear dynamical systems, offers a\ncost-effective alternative to neural networks for complex tasks involving\nprocessing of sequential data, time series modeling, and system identification.\nEcho state networks (ESNs), a type of reservoir computer, mirror neural\nnetworks but simplify training. They apply fixed, random linear transformations\nto the internal state, followed by nonlinear changes. This process, guided by\ninput signals and linear regression, adapts the system to match target\ncharacteristics, reducing computational demands. A potential drawback of ESNs\nis that the fixed reservoir may not offer the complexity needed for specific\nproblems. While directly altering (training) the internal ESN would reintroduce\nthe computational burden, an indirect modification can be achieved by\nredirecting some output as input. This feedback can influence the internal\nreservoir state, yielding ESNs with enhanced complexity suitable for broader\nchallenges. In this paper, we demonstrate that by feeding some component of the\nreservoir state back into the network through the input, we can drastically\nimprove upon the performance of a given ESN. We rigorously prove that, for any\ngiven ESN, feedback will almost always improve the accuracy of the output. For\na set of three tasks, each representing different problem classes, we find that\nwith feedback the average error measures are reduced by $30\\%-60\\%$.\nRemarkably, feedback provides at least an equivalent performance boost to\ndoubling the initial number of computational nodes, a computationally expensive\nand technologically challenging alternative. These results demonstrate the\nbroad applicability and substantial usefulness of this feedback scheme.\n","authors":["Peter J. Ehlers","Hendra I. Nurdin","Daniel Soh"],"pdf_url":"https://arxiv.org/pdf/2312.15141v2.pdf","comment":"36 pages, 6 figures"},{"id":"http://arxiv.org/abs/2306.11207v4","updated":"2025-01-13T18:16:34Z","published":"2023-06-20T00:14:47Z","title":"Quilt-1M: One Million Image-Text Pairs for Histopathology","summary":"  Recent accelerations in multi-modal applications have been made possible with\nthe plethora of image and text data available online. However, the scarcity of\nanalogous data in the medical field, specifically in histopathology, has slowed\ncomparable progress. To enable similar representation learning for\nhistopathology, we turn to YouTube, an untapped resource of videos, offering\n$1,087$ hours of valuable educational histopathology videos from expert\nclinicians. From YouTube, we curate QUILT: a large-scale vision-language\ndataset consisting of $802, 144$ image and text pairs. QUILT was automatically\ncurated using a mixture of models, including large language models, handcrafted\nalgorithms, human knowledge databases, and automatic speech recognition. In\ncomparison, the most comprehensive datasets curated for histopathology amass\nonly around $200$K samples. We combine QUILT with datasets from other sources,\nincluding Twitter, research papers, and the internet in general, to create an\neven larger dataset: QUILT-1M, with $1$M paired image-text samples, marking it\nas the largest vision-language histopathology dataset to date. We demonstrate\nthe value of QUILT-1M by fine-tuning a pre-trained CLIP model. Our model\noutperforms state-of-the-art models on both zero-shot and linear probing tasks\nfor classifying new histopathology images across $13$ diverse patch-level\ndatasets of $8$ different sub-pathologies and cross-modal retrieval tasks.\n","authors":["Wisdom Oluchi Ikezogwo","Mehmet Saygin Seyfioglu","Fatemeh Ghezloo","Dylan Stefan Chan Geva","Fatwir Sheikh Mohammed","Pavan Kumar Anand","Ranjay Krishna","Linda Shapiro"],"pdf_url":"https://arxiv.org/pdf/2306.11207v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07536v1","updated":"2025-01-13T18:16:13Z","published":"2025-01-13T18:16:13Z","title":"ML Mule: Mobile-Driven Context-Aware Collaborative Learning","summary":"  Artificial intelligence has been integrated into nearly every aspect of daily\nlife, powering applications from object detection with computer vision to large\nlanguage models for writing emails and compact models in smart homes. These\nmachine learning models cater to individual users but are often detached from\nthem, as they are typically stored and processed in centralized data centers.\nThis centralized approach raises privacy concerns, incurs high infrastructure\ncosts, and struggles with personalization. Federated and fully decentralized\nlearning methods have been proposed to address these issues, but they still\ndepend on centralized servers or face slow convergence due to communication\nconstraints. To overcome these challenges, we propose ML Mule, a approach that\nutilizes individual mobile devices as 'Mules' to train and transport model\nsnapshots as they move through physical spaces, sharing these models with the\nphysical 'Spaces' they inhabit. This method implicitly forms affinity groups\namong devices associated with users who share particular spaces, enabling\ncollaborative model evolution, and protecting users' privacy. Our approach\naddresses several major shortcomings of traditional, federated, and fully\ndecentralized learning systems. The proposed framework represents a new class\nof machine learning methods that are more robust, distributed, and\npersonalized, bringing the field closer to realizing the original vision of\nintelligent, adaptive, and genuinely context-aware smart environments. The\nresults show that ML Mule converges faster and achieves higher model accuracy\ncompared to other existing methods.\n","authors":["Haoxiang Yu","Javier Berrocal","Christine Julien"],"pdf_url":"https://arxiv.org/pdf/2501.07536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07534v1","updated":"2025-01-13T18:15:01Z","published":"2025-01-13T18:15:01Z","title":"Investigating Map-Based Path Loss Models: A Study of Feature\n  Representations in Convolutional Neural Networks","summary":"  Path loss prediction is a beneficial tool for efficient use of the radio\nfrequency spectrum. Building on prior research on high-resolution map-based\npath loss models, this paper studies convolutional neural network input\nrepresentations in more detail. We investigate different methods of\nrepresenting scalar features in convolutional neural networks. Specifically, we\ncompare using frequency and distance as input channels to convolutional layers\nor as scalar inputs to regression layers. We assess model performance using\nthree different feature configurations and find that representing scalar\nfeatures as image channels results in the strongest generalization.\n","authors":["Ryan G. Dempsey","Jonathan Ethier","Halim Yanikomeroglu"],"pdf_url":"https://arxiv.org/pdf/2501.07534v1.pdf","comment":"4 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2409.08389v2","updated":"2025-01-13T18:14:42Z","published":"2024-09-12T20:37:14Z","title":"Higher-Order Topological Directionality and Directed Simplicial Neural\n  Networks","summary":"  Topological Deep Learning (TDL) has emerged as a paradigm to process and\nlearn from signals defined on higher-order combinatorial topological spaces,\nsuch as simplicial or cell complexes. Although many complex systems have an\nasymmetric relational structure, most TDL models forcibly symmetrize these\nrelationships. In this paper, we first introduce a novel notion of higher-order\ndirectionality and we then design Directed Simplicial Neural Networks\n(Dir-SNNs) based on it. Dir-SNNs are message-passing networks operating on\ndirected simplicial complexes able to leverage directed and possibly asymmetric\ninteractions among the simplices. To our knowledge, this is the first TDL model\nusing a notion of higher-order directionality. We theoretically and empirically\nprove that Dir-SNNs are more expressive than their directed graph counterpart\nin distinguishing isomorphic directed graphs. Experiments on a synthetic source\nlocalization task demonstrate that Dir-SNNs outperform undirected SNNs when the\nunderlying complex is directed, and perform comparably when the underlying\ncomplex is undirected.\n","authors":["Manuel Lecha","Andrea Cavallo","Francesca Dominici","Elvin Isufi","Claudio Battiloro"],"pdf_url":"https://arxiv.org/pdf/2409.08389v2.pdf","comment":"7 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2402.16865v3","updated":"2025-01-13T18:06:23Z","published":"2024-01-21T04:14:54Z","title":"Enhance Eye Disease Detection using Learnable Probabilistic Discrete\n  Latents in Machine Learning Architectures","summary":"  Ocular diseases, including diabetic retinopathy and glaucoma, present a\nsignificant public health challenge due to their high prevalence and potential\nfor causing vision impairment. Early and accurate diagnosis is crucial for\neffective treatment and management. In recent years, deep learning models have\nemerged as powerful tools for analysing medical images, such as retina imaging.\nHowever, challenges persist in model relibability and uncertainty estimation,\nwhich are critical for clinical decision-making. This study leverages the\nprobabilistic framework of Generative Flow Networks (GFlowNets) to learn the\nposterior distribution over latent discrete dropout masks for the\nclassification and analysis of ocular diseases using fundus images. We develop\na robust and generalizable method that utilizes GFlowOut integrated with\nResNet18 and ViT models as the backbone in identifying various ocular\nconditions. This study employs a unique set of dropout masks - none, random,\nbottomup, and topdown - to enhance model performance in analyzing these fundus\nimages. Our results demonstrate that our learnable probablistic latents\nsignificantly improves accuracy, outperforming the traditional dropout\napproach. We utilize a gradient map calculation method, Grad-CAM, to assess\nmodel explainability, observing that the model accurately focuses on critical\nimage regions for predictions. The integration of GFlowOut in neural networks\npresents a promising advancement in the automated diagnosis of ocular diseases,\nwith implications for improving clinical workflows and patient outcomes.\n","authors":["Anirudh Prabhakaran","YeKun Xiao","Ching-Yu Cheng","Dianbo Liu"],"pdf_url":"https://arxiv.org/pdf/2402.16865v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17752v2","updated":"2025-01-13T18:03:46Z","published":"2024-11-25T16:20:49Z","title":"Path Loss Prediction Using Deep Learning","summary":"  Radio deployments and spectrum planning benefit from path loss predictions.\nObstructions along a communications link are often considered implicitly or\nthrough derived metrics such as representative clutter height or total\nobstruction depth. In this paper, we propose a path-specific path loss\nprediction method that uses convolutional neural networks to automatically\nperform feature extraction from high-resolution obstruction height maps. Our\nmethods result in low prediction error in a variety of environments without\nrequiring derived metrics.\n","authors":["Ryan G. Dempsey","Jonathan Ethier","Halim Yanikomeroglu"],"pdf_url":"https://arxiv.org/pdf/2411.17752v2.pdf","comment":"5 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2501.07525v1","updated":"2025-01-13T17:55:32Z","published":"2025-01-13T17:55:32Z","title":"RadAlign: Advancing Radiology Report Generation with Vision-Language\n  Concept Alignment","summary":"  Automated chest radiographs interpretation requires both accurate disease\nclassification and detailed radiology report generation, presenting a\nsignificant challenge in the clinical workflow. Current approaches either focus\non classification accuracy at the expense of interpretability or generate\ndetailed but potentially unreliable reports through image captioning\ntechniques. In this study, we present RadAlign, a novel framework that combines\nthe predictive accuracy of vision-language models (VLMs) with the reasoning\ncapabilities of large language models (LLMs). Inspired by the radiologist's\nworkflow, RadAlign first employs a specialized VLM to align visual features\nwith key medical concepts, achieving superior disease classification with an\naverage AUC of 0.885 across multiple diseases. These recognized medical\nconditions, represented as text-based concepts in the aligned visual-language\nspace, are then used to prompt LLM-based report generation. Enhanced by a\nretrieval-augmented generation mechanism that grounds outputs in similar\nhistorical cases, RadAlign delivers superior report quality with a GREEN score\nof 0.678, outperforming state-of-the-art methods' 0.634. Our framework\nmaintains strong clinical interpretability while reducing hallucinations,\nadvancing automated medical imaging and report analysis through integrated\npredictive and generative AI. Code is available at\nhttps://github.com/difeigu/RadAlign.\n","authors":["Difei Gu","Yunhe Gao","Yang Zhou","Mu Zhou","Dimitris Metaxas"],"pdf_url":"https://arxiv.org/pdf/2501.07525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07752v2","updated":"2025-01-13T17:34:22Z","published":"2024-12-10T18:50:37Z","title":"FlashRNN: Optimizing Traditional RNNs on Modern Hardware","summary":"  While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}\n","authors":["Korbinian Pöppel","Maximilian Beck","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2412.07752v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07508v1","updated":"2025-01-13T17:27:11Z","published":"2025-01-13T17:27:11Z","title":"Improving DeFi Accessibility through Efficient Liquidity Provisioning\n  with Deep Reinforcement Learning","summary":"  This paper applies deep reinforcement learning (DRL) to optimize liquidity\nprovisioning in Uniswap v3, a decentralized finance (DeFi) protocol\nimplementing an automated market maker (AMM) model with concentrated liquidity.\nWe model the liquidity provision task as a Markov Decision Process (MDP) and\ntrain an active liquidity provider (LP) agent using the Proximal Policy\nOptimization (PPO) algorithm. The agent dynamically adjusts liquidity positions\nby using information about price dynamics to balance fee maximization and\nimpermanent loss mitigation. We use a rolling window approach for training and\ntesting, reflecting realistic market conditions and regime shifts. This study\ncompares the data-driven performance of the DRL-based strategy against common\nheuristics adopted by small retail LP actors that do not systematically modify\ntheir liquidity positions. By promoting more efficient liquidity management,\nthis work aims to make DeFi markets more accessible and inclusive for a broader\nrange of participants. Through a data-driven approach to liquidity management,\nthis work seeks to contribute to the ongoing development of more efficient and\nuser-friendly DeFi markets.\n","authors":["Haonan Xu","Alessio Brini"],"pdf_url":"https://arxiv.org/pdf/2501.07508v1.pdf","comment":"9 pages, 5 figures. Accepted at AI for Social Impact: Bridging\n  Innovations in Finance, Social Media, and Crime Prevention Workshop at AAAI\n  2025"},{"id":"http://arxiv.org/abs/2501.07502v1","updated":"2025-01-13T17:19:34Z","published":"2025-01-13T17:19:34Z","title":"RbRL2.0: Integrated Reward and Policy Learning for Rating-based\n  Reinforcement Learning","summary":"  Reinforcement learning (RL), a common tool in decision making, learns\npolicies from various experiences based on the associated cumulative\nreturn/rewards without treating them differently. On the contrary, humans often\nlearn to distinguish from different levels of performance and extract the\nunderlying trends towards improving their decision making for best performance.\nMotivated by this, this paper proposes a novel RL method that mimics humans'\ndecision making process by differentiating among collected experiences for\neffective policy learning. The main idea is to extract important directional\ninformation from experiences with different performance levels, named ratings,\nso that policies can be updated towards desired deviation from these\nexperiences with different ratings. Specifically, we propose a new policy loss\nfunction that penalizes distribution similarities between the current policy\nand failed experiences with different ratings, and assign different weights to\nthe penalty terms based on the rating classes. Meanwhile, reward learning from\nthese rated samples can be integrated with the new policy loss towards an\nintegrated reward and policy learning from rated samples. Optimizing the\nintegrated reward and policy loss function will lead to the discovery of\ndirections for policy improvement towards maximizing cumulative rewards and\npenalizing most from the lowest performance level while least from the highest\nperformance level. To evaluate the effectiveness of the proposed method, we\npresent results for experiments on a few typical environments that show\nimproved convergence and overall performance over the existing rating-based\nreinforcement learning method with only reward learning.\n","authors":["Mingkang Wu","Devin White","Vernon Lawhern","Nicholas R. Waytowich","Yongcan Cao"],"pdf_url":"https://arxiv.org/pdf/2501.07502v1.pdf","comment":"Accepted to the Collaborative AI and Modeling of Humans Bridge\n  Program at AAAI 2025"},{"id":"http://arxiv.org/abs/2405.14496v5","updated":"2025-01-13T17:12:41Z","published":"2024-05-23T12:28:16Z","title":"Hybrid Top-Down Global Causal Discovery with Local Search for Linear and\n  Nonlinear Additive Noise Models","summary":"  Learning the unique directed acyclic graph corresponding to an unknown causal\nmodel is a challenging task. Methods based on functional causal models can\nidentify a unique graph, but either suffer from the curse of dimensionality or\nimpose strong parametric assumptions. To address these challenges, we propose a\nnovel hybrid approach for global causal discovery in observational data that\nleverages local causal substructures. We first present a topological sorting\nalgorithm that leverages ancestral relationships in linear structural causal\nmodels to establish a compact top-down hierarchical ordering, encoding more\ncausal information than linear orderings produced by existing methods. We\ndemonstrate that this approach generalizes to nonlinear settings with arbitrary\nnoise. We then introduce a nonparametric constraint-based algorithm that prunes\nspurious edges by searching for local conditioning sets, achieving greater\naccuracy than current methods. We provide theoretical guarantees for\ncorrectness and worst-case polynomial time complexities, with empirical\nvalidation on synthetic data.\n","authors":["Sujai Hiremath","Jacqueline R. M. A. Maasch","Mengxiao Gao","Promit Ghosal","Kyra Gan"],"pdf_url":"https://arxiv.org/pdf/2405.14496v5.pdf","comment":"To appear at the Thirty-Eighth Annual Conference on Neural\n  Information Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2501.07493v1","updated":"2025-01-13T17:12:38Z","published":"2025-01-13T17:12:38Z","title":"Exploring and Mitigating Adversarial Manipulation of Voting-Based\n  Leaderboards","summary":"  It is now common to evaluate Large Language Models (LLMs) by having humans\nmanually vote to evaluate model outputs, in contrast to typical benchmarks that\nevaluate knowledge or skill at some particular task. Chatbot Arena, the most\npopular benchmark of this type, ranks models by asking users to select the\nbetter response between two randomly selected models (without revealing which\nmodel was responsible for the generations). These platforms are widely trusted\nas a fair and accurate measure of LLM capabilities. In this paper, we show that\nif bot protection and other defenses are not implemented, these voting-based\nbenchmarks are potentially vulnerable to adversarial manipulation.\nSpecifically, we show that an attacker can alter the leaderboard (to promote\ntheir favorite model or demote competitors) at the cost of roughly a thousand\nvotes (verified in a simulated, offline version of Chatbot Arena). Our attack\nconsists of two steps: first, we show how an attacker can determine which model\nwas used to generate a given reply with more than $95\\%$ accuracy; and then,\nthe attacker can use this information to consistently vote for (or against) a\ntarget model. Working with the Chatbot Arena developers, we identify, propose,\nand implement mitigations to improve the robustness of Chatbot Arena against\nadversarial manipulation, which, based on our analysis, substantially increases\nthe cost of such attacks. Some of these defenses were present before our\ncollaboration, such as bot protection with Cloudflare, malicious user\ndetection, and rate limiting. Others, including reCAPTCHA and login are being\nintegrated to strengthen the security in Chatbot Arena.\n","authors":["Yangsibo Huang","Milad Nasr","Anastasios Angelopoulos","Nicholas Carlini","Wei-Lin Chiang","Christopher A. Choquette-Choo","Daphne Ippolito","Matthew Jagielski","Katherine Lee","Ken Ziyu Liu","Ion Stoica","Florian Tramer","Chiyuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.07493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00843v3","updated":"2025-01-13T16:58:43Z","published":"2024-06-30T22:33:47Z","title":"A Unified Approach to Extract Interpretable Rules from Tree Ensembles\n  via Integer Programming","summary":"  Tree ensembles are very popular machine learning models, known for their\neffectiveness in supervised classification and regression tasks. Their\nperformance derives from aggregating predictions of multiple decision trees,\nwhich are renowned for their interpretability properties. However, tree\nensemble models do not reliably exhibit interpretable output. Our work aims to\nextract an optimized list of rules from a trained tree ensemble, providing the\nuser with a condensed, interpretable model that retains most of the predictive\npower of the full model. Our approach consists of solving a set partitioning\nproblem formulated through Integer Programming. The proposed method works with\neither tabular or time series data, for both classification and regression\ntasks, and its flexible formulation can include any arbitrary loss or\nregularization functions. Our extensive computational experiments offer\nstatistically significant evidence that our method is competitive with other\nrule extraction methods in terms of predictive performance and fidelity towards\nthe tree ensemble. Moreover, we empirically show that the proposed method\neffectively extracts interpretable rules from tree ensemble that are designed\nfor time series data.\n","authors":["Lorenzo Bonasera","Emilio Carrizosa"],"pdf_url":"https://arxiv.org/pdf/2407.00843v3.pdf","comment":"- Improved overall manuscript flow and clearness - Added related work\n  on explanation fidelity - Added computational results on fidelity - Fixed\n  some flaws on data inference - Optimization problem with weighted objectives\n  - Added appendix containing qualitative examples - New computational results"},{"id":"http://arxiv.org/abs/2410.16314v3","updated":"2025-01-13T16:53:02Z","published":"2024-10-09T10:09:37Z","title":"Steering Large Language Models using Conceptors: Improving\n  Addition-Based Activation Engineering","summary":"  Large language models have transformed AI, yet reliably controlling their\noutputs remains a challenge. This paper explores activation engineering, where\noutputs of pre-trained LLMs are controlled by manipulating their activations at\ninference time. Unlike traditional methods using a single steering vector, we\nintroduce conceptors - mathematical constructs that represent sets of\nactivation vectors as ellipsoidal regions. Conceptors act as soft projection\nmatrices and offer more precise control over complex activation patterns. Our\nexperiments demonstrate that conceptors outperform traditional methods across\nmultiple steering tasks. We further use Boolean operations on conceptors for\ncombined steering goals that empirically outperform additively combining\nsteering vectors on a set of tasks. These results highlight conceptors as a\npromising tool for more effective steering of LLMs. Our code is available on\ngithub.com/jorispos/conceptorsteering.\n","authors":["Joris Postmus","Steven Abreu"],"pdf_url":"https://arxiv.org/pdf/2410.16314v3.pdf","comment":"Presented at the MINT workshop at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.03033v2","updated":"2025-01-13T16:42:03Z","published":"2024-11-05T12:10:02Z","title":"Rethinking Decoders for Transformer-based Semantic Segmentation: A\n  Compression Perspective","summary":"  State-of-the-art methods for Transformer-based semantic segmentation\ntypically adopt Transformer decoders that are used to extract additional\nembeddings from image embeddings via cross-attention, refine either or both\ntypes of embeddings via self-attention, and project image embeddings onto the\nadditional embeddings via dot-product. Despite their remarkable success, these\nempirical designs still lack theoretical justifications or interpretations,\nthus hindering potentially principled improvements. In this paper, we argue\nthat there are fundamental connections between semantic segmentation and\ncompression, especially between the Transformer decoders and Principal\nComponent Analysis (PCA). From such a perspective, we derive a white-box, fully\nattentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the\ninterpretations as follows: 1) the self-attention operator refines image\nembeddings to construct an ideal principal subspace that aligns with the\nsupervision and retains most information; 2) the cross-attention operator seeks\nto find a low-rank approximation of the refined image embeddings, which is\nexpected to be a set of orthonormal bases of the principal subspace and\ncorresponds to the predefined classes; 3) the dot-product operation yields\ncompact representation for image embeddings as segmentation masks. Experiments\nconducted on dataset ADE20K find that DEPICT consistently outperforms its\nblack-box counterpart, Segmenter, and it is light weight and more robust.\n","authors":["Qishuai Wen","Chun-Guang Li"],"pdf_url":"https://arxiv.org/pdf/2411.03033v2.pdf","comment":"NeurIPS2024. Code:https://github.com/QishuaiWen/DEPICT/"},{"id":"http://arxiv.org/abs/2402.13699v5","updated":"2025-01-13T16:21:58Z","published":"2024-02-21T11:00:23Z","title":"Automation of Quantum Dot Measurement Analysis via Explainable Machine\n  Learning","summary":"  The rapid development of quantum dot (QD) devices for quantum computing has\nnecessitated more efficient and automated methods for device characterization\nand tuning. This work demonstrates the feasibility and advantages of applying\nexplainable machine learning techniques to the analysis of quantum dot\nmeasurements, paving the way for further advances in automated and transparent\nQD device tuning. Many of the measurements acquired during the tuning process\ncome in the form of images that need to be properly analyzed to guide the\nsubsequent tuning steps. By design, features present in such images capture\ncertain behaviors or states of the measured QD devices. When considered\ncarefully, such features can aid the control and calibration of QD devices. An\nimportant example of such images are so-called $\\textit{triangle plots}$, which\nvisually represent current flow and reveal characteristics important for QD\ndevice calibration. While image-based classification tools, such as\nconvolutional neural networks (CNNs), can be used to verify whether a given\nmeasurement is $\\textit{good}$ and thus warrants the initiation of the next\nphase of tuning, they do not provide any insights into how the device should be\nadjusted in the case of $\\textit{bad}$ images. This is because CNNs sacrifice\nprediction and model intelligibility for high accuracy. To ameliorate this\ntrade-off, a recent study introduced an image vectorization approach that\nrelies on the Gabor wavelet transform (Schug $\\textit{et al.}$ 2024\n$\\textit{Proc. XAI4Sci: Explainable Machine Learning for Sciences Workshop\n(AAAI 2024) (Vancouver, Canada)}$ pp 1-6). Here we propose an alternative\nvectorization method that involves mathematical modeling of synthetic triangles\nto mimic the experimental data. Using explainable boosting machines, we show\nthat this new method offers superior explainability of model prediction without\nsacrificing accuracy.\n","authors":["Daniel Schug","Tyler J. Kovach","M. A. Wolfe","Jared Benson","Sanghyeok Park","J. P. Dodson","J. Corrigan","M. A. Eriksson","Justyna P. Zwolak"],"pdf_url":"https://arxiv.org/pdf/2402.13699v5.pdf","comment":"20 pages, 5 figures, abbreviated version published in Proceedings of\n  the XAI4Sci: Explainable machine learning for sciences workshop at AAAI 2024,\n  (Vancouver, Canada)"},{"id":"http://arxiv.org/abs/2501.07447v1","updated":"2025-01-13T16:18:31Z","published":"2025-01-13T16:18:31Z","title":"PrecipDiff: Leveraging image diffusion models to enhance satellite-based\n  precipitation observations","summary":"  A recent report from the World Meteorological Organization (WMO) highlights\nthat water-related disasters have caused the highest human losses among natural\ndisasters over the past 50 years, with over 91\\% of deaths occurring in\nlow-income countries. This disparity is largely due to the lack of adequate\nground monitoring stations, such as weather surveillance radars (WSR), which\nare expensive to install. For example, while the US and Europe combined possess\nover 600 WSRs, Africa, despite having almost one and half times their landmass,\nhas fewer than 40. To address this issue, satellite-based observations offer a\nglobal, near-real-time monitoring solution. However, they face several\nchallenges like accuracy, bias, and low spatial resolution. This study\nleverages the power of diffusion models and residual learning to address these\nlimitations in a unified framework. We introduce the first diffusion model for\ncorrecting the inconsistency between different precipitation products. Our\nmethod demonstrates the effectiveness in downscaling satellite precipitation\nestimates from 10 km to 1 km resolution. Extensive experiments conducted in the\nSeattle region demonstrate significant improvements in accuracy, bias\nreduction, and spatial detail. Importantly, our approach achieves these results\nusing only precipitation data, showcasing the potential of a purely computer\nvision-based approach for enhancing satellite precipitation products and paving\nthe way for further advancements in this domain.\n","authors":["Ting-Yu Dai","Hayato Ushijima-Mwesigwa"],"pdf_url":"https://arxiv.org/pdf/2501.07447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07446v1","updated":"2025-01-13T16:16:53Z","published":"2025-01-13T16:16:53Z","title":"Synthesis and Analysis of Data as Probability Measures with\n  Entropy-Regularized Optimal Transport","summary":"  We consider synthesis and analysis of probability measures using the\nentropy-regularized Wasserstein-2 cost and its unbiased version, the Sinkhorn\ndivergence. The synthesis problem consists of computing the barycenter, with\nrespect to these costs, of $m$ reference measures given a set of coefficients\nbelonging to the $m$-dimensional simplex. The analysis problem consists of\nfinding the coefficients for the closest barycenter in the Wasserstein-2\ndistance to a given measure $\\mu$. Under the weakest assumptions on the\nmeasures thus far in the literature, we compute the derivative of the\nentropy-regularized Wasserstein-2 cost. We leverage this to establish a\ncharacterization of regularized barycenters as solutions to a fixed-point\nequation for the average of the entropic maps from the barycenter to the\nreference measures. This characterization yields a finite-dimensional, convex,\nquadratic program for solving the analysis problem when $\\mu$ is a barycenter.\nIt is shown that these coordinates, as well as the value of the barycenter\nfunctional, can be estimated from samples with dimension-independent rates of\nconvergence, a hallmark of entropy-regularized optimal transport, and we verify\nthese rates experimentally. We also establish that barycentric coordinates are\nstable with respect to perturbations in the Wasserstein-2 metric, suggesting a\nrobustness of these coefficients to corruptions. We employ the barycentric\ncoefficients as features for classification of corrupted point cloud data, and\nshow that compared to neural network baselines, our approach is more efficient\nin small training data regimes.\n","authors":["Brendan Mallery","James M. Murphy","Shuchin Aeron"],"pdf_url":"https://arxiv.org/pdf/2501.07446v1.pdf","comment":"58 pages. Code to reproduce experiments:\n  https://github.com/brendanmallery9/Entropic-Barycenters"},{"id":"http://arxiv.org/abs/2501.07437v1","updated":"2025-01-13T16:05:41Z","published":"2025-01-13T16:05:41Z","title":"Pairwise Comparisons without Stochastic Transitivity: Model, Theory and\n  Applications","summary":"  Most statistical models for pairwise comparisons, including the Bradley-Terry\n(BT) and Thurstone models and many extensions, make a relatively strong\nassumption of stochastic transitivity. This assumption imposes the existence of\nan unobserved global ranking among all the players/teams/items and monotone\nconstraints on the comparison probabilities implied by the global ranking.\nHowever, the stochastic transitivity assumption does not hold in many\nreal-world scenarios of pairwise comparisons, especially games involving\nmultiple skills or strategies. As a result, models relying on this assumption\ncan have suboptimal predictive performance. In this paper, we propose a general\nfamily of statistical models for pairwise comparison data without a stochastic\ntransitivity assumption, substantially extending the BT and Thurstone models.\nIn this model, the pairwise probabilities are determined by a (approximately)\nlow-dimensional skew-symmetric matrix. Likelihood-based estimation methods and\ncomputational algorithms are developed, which allow for sparse data with only a\nsmall proportion of observed pairs. Theoretical analysis shows that the\nproposed estimator achieves minimax-rate optimality, which adapts effectively\nto the sparsity level of the data. The spectral theory for skew-symmetric\nmatrices plays a crucial role in the implementation and theoretical analysis.\nThe proposed method's superiority against the BT model, along with its broad\napplicability across diverse scenarios, is further supported by simulations and\nreal data analysis.\n","authors":["Sze Ming Lee","Yunxiao Chen"],"pdf_url":"https://arxiv.org/pdf/2501.07437v1.pdf","comment":"34 pages, 1 figure"},{"id":"http://arxiv.org/abs/2411.17645v2","updated":"2025-01-13T16:01:14Z","published":"2024-11-26T18:10:51Z","title":"Explainable AI for Classifying UTI Risk Groups Using a Real-World Linked\n  EHR and Pathology Lab Dataset","summary":"  The use of machine learning and AI on electronic health records (EHRs) holds\nsubstantial potential for clinical insight. However, this approach faces\nchallenges due to data heterogeneity, sparsity, temporal misalignment, and\nlimited labeled outcomes. In this context, we leverage a linked EHR dataset of\napproximately one million de-identified individuals from Bristol, North\nSomerset, and South Gloucestershire, UK, to characterize urinary tract\ninfections (UTIs). We implemented a data pre-processing and curation pipeline\nthat transforms the raw EHR data into a structured format suitable for\ndeveloping predictive models focused on data fairness, accountability and\ntransparency. Given the limited availability and biases of ground truth UTI\noutcomes, we introduce a UTI risk estimation framework informed by clinical\nexpertise to estimate UTI risk across individual patient timelines. Pairwise\nXGBoost models are trained using this framework to differentiate UTI risk\ncategories with explainable AI techniques applied to identify key predictors\nand support interpretability. Our findings reveal differences in clinical and\ndemographic predictors across risk groups. While this study highlights the\npotential of AI-driven insights to support UTI clinical decision-making,\nfurther investigation of patient sub-strata and extensive validation are needed\nto ensure robustness and applicability in clinical practice.\n","authors":["Yujie Dai","Brian Sullivan","Axel Montout","Amy Dillon","Chris Waller","Peter Acs","Rachel Denholm","Philip Williams","Alastair D Hay","Raul Santos-Rodriguez","Andrew Dowsey"],"pdf_url":"https://arxiv.org/pdf/2411.17645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07429v1","updated":"2025-01-13T15:52:43Z","published":"2025-01-13T15:52:43Z","title":"Distance Measure Based on an Embedding of the Manifold of K-Component\n  Gaussian Mixture Models into the Manifold of Symmetric Positive Definite\n  Matrices","summary":"  In this paper, a distance between the Gaussian Mixture Models(GMMs) is\nobtained based on an embedding of the K-component Gaussian Mixture Model into\nthe manifold of the symmetric positive definite matrices. Proof of embedding of\nK-component GMMs into the manifold of symmetric positive definite matrices is\ngiven and shown that it is a submanifold. Then, proved that the manifold of\nGMMs with the pullback of induced metric is isometric to the submanifold with\nthe induced metric. Through this embedding we obtain a general lower bound for\nthe Fisher-Rao metric. This lower bound is a distance measure on the manifold\nof GMMs and we employ it for the similarity measure of GMMs. The effectiveness\nof this framework is demonstrated through an experiment on standard machine\nlearning benchmarks, achieving accuracy of 98%, 92%, and 93.33% on the UIUC,\nKTH-TIPS, and UMD texture recognition datasets respectively.\n","authors":["Amit Vishwakarma","KS Subrahamanian Moosath"],"pdf_url":"https://arxiv.org/pdf/2501.07429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07426v1","updated":"2025-01-13T15:47:02Z","published":"2025-01-13T15:47:02Z","title":"MVICAD2: Multi-View Independent Component Analysis with Delays and\n  Dilations","summary":"  Machine learning techniques in multi-view settings face significant\nchallenges, particularly when integrating heterogeneous data, aligning feature\nspaces, and managing view-specific biases. These issues are prominent in\nneuroscience, where data from multiple subjects exposed to the same stimuli are\nanalyzed to uncover brain activity dynamics. In magnetoencephalography (MEG),\nwhere signals are captured at the scalp level, estimating the brain's\nunderlying sources is crucial, especially in group studies where sources are\nassumed to be similar for all subjects. Common methods, such as Multi-View\nIndependent Component Analysis (MVICA), assume identical sources across\nsubjects, but this assumption is often too restrictive due to individual\nvariability and age-related changes. Multi-View Independent Component Analysis\nwith Delays (MVICAD) addresses this by allowing sources to differ up to a\ntemporal delay. However, temporal dilation effects, particularly in auditory\nstimuli, are common in brain dynamics, making the estimation of time delays\nalone insufficient. To address this, we propose Multi-View Independent\nComponent Analysis with Delays and Dilations (MVICAD2), which allows sources to\ndiffer across subjects in both temporal delays and dilations. We present a\nmodel with identifiable sources, derive an approximation of its likelihood in\nclosed form, and use regularization and optimization techniques to enhance\nperformance. Through simulations, we demonstrate that MVICAD2 outperforms\nexisting multi-view ICA methods. We further validate its effectiveness using\nthe Cam-CAN dataset, and showing how delays and dilations are related to aging.\n","authors":["Ambroise Heurtebise","Omar Chehab","Pierre Ablin","Alexandre Gramfort"],"pdf_url":"https://arxiv.org/pdf/2501.07426v1.pdf","comment":"19 pages, 8 figures"},{"id":"http://arxiv.org/abs/2501.07423v1","updated":"2025-01-13T15:43:22Z","published":"2025-01-13T15:43:22Z","title":"An Investigation into Seasonal Variations in Energy Forecasting for\n  Student Residences","summary":"  This research provides an in-depth evaluation of various machine learning\nmodels for energy forecasting, focusing on the unique challenges of seasonal\nvariations in student residential settings. The study assesses the performance\nof baseline models, such as LSTM and GRU, alongside state-of-the-art\nforecasting methods, including Autoregressive Feedforward Neural Networks,\nTransformers, and hybrid approaches. Special attention is given to predicting\nenergy consumption amidst challenges like seasonal patterns, vacations,\nmeteorological changes, and irregular human activities that cause sudden\nfluctuations in usage. The findings reveal that no single model consistently\noutperforms others across all seasons, emphasizing the need for season-specific\nmodel selection or tailored designs. Notably, the proposed Hyper Network based\nLSTM and MiniAutoEncXGBoost models exhibit strong adaptability to seasonal\nvariations, effectively capturing abrupt changes in energy consumption during\nsummer months. This study advances the energy forecasting field by emphasizing\nthe critical role of seasonal dynamics and model-specific behavior in achieving\naccurate predictions.\n","authors":["Muhammad Umair Danish","Mathumitha Sureshkumar","Thanuri Fonseka","Umeshika Uthayakumar","Vinura Galwaduge"],"pdf_url":"https://arxiv.org/pdf/2501.07423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05226v2","updated":"2025-01-13T15:30:39Z","published":"2025-01-09T13:29:54Z","title":"Light Transport-aware Diffusion Posterior Sampling for Single-View\n  Reconstruction of 3D Volumes","summary":"  We introduce a single-view reconstruction technique of volumetric fields in\nwhich multiple light scattering effects are omnipresent, such as in clouds. We\nmodel the unknown distribution of volumetric fields using an unconditional\ndiffusion model trained on a novel benchmark dataset comprising 1,000\nsynthetically simulated volumetric density fields. The neural diffusion model\nis trained on the latent codes of a novel, diffusion-friendly, monoplanar\nrepresentation. The generative model is used to incorporate a tailored\nparametric diffusion posterior sampling technique into different reconstruction\ntasks. A physically-based differentiable volume renderer is employed to provide\ngradients with respect to light transport in the latent space. This stands in\ncontrast to classic NeRF approaches and makes the reconstructions better\naligned with observed data. Through various experiments, we demonstrate\nsingle-view reconstruction of volumetric clouds at a previously unattainable\nquality.\n","authors":["Ludwic Leonard","Nils Thuerey","Ruediger Westermann"],"pdf_url":"https://arxiv.org/pdf/2501.05226v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07405v1","updated":"2025-01-13T15:21:20Z","published":"2025-01-13T15:21:20Z","title":"PROTECT: Protein circadian time prediction using unsupervised learning","summary":"  Circadian rhythms regulate the physiology and behavior of humans and animals.\nDespite advancements in understanding these rhythms and predicting circadian\nphases at the transcriptional level, predicting circadian phases from proteomic\ndata remains elusive. This challenge is largely due to the scarcity of time\nlabels in proteomic datasets, which are often characterized by small sample\nsizes, high dimensionality, and significant noise. Furthermore, existing\nmethods for predicting circadian phases from transcriptomic data typically rely\non prior knowledge of known rhythmic genes, making them unsuitable for\nproteomic datasets. To address this gap, we developed a novel computational\nmethod using unsupervised deep learning techniques to predict circadian sample\nphases from proteomic data without requiring time labels or prior knowledge of\nproteins or genes. Our model involves a two-stage training process optimized\nfor robust circadian phase prediction: an initial greedy one-layer-at-a-time\npre-training which generates informative initial parameters followed by\nfine-tuning. During fine-tuning, a specialized loss function guides the model\nto align protein expression levels with circadian patterns, enabling it to\naccurately capture the underlying rhythmic structure within the data. We tested\nour method on both time-labeled and unlabeled proteomic data. For labeled data,\nwe compared our predictions to the known time labels, achieving high accuracy,\nwhile for unlabeled human datasets, including postmortem brain regions and\nurine samples, we explored circadian disruptions. Notably, our analysis\nidentified disruptions in rhythmic proteins between Alzheimer's disease and\ncontrol subjects across these samples.\n","authors":["Aram Ansary Ogholbake","Qiang Cheng"],"pdf_url":"https://arxiv.org/pdf/2501.07405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07400v1","updated":"2025-01-13T15:17:28Z","published":"2025-01-13T15:17:28Z","title":"Derivation of effective gradient flow equations and dynamical truncation\n  of training data in Deep Learning","summary":"  We derive explicit equations governing the cumulative biases and weights in\nDeep Learning with ReLU activation function, based on gradient descent for the\nEuclidean cost in the input layer, and under the assumption that the weights\nare, in a precise sense, adapted to the coordinate system distinguished by the\nactivations. We show that gradient descent corresponds to a dynamical process\nin the input layer, whereby clusters of data are progressively reduced in\ncomplexity (\"truncated\") at an exponential rate that increases with the number\nof data points that have already been truncated. We provide a detailed\ndiscussion of several types of solutions to the gradient flow equations. A main\nmotivation for this work is to shed light on the interpretability question in\nsupervised learning.\n","authors":["Thomas Chen"],"pdf_url":"https://arxiv.org/pdf/2501.07400v1.pdf","comment":"AMS Latex, 35 pages"},{"id":"http://arxiv.org/abs/2501.07382v1","updated":"2025-01-13T15:01:12Z","published":"2025-01-13T15:01:12Z","title":"Information-Theoretic Dual Memory System for Continual Learning","summary":"  Continuously acquiring new knowledge from a dynamic environment is a\nfundamental capability for animals, facilitating their survival and ability to\naddress various challenges. This capability is referred to as continual\nlearning, which focuses on the ability to learn a sequence of tasks without the\ndetriment of previous knowledge. A prevalent strategy to tackle continual\nlearning involves selecting and storing numerous essential data samples from\nprior tasks within a fixed-size memory buffer. However, the majority of current\nmemory-based techniques typically utilize a single memory buffer, which poses\nchallenges in concurrently managing newly acquired and previously learned\nsamples. Drawing inspiration from the Complementary Learning Systems (CLS)\ntheory, which defines rapid and gradual learning mechanisms for processing\ninformation, we propose an innovative dual memory system called the\nInformation-Theoretic Dual Memory System (ITDMS). This system comprises a fast\nmemory buffer designed to retain temporary and novel samples, alongside a slow\nmemory buffer dedicated to preserving critical and informative samples. The\nfast memory buffer is optimized employing an efficient reservoir sampling\nprocess. Furthermore, we introduce a novel information-theoretic memory\noptimization strategy that selectively identifies and retains diverse and\ninformative data samples for the slow memory buffer. Additionally, we propose a\nnovel balanced sample selection procedure that automatically identifies and\neliminates redundant memorized samples, thus freeing up memory capacity for new\ndata acquisitions, which can deal with a growing array of tasks. Our\nmethodology is rigorously assessed through a series of continual learning\nexperiments, with empirical results underscoring the effectiveness of the\nproposed system.\n","authors":["RunQing Wu","KaiHui Huang","HanYi Zhang","QiHe Liu","GuoJin Yu","JingSong Deng","Fei Ye"],"pdf_url":"https://arxiv.org/pdf/2501.07382v1.pdf","comment":"35 pages, 9 figures, submitted to Knowledge-Based Systems"},{"id":"http://arxiv.org/abs/2402.02429v3","updated":"2025-01-13T14:58:30Z","published":"2024-02-04T09:58:42Z","title":"Towards an Information Theoretic Framework of Context-Based Offline\n  Meta-Reinforcement Learning","summary":"  As a marriage between offline RL and meta-RL, the advent of offline\nmeta-reinforcement learning (OMRL) has shown great promise in enabling RL\nagents to multi-task and quickly adapt while acquiring knowledge safely. Among\nwhich, context-based OMRL (COMRL) as a popular paradigm, aims to learn a\nuniversal policy conditioned on effective task representations. In this work,\nby examining several key milestones in the field of COMRL, we propose to\nintegrate these seemingly independent methodologies into a unified framework.\nMost importantly, we show that the pre-existing COMRL algorithms are\nessentially optimizing the same mutual information objective between the task\nvariable $M$ and its latent representation $Z$ by implementing various\napproximate bounds. Such theoretical insight offers ample design freedom for\nnovel algorithms. As demonstrations, we propose a supervised and a\nself-supervised implementation of $I(Z; M)$, and empirically show that the\ncorresponding optimization algorithms exhibit remarkable generalization across\na broad spectrum of RL benchmarks, context shift scenarios, data qualities and\ndeep learning architectures. This work lays the information theoretic\nfoundation for COMRL methods, leading to a better understanding of task\nrepresentation learning in the context of reinforcement learning. Given its\ngenerality, we envision our framework as a promising offline pre-training\nparadigm of foundation models for decision making.\n","authors":["Lanqing Li","Hai Zhang","Xinyu Zhang","Shatong Zhu","Yang Yu","Junqiao Zhao","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2402.02429v3.pdf","comment":"26 pages, 8 figures, 7 tables. TLDR: We propose a novel information\n  theoretic framework of the context-based offline meta-RL paradigm, which\n  unifies several mainstream methods and leads to two robust algorithm\n  implementations"},{"id":"http://arxiv.org/abs/2501.07373v1","updated":"2025-01-13T14:41:56Z","published":"2025-01-13T14:41:56Z","title":"Dynami-CAL GraphNet: A Physics-Informed Graph Neural Network Conserving\n  Linear and Angular Momentum for Dynamical Systems","summary":"  Accurate, interpretable, and real-time modeling of multi-body dynamical\nsystems is essential for predicting behaviors and inferring physical properties\nin natural and engineered environments. Traditional physics-based models face\nscalability challenges and are computationally demanding, while data-driven\napproaches like Graph Neural Networks (GNNs) often lack physical consistency,\ninterpretability, and generalization. In this paper, we propose Dynami-CAL\nGraphNet, a Physics-Informed Graph Neural Network that integrates the learning\ncapabilities of GNNs with physics-based inductive biases to address these\nlimitations. Dynami-CAL GraphNet enforces pairwise conservation of linear and\nangular momentum for interacting nodes using edge-local reference frames that\nare equivariant to rotational symmetries, invariant to translations, and\nequivariant to node permutations. This design ensures physically consistent\npredictions of node dynamics while offering interpretable, edge-wise linear and\nangular impulses resulting from pairwise interactions. Evaluated on a 3D\ngranular system with inelastic collisions, Dynami-CAL GraphNet demonstrates\nstable error accumulation over extended rollouts, effective extrapolations to\nunseen configurations, and robust handling of heterogeneous interactions and\nexternal forces. Dynami-CAL GraphNet offers significant advantages in fields\nrequiring accurate, interpretable, and real-time modeling of complex multi-body\ndynamical systems, such as robotics, aerospace engineering, and materials\nscience. By providing physically consistent and scalable predictions that\nadhere to fundamental conservation laws, it enables the inference of forces and\nmoments while efficiently handling heterogeneous interactions and external\nforces.\n","authors":["Vinay Sharma","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2501.07373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07371v1","updated":"2025-01-13T14:40:42Z","published":"2025-01-13T14:40:42Z","title":"Simulating the Hubbard Model with Equivariant Normalizing Flows","summary":"  Generative models, particularly normalizing flows, have shown exceptional\nperformance in learning probability distributions across various domains of\nphysics, including statistical mechanics, collider physics, and lattice field\ntheory. In the context of lattice field theory, normalizing flows have been\nsuccessfully applied to accurately learn the Boltzmann distribution, enabling a\nrange of tasks such as direct estimation of thermodynamic observables and\nsampling independent and identically distributed (i.i.d.) configurations.\n  In this work, we present a proof-of-concept demonstration that normalizing\nflows can be used to learn the Boltzmann distribution for the Hubbard model.\nThis model is widely employed to study the electronic structure of graphene and\nother carbon nanomaterials. State-of-the-art numerical simulations of the\nHubbard model, such as those based on Hybrid Monte Carlo (HMC) methods, often\nsuffer from ergodicity issues, potentially leading to biased estimates of\nphysical observables. Our numerical experiments demonstrate that leveraging\ni.i.d.\\ sampling from the normalizing flow effectively addresses these issues.\n","authors":["Dominic Schuh","Janik Kreit","Evan Berkowitz","Lena Funcke","Thomas Luu","Kim A. Nicoli","Marcel Rodekamp"],"pdf_url":"https://arxiv.org/pdf/2501.07371v1.pdf","comment":"14 pages, 5 figures, contribution to the 41st International Symposium\n  on Lattice Field Theory (Lattice 2024), July 28th - August 3rd, 2024,\n  Liverpool, UK"},{"id":"http://arxiv.org/abs/2411.06376v2","updated":"2025-01-13T14:39:34Z","published":"2024-11-10T07:15:03Z","title":"Project Tracyn: Generative Artificial Intelligence based Peripherals\n  Trace Synthesizer","summary":"  Peripheral Component Interconnect Express (PCIe) is the de facto interconnect\nstandard for high-speed peripherals and CPUs. Prototyping and optimizing PCIe\ndevices for emerging scenarios is an ongoing challenge. Since Transaction Layer\nPackets (TLPs) capture device-CPU interactions, it is crucial to analyze and\ngenerate realistic TLP traces for effective device design and optimization.\nGenerative AI offers a promising approach for creating intricate, custom TLP\ntraces necessary for PCIe hardware and software development. However, existing\nmodels often generate impractical traces due to the absence of PCIe-specific\nconstraints, such as TLP ordering and causality. This paper presents Phantom,\nthe first framework that treats TLP trace generation as a generative AI problem\nwhile incorporating PCIe-specific constraints. We validate Phantom's\neffectiveness by generating TLP traces for an actual PCIe network interface\ncard. Experimental results show that Phantom produces practical, large-scale\nTLP traces, significantly outperforming existing models, with improvements of\nup to 1000$\\times$ in task-specific metrics and up to 2.19$\\times$ in Frechet\nInception Distance (FID) compared to backbone-only methods.\n","authors":["Zhibai Huang","Yihan Shen","Yongchen Xie","Zhixiang Wei","Yun wang","Fangxin Liu","Tao Song","Zhengwei Qi"],"pdf_url":"https://arxiv.org/pdf/2411.06376v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10874v2","updated":"2025-01-13T14:38:07Z","published":"2024-02-16T18:20:33Z","title":"Design of 2D Skyrmionic Metamaterial Through Controlled Assembly","summary":"  Despite extensive research on magnetic skyrmions and antiskyrmions, a\nsignificant challenge remains in crafting nontrivial high-order skyrmionic\ntextures with varying, or even tailor-made, topologies. We address this\nchallenge, by focusing on a construction pathway of skyrmionic metamaterials\nwithin a monolayer thin film and suggest several skyrmionic metamaterials that\nare surprisingly stable, i.e., long-lived, due to a self-stabilization\nmechanism. This makes these new textures promising for applications. Central to\nour approach is the concept of 'simulated controlled assembly', in short, a\nprotocol inspired by 'click chemistry' that allows for positioning topological\nmagnetic structures where one likes, and then allowing for energy minimization\nto elucidate the stability. Utilizing high-throughput atomistic-spin-dynamic\nsimulations alongside state-of-the-art AI-driven tools, we have isolated\nskyrmions (topological charge Q=1), antiskyrmions (Q=-1), and skyrmionium\n(Q=0). These entities serve as foundational 'skyrmionic building blocks' to\nform the here reported intricate textures. In this work, two key contributions\nare introduced to the field of skyrmionic systems. First, we present a a novel\ncombination of atomistic spin dynamics simulations and controlled assembly\nprotocols for the stabilization and investigation of new topological magnets.\nSecond, using the aforementioned methods we report on the discovery of\nskyrmionic metamaterials.\n","authors":["Qichen Xu","Zhuanglin Shen","Alexander Edström","I. P. Miranda","Zhiwei Lu","Anders Bergman","Danny Thonig","Wanjian Yin","Olle Eriksson","Anna Delin"],"pdf_url":"https://arxiv.org/pdf/2402.10874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09718v2","updated":"2025-01-13T14:37:52Z","published":"2024-12-12T20:48:06Z","title":"BayesAdapter: enhanced uncertainty estimation in CLIP few-shot\n  adaptation","summary":"  The emergence of large pre-trained vision-language models (VLMs) represents a\nparadigm shift in machine learning, with unprecedented results in a broad span\nof visual recognition tasks. CLIP, one of the most popular VLMs, has exhibited\nremarkable zero-shot and transfer learning capabilities in classification. To\ntransfer CLIP to downstream tasks, adapters constitute a parameter-efficient\napproach that avoids backpropagation through the large model (unlike related\nprompt learning methods). However, CLIP adapters have been developed to target\ndiscriminative performance, and the quality of their uncertainty estimates has\nbeen overlooked. In this work we show that the discriminative performance of\nstate-of-the-art CLIP adapters does not always correlate with their uncertainty\nestimation capabilities, which are essential for a safe deployment in\nreal-world scenarios. We also demonstrate that one of such adapters is obtained\nthrough MAP inference from a more general probabilistic framework. Based on\nthis observation we introduce BayesAdapter, which leverages Bayesian inference\nto estimate a full probability distribution instead of a single point, better\ncapturing the variability inherent in the parameter space. In a comprehensive\nempirical evaluation we show that our approach obtains high quality uncertainty\nestimates in the predictions, standing out in calibration and selective\nclassification. Our code will be publicly available upon acceptance of the\npaper.\n","authors":["Pablo Morales-Álvarez","Stergios Christodoulidis","Maria Vakalopoulou","Pablo Piantanida","Jose Dolz"],"pdf_url":"https://arxiv.org/pdf/2412.09718v2.pdf","comment":"30 pages, 5 figures, 23 tables"},{"id":"http://arxiv.org/abs/2501.07365v1","updated":"2025-01-13T14:34:26Z","published":"2025-01-13T14:34:26Z","title":"Multimodal semantic retrieval for product search","summary":"  Semantic retrieval (also known as dense retrieval) based on textual data has\nbeen extensively studied for both web search and product search application\nfields, where the relevance of a query and a potential target document is\ncomputed by their dense vector representation comparison. Product image is\ncrucial for e-commence search interactions and is a key factor for customers at\nproduct explorations. But its impact for semantic retrieval has not been well\nstudied yet. In this research, we build a multimodal representation for product\nitems in e-commerece search in contrast to pure-text representation of\nproducts, and investigate the impact of such representations. The models are\ndeveloped and evaluated on e-commerce datasets. We demonstrate that a\nmultimodal representation scheme for a product can show improvement either on\npurchase recall or relevance accuracy in semantic retrieval. Additionally, we\nprovide numerical analysis for exclusive matches retrieved by a multimodal\nsemantic retrieval model versus a text-only semantic retrieval model, to\ndemonstrate the validation of multimodal solutions.\n","authors":["Dong Liu","Esther Lopez Ramos"],"pdf_url":"https://arxiv.org/pdf/2501.07365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13241v2","updated":"2025-01-13T14:32:28Z","published":"2024-09-20T05:57:50Z","title":"Exploring energy minimization to model strain localization as a strong\n  discontinuity using Physics Informed Neural Networks","summary":"  We explore the possibilities of using energy minimization for the numerical\nmodeling of strain localization in solids as a sharp discontinuity in the\ndisplacement field. For this purpose, we consider (regularized) strong\ndiscontinuity kinematics in elastoplastic solids. The corresponding\nmathematical model is discretized using Artificial Neural Networks (ANNs),\naiming to predict both the magnitude and location of the displacement jump from\nenergy minimization, $\\textit{i.e.}$, within a variational setting. The\narchitecture takes care of the kinematics, while the loss function takes care\nof the variational statement of the boundary value problem. The main idea\nbehind this approach is to solve both the equilibrium problem and the location\nof the localization band by means of trainable parameters in the ANN. As a\nproof of concept, we show through both 1D and 2D numerical examples that the\ncomputational modeling of strain localization for elastoplastic solids using\nenergy minimization is feasible.\n","authors":["Omar León","Víctor Rivera","Angel Vázquez-Patiño","Jacinto Ulloa","Esteban Samaniego"],"pdf_url":"https://arxiv.org/pdf/2409.13241v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07360v1","updated":"2025-01-13T14:30:01Z","published":"2025-01-13T14:30:01Z","title":"TimberVision: A Multi-Task Dataset and Framework for Log-Component\n  Segmentation and Tracking in Autonomous Forestry Operations","summary":"  Timber represents an increasingly valuable and versatile resource. However,\nforestry operations such as harvesting, handling and measuring logs still\nrequire substantial human labor in remote environments posing significant\nsafety risks. Progressively automating these tasks has the potential of\nincreasing their efficiency as well as safety, but requires an accurate\ndetection of individual logs as well as live trees and their context. Although\ninitial approaches have been proposed for this challenging application domain,\nspecialized data and algorithms are still too scarce to develop robust\nsolutions. To mitigate this gap, we introduce the TimberVision dataset,\nconsisting of more than 2k annotated RGB images containing a total of 51k trunk\ncomponents including cut and lateral surfaces, thereby surpassing any existing\ndataset in this domain in terms of both quantity and detail by a large margin.\nBased on this data, we conduct a series of ablation experiments for oriented\nobject detection and instance segmentation and evaluate the influence of\nmultiple scene parameters on model performance. We introduce a generic\nframework to fuse the components detected by our models for both tasks into\nunified trunk representations. Furthermore, we automatically derive geometric\nproperties and apply multi-object tracking to further enhance robustness. Our\ndetection and tracking approach provides highly descriptive and accurate trunk\nrepresentations solely from RGB image data, even under challenging\nenvironmental conditions. Our solution is suitable for a wide range of\napplication scenarios and can be readily combined with other sensor modalities.\n","authors":["Daniel Steininger","Julia Simon","Andreas Trondl","Markus Murschitz"],"pdf_url":"https://arxiv.org/pdf/2501.07360v1.pdf","comment":"Accepted at Winter Conference on Applications of Computer Vision\n  (WACV) 2025. Code and dataset available at\n  https://github.com/timbervision/timbervision"},{"id":"http://arxiv.org/abs/2501.07358v1","updated":"2025-01-13T14:26:39Z","published":"2025-01-13T14:26:39Z","title":"Deep Generative Clustering with VAEs and Expectation-Maximization","summary":"  We propose a novel deep clustering method that integrates Variational\nAutoencoders (VAEs) into the Expectation-Maximization (EM) framework. Our\napproach models the probability distribution of each cluster with a VAE and\nalternates between updating model parameters by maximizing the Evidence Lower\nBound (ELBO) of the log-likelihood and refining cluster assignments based on\nthe learned distributions. This enables effective clustering and generation of\nnew samples from each cluster. Unlike existing VAE-based methods, our approach\neliminates the need for a Gaussian Mixture Model (GMM) prior or additional\nregularization techniques. Experiments on MNIST and FashionMNIST demonstrate\nsuperior clustering performance compared to state-of-the-art methods.\n","authors":["Michael Adipoetra","Ségolène Martin"],"pdf_url":"https://arxiv.org/pdf/2501.07358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06782v2","updated":"2025-01-13T14:11:49Z","published":"2024-11-11T08:19:54Z","title":"QuadWBG: Generalizable Quadrupedal Whole-Body Grasping","summary":"  Legged robots with advanced manipulation capabilities have the potential to\nsignificantly improve household duties and urban maintenance. Despite\nconsiderable progress in developing robust locomotion and precise manipulation\nmethods, seamlessly integrating these into cohesive whole-body control for\nreal-world applications remains challenging. In this paper, we present a\nmodular framework for robust and generalizable whole-body loco-manipulation\ncontroller based on a single arm-mounted camera. By using reinforcement\nlearning (RL), we enable a robust low-level policy for command execution over 5\ndimensions (5D) and a grasp-aware high-level policy guided by a novel metric,\nGeneralized Oriented Reachability Map (GORM). The proposed system achieves\nstate-of-the-art one-time grasping accuracy of 89% in the real world, including\nchallenging tasks such as grasping transparent objects. Through extensive\nsimulations and real-world experiments, we demonstrate that our system can\neffectively manage a large workspace, from floor level to above body height,\nand perform diverse whole-body loco-manipulation tasks.\n","authors":["Jilong Wang","Javokhirbek Rajabov","Chaoyi Xu","Yiming Zheng","He Wang"],"pdf_url":"https://arxiv.org/pdf/2411.06782v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07346v1","updated":"2025-01-13T14:11:12Z","published":"2025-01-13T14:11:12Z","title":"Enhancing Online Reinforcement Learning with Meta-Learned Objective from\n  Offline Data","summary":"  A major challenge in Reinforcement Learning (RL) is the difficulty of\nlearning an optimal policy from sparse rewards. Prior works enhance online RL\nwith conventional Imitation Learning (IL) via a handcrafted auxiliary\nobjective, at the cost of restricting the RL policy to be sub-optimal when the\noffline data is generated by a non-expert policy. Instead, to better leverage\nvaluable information in offline data, we develop Generalized Imitation Learning\nfrom Demonstration (GILD), which meta-learns an objective that distills\nknowledge from offline data and instills intrinsic motivation towards the\noptimal policy. Distinct from prior works that are exclusive to a specific RL\nalgorithm, GILD is a flexible module intended for diverse vanilla off-policy RL\nalgorithms. In addition, GILD introduces no domain-specific hyperparameter and\nminimal increase in computational cost. In four challenging MuJoCo tasks with\nsparse rewards, we show that three RL algorithms enhanced with GILD\nsignificantly outperform state-of-the-art methods.\n","authors":["Shilong Deng","Zetao Zheng","Hongcai He","Paul Weng","Jie Shao"],"pdf_url":"https://arxiv.org/pdf/2501.07346v1.pdf","comment":"Accepted by AAAI 2025 (this version includes supplementary material)"},{"id":"http://arxiv.org/abs/2410.05450v2","updated":"2025-01-13T13:54:31Z","published":"2024-10-07T19:34:25Z","title":"AI-Driven Early Mental Health Screening: Analyzing Selfies of Pregnant\n  Women","summary":"  Major Depressive Disorder and anxiety disorders affect millions globally,\ncontributing significantly to the burden of mental health issues. Early\nscreening is crucial for effective intervention, as timely identification of\nmental health issues can significantly improve treatment outcomes. Artificial\nintelligence (AI) can be valuable for improving the screening of mental\ndisorders, enabling early intervention and better treatment outcomes. AI-driven\nscreening can leverage the analysis of multiple data sources, including facial\nfeatures in digital images. However, existing methods often rely on controlled\nenvironments or specialized equipment, limiting their broad applicability. This\nstudy explores the potential of AI models for ubiquitous depression-anxiety\nscreening given face-centric selfies. The investigation focuses on high-risk\npregnant patients, a population that is particularly vulnerable to mental\nhealth issues. To cope with limited training data resulting from our clinical\nsetup, pre-trained models were utilized in two different approaches:\nfine-tuning convolutional neural networks (CNNs) originally designed for facial\nexpression recognition and employing vision-language models (VLMs) for\nzero-shot analysis of facial expressions. Experimental results indicate that\nthe proposed VLM-based method significantly outperforms CNNs, achieving an\naccuracy of 77.6%. Although there is significant room for improvement, the\nresults suggest that VLMs can be a promising approach for mental health\nscreening.\n","authors":["Gustavo A. Basílio","Thiago B. Pereira","Alessandro L. Koerich","Hermano Tavares","Ludmila Dias","Maria das Graças da S. Teixeira","Rafael T. Sousa","Wilian H. Hisatugu","Amanda S. Mota","Anilton S. Garcia","Marco Aurélio K. Galletta","Thiago M. Paixão"],"pdf_url":"https://arxiv.org/pdf/2410.05450v2.pdf","comment":"This article has been accepted for publication in HEALTHINF25 at the\n  18th International Joint Conference on Biomedical Engineering Systems and\n  Technologies (BIOSTEC 2025)"},{"id":"http://arxiv.org/abs/2501.07337v1","updated":"2025-01-13T13:48:35Z","published":"2025-01-13T13:48:35Z","title":"Digital Operating Mode Classification of Real-World Amateur Radio\n  Transmissions","summary":"  This study presents an ML approach for classifying digital radio operating\nmodes evaluated on real-world transmissions. We generated 98 different\nparameterized radio signals from 17 digital operating modes, transmitted each\nof them on the 70 cm (UHF) amateur radio band, and recorded our transmissions\nwith two different architectures of SDR receivers. Three lightweight ML models\nwere trained exclusively on spectrograms of limited non-transmitted signals\nwith random characters as payloads. This training involved an online data\naugmentation pipeline to simulate various radio channel impairments. Our best\nmodel, EfficientNetB0, achieved an accuracy of 93.80% across the 17 operating\nmodes and 85.47% across all 98 parameterized radio signals, evaluated on our\nreal-world transmissions with Wikipedia articles as payloads. Furthermore, we\nanalyzed the impact of varying signal durations & the number of FFT bins on\nclassification, assessed the effectiveness of our simulated channel\nimpairments, and tested our models across multiple simulated SNRs.\n","authors":["Maximilian Bundscherer","Thomas H. Schmitt","Ilja Baumann","Tobias Bocklet"],"pdf_url":"https://arxiv.org/pdf/2501.07337v1.pdf","comment":"Conference IEEE ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.07335v1","updated":"2025-01-13T13:47:05Z","published":"2025-01-13T13:47:05Z","title":"TempoGPT: Enhancing Temporal Reasoning via Quantizing Embedding","summary":"  Multi-modal language model has made advanced progress in vision and audio,\nbut still faces significant challenges in dealing with complex reasoning tasks\nin the time series domain. The reasons are twofold. First, labels for\nmulti-modal time series data are coarse and devoid of analysis or reasoning\nprocesses. Training with these data cannot improve the model's reasoning\ncapabilities. Second, due to the lack of precise tokenization in processing\ntime series, the representation patterns for temporal and textual information\nare inconsistent, which hampers the effectiveness of multi-modal alignment. To\naddress these challenges, we propose a multi-modal time series data\nconstruction approach and a multi-modal time series language model (TLM),\nTempoGPT. Specially, we construct multi-modal data for complex reasoning tasks\nby analyzing the variable-system relationships within a white-box system.\nAdditionally, proposed TempoGPT achieves consistent representation between\ntemporal and textual information by quantizing temporal embeddings, where\ntemporal embeddings are quantized into a series of discrete tokens using a\npredefined codebook; subsequently, a shared embedding layer processes both\ntemporal and textual tokens. Extensive experiments demonstrate that TempoGPT\naccurately perceives temporal information, logically infers conclusions, and\nachieves state-of-the-art in the constructed complex time series reasoning\ntasks. Moreover, we quantitatively demonstrate the effectiveness of quantizing\ntemporal embeddings in enhancing multi-modal alignment and the reasoning\ncapabilities of TLMs. Code and data are available at\nhttps://github.com/zhanghaochuan20/TempoGPT.\n","authors":["Haochuan Zhang","Chunhua Yang","Jie Han","Liyang Qin","Xiaoli Wang"],"pdf_url":"https://arxiv.org/pdf/2501.07335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09541v3","updated":"2025-01-13T13:41:53Z","published":"2024-05-15T17:55:05Z","title":"Spectral complexity of deep neural networks","summary":"  It is well-known that randomly initialized, push-forward, fully-connected\nneural networks weakly converge to isotropic Gaussian processes, in the limit\nwhere the width of all layers goes to infinity. In this paper, we propose to\nuse the angular power spectrum of the limiting field to characterize the\ncomplexity of the network architecture. In particular, we define sequences of\nrandom variables associated with the angular power spectrum, and provide a full\ncharacterization of the network complexity in terms of the asymptotic\ndistribution of these sequences as the depth diverges. On this basis, we\nclassify neural networks as low-disorder, sparse, or high-disorder; we show how\nthis classification highlights a number of distinct features for standard\nactivation functions, and in particular, sparsity properties of ReLU networks.\nOur theoretical results are also validated by numerical simulations.\n","authors":["Simmaco Di Lillo","Domenico Marinucci","Michele Salvi","Stefano Vigogna"],"pdf_url":"https://arxiv.org/pdf/2405.09541v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07324v1","updated":"2025-01-13T13:36:17Z","published":"2025-01-13T13:36:17Z","title":"Foundation Models at Work: Fine-Tuning for Fairness in Algorithmic\n  Hiring","summary":"  Foundation models require fine-tuning to ensure their generative outputs\nalign with intended results for specific tasks. Automating this fine-tuning\nprocess is challenging, as it typically needs human feedback that can be\nexpensive to acquire. We present AutoRefine, a method that leverages\nreinforcement learning for targeted fine-tuning, utilizing direct feedback from\nmeasurable performance improvements in specific downstream tasks. We\ndemonstrate the method for a problem arising in algorithmic hiring platforms\nwhere linguistic biases influence a recommendation system. In this setting, a\ngenerative model seeks to rewrite given job specifications to receive more\ndiverse candidate matches from a recommendation engine which matches jobs to\ncandidates. Our model detects and regulates biases in job descriptions to meet\ndiversity and fairness criteria. The experiments on a public hiring dataset and\na real-world hiring platform showcase how large language models can assist in\nidentifying and mitigation biases in the real world.\n","authors":["Buse Sibel Korkmaz","Rahul Nair","Elizabeth M. Daly","Evangelos Anagnostopoulos","Christos Varytimidis","Antonio del Rio Chanona"],"pdf_url":"https://arxiv.org/pdf/2501.07324v1.pdf","comment":"Accepted to AAAI 2025, AI Governance Workshop"},{"id":"http://arxiv.org/abs/2403.15517v2","updated":"2025-01-13T13:32:48Z","published":"2024-03-22T11:14:30Z","title":"Improving Forward Compatibility in Class Incremental Learning by\n  Increasing Representation Rank and Feature Richness","summary":"  Class Incremental Learning (CIL) constitutes a pivotal subfield within\ncontinual learning, aimed at enabling models to progressively learn new\nclassification tasks while retaining knowledge obtained from prior tasks.\nAlthough previous studies have predominantly focused on backward compatible\napproaches to mitigate catastrophic forgetting, recent investigations have\nintroduced forward compatible methods to enhance performance on novel tasks and\ncomplement existing backward compatible methods. In this study, we introduce an\neffective-Rank based Feature Richness enhancement (RFR) method, designed for\nimproving forward compatibility. Specifically, this method increases the\neffective rank of representations during the base session, thereby facilitating\nthe incorporation of more informative features pertinent to unseen novel tasks.\nConsequently, RFR achieves dual objectives in backward and forward\ncompatibility: minimizing feature extractor modifications and enhancing novel\ntask performance, respectively. To validate the efficacy of our approach, we\nestablish a theoretical connection between effective rank and the Shannon\nentropy of representations. Subsequently, we conduct comprehensive experiments\nby integrating RFR into eleven well-known CIL methods. Our results demonstrate\nthe effectiveness of our approach in enhancing novel-task performance while\nmitigating catastrophic forgetting. Furthermore, our method notably improves\nthe average incremental accuracy across all eleven cases examined.\n","authors":["Jaeill Kim","Wonseok Lee","Moonjung Eo","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.15517v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07317v1","updated":"2025-01-13T13:28:03Z","published":"2025-01-13T13:28:03Z","title":"Evaluation of Artificial Intelligence Methods for Lead Time Prediction\n  in Non-Cycled Areas of Automotive Production","summary":"  The present study examines the effectiveness of applying Artificial\nIntelligence methods in an automotive production environment to predict unknown\nlead times in a non-cycle-controlled production area. Data structures are\nanalyzed to identify contextual features and then preprocessed using one-hot\nencoding. Methods selection focuses on supervised machine learning techniques.\nIn supervised learning methods, regression and classification methods are\nevaluated. Continuous regression based on target size distribution is not\nfeasible. Classification methods analysis shows that Ensemble Learning and\nSupport Vector Machines are the most suitable. Preliminary study results\nindicate that gradient boosting algorithms LightGBM, XGBoost, and CatBoost\nyield the best results. After further testing and extensive hyperparameter\noptimization, the final method choice is the LightGBM algorithm. Depending on\nfeature availability and prediction interval granularity, relative prediction\naccuracies of up to 90% can be achieved. Further tests highlight the importance\nof periodic retraining of AI models to accurately represent complex production\nprocesses using the database. The research demonstrates that AI methods can be\neffectively applied to highly variable production data, adding business value\nby providing an additional metric for various control tasks while outperforming\ncurrent non AI-based systems.\n","authors":["Cornelius Hake","Jonas Weigele","Frederik Reichert","Christian Friedrich"],"pdf_url":"https://arxiv.org/pdf/2501.07317v1.pdf","comment":"7 pages, 4 figures, CLC2024 Conference"},{"id":"http://arxiv.org/abs/2405.00304v3","updated":"2025-01-13T13:17:47Z","published":"2024-05-01T04:00:09Z","title":"QUACK: Quantum Aligned Centroid Kernel","summary":"  Quantum computing (QC) seems to show potential for application in machine\nlearning (ML). In particular quantum kernel methods (QKM) exhibit promising\nproperties for use in supervised ML tasks. However, a major disadvantage of\nkernel methods is their unfavorable quadratic scaling with the number of\ntraining samples. Together with the limits imposed by currently available\nquantum hardware (NISQ devices) with their low qubit coherence times, small\nnumber of qubits, and high error rates, the use of QC in ML at an industrially\nrelevant scale is currently impossible. As a small step in improving the\npotential applications of QKMs, we introduce QUACK, a quantum kernel algorithm\nwhose time complexity scales linear with the number of samples during training,\nand independent of the number of training samples in the inference stage. In\nthe training process, only the kernel entries for the samples and the centers\nof the classes are calculated, i.e. the maximum shape of the kernel for n\nsamples and c classes is (n, c). During training, the parameters of the quantum\nkernel and the positions of the centroids are optimized iteratively. In the\ninference stage, for every new sample the circuit is only evaluated for every\ncentroid, i.e. c times. We show that the QUACK algorithm nevertheless provides\nsatisfactory results and can perform at a similar level as classical kernel\nmethods with quadratic scaling during training. In addition, our (simulated)\nalgorithm is able to handle high-dimensional datasets such as MNIST with 784\nfeatures without any dimensionality reduction.\n","authors":["Kilian Tscharke","Sebastian Issel","Pascal Debus"],"pdf_url":"https://arxiv.org/pdf/2405.00304v3.pdf","comment":"2nd place Best Paper award in QML track @ IEEE International\n  Conference on Quantum Computing and Engineering (QCE) 2024"},{"id":"http://arxiv.org/abs/2501.07306v1","updated":"2025-01-13T13:16:12Z","published":"2025-01-13T13:16:12Z","title":"Variable Bregman Majorization-Minimization Algorithm and its Application\n  to Dirichlet Maximum Likelihood Estimation","summary":"  We propose a novel Bregman descent algorithm for minimizing a convex function\nthat is expressed as the sum of a differentiable part (defined over an open\nset) and a possibly nonsmooth term. The approach, referred to as the Variable\nBregman Majorization-Minimization (VBMM) algorithm, extends the Bregman\nProximal Gradient method by allowing the Bregman function used in the\ndivergence to adaptively vary at each iteration, provided it satisfies a\nmajorizing condition on the objective function. This adaptive framework enables\nthe algorithm to approximate the objective more precisely at each iteration,\nthereby allowing for accelerated convergence compared to the traditional\nBregman Proximal Gradient descent. We establish the convergence of the VBMM\nalgorithm to a minimizer under mild assumptions on the family of metrics used.\nFurthermore, we introduce a novel application of both the Bregman Proximal\nGradient method and the VBMM algorithm to the estimation of the\nmultidimensional parameters of a Dirichlet distribution through the\nmaximization of its log-likelihood. Numerical experiments confirm that the VBMM\nalgorithm outperforms existing approaches in terms of convergence speed.\n","authors":["Ségolène Martin","Jean-Christophe Pesquet","Gabriele Steidl","Ismail Ben Ayed"],"pdf_url":"https://arxiv.org/pdf/2501.07306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03387v2","updated":"2025-01-13T13:13:38Z","published":"2024-11-05T18:14:49Z","title":"Quantifying Aleatoric Uncertainty of the Treatment Effect: A Novel\n  Orthogonal Learner","summary":"  Estimating causal quantities from observational data is crucial for\nunderstanding the safety and effectiveness of medical treatments. However, to\nmake reliable inferences, medical practitioners require not only estimating\naveraged causal quantities, such as the conditional average treatment effect,\nbut also understanding the randomness of the treatment effect as a random\nvariable. This randomness is referred to as aleatoric uncertainty and is\nnecessary for understanding the probability of benefit from treatment or\nquantiles of the treatment effect. Yet, the aleatoric uncertainty of the\ntreatment effect has received surprisingly little attention in the causal\nmachine learning community. To fill this gap, we aim to quantify the aleatoric\nuncertainty of the treatment effect at the covariate-conditional level, namely,\nthe conditional distribution of the treatment effect (CDTE). Unlike average\ncausal quantities, the CDTE is not point identifiable without strong additional\nassumptions. As a remedy, we employ partial identification to obtain sharp\nbounds on the CDTE and thereby quantify the aleatoric uncertainty of the\ntreatment effect. We then develop a novel, orthogonal learner for the bounds on\nthe CDTE, which we call AU-learner. We further show that our AU-learner has\nseveral strengths in that it satisfies Neyman-orthogonality and, thus,\nquasi-oracle efficiency. Finally, we propose a fully-parametric deep learning\ninstantiation of our AU-learner.\n","authors":["Valentyn Melnychuk","Stefan Feuerriegel","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2411.03387v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07304v1","updated":"2025-01-13T13:12:18Z","published":"2025-01-13T13:12:18Z","title":"Code and Pixels: Multi-Modal Contrastive Pre-training for Enhanced\n  Tabular Data Analysis","summary":"  Learning from tabular data is of paramount importance, as it complements the\nconventional analysis of image and video data by providing a rich source of\nstructured information that is often critical for comprehensive understanding\nand decision-making processes. We present Multi-task Contrastive Masked Tabular\nModeling (MT-CMTM), a novel method aiming to enhance tabular models by\nleveraging the correlation between tabular data and corresponding images.\nMT-CMTM employs a dual strategy combining contrastive learning with masked\ntabular modeling, optimizing the synergy between these data modalities.\n  Central to our approach is a 1D Convolutional Neural Network with residual\nconnections and an attention mechanism (1D-ResNet-CBAM), designed to\nefficiently process tabular data without relying on images. This enables\nMT-CMTM to handle purely tabular data for downstream tasks, eliminating the\nneed for potentially costly image acquisition and processing.\n  We evaluated MT-CMTM on the DVM car dataset, which is uniquely suited for\nthis particular scenario, and the newly developed HIPMP dataset, which connects\nmembrane fabrication parameters with image data. Our MT-CMTM model outperforms\nthe proposed tabular 1D-ResNet-CBAM, which is trained from scratch, achieving a\nrelative 1.48% improvement in relative MSE on HIPMP and a 2.38% increase in\nabsolute accuracy on DVM. These results demonstrate MT-CMTM's robustness and\nits potential to advance the field of multi-modal learning.\n","authors":["Kankana Roy","Lars Krämer","Sebastian Domaschke","Malik Haris","Roland Aydin","Fabian Isensee","Martin Held"],"pdf_url":"https://arxiv.org/pdf/2501.07304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20287v5","updated":"2025-01-13T13:12:17Z","published":"2024-03-29T16:58:13Z","title":"Benchmarking Counterfactual Image Generation","summary":"  Generative AI has revolutionised visual content editing, empowering users to\neffortlessly modify images and videos. However, not all edits are equal. To\nperform realistic edits in domains such as natural image or medical imaging,\nmodifications must respect causal relationships inherent to the data generation\nprocess. Such image editing falls into the counterfactual image generation\nregime. Evaluating counterfactual image generation is substantially complex:\nnot only it lacks observable ground truths, but also requires adherence to\ncausal constraints. Although several counterfactual image generation methods\nand evaluation metrics exist, a comprehensive comparison within a unified\nsetting is lacking. We present a comparison framework to thoroughly benchmark\ncounterfactual image generation methods. We integrate all models that have been\nused for the task at hand and expand them to novel datasets and causal graphs,\ndemonstrating the superiority of Hierarchical VAEs across most datasets and\nmetrics. Our framework is implemented in a user-friendly Python package that\ncan be extended to incorporate additional SCMs, causal methods, generative\nmodels, and datasets for the community to build on. Code:\nhttps://github.com/gulnazaki/counterfactual-benchmark.\n","authors":["Thomas Melistas","Nikos Spyrou","Nefeli Gkouti","Pedro Sanchez","Athanasios Vlontzos","Yannis Panagakis","Giorgos Papanastasiou","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2403.20287v5.pdf","comment":"Published as a conference paper at NeurIPS 2024 Datasets and\n  Benchmarks Track https://openreview.net/forum?id=0T8xRFrScB Project page:\n  https://gulnazaki.github.io/counterfactual-benchmark"},{"id":"http://arxiv.org/abs/2501.07301v1","updated":"2025-01-13T13:10:16Z","published":"2025-01-13T13:10:16Z","title":"The Lessons of Developing Process Reward Models in Mathematical\n  Reasoning","summary":"  Process Reward Models (PRMs) emerge as a promising approach for process\nsupervision in mathematical reasoning of Large Language Models (LLMs), which\naim to identify and mitigate intermediate errors in the reasoning processes.\nHowever, the development of effective PRMs faces significant challenges,\nparticularly in data annotation and evaluation methodologies. In this paper,\nthrough extensive experiments, we demonstrate that commonly used Monte Carlo\n(MC) estimation-based data synthesis for PRMs typically yields inferior\nperformance and generalization compared to LLM-as-a-judge and human annotation\nmethods. MC estimation relies on completion models to evaluate current-step\ncorrectness, leading to inaccurate step verification. Furthermore, we identify\npotential biases in conventional Best-of-N (BoN) evaluation strategies for\nPRMs: (1) The unreliable policy models generate responses with correct answers\nbut flawed processes, leading to a misalignment between the evaluation criteria\nof BoN and the PRM objectives of process verification. (2) The tolerance of\nPRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a\nsignificant proportion of minimum scores concentrated on the final answer\nsteps, revealing the shift from process to outcome-based assessment in BoN\nOptimized PRMs. To address these challenges, we develop a consensus filtering\nmechanism that effectively integrates MC estimation with LLM-as-a-judge and\nadvocates a more comprehensive evaluation framework that combines\nresponse-level and step-level metrics. Based on the mechanisms, we\nsignificantly improve both model performance and data efficiency in the BoN\nevaluation and the step-wise error identification task. Finally, we release a\nnew state-of-the-art PRM that outperforms existing open-source alternatives and\nprovides practical guidelines for future research in building process\nsupervision models.\n","authors":["Zhenru Zhang","Chujie Zheng","Yangzhen Wu","Beichen Zhang","Runji Lin","Bowen Yu","Dayiheng Liu","Jingren Zhou","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2501.07301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07294v1","updated":"2025-01-13T13:01:00Z","published":"2025-01-13T13:01:00Z","title":"Dataset-Agnostic Recommender Systems","summary":"  [This is a position paper and does not contain any empirical or theoretical\nresults] Recommender systems have become a cornerstone of personalized user\nexperiences, yet their development typically involves significant manual\nintervention, including dataset-specific feature engineering, hyperparameter\ntuning, and configuration. To this end, we introduce a novel paradigm:\nDataset-Agnostic Recommender Systems (DAReS) that aims to enable a single\ncodebase to autonomously adapt to various datasets without the need for\nfine-tuning, for a given recommender system task. Central to this approach is\nthe Dataset Description Language (DsDL), a structured format that provides\nmetadata about the dataset's features and labels, and allow the system to\nunderstand dataset's characteristics, allowing it to autonomously manage\nprocesses like feature selection, missing values imputation, noise removal, and\nhyperparameter optimization. By reducing the need for domain-specific expertise\nand manual adjustments, DAReS offers a more efficient and scalable solution for\nbuilding recommender systems across diverse application domains. It addresses\ncritical challenges in the field, such as reusability, reproducibility, and\naccessibility for non-expert users or entry-level researchers.\n","authors":["Tri Kurniawan Wijaya","Edoardo D'Amico","Xinyang Shao"],"pdf_url":"https://arxiv.org/pdf/2501.07294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07292v1","updated":"2025-01-13T13:00:24Z","published":"2025-01-13T13:00:24Z","title":"Estimating quantum relative entropies on quantum computers","summary":"  Quantum relative entropy, a quantum generalization of the well-known\nKullback-Leibler divergence, serves as a fundamental measure of the\ndistinguishability between quantum states and plays a pivotal role in quantum\ninformation science. Despite its importance, efficiently estimating quantum\nrelative entropy between two quantum states on quantum computers remains a\nsignificant challenge. In this work, we propose the first quantum algorithm for\nestimating quantum relative entropy and Petz R\\'{e}nyi divergence from two\nunknown quantum states on quantum computers, addressing open problems\nhighlighted in [Phys. Rev. A 109, 032431 (2024)] and [IEEE Trans. Inf. Theory\n70, 5653-5680 (2024)]. This is achieved by combining quadrature approximations\nof relative entropies, the variational representation of quantum f-divergences,\nand a new technique for parameterizing Hermitian polynomial operators to\nestimate their traces with quantum states. Notably, the circuit size of our\nalgorithm is at most 2n+1 with n being the number of qubits in the quantum\nstates and it is directly applicable to distributed scenarios, where quantum\nstates to be compared are hosted on cross-platform quantum computers. We\nvalidate our algorithm through numerical simulations, laying the groundwork for\nits future deployment on quantum hardware devices.\n","authors":["Yuchen Lu","Kun Fang"],"pdf_url":"https://arxiv.org/pdf/2501.07292v1.pdf","comment":"24 pages, 10 figures; comments are welcome"},{"id":"http://arxiv.org/abs/2501.07276v1","updated":"2025-01-13T12:41:27Z","published":"2025-01-13T12:41:27Z","title":"Bridging Smart Meter Gaps: A Benchmark of Statistical, Machine Learning\n  and Time Series Foundation Models for Data Imputation","summary":"  The integrity of time series data in smart grids is often compromised by\nmissing values due to sensor failures, transmission errors, or disruptions.\nGaps in smart meter data can bias consumption analyses and hinder reliable\npredictions, causing technical and economic inefficiencies. As smart meter data\ngrows in volume and complexity, conventional techniques struggle with its\nnonlinear and nonstationary patterns. In this context, Generative Artificial\nIntelligence offers promising solutions that may outperform traditional\nstatistical methods. In this paper, we evaluate two general-purpose Large\nLanguage Models and five Time Series Foundation Models for smart meter data\nimputation, comparing them with conventional Machine Learning and statistical\nmodels. We introduce artificial gaps (30 minutes to one day) into an anonymized\npublic dataset to test inference capabilities. Results show that Time Series\nFoundation Models, with their contextual understanding and pattern recognition,\ncould significantly enhance imputation accuracy in certain cases. However, the\ntrade-off between computational cost and performance gains remains a critical\nconsideration.\n","authors":["Amir Sartipi","Joaquin Delgado Fernandez","Sergio Potenciano Menci","Alessio Magitteri"],"pdf_url":"https://arxiv.org/pdf/2501.07276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07275v1","updated":"2025-01-13T12:40:52Z","published":"2025-01-13T12:40:52Z","title":"Generating Poisoning Attacks against Ridge Regression Models with\n  Categorical Features","summary":"  Machine Learning (ML) models have become a very powerful tool to extract\ninformation from large datasets and use it to make accurate predictions and\nautomated decisions. However, ML models can be vulnerable to external attacks,\ncausing them to underperform or deviate from their expected tasks. One way to\nattack ML models is by injecting malicious data to mislead the algorithm during\nthe training phase, which is referred to as a poisoning attack. We can prepare\nfor such situations by designing anticipated attacks, which are later used for\ncreating and testing defence strategies. In this paper, we propose an algorithm\nto generate strong poisoning attacks for a ridge regression model containing\nboth numerical and categorical features that explicitly models and poisons\ncategorical features. We model categorical features as SOS-1 sets and formulate\nthe problem of designing poisoning attacks as a bilevel optimization problem\nthat is nonconvex mixed-integer in the upper-level and unconstrained convex\nquadratic in the lower-level. We present the mathematical formulation of the\nproblem, introduce a single-level reformulation based on the Karush-Kuhn-Tucker\n(KKT) conditions of the lower level, find bounds for the lower-level variables\nto accelerate solver performance, and propose a new algorithm to poison\ncategorical features. Numerical experiments show that our method improves the\nmean squared error of all datasets compared to the previous benchmark in the\nliterature.\n","authors":["Monse Guedes-Ayala","Lars Schewe","Zeynep Suvak","Miguel Anjos"],"pdf_url":"https://arxiv.org/pdf/2501.07275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20351v3","updated":"2025-01-13T12:27:56Z","published":"2024-05-28T06:59:16Z","title":"Imitating from auxiliary imperfect demonstrations via Adversarial\n  Density Weighted Regression","summary":"  We propose a novel one-step supervised imitation learning (IL) framework\ncalled Adversarial Density Regression (ADR). This IL framework aims to correct\nthe policy learned on unknown-quality to match the expert distribution by\nutilizing demonstrations, without relying on the Bellman operator.\nSpecifically, ADR addresses several limitations in previous IL algorithms:\nFirst, most IL algorithms are based on the Bellman operator, which inevitably\nsuffer from cumulative offsets from sub-optimal rewards during multi-step\nupdate processes. Additionally, off-policy training frameworks suffer from\nOut-of-Distribution (OOD) state-actions. Second, while conservative terms help\nsolve the OOD issue, balancing the conservative term is difficult. To address\nthese limitations, we fully integrate a one-step density-weighted Behavioral\nCloning (BC) objective for IL with auxiliary imperfect demonstration.\nTheoretically, we demonstrate that this adaptation can effectively correct the\ndistribution of policies trained on unknown-quality datasets to align with the\nexpert policy's distribution. Moreover, the difference between the empirical\nand the optimal value function is proportional to the upper bound of ADR's\nobjective, indicating that minimizing ADR's objective is akin to approaching\nthe optimal value. Experimentally, we validated the performance of ADR by\nconducting extensive evaluations. Specifically, ADR outperforms all of the\nselected IL algorithms on tasks from the Gym-Mujoco domain. Meanwhile, it\nachieves an 89.5% improvement over IQL when utilizing ground truth rewards on\ntasks from the Adroit and Kitchen domains. Our codebase will be released at:\nhttps://github.com/stevezhangzA/Adverserial_Density_Regression.\n","authors":["Ziqi Zhang","Zifeng Zhuang","Jingzehua Xu","Yiyuan Yang","Yubo Huang","Donglin Wang","Shuai Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.20351v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05068v2","updated":"2025-01-13T12:06:15Z","published":"2025-01-09T08:44:06Z","title":"D3RM: A Discrete Denoising Diffusion Refinement Model for Piano\n  Transcription","summary":"  Diffusion models have been widely used in the generative domain due to their\nconvincing performance in modeling complex data distributions. Moreover, they\nhave shown competitive results on discriminative tasks, such as image\nsegmentation. While diffusion models have also been explored for automatic\nmusic transcription, their performance has yet to reach a competitive level. In\nthis paper, we focus on discrete diffusion model's refinement capabilities and\npresent a novel architecture for piano transcription. Our model utilizes\nNeighborhood Attention layers as the denoising module, gradually predicting the\ntarget high-resolution piano roll, conditioned on the finetuned features of a\npretrained acoustic model. To further enhance refinement, we devise a novel\nstrategy which applies distinct transition states during training and inference\nstage of discrete diffusion models. Experiments on the MAESTRO dataset show\nthat our approach outperforms previous diffusion-based piano transcription\nmodels and the baseline model in terms of F1 score. Our code is available in\nhttps://github.com/hanshounsu/d3rm.\n","authors":["Hounsu Kim","Taegyun Kwon","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2501.05068v2.pdf","comment":"Accepted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.07251v1","updated":"2025-01-13T12:00:34Z","published":"2025-01-13T12:00:34Z","title":"MOS-Attack: A Scalable Multi-objective Adversarial Attack Framework","summary":"  Crafting adversarial examples is crucial for evaluating and enhancing the\nrobustness of Deep Neural Networks (DNNs), presenting a challenge equivalent to\nmaximizing a non-differentiable 0-1 loss function.\n  However, existing single objective methods, namely adversarial attacks focus\non a surrogate loss function, do not fully harness the benefits of engaging\nmultiple loss functions, as a result of insufficient understanding of their\nsynergistic and conflicting nature.\n  To overcome these limitations, we propose the Multi-Objective Set-based\nAttack (MOS Attack), a novel adversarial attack framework leveraging multiple\nloss functions and automatically uncovering their interrelations.\n  The MOS Attack adopts a set-based multi-objective optimization strategy,\nenabling the incorporation of numerous loss functions without additional\nparameters.\n  It also automatically mines synergistic patterns among various losses,\nfacilitating the generation of potent adversarial attacks with fewer\nobjectives.\n  Extensive experiments have shown that our MOS Attack outperforms\nsingle-objective attacks. Furthermore, by harnessing the identified synergistic\npatterns, MOS Attack continues to show superior results with a reduced number\nof loss functions.\n","authors":["Ping Guo","Cheng Gong","Xi Lin","Fei Liu","Zhichao Lu","Qingfu Zhang","Zhenkun Wang"],"pdf_url":"https://arxiv.org/pdf/2501.07251v1.pdf","comment":"Under Review of CVPR 2025"},{"id":"http://arxiv.org/abs/2501.07247v1","updated":"2025-01-13T11:55:04Z","published":"2025-01-13T11:55:04Z","title":"Interpretable machine-learning for predicting molecular weight of PLA\n  based on artificial bee colony optimization algorithm and adaptive neurofuzzy\n  inference system","summary":"  This article discusses the integration of the Artificial Bee Colony (ABC)\nalgorithm with two supervised learning methods, namely Artificial Neural\nNetworks (ANNs) and Adaptive Network-based Fuzzy Inference System (ANFIS), for\nfeature selection from Near-Infrared (NIR) spectra for predicting the molecular\nweight of medical-grade Polylactic Acid (PLA). During extrusion processing of\nPLA, in-line NIR spectra were captured along with extrusion process and machine\nsetting data. With a dataset comprising 63 observations and 512 input features,\nappropriate machine learning tools are essential for interpreting data and\nselecting features to improve prediction accuracy. Initially, the ABC\noptimization algorithm is coupled with ANN/ANFIS to forecast PLA molecular\nweight. The objective functions of the ABC algorithm are to minimize the root\nmean square error (RMSE) between experimental and predicted PLA molecular\nweights while also minimizing the number of input features. Results indicate\nthat employing ABC-ANFIS yields the lowest RMSE of 282 Da and identifies four\nsignificant parameters (NIR wavenumbers 6158 cm-1, 6310 cm-1, 6349 cm-1, and\nmelt temperature) for prediction. These findings demonstrate the effectiveness\nof using the ABC algorithm with ANFIS for selecting a minimal set of features\nto predict PLA molecular weight with high accuracy during processing\n","authors":["Amir Pouya Masoumi","Leo Creedon","Ramen Ghosh","Nimra Munir","Ross McMorrow","Marion McAfee"],"pdf_url":"https://arxiv.org/pdf/2501.07247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12094v2","updated":"2025-01-13T11:46:59Z","published":"2024-03-15T06:57:08Z","title":"Are LLMs Good Cryptic Crossword Solvers?","summary":"  Cryptic crosswords are puzzles that rely not only on general knowledge but\nalso on the solver's ability to manipulate language on different levels and\ndeal with various types of wordplay. Previous research suggests that solving\nsuch puzzles is a challenge even for modern NLP models. However, the abilities\nof large language models (LLMs) have not yet been tested on this task. In this\npaper, we establish the benchmark results for three popular LLMs -- LLaMA2,\nMistral, and ChatGPT -- showing that their performance on this task is still\nfar from that of humans.\n","authors":["Abdelrahman Sadallah","Daria Kotova","Ekaterina Kochmar"],"pdf_url":"https://arxiv.org/pdf/2403.12094v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20104v2","updated":"2025-01-13T11:46:06Z","published":"2024-12-28T10:12:12Z","title":"SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object\n  Interaction Synthesis","summary":"  Synthesizing realistic human-object interaction motions is a critical problem\nin VR/AR and human animation. Unlike the commonly studied scenarios involving a\nsingle human or hand interacting with one object, we address a more generic\nmulti-body setting with arbitrary numbers of humans, hands, and objects. This\ncomplexity introduces significant challenges in synchronizing motions due to\nthe high correlations and mutual influences among bodies. To address these\nchallenges, we introduce SyncDiff, a novel method for multi-body interaction\nsynthesis using a synchronized motion diffusion strategy. SyncDiff employs a\nsingle diffusion model to capture the joint distribution of multi-body motions.\nTo enhance motion fidelity, we propose a frequency-domain motion decomposition\nscheme. Additionally, we introduce a new set of alignment scores to emphasize\nthe synchronization of different body motions. SyncDiff jointly optimizes both\ndata sample likelihood and alignment likelihood through an explicit\nsynchronization strategy. Extensive experiments across four datasets with\nvarious multi-body configurations demonstrate the superiority of SyncDiff over\nexisting state-of-the-art motion synthesis methods.\n","authors":["Wenkun He","Yun Liu","Ruitao Liu","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2412.20104v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05409v5","updated":"2025-01-13T11:35:37Z","published":"2024-05-08T20:23:24Z","title":"Initialization is Critical to Whether Transformers Fit Composite\n  Functions by Reasoning or Memorizing","summary":"  Transformers have shown impressive capabilities across various tasks, but\ntheir performance on compositional problems remains a topic of debate. In this\nwork, we investigate the mechanisms of how transformers behave on unseen\ncompositional tasks. We discover that the parameter initialization scale plays\na critical role in determining whether the model learns inferential\n(reasoning-based) solutions, which capture the underlying compositional\nprimitives, or symmetric (memory-based) solutions, which simply memorize\nmappings without understanding the compositional structure. By analyzing the\ninformation flow and vector representations within the model, we reveal the\ndistinct mechanisms underlying these solution types. We further find that\ninferential (reasoning-based) solutions exhibit low complexity bias, which we\nhypothesize is a key factor enabling them to learn individual mappings for\nsingle anchors. We validate our conclusions on various real-world datasets. Our\nfindings provide valuable insights into the role of initialization scale in\ntuning the reasoning and memorizing ability and we propose the initialization\nrate $\\gamma$ to be a convenient tunable hyper-parameter in common deep\nlearning frameworks, where $1/d_{\\mathrm{in}}^\\gamma$ is the standard deviation\nof parameters of the layer with $d_{\\mathrm{in}}$ input neurons.\n","authors":["Zhongwang Zhang","Pengxiao Lin","Zhiwei Wang","Yaoyu Zhang","Zhi-Qin John Xu"],"pdf_url":"https://arxiv.org/pdf/2405.05409v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07237v1","updated":"2025-01-13T11:35:09Z","published":"2025-01-13T11:35:09Z","title":"Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs\n  Training","summary":"  Large language models (LLMs) have shown impressive performance across a range\nof natural language processing tasks. However, their vast number of parameters\nintroduces significant memory challenges during training, particularly when\nusing memory-intensive optimizers like Adam. Existing memory-efficient\nalgorithms often rely on techniques such as singular value decomposition\nprojection or weight freezing. While these approaches help alleviate memory\nconstraints, they generally produce suboptimal results compared to full-rank\nupdates. In this paper, we investigate the memory-efficient method beyond\nlow-rank training, proposing a novel solution called Gradient Wavelet Transform\n(GWT), which applies wavelet transforms to gradients in order to significantly\nreduce the memory requirements for maintaining optimizer states. We demonstrate\nthat GWT can be seamlessly integrated with memory-intensive optimizers,\nenabling efficient training without sacrificing performance. Through extensive\nexperiments on both pre-training and fine-tuning tasks, we show that GWT\nachieves state-of-the-art performance compared with advanced memory-efficient\noptimizers and full-rank approaches in terms of both memory usage and training\nperformance.\n","authors":["Ziqing Wen","Ping Luo","Jiahuan Wang","Xiaoge Deng","Jinping Zou","Kun Yuan","Tao Sun","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2501.07237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15622v2","updated":"2025-01-13T11:17:38Z","published":"2023-05-25T00:03:22Z","title":"GFairHint: Improving Individual Fairness for Graph Neural Networks via\n  Fairness Hint","summary":"  Given the growing concerns about fairness in machine learning and the\nimpressive performance of Graph Neural Networks (GNNs) on graph data learning,\nalgorithmic fairness in GNNs has attracted significant attention. While many\nexisting studies improve fairness at the group level, only a few works promote\nindividual fairness, which renders similar outcomes for similar individuals. A\ndesirable framework that promotes individual fairness should (1) balance\nbetween fairness and performance, (2) accommodate two commonly-used individual\nsimilarity measures (externally annotated and computed from input features),\n(3) generalize across various GNN models, and (4) be computationally efficient.\nUnfortunately, none of the prior work achieves all the desirables. In this\nwork, we propose a novel method, GFairHint, which promotes individual fairness\nin GNNs and achieves all aforementioned desirables. GFairHint learns fairness\nrepresentations through an auxiliary link prediction task, and then\nconcatenates the representations with the learned node embeddings in original\nGNNs as a \"fairness hint\". Through extensive experimental investigations on\nfive real-world graph datasets under three prevalent GNN models covering both\nindividual similarity measures above, GFairHint achieves the best fairness\nresults in almost all combinations of datasets with various backbone models,\nwhile generating comparable utility results, with much less computational cost\ncompared to the previous state-of-the-art (SoTA) method.\n","authors":["Paiheng Xu","Yuhang Zhou","Bang An","Wei Ai","Furong Huang"],"pdf_url":"https://arxiv.org/pdf/2305.15622v2.pdf","comment":"Accepted by the ACM Transactions on Knowledge Discovery from Data\n  (TKDD 2025)"},{"id":"http://arxiv.org/abs/2501.06007v2","updated":"2025-01-13T11:02:23Z","published":"2025-01-10T14:42:08Z","title":"CoNOAir: A Neural Operator for Forecasting Carbon Monoxide Evolution in\n  Cities","summary":"  Carbon Monoxide (CO) is a dominant pollutant in urban areas due to the energy\ngeneration from fossil fuels for industry, automobile, and domestic\nrequirements. Forecasting the evolution of CO in real-time can enable the\ndeployment of effective early warning systems and intervention strategies.\nHowever, the computational cost associated with the physics and chemistry-based\nsimulation makes it prohibitive to implement such a model at the city and\ncountry scale. To address this challenge, here, we present a machine learning\nmodel based on neural operator, namely, Complex Neural Operator for Air Quality\n(CoNOAir), that can effectively forecast CO concentrations. We demonstrate this\nby developing a country-level model for short-term (hourly) and long-term\n(72-hour) forecasts of CO concentrations. Our model outperforms\nstate-of-the-art models such as Fourier neural operators (FNO) and provides\nreliable predictions for both short and long-term forecasts. We further analyse\nthe capability of the model to capture extreme events and generate forecasts in\nurban cities in India. Interestingly, we observe that the model predicts the\nnext hour CO concentrations with R2 values greater than 0.95 for all the cities\nconsidered. The deployment of such a model can greatly assist the governing\nbodies to provide early warning, plan intervention strategies, and develop\neffective strategies by considering several what-if scenarios. Altogether, the\npresent approach could provide a fillip to real-time predictions of CO\npollution in urban cities.\n","authors":["Sanchit Bedi","Karn Tiwari","Prathosh A. P.","Sri Harsha Kota","N. M. Anoop Krishnan"],"pdf_url":"https://arxiv.org/pdf/2501.06007v2.pdf","comment":"28 pages, 14 figures, under submission process"},{"id":"http://arxiv.org/abs/2501.07206v1","updated":"2025-01-13T11:00:31Z","published":"2025-01-13T11:00:31Z","title":"A data-driven approach to discover and quantify systemic lupus\n  erythematosus etiological heterogeneity from electronic health records","summary":"  Systemic lupus erythematosus (SLE) is a complex heterogeneous disease with\nmany manifestational facets. We propose a data-driven approach to discover\nprobabilistic independent sources from multimodal imperfect EHR data. These\nsources represent exogenous variables in the data generation process causal\ngraph that estimate latent root causes of the presence of SLE in the health\nrecord. We objectively evaluated the sources against the original variables\nfrom which they were discovered by training supervised models to discriminate\nSLE from negative health records using a reduced set of labelled instances. We\nfound 19 predictive sources with high clinical validity and whose EHR\nsignatures define independent factors of SLE heterogeneity. Using the sources\nas input patient data representation enables models to provide with rich\nexplanations that better capture the clinical reasons why a particular record\nis (not) an SLE case. Providers may be willing to trade patient-level\ninterpretability for discrimination especially in challenging cases.\n","authors":["Marco Barbero Mota","John M. Still","Jorge L. Gamboa","Eric V. Strobl","Charles M. Stein","Vivian K. Kawai","Thomas A. Lasko"],"pdf_url":"https://arxiv.org/pdf/2501.07206v1.pdf","comment":"Received Runner-up Knowledge Discovery and Data Mining Innovation\n  Award at the American Medical Informatics Association Annual Symposium 2024"},{"id":"http://arxiv.org/abs/2501.07201v1","updated":"2025-01-13T10:53:19Z","published":"2025-01-13T10:53:19Z","title":"An Enhanced Zeroth-Order Stochastic Frank-Wolfe Framework for\n  Constrained Finite-Sum Optimization","summary":"  We propose an enhanced zeroth-order stochastic Frank-Wolfe framework to\naddress constrained finite-sum optimization problems, a structure prevalent in\nlarge-scale machine-learning applications. Our method introduces a novel double\nvariance reduction framework that effectively reduces the gradient\napproximation variance induced by zeroth-order oracles and the stochastic\nsampling variance from finite-sum objectives. By leveraging this framework, our\nalgorithm achieves significant improvements in query efficiency, making it\nparticularly well-suited for high-dimensional optimization tasks. Specifically,\nfor convex objectives, the algorithm achieves a query complexity of O(d\n\\sqrt{n}/\\epsilon ) to find an epsilon-suboptimal solution, where d is the\ndimensionality and n is the number of functions in the finite-sum objective.\nFor non-convex objectives, it achieves a query complexity of\nO(d^{3/2}\\sqrt{n}/\\epsilon^2 ) without requiring the computation ofd partial\nderivatives at each iteration. These complexities are the best known among\nzeroth-order stochastic Frank-Wolfe algorithms that avoid explicit gradient\ncalculations. Empirical experiments on convex and non-convex machine learning\ntasks, including sparse logistic regression, robust classification, and\nadversarial attacks on deep networks, validate the computational efficiency and\nscalability of our approach. Our algorithm demonstrates superior performance in\nboth convergence rate and query complexity compared to existing methods.\n","authors":["Haishan Ye","Yinghui Huang","Hao Di","Xiangyu Chang"],"pdf_url":"https://arxiv.org/pdf/2501.07201v1.pdf","comment":"35 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2501.07197v1","updated":"2025-01-13T10:44:08Z","published":"2025-01-13T10:44:08Z","title":"Lung Cancer detection using Deep Learning","summary":"  In this paper we discuss lung cancer detection using hybrid model of\nConvolutional-Neural-Networks (CNNs) and Support-Vector-Machines-(SVMs) in\norder to gain early detection of tumors, benign or malignant. The work uses\nthis hybrid model by training upon the Computed Tomography scans (CT scans) as\ndataset. Using deep learning for detecting lung cancer early is a cutting-edge\nmethod.\n","authors":["Aryan Chaudhari","Ankush Singh","Sanchi Gajbhiye","Pratham Agrawal"],"pdf_url":"https://arxiv.org/pdf/2501.07197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06076v2","updated":"2025-01-13T10:42:59Z","published":"2025-01-10T16:13:57Z","title":"A monthly sub-national Harmonized Food Insecurity Dataset for\n  comprehensive analysis and predictive modeling","summary":"  Food security is a complex, multidimensional concept challenging to measure\ncomprehensively. Effective anticipation, monitoring, and mitigation of food\ncrises require timely and comprehensive global data. This paper introduces the\nHarmonized Food Insecurity Dataset (HFID), an open-source resource\nconsolidating four key data sources: the Integrated Food Security Phase\nClassification (IPC)/Cadre Harmonis\\'e (CH) phases, the Famine Early Warning\nSystems Network (FEWS NET) IPC-compatible phases, and the World Food Program's\n(WFP) Food Consumption Score (FCS) and reduced Coping Strategy Index (rCSI).\nUpdated monthly and using a common reference system for administrative units,\nthe HFID offers extensive spatial and temporal coverage. It serves as a vital\ntool for food security experts and humanitarian agencies, providing a unified\nresource for analyzing food security conditions and highlighting global data\ndisparities. The scientific community can also leverage the HFID to develop\ndata-driven predictive models, enhancing the capacity to forecast and prevent\nfuture food crises.\n","authors":["Mélissande Machefer","Michele Ronco","Anne-Claire Thomas","Michael Assouline","Melanie Rabier","Christina Corbane","Felix Rembold"],"pdf_url":"https://arxiv.org/pdf/2501.06076v2.pdf","comment":"The authors Melissande Machefer and Michele Ronco have contributed\n  equally as both first authors to this work. This work is currently being\n  reviewed in a peer-reviewed journal"},{"id":"http://arxiv.org/abs/2501.07191v1","updated":"2025-01-13T10:38:12Z","published":"2025-01-13T10:38:12Z","title":"Pre-Trained Large Language Model Based Remaining Useful Life Transfer\n  Prediction of Bearing","summary":"  Accurately predicting the remaining useful life (RUL) of rotating machinery,\nsuch as bearings, is essential for ensuring equipment reliability and\nminimizing unexpected industrial failures. Traditional data-driven deep\nlearning methods face challenges in practical settings due to inconsistent\ntraining and testing data distributions and limited generalization for\nlong-term predictions.\n","authors":["Laifa Tao","Zhengduo Zhao","Xuesong Wang","Bin Li","Wenchao Zhan","Xuanyuan Su","Shangyu Li","Qixuan Huang","Haifei Liu","Chen Lu","Zhixuan Lian"],"pdf_url":"https://arxiv.org/pdf/2501.07191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10496v2","updated":"2025-01-13T10:34:16Z","published":"2024-09-16T17:28:21Z","title":"MusicLIME: Explainable Multimodal Music Understanding","summary":"  Multimodal models are critical for music understanding tasks, as they capture\nthe complex interplay between audio and lyrics. However, as these models become\nmore prevalent, the need for explainability grows-understanding how these\nsystems make decisions is vital for ensuring fairness, reducing bias, and\nfostering trust. In this paper, we introduce MusicLIME, a model-agnostic\nfeature importance explanation method designed for multimodal music models.\nUnlike traditional unimodal methods, which analyze each modality separately\nwithout considering the interaction between them, often leading to incomplete\nor misleading explanations, MusicLIME reveals how audio and lyrical features\ninteract and contribute to predictions, providing a holistic view of the\nmodel's decision-making. Additionally, we enhance local explanations by\naggregating them into global explanations, giving users a broader perspective\nof model behavior. Through this work, we contribute to improving the\ninterpretability of multimodal music models, empowering users to make informed\nchoices, and fostering more equitable, fair, and transparent music\nunderstanding systems.\n","authors":["Theodoros Sotirou","Vassilis Lyberatos","Orfeas Menis Mastromichalakis","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2409.10496v2.pdf","comment":"GitHub repository: https://github.com/IamTheo2000/MusicLIME. To be\n  presented at ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.07186v1","updated":"2025-01-13T10:31:36Z","published":"2025-01-13T10:31:36Z","title":"Generalizable Graph Neural Networks for Robust Power Grid Topology\n  Control","summary":"  The energy transition necessitates new congestion management methods. One\nsuch method is controlling the grid topology with machine learning (ML). This\napproach has gained popularity following the Learning to Run a Power Network\n(L2RPN) competitions. Graph neural networks (GNNs) are a class of ML models\nthat reflect graph structure in their computation, which makes them suitable\nfor power grid modeling. Various GNN approaches for topology control have thus\nbeen proposed. We propose the first GNN model for grid topology control that\nuses only GNN layers. Additionally, we identify the busbar information\nasymmetry problem that the popular homogeneous graph representation suffers\nfrom, and propose a heterogeneous graph representation to resolve it. We train\nboth homogeneous and heterogeneous GNNs and fully connected neural networks\n(FCNN) baselines on an imitation learning task. We evaluate the models\naccording to their classification accuracy and grid operation ability. We find\nthat the heterogeneous GNNs perform best on in-distribution networks, followed\nby the FCNNs, and lastly, the homogeneous GNNs. We also find that both GNN\ntypes generalize better to out-of-distribution networks than FCNNs.\n","authors":["Matthijs de Jong","Jan Viebahn","Yuliya Shapovalova"],"pdf_url":"https://arxiv.org/pdf/2501.07186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07185v1","updated":"2025-01-13T10:30:10Z","published":"2025-01-13T10:30:10Z","title":"Uncertainty Guarantees on Automated Precision Weeding using Conformal\n  Prediction","summary":"  Precision agriculture in general, and precision weeding in particular, have\ngreatly benefited from the major advancements in deep learning and computer\nvision. A large variety of commercial robotic solutions are already available\nand deployed. However, the adoption by farmers of such solutions is still low\nfor many reasons, an important one being the lack of trust in these systems.\nThis is in great part due to the opaqueness and complexity of deep neural\nnetworks and the manufacturers' inability to provide valid guarantees on their\nperformance. Conformal prediction, a well-established methodology in the\nmachine learning community, is an efficient and reliable strategy for providing\ntrustworthy guarantees on the predictions of any black-box model under very\nminimal constraints. Bridging the gap between the safe machine learning and\nprecision agriculture communities, this article showcases conformal prediction\nin action on the task of precision weeding through deep learning-based image\nclassification. After a detailed presentation of the conformal prediction\nmethodology and the development of a precision spraying pipeline based on a\n''conformalized'' neural network and well-defined spraying decision rules, the\narticle evaluates this pipeline on two real-world scenarios: one under\nin-distribution conditions, the other reflecting a near out-of-distribution\nsetting. The results show that we are able to provide formal, i.e. certifiable,\nguarantees on spraying at least 90% of the weeds.\n","authors":["Paul Melki","Lionel Bombrun","Boubacar Diallo","Jérôme Dias","Jean-Pierre da Costa"],"pdf_url":"https://arxiv.org/pdf/2501.07185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.07661v4","updated":"2025-01-13T10:20:10Z","published":"2022-10-14T09:25:47Z","title":"CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling","summary":"  Transformer has achieved remarkable success in language, image, and speech\nprocessing. Recently, various efficient attention architectures have been\nproposed to improve transformer's efficiency while largely preserving its\nefficacy, especially in modeling long sequences. A widely-used benchmark to\ntest these efficient methods' capability on long-range modeling is Long Range\nArena (LRA). However, LRA only focuses on the standard bidirectional (or\nnoncausal) self attention, and completely ignores cross attentions and\nunidirectional (or causal) attentions, which are equally important to\ndownstream applications. In this paper, we propose Comprehensive Attention\nBenchmark (CAB) under a fine-grained attention taxonomy with four\ndistinguishable attention patterns, namely, noncausal self, causal self,\nnoncausal cross, and causal cross attentions. CAB collects seven real-world\ntasks from different research areas to evaluate efficient attentions under the\nfour attention patterns. Among these tasks, CAB validates efficient attentions\nin eight backbone networks to show their generalization across neural\narchitectures. We conduct exhaustive experiments to benchmark the performances\nof nine widely-used efficient attention architectures designed with different\nphilosophies on CAB. Extensive experimental results also shed light on the\nfundamental problems of efficient attentions, such as efficiency length against\nvanilla attention, performance consistency across attention patterns, the\nbenefit of attention mechanisms, and interpolation/extrapolation on\nlong-context language modeling.\n","authors":["Jun Zhang","Shuyang Jiang","Jiangtao Feng","Lin Zheng","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2210.07661v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20971v2","updated":"2025-01-13T10:14:27Z","published":"2024-05-31T16:18:46Z","title":"Amortizing intractable inference in diffusion models for vision,\n  language, and control","summary":"  Diffusion models have emerged as effective distribution estimators in vision,\nlanguage, and reinforcement learning, but their use as priors in downstream\ntasks poses an intractable posterior inference problem. This paper studies\namortized sampling of the posterior over data, $\\mathbf{x}\\sim p^{\\rm\npost}(\\mathbf{x})\\propto p(\\mathbf{x})r(\\mathbf{x})$, in a model that consists\nof a diffusion generative model prior $p(\\mathbf{x})$ and a black-box\nconstraint or likelihood function $r(\\mathbf{x})$. We state and prove the\nasymptotic correctness of a data-free learning objective, relative trajectory\nbalance, for training a diffusion model that samples from this posterior, a\nproblem that existing methods solve only approximately or in restricted cases.\nRelative trajectory balance arises from the generative flow network perspective\non diffusion models, which allows the use of deep reinforcement learning\ntechniques to improve mode coverage. Experiments illustrate the broad potential\nof unbiased inference of arbitrary posteriors under diffusion priors: in vision\n(classifier guidance), language (infilling under a discrete diffusion LLM), and\nmultimodal data (text-to-image generation). Beyond generative modeling, we\napply relative trajectory balance to the problem of continuous control with a\nscore-based behavior prior, achieving state-of-the-art results on benchmarks in\noffline reinforcement learning.\n","authors":["Siddarth Venkatraman","Moksh Jain","Luca Scimeca","Minsu Kim","Marcin Sendera","Mohsin Hasan","Luke Rowe","Sarthak Mittal","Pablo Lemos","Emmanuel Bengio","Alexandre Adam","Jarrid Rector-Brooks","Yoshua Bengio","Glen Berseth","Nikolay Malkin"],"pdf_url":"https://arxiv.org/pdf/2405.20971v2.pdf","comment":"NeurIPS 2024; code: https://github.com/GFNOrg/diffusion-finetuning"},{"id":"http://arxiv.org/abs/2501.07173v1","updated":"2025-01-13T10:05:47Z","published":"2025-01-13T10:05:47Z","title":"Knowledge Distillation and Enhanced Subdomain Adaptation Using Graph\n  Convolutional Network for Resource-Constrained Bearing Fault Diagnosis","summary":"  Bearing fault diagnosis under varying working conditions faces challenges,\nincluding a lack of labeled data, distribution discrepancies, and resource\nconstraints. To address these issues, we propose a progressive knowledge\ndistillation framework that transfers knowledge from a complex teacher model,\nutilizing a Graph Convolutional Network (GCN) with Autoregressive moving\naverage (ARMA) filters, to a compact and efficient student model. To mitigate\ndistribution discrepancies and labeling uncertainty, we introduce Enhanced\nLocal Maximum Mean Squared Discrepancy (ELMMSD), which leverages mean and\nvariance statistics in the Reproducing Kernel Hilbert Space (RKHS) and\nincorporates a priori probability distributions between labels. This approach\nincreases the distance between clustering centers, bridges subdomain gaps, and\nenhances subdomain alignment reliability. Experimental results on benchmark\ndatasets (CWRU and JNU) demonstrate that the proposed method achieves superior\ndiagnostic accuracy while significantly reducing computational costs.\nComprehensive ablation studies validate the effectiveness of each component,\nhighlighting the robustness and adaptability of the approach across diverse\nworking conditions.\n","authors":["Mohammadreza Kavianpour","Parisa Kavianpour","Amin Ramezani","Mohammad TH Beheshti"],"pdf_url":"https://arxiv.org/pdf/2501.07173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07172v1","updated":"2025-01-13T10:04:55Z","published":"2025-01-13T10:04:55Z","title":"Anomalous Agreement: How to find the Ideal Number of Anomaly Classes in\n  Correlated, Multivariate Time Series Data","summary":"  Detecting and classifying abnormal system states is critical for condition\nmonitoring, but supervised methods often fall short due to the rarity of\nanomalies and the lack of labeled data. Therefore, clustering is often used to\ngroup similar abnormal behavior. However, evaluating cluster quality without\nground truth is challenging, as existing measures such as the Silhouette Score\n(SSC) only evaluate the cohesion and separation of clusters and ignore possible\nprior knowledge about the data. To address this challenge, we introduce the\nSynchronized Anomaly Agreement Index (SAAI), which exploits the synchronicity\nof anomalies across multivariate time series to assess cluster quality. We\ndemonstrate the effectiveness of SAAI by showing that maximizing SAAI improves\naccuracy on the task of finding the true number of anomaly classes K in\ncorrelated time series by 0.23 compared to SSC and by 0.32 compared to X-Means.\nWe also show that clusters obtained by maximizing SAAI are easier to interpret\ncompared to SSC.\n","authors":["Ferdinand Rewicki","Joachim Denzler","Julia Niebling"],"pdf_url":"https://arxiv.org/pdf/2501.07172v1.pdf","comment":"Acccepted at AAAI Workshop on AI for Time Series Analysis (AI4TS)\n  2025"},{"id":"http://arxiv.org/abs/2409.01990v3","updated":"2025-01-13T10:02:27Z","published":"2024-09-03T15:35:01Z","title":"Efficient Large Foundation Models Design: A Perspective From Model and\n  System Co-Design","summary":"  This paper focuses on modern efficient training and inference technologies on\nfoundation models and illustrates them from two perspectives: model and system\ndesign. Model and System Design optimize LLM training and inference from\ndifferent aspects to save computational resources, making LLMs more efficient,\naffordable, and more accessible. The paper list repository is available at\n\\url{https://github.com/NoakLiu/Efficient-Foundation-Models-Survey}\n","authors":["Dong Liu","Yanxuan Yu","Zhixin Lai","Yite Wang","Jing Wu","Zhongwei Wan","Sina Alinejad","Benjamin Lengerich","Ying Nian Wu"],"pdf_url":"https://arxiv.org/pdf/2409.01990v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07155v1","updated":"2025-01-13T09:28:47Z","published":"2025-01-13T09:28:47Z","title":"AlphaNet: Scaling Up Local Frame-based Atomistic Foundation Model","summary":"  We present AlphaNet, a local frame-based equivariant model designed to\nachieve both accurate and efficient simulations for atomistic systems.\nRecently, machine learning force fields (MLFFs) have gained prominence in\nmolecular dynamics simulations due to their advantageous efficiency-accuracy\nbalance compared to classical force fields and quantum mechanical calculations,\nalongside their transferability across various systems. Despite the\nadvancements in improving model accuracy, the efficiency and scalability of\nMLFFs remain significant obstacles in practical applications. AlphaNet enhances\ncomputational efficiency and accuracy by leveraging the local geometric\nstructures of atomic environments through the construction of equivariant local\nframes and learnable frame transitions. We substantiate the efficacy of\nAlphaNet across diverse datasets, including defected graphene, formate\ndecomposition, zeolites, and surface reactions. AlphaNet consistently surpasses\nwell-established models, such as NequIP and DeepPot, in terms of both energy\nand force prediction accuracy. Notably, AlphaNet offers one of the best\ntrade-offs between computational efficiency and accuracy among existing models.\nMoreover, AlphaNet exhibits scalability across a broad spectrum of system and\ndataset sizes, affirming its versatility.\n","authors":["Bangchen Yin","Jiaao Wang","Weitao Du","Pengbo Wang","Penghua Ying","Haojun Jia","Zisheng Zhang","Yuanqi Du","Carla P. Gomes","Chenru Duan","Hai Xiao","Graeme Henkelman"],"pdf_url":"https://arxiv.org/pdf/2501.07155v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.05223v2","updated":"2025-01-13T09:27:23Z","published":"2025-01-09T13:19:59Z","title":"EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic\n  Regression on Heterogeneous Database","summary":"  Accurate nonlinear computation is a key challenge in privacy-preserving\nmachine learning (PPML). Most existing frameworks approximate it through linear\noperations, resulting in significant precision loss. This paper proposes an\nefficient, verifiable and accurate security 2-party logistic regression\nframework (EVA-S2PLoR), which achieves accurate nonlinear function computation\nthrough a novel secure element-wise multiplication protocol and its derived\nprotocols. Our framework primarily includes secure 2-party vector element-wise\nmultiplication, addition to multiplication, reciprocal, and sigmoid function\nbased on data disguising technology, where high efficiency and accuracy are\nguaranteed by the simple computation flow based on the real number domain and\nthe few number of fixed communication rounds. We provide secure and robust\nanomaly detection through dimension transformation and Monte Carlo methods.\nEVA-S2PLoR outperforms many advanced frameworks in terms of precision\n(improving the performance of the sigmoid function by about 10 orders of\nmagnitude compared to most frameworks) and delivers the best overall\nperformance in secure logistic regression experiments.\n","authors":["Tianle Tao","Shizhao Peng","Tianyu Mei","Shoumo Li","Haogang Zhu"],"pdf_url":"https://arxiv.org/pdf/2501.05223v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13139v3","updated":"2025-01-13T09:19:48Z","published":"2024-08-23T15:01:37Z","title":"Optimally Solving Simultaneous-Move Dec-POMDPs: The Sequential Central\n  Planning Approach","summary":"  The centralized training for decentralized execution paradigm emerged as the\nstate-of-the-art approach to $\\epsilon$-optimally solving decentralized\npartially observable Markov decision processes. However, scalability remains a\nsignificant issue. This paper presents a novel and more scalable alternative,\nnamely the sequential-move centralized training for decentralized execution.\nThis paradigm further pushes the applicability of the Bellman's principle of\noptimality, raising three new properties. First, it allows a central planner to\nreason upon sufficient sequential-move statistics instead of prior\nsimultaneous-move ones. Next, it proves that $\\epsilon$-optimal value functions\nare piecewise linear and convex in such sufficient sequential-move statistics.\nFinally, it drops the complexity of the backup operators from double\nexponential to polynomial at the expense of longer planning horizons. Besides,\nit makes it easy to use single-agent methods, e.g., SARSA algorithm enhanced\nwith these findings, while still preserving convergence guarantees. Experiments\non two- as well as many-agent domains from the literature against\n$\\epsilon$-optimal simultaneous-move solvers confirm the superiority of our\nnovel approach. This paradigm opens the door for efficient planning and\nreinforcement learning methods for multi-agent systems.\n","authors":["Johan Peralez","Aurèlien Delage","Jacopo Castellini","Rafael F. Cunha","Jilles S. Dibangoye"],"pdf_url":"https://arxiv.org/pdf/2408.13139v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07146v1","updated":"2025-01-13T09:11:33Z","published":"2025-01-13T09:11:33Z","title":"TIMRL: A Novel Meta-Reinforcement Learning Framework for Non-Stationary\n  and Multi-Task Environments","summary":"  In recent years, meta-reinforcement learning (meta-RL) algorithm has been\nproposed to improve sample efficiency in the field of decision-making and\ncontrol, enabling agents to learn new knowledge from a small number of samples.\nHowever, most research uses the Gaussian distribution to extract task\nrepresentation, which is poorly adapted to tasks that change in non-stationary\nenvironment. To address this problem, we propose a novel meta-reinforcement\nlearning method by leveraging Gaussian mixture model and the transformer\nnetwork to construct task inference model. The Gaussian mixture model is\nutilized to extend the task representation and conduct explicit encoding of\ntasks. Specifically, the classification of tasks is encoded through transformer\nnetwork to determine the Gaussian component corresponding to the task. By\nleveraging task labels, the transformer network is trained using supervised\nlearning. We validate our method on MuJoCo benchmarks with non-stationary and\nmulti-task environments. Experimental results demonstrate that the proposed\nmethod dramatically improves sample efficiency and accurately recognizes the\nclassification of the tasks, while performing excellently in the environment.\n","authors":["Chenyang Qi","Huiping Li","Panfeng Huang"],"pdf_url":"https://arxiv.org/pdf/2501.07146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07145v1","updated":"2025-01-13T09:11:13Z","published":"2025-01-13T09:11:13Z","title":"A User's Guide to $\\texttt{KSig}$: GPU-Accelerated Computation of the\n  Signature Kernel","summary":"  The signature kernel is a positive definite kernel for sequential and\ntemporal data that has become increasingly popular in machine learning\napplications due to powerful theoretical guarantees, strong empirical\nperformance, and recently introduced various scalable variations. In this\nchapter, we give a short introduction to $\\texttt{KSig}$, a\n$\\texttt{Scikit-Learn}$ compatible Python package that implements various\nGPU-accelerated algorithms for computing signature kernels, and performing\ndownstream learning tasks. We also introduce a new algorithm based on tensor\nsketches which gives strong performance compared to existing algorithms. The\npackage is available at\n$\\href{https://github.com/tgcsaba/ksig}{\\texttt{https://github.com/tgcsaba/ksig}}$.\n","authors":["Csaba Tóth","Danilo Jr Dela Cruz","Harald Oberhauser"],"pdf_url":"https://arxiv.org/pdf/2501.07145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10676v2","updated":"2025-01-13T09:10:18Z","published":"2024-11-16T02:41:12Z","title":"Exploring Feature-based Knowledge Distillation for Recommender System: A\n  Frequency Perspective","summary":"  In this paper, we analyze the feature-based knowledge distillation for\nrecommendation from the frequency perspective. By defining knowledge as\ndifferent frequency components of the features, we theoretically demonstrate\nthat regular feature-based knowledge distillation is equivalent to equally\nminimizing losses on all knowledge and further analyze how this equal loss\nweight allocation method leads to important knowledge being overlooked. In\nlight of this, we propose to emphasize important knowledge by redistributing\nknowledge weights. Furthermore, we propose FreqD, a lightweight knowledge\nreweighting method, to avoid the computational cost of calculating losses on\neach knowledge. Extensive experiments demonstrate that FreqD consistently and\nsignificantly outperforms state-of-the-art knowledge distillation methods for\nrecommender systems. Our code is available at https://github.com/woriazzc/KDs.\n","authors":["Zhangchi Zhu","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.10676v2.pdf","comment":"ACM KDD 2025 Accepted"},{"id":"http://arxiv.org/abs/2409.08303v2","updated":"2025-01-13T09:03:56Z","published":"2024-09-10T20:04:27Z","title":"Explainable Metrics for the Assessment of Neurodegenerative Diseases\n  through Handwriting Analysis","summary":"  Motor dysfunction is a common sign of neurodegenerative diseases (NDs) such\nas Parkinson's disease (PD) and Alzheimer's disease (AD), but may be difficult\nto detect, especially in the early stages. In this work, we examine the\nbehavior of a wide array of explainable metrics extracted from the handwriting\nsignals of 113 subjects performing multiple tasks on a digital tablet, as part\nof the Neurological Signals dataset. The aim is to measure their effectiveness\nin characterizing NDs, including AD and PD. To this end, task-agnostic and\ntask-specific metrics are extracted from 14 distinct tasks. Subsequently,\nthrough statistical analysis and a series of classification experiments, we\ninvestigate which metrics provide greater discriminative power between NDs and\nhealthy controls and amongst different NDs. Preliminary results indicate that\nthe tasks at hand can all be effectively leveraged to distinguish between the\nconsidered set of NDs, specifically by measuring the stability, the speed of\nwriting, the time spent not writing, and the pressure variations between groups\nfrom our handcrafted explainable metrics, which shows p-values lower than\n0.0001 for multiple tasks. Using various binary classification algorithms on\nthe computed metrics, we obtain up to 87 % accuracy for the discrimination\nbetween AD and healthy controls (CTL), and up to 69 % for the discrimination\nbetween PD and CTL.\n","authors":["Thomas Thebaud","Anna Favaro","Casey Chen","Gabrielle Chavez","Laureano Moro-Velazquez","Ankur Butala","Najim Dehak"],"pdf_url":"https://arxiv.org/pdf/2409.08303v2.pdf","comment":"14 pages including references, under review in IEEE JHBI"},{"id":"http://arxiv.org/abs/2404.14047v3","updated":"2025-01-13T09:01:13Z","published":"2024-04-22T10:03:03Z","title":"An empirical study of LLaMA3 quantization: from LLMs to MLLMs","summary":"  The LLaMA family, a collection of foundation language models ranging from 7B\nto 65B parameters, has become one of the most powerful open-source large\nlanguage models (LLMs) and the popular LLM backbone of multi-modal large\nlanguage models (MLLMs), widely used in computer vision and natural language\nunderstanding tasks. In particular, LLaMA3 models have recently been released\nand have achieved impressive performance in various domains with super-large\nscale pre-training on over 15T tokens of data. Given the wide application of\nlow-bit quantization for LLMs in resource-constrained scenarios, we explore\nLLaMA3's capabilities when quantized to low bit-width. This exploration can\npotentially provide new insights and challenges for the low-bit quantization of\nLLaMA3 and other future LLMs, especially in addressing performance degradation\nissues that suffer in LLM compression. Specifically, we comprehensively\nevaluate the 10 existing post-training quantization and LoRA fine-tuning\n(LoRA-FT) methods of LLaMA3 on 1-8 bits and various datasets to reveal the\nlow-bit quantization performance of LLaMA3. To uncover the capabilities of\nlow-bit quantized MLLM, we assessed the performance of the LLaMA3-based\nLLaVA-Next-8B model under 2-4 ultra-low bits with post-training quantization\nmethods. Our experimental results indicate that LLaMA3 still suffers from\nnon-negligible degradation in linguistic and visual contexts, particularly\nunder ultra-low bit widths. This highlights the significant performance gap at\nlow bit-width that needs to be addressed in future developments. We expect that\nthis empirical study will prove valuable in advancing future models, driving\nLLMs and MLLMs to achieve higher accuracy at lower bit to enhance practicality.\nOur project is released on https://github.com/Macaronlin/LLaMA3-Quantization ,\nand quantized models are released at https://huggingface.co/Efficient-ML .\n","authors":["Wei Huang","Xingyu Zheng","Xudong Ma","Haotong Qin","Chengtao Lv","Hong Chen","Jie Luo","Xiaojuan Qi","Xianglong Liu","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2404.14047v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07124v1","updated":"2025-01-13T08:26:43Z","published":"2025-01-13T08:26:43Z","title":"LLM360 K2: Scaling Up 360-Open-Source Large Language Models","summary":"  We detail the training of the LLM360 K2-65B model, scaling up our 360-degree\nOPEN SOURCE approach to the largest and most powerful models under project\nLLM360. While open-source LLMs continue to advance, the answer to \"How are the\nlargest LLMs trained?\" remains unclear within the community. The implementation\ndetails for such high-capacity models are often protected due to business\nconsiderations associated with their high cost. This lack of transparency\nprevents LLM researchers from leveraging valuable insights from prior\nexperience, e.g., \"What are the best practices for addressing loss spikes?\" The\nLLM360 K2 project addresses this gap by providing full transparency and access\nto resources accumulated during the training of LLMs at the largest scale. This\nreport highlights key elements of the K2 project, including our first model, K2\nDIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals\nLLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the\nimplementation steps and present a longitudinal analysis of K2 DIAMOND's\ncapabilities throughout its training process. We also outline ongoing projects\nsuch as TXT360, setting the stage for future models in the series. By offering\npreviously unavailable resources, the K2 project also resonates with the\n360-degree OPEN SOURCE principles of transparency, reproducibility, and\naccessibility, which we believe are vital in the era of resource-intensive AI\nresearch.\n","authors":["Zhengzhong Liu","Bowen Tan","Hongyi Wang","Willie Neiswanger","Tianhua Tao","Haonan Li","Fajri Koto","Yuqi Wang","Suqi Sun","Omkar Pangarkar","Richard Fan","Yi Gu","Victor Miller","Liqun Ma","Liping Tang","Nikhil Ranjan","Yonghao Zhuang","Guowei He","Renxi Wang","Mingkai Deng","Robin Algayres","Yuanzhi Li","Zhiqiang Shen","Preslav Nakov","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2501.07124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07123v1","updated":"2025-01-13T08:25:14Z","published":"2025-01-13T08:25:14Z","title":"Inferring Interpretable Models of Fragmentation Functions using Symbolic\n  Regression","summary":"  Machine learning is rapidly making its path into natural sciences, including\nhigh-energy physics. We present the first study that infers, directly from\nexperimental data, a functional form of fragmentation functions. The latter\nrepresent a key ingredient to describe physical observables measured in\nhigh-energy physics processes that involve hadron production, and predict their\nvalues at different energy. Fragmentation functions can not be calculated in\ntheory and have to be determined instead from data. Traditional approaches rely\non global fits of experimental data using a pre-assumed functional form\ninspired from phenomenological models to learn its parameters. This novel\napproach uses a ML technique, namely symbolic regression, to learn an\nanalytical model from measured charged hadron multiplicities. The function\nlearned by symbolic regression resembles the Lund string function and describes\nthe data well, thus representing a potential candidate for use in global FFs\nfits. This study represents an approach to follow in such QCD-related\nphenomenology studies and more generally in sciences.\n","authors":["Nour Makke","Sanjay Chawla"],"pdf_url":"https://arxiv.org/pdf/2501.07123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16264v2","updated":"2025-01-13T08:00:49Z","published":"2024-12-20T09:22:07Z","title":"Continual Learning with Strategic Selection and Forgetting for Network\n  Intrusion Detection","summary":"  Intrusion Detection Systems (IDS) are crucial for safeguarding digital\ninfrastructure. In dynamic network environments, both threat landscapes and\nnormal operational behaviors are constantly changing, resulting in concept\ndrift. While continuous learning mitigates the adverse effects of concept\ndrift, insufficient attention to drift patterns and excessive preservation of\noutdated knowledge can still hinder the IDS's adaptability. In this paper, we\npropose SSF (Strategic Selection and Forgetting), a novel continual learning\nmethod for IDS, providing continuous model updates with a constantly refreshed\nmemory buffer. Our approach features a strategic sample selection algorithm to\nselect representative new samples and a strategic forgetting mechanism to drop\noutdated samples. The proposed strategic sample selection algorithm prioritizes\nnew samples that cause the `drifted' pattern, enabling the model to better\nunderstand the evolving landscape. Additionally, we introduce strategic\nforgetting upon detecting significant drift by discarding outdated samples to\nfree up memory, allowing the incorporation of more recent data. SSF captures\nevolving patterns effectively and ensures the model is aligned with the change\nof data patterns, significantly enhancing the IDS's adaptability to concept\ndrift. The state-of-the-art performance of SSF on NSL-KDD and UNSW-NB15\ndatasets demonstrates its superior adaptability to concept drift for network\nintrusion detection.\n","authors":["Xinchen Zhang","Running Zhao","Zhihan Jiang","Handi Chen","Yulong Ding","Edith C. H. Ngai","Shuang-Hua Yang"],"pdf_url":"https://arxiv.org/pdf/2412.16264v2.pdf","comment":"Accepted by IEEE International Conference on Computer Communications\n  (INFOCOM) 2025"},{"id":"http://arxiv.org/abs/2412.13973v2","updated":"2025-01-13T07:50:54Z","published":"2024-12-18T15:50:50Z","title":"Model-Agnostic Cosmological Inference with SDSS-IV eBOSS: Simultaneous\n  Probing for Background and Perturbed Universe","summary":"  Here we explore certain subtle features imprinted in data from the completed\nSloan Digital Sky Survey IV (SDSS-IV) extended Baryon Oscillation Spectroscopic\nSurvey (eBOSS) as a combined probe for the background and perturbed Universe.\nWe reconstruct the baryon Acoustic Oscillation (BAO) and Redshift Space\nDistortion (RSD) observables as functions of redshift, using measurements from\nSDSS alone. We apply the Multi-Task Gaussian Process (MTGP) framework to model\nthe interdependencies of cosmological observables $D_M(z)/r_d$, $D_H(z)/r_d$,\nand $f\\sigma_8(z)$, and track their evolution across different redshifts.\nSubsequently, we obtain constrained three-dimensional phase space containing\n$D_M(z)/r_d$, $D_H(z)/r_d$, and $f\\sigma_8(z)$ at different redshifts probed by\nthe SDSS-IV eBOSS survey. Furthermore, assuming the $\\Lambda$CDM model, we\nobtain constraints on model parameters $\\Omega_{m}$, $H_{0}r_{d}$, $\\sigma_{8}$\nand $S_{8}$ at each redshift probed by SDSS-IV eBOSS. This indicates\nredshift-dependent trends in $H_0$, $\\Omega_m$, $\\sigma_8$ and $S_8$ in the\n$\\Lambda$CDM model, suggesting a possible inconsistency in the $\\Lambda$CDM\nmodel. Ours is a template for model-independent extraction of information for\nboth background and perturbed Universe using a single galaxy survey taking into\naccount all the existing correlations between background and perturbed\nobservables and this can be easily extended to future DESI-3YR as well as\nEuclid results.\n","authors":["Purba Mukherjee","Anjan A. Sen"],"pdf_url":"https://arxiv.org/pdf/2412.13973v2.pdf","comment":"14 pages, 7 sets of figures, 3 tables. Comments are welcome. New\n  references added"},{"id":"http://arxiv.org/abs/2501.05809v2","updated":"2025-01-13T07:49:28Z","published":"2025-01-10T09:19:10Z","title":"AdaPRL: Adaptive Pairwise Regression Learning with Uncertainty\n  Estimation for Universal Regression Tasks","summary":"  Current deep regression models usually learn in point-wise way that treat\neach sample as an independent input, neglecting the relative ordering among\ndifferent data. Consequently, the regression model could neglect the data 's\ninterrelationships, potentially resulting in suboptimal performance. Moreover,\nthe existence of aleatoric uncertainty in the training data may drive the model\nto capture non-generalizable patterns, contributing to increased overfitting.\nTo address these issues, we propose a novel adaptive pairwise learning\nframework (AdaPRL) for regression tasks which leverages the relative\ndifferences between data points and integrates with deep probabilistic models\nto quantify the uncertainty associated with the predictions. Additionally, we\nadapt AdaPRL for applications in multi-task learning and multivariate time\nseries forecasting. Extensive experiments with several real-world regression\ndatasets including recommendation systems, age estimation, time series\nforecasting, natural language understanding, finance, and industry datasets\nshow that AdaPRL is compatible with different backbone networks in various\ntasks and achieves state-of-the-art performance on the vast majority of tasks,\nhighlighting its notable potential including enhancing prediction accuracy and\nranking ability, increasing generalization capability, improving robustness to\nnoisy data, improving resilience to reduced data, and enhancing\ninterpretability, etc.\n","authors":["Fuhang Liang","Rucong Xu","Deng Lin"],"pdf_url":"https://arxiv.org/pdf/2501.05809v2.pdf","comment":"22 pages, 11 figures"},{"id":"http://arxiv.org/abs/2409.17692v3","updated":"2025-01-13T07:41:44Z","published":"2024-09-26T09:57:16Z","title":"MIO: A Foundation Model on Multimodal Tokens","summary":"  In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.\n","authors":["Zekun Wang","King Zhu","Chunpu Xu","Wangchunshu Zhou","Jiaheng Liu","Yibo Zhang","Jiashuo Wang","Ning Shi","Siyu Li","Yizhi Li","Haoran Que","Zhaoxiang Zhang","Yuanxing Zhang","Ge Zhang","Ke Xu","Jie Fu","Wenhao Huang"],"pdf_url":"https://arxiv.org/pdf/2409.17692v3.pdf","comment":"Technical Report. Codes and models are available in\n  https://github.com/MIO-Team/MIO"},{"id":"http://arxiv.org/abs/2411.14789v2","updated":"2025-01-13T07:29:53Z","published":"2024-11-22T08:17:46Z","title":"Simplifying CLIP: Unleashing the Power of Large-Scale Models on\n  Consumer-level Computers","summary":"  Contrastive Language-Image Pre-training (CLIP) has attracted a surge of\nattention for its superior zero-shot performance and excellent transferability\nto downstream tasks. However, training such large-scale models usually requires\nsubstantial computation and storage, which poses barriers for general users\nwith consumer-level computers. Motivated by this observation, in this paper we\ninvestigate how to achieve competitive performance on only one Nvidia RTX3090\nGPU and with one terabyte for storing dataset. On one hand, we simplify the\ntransformer block structure and combine Weight Inheritance with multi-stage\nKnowledge Distillation (WIKD), thereby reducing the parameters and improving\nthe inference speed during training along with deployment. On the other hand,\nconfronted with the convergence challenge posed by small dataset, we generate\nsynthetic captions for each sample as data augmentation, and devise a novel\nPair Matching (PM) loss to fully exploit the distinguishment among positive and\nnegative image-text pairs. Extensive experiments demonstrate that our model can\nachieve a new state-of-the-art datascale-parameter-accuracy tradeoff, which\ncould further popularize the CLIP model in the related research community.\n","authors":["Hongbo Liu"],"pdf_url":"https://arxiv.org/pdf/2411.14789v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07249v2","updated":"2025-01-13T07:22:02Z","published":"2024-12-10T07:18:51Z","title":"Buster: Implanting Semantic Backdoor into Text Encoder to Mitigate NSFW\n  Content Generation","summary":"  The rise of deep learning models in the digital era has raised substantial\nconcerns regarding the generation of Not-Safe-for-Work (NSFW) content. Existing\ndefense methods primarily involve model fine-tuning and post-hoc content\nmoderation. Nevertheless, these approaches largely lack scalability in\neliminating harmful content, degrade the quality of benign image generation, or\nincur high inference costs. To address these challenges, we propose an\ninnovative framework named \\textit{Buster}, which injects backdoors into the\ntext encoder to prevent NSFW content generation. Buster leverages deep semantic\ninformation rather than explicit prompts as triggers, redirecting NSFW prompts\ntowards targeted benign prompts. Additionally, Buster employs energy-based\ntraining data generation through Langevin dynamics for adversarial knowledge\naugmentation, thereby ensuring robustness in harmful concept definition. This\napproach demonstrates exceptional resilience and scalability in mitigating NSFW\ncontent. Particularly, Buster fine-tunes the text encoder of Text-to-Image\nmodels within merely five minutes, showcasing its efficiency. Our extensive\nexperiments denote that Buster outperforms nine state-of-the-art baselines,\nachieving a superior NSFW content removal rate of at least 91.2\\% while\npreserving the quality of harmless images.\n","authors":["Xin Zhao","Xiaojun Chen","Yuexin Xuan","Zhendong Zhao","Xiaojun Jia","Xinfeng Li","Xiaofeng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.07249v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19943v3","updated":"2025-01-13T06:53:56Z","published":"2024-11-29T18:58:22Z","title":"Critical Tokens Matter: Token-Level Contrastive Estimation Enhances\n  LLM's Reasoning Capability","summary":"  Mathematical reasoning tasks pose significant challenges for large language\nmodels (LLMs) because they require precise logical deduction and sequence\nanalysis. In this work, we introduce the concept of critical tokens -- elements\nwithin reasoning trajectories that significantly influence incorrect outcomes.\nWe present a novel framework for identifying these tokens through rollout\nsampling and demonstrate their substantial divergence from traditional error\ntokens. Through extensive experiments on datasets such as GSM8K and MATH500, we\nshow that identifying and replacing critical tokens significantly improves\nmodel accuracy. We propose an efficient methodology for pinpointing these\ntokens in large-scale datasets using contrastive estimation and extend this\nframework to enhance model training processes with direct preference\noptimization (DPO). Experimental results on GSM8K and MATH500 benchmarks with\nthe widely used models Llama-3 (8B and 70B) and Deepseek-math (7B) demonstrate\nthe effectiveness of the proposed approach, cDPO. Our results underscore the\npotential of leveraging critical tokens to reduce errors in reasoning tasks,\nadvancing the development of AI systems capable of robust logical deduction.\nOur code, annotated datasets, and trained models are available at\nhttps://github.com/chenzhiling9954/Critical-Tokens-Matter to support and\nencourage future research in this promising field.\n","authors":["Zicheng Lin","Tian Liang","Jiahao Xu","Qiuzhi Lin","Xing Wang","Ruilin Luo","Chufan Shi","Siheng Li","Yujiu Yang","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2411.19943v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.10182v5","updated":"2025-01-13T06:51:13Z","published":"2024-03-15T10:38:48Z","title":"Fast and reliable uncertainty quantification with neural network\n  ensembles for industrial image classification","summary":"  Image classification with neural networks (NNs) is widely used in industrial\nprocesses, situations where the model likely encounters unknown objects during\ndeployment, i.e., out-of-distribution (OOD) data. Worryingly, NNs tend to make\nconfident yet incorrect predictions when confronted with OOD data. To increase\nthe models' reliability, they should quantify the uncertainty in their own\npredictions, communicating when the output should (not) be trusted. Deep\nensembles, composed of multiple independent NNs, have been shown to perform\nstrongly but are computationally expensive. Recent research has proposed more\nefficient NN ensembles, namely the snapshot, batch, and multi-input\nmulti-output ensemble. This study investigates the predictive and uncertainty\nperformance of efficient NN ensembles in the context of image classification\nfor industrial processes. It is the first to provide a comprehensive comparison\nand it proposes a novel Diversity Quality metric to quantify the ensembles'\nperformance on the in-distribution and OOD sets in one single metric. The\nresults highlight the batch ensemble as a cost-effective and competitive\nalternative to the deep ensemble. It matches the deep ensemble in both\nuncertainty and accuracy while exhibiting considerable savings in training\ntime, test time, and memory storage.\n","authors":["Arthur Thuy","Dries F. Benoit"],"pdf_url":"https://arxiv.org/pdf/2403.10182v5.pdf","comment":"Accepted Manuscript version of an article published in Annals of\n  Operations Research"},{"id":"http://arxiv.org/abs/2405.13796v5","updated":"2025-01-13T06:35:54Z","published":"2024-05-22T16:21:02Z","title":"Generalizing Weather Forecast to Fine-grained Temporal Scales via\n  Physics-AI Hybrid Modeling","summary":"  Data-driven artificial intelligence (AI) models have made significant\nadvancements in weather forecasting, particularly in medium-range and\nnowcasting. However, most data-driven weather forecasting models are black-box\nsystems that focus on learning data mapping rather than fine-grained physical\nevolution in the time dimension. Consequently, the limitations in the temporal\nscale of datasets prevent these models from forecasting at finer time scales.\nThis paper proposes a physics-AI hybrid model (i.e., WeatherGFT) which\ngeneralizes weather forecasts to finer-grained temporal scales beyond training\ndataset. Specifically, we employ a carefully designed PDE kernel to simulate\nphysical evolution on a small time scale (e.g., 300 seconds) and use a parallel\nneural networks with a learnable router for bias correction. Furthermore, we\nintroduce a lead time-aware training framework to promote the generalization of\nthe model at different lead times. The weight analysis of physics-AI modules\nindicates that physics conducts major evolution while AI performs corrections\nadaptively. Extensive experiments show that WeatherGFT trained on an hourly\ndataset, effectively generalizes forecasts across multiple time scales,\nincluding 30-minute, which is even smaller than the dataset's temporal\nresolution.\n","authors":["Wanghan Xu","Fenghua Ling","Wenlong Zhang","Tao Han","Hao Chen","Wanli Ouyang","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2405.13796v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07959v2","updated":"2025-01-13T06:25:57Z","published":"2024-11-12T17:36:20Z","title":"On the Convergence of Continual Federated Learning Using Incrementally\n  Aggregated Gradients","summary":"  The holy grail of machine learning is to enable Continual Federated Learning\n(CFL) to enhance the efficiency, privacy, and scalability of AI systems while\nlearning from streaming data. The primary challenge of a CFL system is to\novercome global catastrophic forgetting, wherein the accuracy of the global\nmodel trained on new tasks declines on the old tasks. In this work, we propose\nContinual Federated Learning with Aggregated Gradients (C-FLAG), a novel\nreplay-memory based federated strategy consisting of edge-based gradient\nupdates on memory and aggregated gradients on the current data. We provide\nconvergence analysis of the C-FLAG approach which addresses forgetting and bias\nwhile converging at a rate of $O(1/\\sqrt{T})$ over $T$ communication rounds. We\nformulate an optimization sub-problem that minimizes catastrophic forgetting,\ntranslating CFL into an iterative algorithm with adaptive learning rates that\nensure seamless learning across tasks. We empirically show that C-FLAG\noutperforms several state-of-the-art baselines on both task and\nclass-incremental settings with respect to metrics such as accuracy and\nforgetting.\n","authors":["Satish Kumar Keshri","Nazreen Shah","Ranjitha Prasad"],"pdf_url":"https://arxiv.org/pdf/2411.07959v2.pdf","comment":"30 pages, 7 figures"},{"id":"http://arxiv.org/abs/2501.07077v1","updated":"2025-01-13T06:16:11Z","published":"2025-01-13T06:16:11Z","title":"D3MES: Diffusion Transformer with multihead equivariant self-attention\n  for 3D molecule generation","summary":"  Understanding and predicting the diverse conformational states of molecules\nis crucial for advancing fields such as chemistry, material science, and drug\ndevelopment. Despite significant progress in generative models, accurately\ngenerating complex and biologically or material-relevant molecular structures\nremains a major challenge. In this work, we introduce a diffusion model for\nthree-dimensional (3D) molecule generation that combines a classifiable\ndiffusion model, Diffusion Transformer, with multihead equivariant\nself-attention. This method addresses two key challenges: correctly attaching\nhydrogen atoms in generated molecules through learning representations of\nmolecules after hydrogen atoms are removed; and overcoming the limitations of\nexisting models that cannot generate molecules across multiple classes\nsimultaneously. The experimental results demonstrate that our model not only\nachieves state-of-the-art performance across several key metrics but also\nexhibits robustness and versatility, making it highly suitable for early-stage\nlarge-scale generation processes in molecular design, followed by validation\nand further screening to obtain molecules with specific properties.\n","authors":["Zhejun Zhang","Yuanping Chen","Shibing Chu"],"pdf_url":"https://arxiv.org/pdf/2501.07077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06293v3","updated":"2025-01-13T04:59:24Z","published":"2024-02-09T10:14:18Z","title":"Probabilistic Forecasting of Irregular Time Series via Conditional Flows","summary":"  Probabilistic forecasting of irregularly sampled multivariate time series\nwith missing values is an important problem in many fields, including health\ncare, astronomy, and climate. State-of-the-art methods for the task estimate\nonly marginal distributions of observations in single channels and at single\ntimepoints, assuming a fixed-shape parametric distribution. In this work, we\npropose a novel model, ProFITi, for probabilistic forecasting of irregularly\nsampled time series with missing values using conditional normalizing flows.\nThe model learns joint distributions over the future values of the time series\nconditioned on past observations and queried channels and times, without\nassuming any fixed shape of the underlying distribution. As model components,\nwe introduce a novel invertible triangular attention layer and an invertible\nnon-linear activation function on and onto the whole real line. We conduct\nextensive experiments on four datasets and demonstrate that the proposed model\nprovides $4$ times higher likelihood over the previously best model.\n","authors":["Vijaya Krishna Yalavarthi","Randolf Scholz","Stefan Born","Lars Schmidt-Thieme"],"pdf_url":"https://arxiv.org/pdf/2402.06293v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06838v2","updated":"2025-01-13T04:32:01Z","published":"2024-12-07T06:12:23Z","title":"Hardware implementation of timely reliable Bayesian decision-making\n  using memristors","summary":"  Brains perform decision-making by Bayes theorem. The theorem quantifies\nevents as probabilities and, based on probability rules, renders the decisions.\nLearning from this, Bayes theorem can be applied to enable efficient user-scene\ninteractions. However, given the probabilistic nature, implementing Bayes\ntheorem in hardware using conventional deterministic computing can incur\nexcessive computational cost and decision latency. Though challenging, here we\npresent a probabilistic computing approach based on memristors to implement the\nBayes theorem. We integrate memristors with Boolean logics and, by exploiting\nthe volatile stochastic switching of the memristors, realise probabilistic\nlogic operations, key for hardware Bayes theorem implementation. To empirically\nvalidate the efficacy of the hardware Bayes theorem in user-scene interactions,\nwe develop lightweight Bayesian inference and fusion hardware operators using\nthe probabilistic logics and apply the operators in road scene parsing for\nself-driving, including route planning and obstacle detection. The results show\nour operators can achieve reliable decisions in less than 0.4 ms (or\nequivalently 2,500 fps), outperforming human decision-making and the existing\ndriving assistance systems.\n","authors":["Lekai Song","Pengyu Liu","Yang Liu","Jingfang Pei","Wenyu Cui","Songwei Liu","Yingyi Wen","Teng Ma","Kong-Pang Pun","Leonard W. T. Ng","Guohua Hu"],"pdf_url":"https://arxiv.org/pdf/2412.06838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07055v1","updated":"2025-01-13T04:30:41Z","published":"2025-01-13T04:30:41Z","title":"SFC-GAN: A Generative Adversarial Network for Brain Functional and\n  Structural Connectome Translation","summary":"  Modern brain imaging technologies have enabled the detailed reconstruction of\nhuman brain connectomes, capturing structural connectivity (SC) from diffusion\nMRI and functional connectivity (FC) from functional MRI. Understanding the\nintricate relationships between SC and FC is vital for gaining deeper insights\ninto the brain's functional and organizational mechanisms. However, obtaining\nboth SC and FC modalities simultaneously remains challenging, hindering\ncomprehensive analyses. Existing deep generative models typically focus on\nsynthesizing a single modality or unidirectional translation between FC and SC,\nthereby missing the potential benefits of bi-directional translation,\nespecially in scenarios where only one connectome is available. Therefore, we\npropose Structural-Functional Connectivity GAN (SFC-GAN), a novel framework for\nbidirectional translation between SC and FC. This approach leverages the\nCycleGAN architecture, incorporating convolutional layers to effectively\ncapture the spatial structures of brain connectomes. To preserve the\ntopological integrity of these connectomes, we employ a structure-preserving\nloss that guides the model in capturing both global and local connectome\npatterns while maintaining symmetry. Our framework demonstrates superior\nperformance in translating between SC and FC, outperforming baseline models in\nsimilarity and graph property evaluations compared to ground truth data, each\ntranslated modality can be effectively utilized for downstream classification.\n","authors":["Yee-Fan Tan","Jun Lin Liow","Pei-Sze Tan","Fuad Noman","Raphael C. -W. Phan","Hernando Ombao","Chee-Ming Ting"],"pdf_url":"https://arxiv.org/pdf/2501.07055v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2206.08523v4","updated":"2025-01-13T04:26:07Z","published":"2022-06-17T03:11:18Z","title":"A Spatio-Temporal Neural Network Forecasting Approach for Emulation of\n  Firefront Models","summary":"  Computational simulations of wildfire spread typically employ empirical\nrate-of-spread calculations under various conditions (such as terrain, fuel\ntype, weather). Small perturbations in conditions can often lead to significant\nchanges in fire spread (such as speed and direction), necessitating a\ncomputationally expensive large set of simulations to quantify uncertainty.\nModel emulation seeks alternative representations of physical models using\nmachine learning, aiming to provide more efficient and/or simplified surrogate\nmodels. We propose a dedicated spatio-temporal neural network based framework\nfor model emulation, able to capture the complex behaviour of fire spread\nmodels. The proposed approach can approximate forecasts at fine spatial and\ntemporal resolutions that are often challenging for neural network based\napproaches. Furthermore, the proposed approach is robust even with small\ntraining sets, due to novel data augmentation methods. Empirical experiments\nshow good agreement between simulated and emulated firefronts, with an average\nJaccard score of 0.76.\n","authors":["Andrew Bolt","Carolyn Huston","Petra Kuhnert","Joel Janek Dabrowski","James Hilton","Conrad Sanderson"],"pdf_url":"https://arxiv.org/pdf/2206.08523v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07046v1","updated":"2025-01-13T04:05:19Z","published":"2025-01-13T04:05:19Z","title":"Differentially Private Kernelized Contextual Bandits","summary":"  We consider the problem of contextual kernel bandits with stochastic\ncontexts, where the underlying reward function belongs to a known Reproducing\nKernel Hilbert Space (RKHS). We study this problem under the additional\nconstraint of joint differential privacy, where the agents needs to ensure that\nthe sequence of query points is differentially private with respect to both the\nsequence of contexts and rewards. We propose a novel algorithm that improves\nupon the state of the art and achieves an error rate of\n$\\mathcal{O}\\left(\\sqrt{\\frac{\\gamma_T}{T}} + \\frac{\\gamma_T}{T\n\\varepsilon}\\right)$ after $T$ queries for a large class of kernel families,\nwhere $\\gamma_T$ represents the effective dimensionality of the kernel and\n$\\varepsilon > 0$ is the privacy parameter. Our results are based on a novel\nestimator for the reward function that simultaneously enjoys high utility along\nwith a low-sensitivity to observed rewards and contexts, which is crucial to\nobtain an order optimal learning performance with improved dependence on the\nprivacy parameter.\n","authors":["Nikola Pavlovic","Sudeep Salgia","Qing Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.07046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07045v1","updated":"2025-01-13T03:55:59Z","published":"2025-01-13T03:55:59Z","title":"ACCon: Angle-Compensated Contrastive Regularizer for Deep Regression","summary":"  In deep regression, capturing the relationship among continuous labels in\nfeature space is a fundamental challenge that has attracted increasing\ninterest. Addressing this issue can prevent models from converging to\nsuboptimal solutions across various regression tasks, leading to improved\nperformance, especially for imbalanced regression and under limited sample\nsizes. However, existing approaches often rely on order-aware representation\nlearning or distance-based weighting. In this paper, we hypothesize a linear\nnegative correlation between label distances and representation similarities in\nregression tasks. To implement this, we propose an angle-compensated\ncontrastive regularizer for deep regression, which adjusts the cosine distance\nbetween anchor and negative samples within the contrastive learning framework.\nOur method offers a plug-and-play compatible solution that extends most\nexisting contrastive learning methods for regression tasks. Extensive\nexperiments and theoretical analysis demonstrate that our proposed\nangle-compensated contrastive regularizer not only achieves competitive\nregression performance but also excels in data efficiency and effectiveness on\nimbalanced datasets.\n","authors":["Botao Zhao","Xiaoyang Qu","Zuheng Kang","Junqing Peng","Jing Xiao","Jianzong Wang"],"pdf_url":"https://arxiv.org/pdf/2501.07045v1.pdf","comment":"Accept by AAAI-2025 (The 39th Annual AAAI Conference on Artificial\n  Intelligence)"},{"id":"http://arxiv.org/abs/2501.07044v1","updated":"2025-01-13T03:54:19Z","published":"2025-01-13T03:54:19Z","title":"Protego: Detecting Adversarial Examples for Vision Transformers via\n  Intrinsic Capabilities","summary":"  Transformer models have excelled in natural language tasks, prompting the\nvision community to explore their implementation in computer vision problems.\nHowever, these models are still influenced by adversarial examples. In this\npaper, we investigate the attack capabilities of six common adversarial attacks\non three pretrained ViT models to reveal the vulnerability of ViT models. To\nunderstand and analyse the bias in neural network decisions when the input is\nadversarial, we use two visualisation techniques that are attention rollout and\ngrad attention rollout. To prevent ViT models from adversarial attack, we\npropose Protego, a detection framework that leverages the transformer intrinsic\ncapabilities to detection adversarial examples of ViT models. Nonetheless, this\nis challenging due to a diversity of attack strategies that may be adopted by\nadversaries. Inspired by the attention mechanism, we know that the token of\nprediction contains all the information from the input sample. Additionally,\nthe attention region for adversarial examples differs from that of normal\nexamples. Given these points, we can train a detector that achieves superior\nperformance than existing detection methods to identify adversarial examples.\nOur experiments have demonstrated the high effectiveness of our detection\nmethod. For these six adversarial attack methods, our detector's AUC scores all\nexceed 0.95. Protego may advance investigations in metaverse security.\n","authors":["Jialin Wu","Kaikai Pan","Yanjiao Chen","Jiangyi Deng","Shengyuan Pang","Wenyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2501.07044v1.pdf","comment":"Accepted by IEEE MetaCom 2024"},{"id":"http://arxiv.org/abs/2501.01312v3","updated":"2025-01-13T03:53:34Z","published":"2025-01-02T15:53:25Z","title":"Learning Spectral Methods by Transformers","summary":"  Transformers demonstrate significant advantages as the building block of\nmodern LLMs. In this work, we study the capacities of Transformers in\nperforming unsupervised learning. We show that multi-layered Transformers,\ngiven a sufficiently large set of pre-training instances, are able to learn the\nalgorithms themselves and perform statistical estimation tasks given new\ninstances. This learning paradigm is distinct from the in-context learning\nsetup and is similar to the learning procedure of human brains where skills are\nlearned through past experience. Theoretically, we prove that pre-trained\nTransformers can learn the spectral methods and use the classification of\nbi-class Gaussian mixture model as an example. Our proof is constructive using\nalgorithmic design techniques. Our results are built upon the similarities of\nmulti-layered Transformer architecture with the iterative recovery algorithms\nused in practice. Empirically, we verify the strong capacity of the\nmulti-layered (pre-trained) Transformer on unsupervised learning through the\nlens of both the PCA and the Clustering tasks performed on the synthetic and\nreal-world datasets.\n","authors":["Yihan He","Yuan Cao","Hong-Yu Chen","Dennis Wu","Jianqing Fan","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2501.01312v3.pdf","comment":"77 pages, 12 figures"},{"id":"http://arxiv.org/abs/2501.07034v1","updated":"2025-01-13T03:13:32Z","published":"2025-01-13T03:13:32Z","title":"Explore the Use of Time Series Foundation Model for Car-Following\n  Behavior Analysis","summary":"  Modeling car-following behavior is essential for traffic simulation,\nanalyzing driving patterns, and understanding complex traffic flows with\nvarying levels of autonomous vehicles. Traditional models like the Safe\nDistance Model and Intelligent Driver Model (IDM) require precise parameter\ncalibration and often lack generality due to simplified assumptions about\ndriver behavior. While machine learning and deep learning methods capture\ncomplex patterns, they require large labeled datasets. Foundation models\nprovide a more efficient alternative. Pre-trained on vast, diverse time series\ndatasets, they can be applied directly to various tasks without the need for\nextensive re-training. These models generalize well across domains, and with\nminimal fine-tuning, they can be adapted to specific tasks like car-following\nbehavior prediction. In this paper, we apply Chronos, a state-of-the-art public\ntime series foundation model, to analyze car-following behavior using the Open\nACC dataset. Without fine-tuning, Chronos outperforms traditional models like\nIDM and Exponential smoothing with trend and seasonality (ETS), and achieves\nsimilar results to deep learning models such as DeepAR and TFT, with an RMSE of\n0.60. After fine-tuning, Chronos reduces the error to an RMSE of 0.53,\nrepresenting a 33.75% improvement over IDM and a 12-37% reduction compared to\nmachine learning models like ETS and deep learning models including DeepAR,\nWaveNet, and TFT. This demonstrates the potential of foundation models to\nsignificantly advance transportation research, offering a scalable, adaptable,\nand highly accurate approach to predicting and simulating car-following\nbehaviors.\n","authors":["Luwei Zeng","Runze Yan"],"pdf_url":"https://arxiv.org/pdf/2501.07034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07033v1","updated":"2025-01-13T03:10:54Z","published":"2025-01-13T03:10:54Z","title":"Detection of AI Deepfake and Fraud in Online Payments Using GAN-Based\n  Models","summary":"  This study explores the use of Generative Adversarial Networks (GANs) to\ndetect AI deepfakes and fraudulent activities in online payment systems. With\nthe growing prevalence of deepfake technology, which can manipulate facial\nfeatures in images and videos, the potential for fraud in online transactions\nhas escalated. Traditional security systems struggle to identify these\nsophisticated forms of fraud. This research proposes a novel GAN-based model\nthat enhances online payment security by identifying subtle manipulations in\npayment images. The model is trained on a dataset consisting of real-world\nonline payment images and deepfake images generated using advanced GAN\narchitectures, such as StyleGAN and DeepFake. The results demonstrate that the\nproposed model can accurately distinguish between legitimate transactions and\ndeepfakes, achieving a high detection rate above 95%. This approach\nsignificantly improves the robustness of payment systems against AI-driven\nfraud. The paper contributes to the growing field of digital security, offering\ninsights into the application of GANs for fraud detection in financial\nservices. Keywords- Payment Security, Image Recognition, Generative Adversarial\nNetworks, AI Deepfake, Fraudulent Activities\n","authors":["Zong Ke","Shicheng Zhou","Yining Zhou","Chia Hong Chang","Rong Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.07033v1.pdf","comment":"The paper will be published and indexed by IEEE at 2025 8th\n  International Conference on Advanced Algorithms and Control Engineering\n  (ICAACE 2025)"},{"id":"http://arxiv.org/abs/2311.11200v2","updated":"2025-01-13T03:08:53Z","published":"2023-11-19T02:26:16Z","title":"Beyond the Power Law: Estimation, Goodness-of-Fit, and a Semiparametric\n  Extension in Complex Networks","summary":"  Scale-free networks play a fundamental role in the study of complex networks\nand various applied fields due to their ability to model a wide range of\nreal-world systems. A key characteristic of these networks is their degree\ndistribution, which often follows a power-law distribution, where the\nprobability mass function is proportional to $x^{-\\alpha}$, with $\\alpha$\ntypically ranging between $2 < \\alpha < 3$. In this paper, we introduce\nBayesian inference methods to obtain more accurate estimates than those\nobtained using traditional methods, which often yield biased estimates, and\nprecise credible intervals. Through a simulation study, we demonstrate that our\napproach provides nearly unbiased estimates for the scaling parameter,\nenhancing the reliability of inferences. We also evaluate new goodness-of-fit\ntests to improve the effectiveness of the Kolmogorov-Smirnov test, commonly\nused for this purpose. Our findings show that the Watson test offers superior\npower while maintaining a controlled type I error rate, enabling us to better\ndetermine whether data adheres to a power-law distribution. Finally, we propose\na piecewise extension of this model to provide greater flexibility, evaluating\nthe estimation and its goodness-of-fit features as well. In the complex\nnetworks field, this extension allows us to model the full degree distribution,\ninstead of just focusing on the tail, as is commonly done. We demonstrate the\nutility of these novel methods through applications to two real-world datasets,\nshowcasing their practical relevance and potential to advance the analysis of\npower-law behavior.\n","authors":["Nixon Jerez-Lillo","Francisco A. Rodrigues","Paulo H. Ferreira","Pedro L. Ramos"],"pdf_url":"https://arxiv.org/pdf/2311.11200v2.pdf","comment":"33 pages, 11 figures"},{"id":"http://arxiv.org/abs/2501.07032v1","updated":"2025-01-13T03:07:39Z","published":"2025-01-13T03:07:39Z","title":"PRKAN: Parameter-Reduced Kolmogorov-Arnold Networks","summary":"  Kolmogorov-Arnold Networks (KANs) represent an innovation in neural network\narchitectures, offering a compelling alternative to Multi-Layer Perceptrons\n(MLPs) in models such as Convolutional Neural Networks (CNNs), Recurrent Neural\nNetworks (RNNs), and Transformers. By advancing network design, KANs are\ndriving groundbreaking research and enabling transformative applications across\nvarious scientific domains involving neural networks. However, existing KANs\noften require significantly more parameters in their network layers compared to\nMLPs. To address this limitation, this paper introduces PRKANs\n(\\textbf{P}arameter-\\textbf{R}educed \\textbf{K}olmogorov-\\textbf{A}rnold\n\\textbf{N}etworks), which employ several methods to reduce the parameter count\nin KAN layers, making them comparable to MLP layers. Experimental results on\nthe MNIST and Fashion-MNIST datasets demonstrate that PRKANs with attention\nmechanisms outperform several existing KANs and rival the performance of MLPs,\nalbeit with slightly longer training times. Furthermore, the study highlights\nthe advantages of Gaussian Radial Basis Functions (GRBFs) and layer\nnormalization in KAN designs. The repository for this work is available at:\n\\url{https://github.com/hoangthangta/All-KAN}.\n","authors":["Hoang-Thang Ta","Duy-Quy Thai","Anh Tran","Grigori Sidorov","Alexander Gelbukh"],"pdf_url":"https://arxiv.org/pdf/2501.07032v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2412.07819v2","updated":"2025-01-13T03:03:57Z","published":"2024-12-10T12:14:38Z","title":"Intelligent System for Automated Molecular Patent Infringement\n  Assessment","summary":"  Automated drug discovery offers significant potential for accelerating the\ndevelopment of novel therapeutics by substituting labor-intensive human\nworkflows with machine-driven processes. However, molecules generated by\nartificial intelligence may unintentionally infringe on existing patents,\nposing legal and financial risks that impede the full automation of drug\ndiscovery pipelines. This paper introduces PatentFinder, a novel multi-agent\nand tool-enhanced intelligence system that can accurately and comprehensively\nevaluate small molecules for patent infringement. PatentFinder features five\nspecialized agents that collaboratively analyze patent claims and molecular\nstructures with heuristic and model-based tools, generating interpretable\ninfringement reports. To support systematic evaluation, we curate\nMolPatent-240, a benchmark dataset tailored for patent infringement assessment\nalgorithms. On this benchmark, PatentFinder outperforms baseline methods that\nrely solely on large language models or specialized chemical tools, achieving a\n13.8% improvement in F1-score and a 12% increase in accuracy. Additionally,\nPatentFinder autonomously generates detailed and interpretable patent\ninfringement reports, showcasing enhanced accuracy and improved\ninterpretability. The high accuracy and interpretability of PatentFinder make\nit a valuable and reliable tool for automating patent infringement assessments,\noffering a practical solution for integrating patent protection analysis into\nthe drug discovery pipeline.\n","authors":["Yaorui Shi","Sihang Li","Taiyan Zhang","Xi Fang","Jiankun Wang","Zhiyuan Liu","Guojiang Zhao","Zhengdan Zhu","Zhifeng Gao","Renxin Zhong","Linfeng Zhang","Guolin Ke","Weinan E","Hengxing Cai","Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.07819v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07030v1","updated":"2025-01-13T03:02:15Z","published":"2025-01-13T03:02:15Z","title":"Erasing Noise in Signal Detection with Diffusion Model: From Theory to\n  Application","summary":"  In this paper, a signal detection method based on the denoise diffusion model\n(DM) is proposed, which outperforms the maximum likelihood (ML) estimation\nmethod that has long been regarded as the optimal signal detection technique.\nTheoretically, a novel mathematical theory for intelligent signal detection\nbased on stochastic differential equations (SDEs) is established in this paper,\ndemonstrating the effectiveness of DM in reducing the additive white Gaussian\nnoise in received signals. Moreover, a mathematical relationship between the\nsignal-to-noise ratio (SNR) and the timestep in DM is established, revealing\nthat for any given SNR, a corresponding optimal timestep can be identified.\nFurthermore, to address potential issues with out-of-distribution inputs in the\nDM, we employ a mathematical scaling technique that allows the trained DM to\nhandle signal detection across a wide range of SNRs without any fine-tuning.\nBuilding on the above theoretical foundation, we propose a DM-based signal\ndetection method, with the diffusion transformer (DiT) serving as the backbone\nneural network, whose computational complexity of this method is\n$\\mathcal{O}(n^2)$. Simulation results demonstrate that, for BPSK and QAM\nmodulation schemes, the DM-based method achieves a significantly lower symbol\nerror rate (SER) compared to ML estimation, while maintaining a much lower\ncomputational complexity.\n","authors":["Xiucheng Wang","Peilin Zheng","Nan Cheng"],"pdf_url":"https://arxiv.org/pdf/2501.07030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02422v4","updated":"2025-01-13T02:53:39Z","published":"2023-09-05T17:51:00Z","title":"Integral Probability Metrics Meet Neural Networks: The\n  Radon-Kolmogorov-Smirnov Test","summary":"  Integral probability metrics (IPMs) constitute a general class of\nnonparametric two-sample tests that are based on maximizing the mean difference\nbetween samples from one distribution $P$ versus another $Q$, over all choices\nof data transformations $f$ living in some function space $\\mathcal{F}$.\nInspired by recent work that connects what are known as functions of\n$\\textit{Radon bounded variation}$ (RBV) and neural networks (Parhi and Nowak,\n2021, 2023), we study the IPM defined by taking $\\mathcal{F}$ to be the unit\nball in the RBV space of a given smoothness degree $k \\geq 0$. This test, which\nwe refer to as the $\\textit{Radon-Kolmogorov-Smirnov}$ (RKS) test, can be\nviewed as a generalization of the well-known and classical Kolmogorov-Smirnov\n(KS) test to multiple dimensions and higher orders of smoothness. It is also\nintimately connected to neural networks: we prove that the witness in the RKS\ntest -- the function $f$ achieving the maximum mean difference -- is always a\nridge spline of degree $k$, i.e., a single neuron in a neural network. We can\nthus leverage the power of modern neural network optimization toolkits to\n(approximately) maximize the criterion that underlies the RKS test. We prove\nthat the RKS test has asymptotically full power at distinguishing any distinct\npair $P \\not= Q$ of distributions, derive its asymptotic null distribution, and\ncarry out experiments to elucidate the strengths and weaknesses of the RKS test\nversus the more traditional kernel MMD test.\n","authors":["Seunghoon Paik","Michael Celentano","Alden Green","Ryan J. Tibshirani"],"pdf_url":"https://arxiv.org/pdf/2309.02422v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07022v1","updated":"2025-01-13T02:48:58Z","published":"2025-01-13T02:48:58Z","title":"Improved Regret Bounds for Online Fair Division with Bandit Learning","summary":"  We study online fair division when there are a finite number of item types\nand the player values for the items are drawn randomly from distributions with\nunknown means. In this setting, a sequence of indivisible items arrives\naccording to a random online process, and each item must be allocated to a\nsingle player. The goal is to maximize expected social welfare while\nmaintaining that the allocation satisfies proportionality in expectation. When\nplayer values are normalized, we show that it is possible to with high\nprobability guarantee proportionality constraint satisfaction and achieve\n$\\tilde{O}(\\sqrt{T})$ regret. To achieve this result, we present an upper\nconfidence bound (UCB) algorithm that uses two rounds of linear optimization.\nThis algorithm highlights fundamental aspects of proportionality constraints\nthat allow for a UCB algorithm despite the presence of many (potentially tight)\nconstraints. This result improves upon the previous best regret rate of\n$\\tilde{O}(T^{2/3})$.\n","authors":["Benjamin Schiffer","Shirley Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.07022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07021v1","updated":"2025-01-13T02:47:49Z","published":"2025-01-13T02:47:49Z","title":"Neural Probabilistic Circuits: Enabling Compositional and Interpretable\n  Predictions through Logical Reasoning","summary":"  End-to-end deep neural networks have achieved remarkable success across\nvarious domains but are often criticized for their lack of interpretability.\nWhile post hoc explanation methods attempt to address this issue, they often\nfail to accurately represent these black-box models, resulting in misleading or\nincomplete explanations. To overcome these challenges, we propose an inherently\ntransparent model architecture called Neural Probabilistic Circuits (NPCs),\nwhich enable compositional and interpretable predictions through logical\nreasoning. In particular, an NPC consists of two modules: an attribute\nrecognition model, which predicts probabilities for various attributes, and a\ntask predictor built on a probabilistic circuit, which enables logical\nreasoning over recognized attributes to make class predictions. To train NPCs,\nwe introduce a three-stage training algorithm comprising attribute recognition,\ncircuit construction, and joint optimization. Moreover, we theoretically\ndemonstrate that an NPC's error is upper-bounded by a linear combination of the\nerrors from its modules. To further demonstrate the interpretability of NPC, we\nprovide both the most probable explanations and the counterfactual\nexplanations. Empirical results on four benchmark datasets show that NPCs\nstrike a balance between interpretability and performance, achieving results\ncompetitive even with those of end-to-end black-box models while providing\nenhanced interpretability.\n","authors":["Weixin Chen","Simon Yu","Huajie Shao","Lui Sha","Han Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.07021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19714v2","updated":"2025-01-13T02:43:47Z","published":"2024-11-29T14:02:00Z","title":"The Streetscape Application Services Stack (SASS): Towards a Distributed\n  Sensing Architecture for Urban Applications","summary":"  As urban populations grow, cities are becoming more complex, driving the\ndeployment of interconnected sensing systems to realize the vision of smart\ncities. These systems aim to improve safety, mobility, and quality of life\nthrough applications that integrate diverse sensors with real-time\ndecision-making. Streetscape applications-focusing on challenges like\npedestrian safety and adaptive traffic management-depend on managing\ndistributed, heterogeneous sensor data, aligning information across time and\nspace, and enabling real-time processing. These tasks are inherently complex\nand often difficult to scale. The Streetscape Application Services Stack (SASS)\naddresses these challenges with three core services: multimodal data\nsynchronization, spatiotemporal data fusion, and distributed edge computing. By\nstructuring these capabilities as clear, composable abstractions with clear\nsemantics, SASS allows developers to scale streetscape applications efficiently\nwhile minimizing the complexity of multimodal integration.\n  We evaluated SASS in two real-world testbed environments: a controlled\nparking lot and an urban intersection in a major U.S. city. These testbeds\nallowed us to test SASS under diverse conditions, demonstrating its practical\napplicability. The Multimodal Data Synchronization service reduced temporal\nmisalignment errors by 88%, achieving synchronization accuracy within 50\nmilliseconds. Spatiotemporal Data Fusion service improved detection accuracy\nfor pedestrians and vehicles by over 10%, leveraging multicamera integration.\nThe Distributed Edge Computing service increased system throughput by more than\nan order of magnitude. Together, these results show how SASS provides the\nabstractions and performance needed to support real-time, scalable urban\napplications, bridging the gap between sensing infrastructure and actionable\nstreetscape intelligence.\n","authors":["Navid Salami Pargoo","Mahshid Ghasemi","Shuren Xia","Mehmet Kerem Turkcan","Taqiya Ehsan","Chengbo Zang","Yuan Sun","Javad Ghaderi","Gil Zussman","Zoran Kostic","Jorge Ortiz"],"pdf_url":"https://arxiv.org/pdf/2411.19714v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07014v1","updated":"2025-01-13T02:17:01Z","published":"2025-01-13T02:17:01Z","title":"AlgoRxplorers | Precision in Mutation -- Enhancing Drug Design with\n  Advanced Protein Stability Prediction Tools","summary":"  Predicting the impact of single-point amino acid mutations on protein\nstability is essential for understanding disease mechanisms and advancing drug\ndevelopment. Protein stability, quantified by changes in Gibbs free energy\n($\\Delta\\Delta G$), is influenced by these mutations. However, the scarcity of\ndata and the complexity of model interpretation pose challenges in accurately\npredicting stability changes. This study proposes the application of deep\nneural networks, leveraging transfer learning and fusing complementary\ninformation from different models, to create a feature-rich representation of\nthe protein stability landscape. We developed four models, with our third\nmodel, ThermoMPNN+, demonstrating the best performance in predicting\n$\\Delta\\Delta G$ values. This approach, which integrates diverse feature sets\nand embeddings through latent transfusion techniques, aims to refine\n$\\Delta\\Delta G$ predictions and contribute to a deeper understanding of\nprotein dynamics, potentially leading to advancements in disease research and\ndrug discovery.\n","authors":["Karishma Thakrar","Jiangqin Ma","Max Diamond","Akash Patel"],"pdf_url":"https://arxiv.org/pdf/2501.07014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07005v1","updated":"2025-01-13T01:49:17Z","published":"2025-01-13T01:49:17Z","title":"Global Search for Optimal Low Thrust Spacecraft Trajectories using\n  Diffusion Models and the Indirect Method","summary":"  Long time-duration low-thrust nonlinear optimal spacecraft trajectory global\nsearch is a computationally and time expensive problem characterized by\nclustering patterns in locally optimal solutions. During preliminary mission\ndesign, mission parameters are subject to frequent changes, necessitating that\ntrajectory designers efficiently generate high-quality control solutions for\nthese new scenarios. Generative machine learning models can be trained to learn\nhow the solution structure varies with respect to a conditional parameter,\nthereby accelerating the global search for missions with updated parameters. In\nthis work, state-of-the-art diffusion models are integrated with the indirect\napproach for trajectory optimization within a global search framework. This\nframework is tested on two low-thrust transfers of different complexity in the\ncircular restricted three-body problem. By generating and analyzing a training\ndata set, we develop mathematical relations and techniques to understand the\ncomplex structures in the costate domain of locally optimal solutions for these\nproblems. A diffusion model is trained on this data and successfully\naccelerates the global search for both problems. The model predicts how the\ncostate solution structure changes, based on the maximum spacecraft thrust\nmagnitude. Warm-starting a numerical solver with diffusion model samples for\nthe costates at the initial time increases the number of solutions generated\nper minute for problems with unseen thrust magnitudes by one to two orders of\nmagnitude in comparison to samples from a uniform distribution and from an\nadjoint control transformation.\n","authors":["Jannik Graebner","Ryne Beeson"],"pdf_url":"https://arxiv.org/pdf/2501.07005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00088v2","updated":"2025-01-13T01:43:15Z","published":"2024-11-27T09:37:33Z","title":"Stochastic Taylor Derivative Estimator: Efficient amortization for\n  arbitrary differential operators","summary":"  Optimizing neural networks with loss that contain high-dimensional and\nhigh-order differential operators is expensive to evaluate with\nback-propagation due to $\\mathcal{O}(d^{k})$ scaling of the derivative tensor\nsize and the $\\mathcal{O}(2^{k-1}L)$ scaling in the computation graph, where\n$d$ is the dimension of the domain, $L$ is the number of ops in the forward\ncomputation graph, and $k$ is the derivative order. In previous works, the\npolynomial scaling in $d$ was addressed by amortizing the computation over the\noptimization process via randomization. Separately, the exponential scaling in\n$k$ for univariate functions ($d=1$) was addressed with high-order\nauto-differentiation (AD). In this work, we show how to efficiently perform\narbitrary contraction of the derivative tensor of arbitrary order for\nmultivariate functions, by properly constructing the input tangents to\nunivariate high-order AD, which can be used to efficiently randomize any\ndifferential operator. When applied to Physics-Informed Neural Networks\n(PINNs), our method provides >1000$\\times$ speed-up and >30$\\times$ memory\nreduction over randomization with first-order AD, and we can now solve\n\\emph{1-million-dimensional PDEs in 8 minutes on a single NVIDIA A100 GPU}.\nThis work opens the possibility of using high-order differential operators in\nlarge-scale problems.\n","authors":["Zekun Shi","Zheyuan Hu","Min Lin","Kenji Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2412.00088v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06999v1","updated":"2025-01-13T01:20:23Z","published":"2025-01-13T01:20:23Z","title":"Likelihood Training of Cascaded Diffusion Models via Hierarchical\n  Volume-preserving Maps","summary":"  Cascaded models are multi-scale generative models with a marked capacity for\nproducing perceptually impressive samples at high resolutions. In this work, we\nshow that they can also be excellent likelihood models, so long as we overcome\na fundamental difficulty with probabilistic multi-scale models: the\nintractability of the likelihood function. Chiefly, in cascaded models each\nintermediary scale introduces extraneous variables that cannot be tractably\nmarginalized out for likelihood evaluation. This issue vanishes by modeling the\ndiffusion process on latent spaces induced by a class of transformations we\ncall hierarchical volume-preserving maps, which decompose spatially structured\ndata in a hierarchical fashion without introducing local distortions in the\nlatent space. We demonstrate that two such maps are well-known in the\nliterature for multiscale modeling: Laplacian pyramids and wavelet transforms.\nNot only do such reparameterizations allow the likelihood function to be\ndirectly expressed as a joint likelihood over the scales, we show that the\nLaplacian pyramid and wavelet transform also produces significant improvements\nto the state-of-the-art on a selection of benchmarks in likelihood modeling,\nincluding density estimation, lossless compression, and out-of-distribution\ndetection. Investigating the theoretical basis of our empirical gains we\nuncover deep connections to score matching under the Earth Mover's Distance\n(EMD), which is a well-known surrogate for perceptual similarity. Code can be\nfound at \\href{https://github.com/lihenryhfl/pcdm}{this https url}.\n","authors":["Henry Li","Ronen Basri","Yuval Kluger"],"pdf_url":"https://arxiv.org/pdf/2501.06999v1.pdf","comment":"Spotlight at ICLR 2024"},{"id":"http://arxiv.org/abs/2501.06994v1","updated":"2025-01-13T01:01:44Z","published":"2025-01-13T01:01:44Z","title":"Motion Tracks: A Unified Representation for Human-Robot Transfer in\n  Few-Shot Imitation Learning","summary":"  Teaching robots to autonomously complete everyday tasks remains a challenge.\nImitation Learning (IL) is a powerful approach that imbues robots with skills\nvia demonstrations, but is limited by the labor-intensive process of collecting\nteleoperated robot data. Human videos offer a scalable alternative, but it\nremains difficult to directly train IL policies from them due to the lack of\nrobot action labels. To address this, we propose to represent actions as\nshort-horizon 2D trajectories on an image. These actions, or motion tracks,\ncapture the predicted direction of motion for either human hands or robot\nend-effectors. We instantiate an IL policy called Motion Track Policy (MT-pi)\nwhich receives image observations and outputs motion tracks as actions. By\nleveraging this unified, cross-embodiment action space, MT-pi completes tasks\nwith high success given just minutes of human video and limited additional\nrobot demonstrations. At test time, we predict motion tracks from two camera\nviews, recovering 6DoF trajectories via multi-view synthesis. MT-pi achieves an\naverage success rate of 86.5% across 4 real-world tasks, outperforming\nstate-of-the-art IL baselines which do not leverage human data or our action\nspace by 40%, and generalizes to scenarios seen only in human videos. Code and\nvideos are available on our website\nhttps://portal-cornell.github.io/motion_track_policy/.\n","authors":["Juntao Ren","Priya Sundaresan","Dorsa Sadigh","Sanjiban Choudhury","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2501.06994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03172v2","updated":"2025-01-13T00:43:45Z","published":"2024-11-05T15:20:23Z","title":"Blind Estimation of Sub-band Acoustic Parameters from Ambisonics\n  Recordings using Spectro-Spatial Covariance Features","summary":"  Estimating frequency-varying acoustic parameters is essential for enhancing\nimmersive perception in realistic spatial audio creation. In this paper, we\npropose a unified framework that blindly estimates reverberation time (T60),\ndirect-to-reverberant ratio (DRR), and clarity (C50) across 10 frequency bands\nusing first-order Ambisonics (FOA) speech recordings as inputs. The proposed\nframework utilizes a novel feature named Spectro-Spatial Covariance Vector\n(SSCV), efficiently representing temporal, spectral as well as spatial\ninformation of the FOA signal. Our models significantly outperform existing\nsingle-channel methods with only spectral information, reducing estimation\nerrors by more than half for all three acoustic parameters. Additionally, we\nintroduce FOA-Conv3D, a novel back-end network for effectively utilising the\nSSCV feature with a 3D convolutional encoder. FOA-Conv3D outperforms the\nconvolutional neural network (CNN) and recurrent convolutional neural network\n(CRNN) backends, achieving lower estimation errors and accounting for a higher\nproportion of variance (PoV) for all 3 acoustic parameters.\n","authors":["Hanyu Meng","Jeroen Breebaart","Jeremy Stoddard","Vidhyasaharan Sethu","Eliathamby Ambikairajah"],"pdf_url":"https://arxiv.org/pdf/2411.03172v2.pdf","comment":"Accepted by ICASSP2025"},{"id":"http://arxiv.org/abs/2408.03330v3","updated":"2025-01-13T00:43:34Z","published":"2024-07-19T15:32:15Z","title":"Modeling Latent Neural Dynamics with Gaussian Process Switching Linear\n  Dynamical Systems","summary":"  Understanding how the collective activity of neural populations relates to\ncomputation and ultimately behavior is a key goal in neuroscience. To this end,\nstatistical methods which describe high-dimensional neural time series in terms\nof low-dimensional latent dynamics have played a fundamental role in\ncharacterizing neural systems. Yet, what constitutes a successful method\ninvolves two opposing criteria: (1) methods should be expressive enough to\ncapture complex nonlinear dynamics, and (2) they should maintain a notion of\ninterpretability often only warranted by simpler linear models. In this paper,\nwe develop an approach that balances these two objectives: the Gaussian Process\nSwitching Linear Dynamical System (gpSLDS). Our method builds on previous work\nmodeling the latent state evolution via a stochastic differential equation\nwhose nonlinear dynamics are described by a Gaussian process (GP-SDEs). We\npropose a novel kernel function which enforces smoothly interpolated locally\nlinear dynamics, and therefore expresses flexible -- yet interpretable --\ndynamics akin to those of recurrent switching linear dynamical systems (rSLDS).\nOur approach resolves key limitations of the rSLDS such as artifactual\noscillations in dynamics near discrete state boundaries, while also providing\nposterior uncertainty estimates of the dynamics. To fit our models, we leverage\na modified learning objective which improves the estimation accuracy of kernel\nhyperparameters compared to previous GP-SDE fitting approaches. We apply our\nmethod to synthetic data and data recorded in two neuroscience experiments and\ndemonstrate favorable performance in comparison to the rSLDS.\n","authors":["Amber Hu","David Zoltowski","Aditya Nair","David Anderson","Lea Duncker","Scott Linderman"],"pdf_url":"https://arxiv.org/pdf/2408.03330v3.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)"},{"id":"http://arxiv.org/abs/2501.06980v1","updated":"2025-01-13T00:03:20Z","published":"2025-01-13T00:03:20Z","title":"Combining LLM decision and RL action selection to improve RL policy for\n  adaptive interventions","summary":"  Reinforcement learning (RL) is increasingly being used in the healthcare\ndomain, particularly for the development of personalized health adaptive\ninterventions. Inspired by the success of Large Language Models (LLMs), we are\ninterested in using LLMs to update the RL policy in real time, with the goal of\naccelerating personalization. We use the text-based user preference to\ninfluence the action selection on the fly, in order to immediately incorporate\nthe user preference. We use the term \"user preference\" as a broad term to refer\nto a user personal preference, constraint, health status, or a statement\nexpressing like or dislike, etc. Our novel approach is a hybrid method that\ncombines the LLM response and the RL action selection to improve the RL policy.\nGiven an LLM prompt that incorporates the user preference, the LLM acts as a\nfilter in the typical RL action selection. We investigate different prompting\nstrategies and action selection strategies. To evaluate our approach, we\nimplement a simulation environment that generates the text-based user\npreferences and models the constraints that impact behavioral dynamics. We show\nthat our approach is able to take into account the text-based user preferences,\nwhile improving the RL policy, thus improving personalization in adaptive\nintervention.\n","authors":["Karine Karine","Benjamin M. Marlin"],"pdf_url":"https://arxiv.org/pdf/2501.06980v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.11222v2","updated":"2025-01-13T18:20:35Z","published":"2024-11-18T01:19:37Z","title":"The Sound of Water: Inferring Physical Properties from Pouring Liquids","summary":"  We study the connection between audio-visual observations and the underlying\nphysics of a mundane yet intriguing everyday activity: pouring liquids. Given\nonly the sound of liquid pouring into a container, our objective is to\nautomatically infer physical properties such as the liquid level, the shape and\nsize of the container, the pouring rate and the time to fill. To this end, we:\n(i) show in theory that these properties can be determined from the fundamental\nfrequency (pitch); (ii) train a pitch detection model with supervision from\nsimulated data and visual data with a physics-inspired objective; (iii)\nintroduce a new large dataset of real pouring videos for a systematic study;\n(iv) show that the trained model can indeed infer these physical properties for\nreal data; and finally, (v) we demonstrate strong generalization to various\ncontainer shapes, other datasets, and in-the-wild YouTube videos. Our work\npresents a keen understanding of a narrow yet rich problem at the intersection\nof acoustics, physics, and learning. It opens up applications to enhance\nmultisensory perception in robotic pouring.\n","authors":["Piyush Bagad","Makarand Tapaswi","Cees G. M. Snoek","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2411.11222v2.pdf","comment":"Project page at https://bpiyush.github.io/pouring-water-website.\n  Short version accepted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.07246v1","updated":"2025-01-13T11:54:40Z","published":"2025-01-13T11:54:40Z","title":"Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language\n  Model","summary":"  Large Audio-Language Models (LALMs) have demonstrated remarkable performance\nin tasks involving audio perception and understanding, such as speech\nrecognition and audio captioning. However, their reasoning capabilities -\ncritical for solving complex real-world problems - remain underexplored. In\nthis work, we conduct the first exploration into integrating Chain-of-Thought\n(CoT) reasoning into LALMs to enhance their reasoning ability across auditory\nmodalities. We evaluate representative CoT methods, analyzing their performance\nin both information extraction and reasoning tasks across sound, music, and\nspeech domains. Our findings reveal that CoT methods significantly improve\nperformance on easy and medium tasks but encounter challenges with hard tasks,\nwhere reasoning chains can confuse the model rather than improve accuracy.\nAdditionally, we identify a positive correlation between reasoning path length\nand accuracy, demonstrating the potential of scaling inference for advanced\ninstruction-following and reasoning. This study not only highlights the promise\nof CoT in enhancing LALM reasoning capabilities but also identifies key\nlimitations and provides actionable directions for future research.\n","authors":["Ziyang Ma","Zhuo Chen","Yuping Wang","Eng Siong Chng","Xie Chen"],"pdf_url":"https://arxiv.org/pdf/2501.07246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07245v1","updated":"2025-01-13T11:54:26Z","published":"2025-01-13T11:54:26Z","title":"Depth and Image Fusion for Road Obstacle Detection Using Stereo Camera","summary":"  This paper is devoted to the detection of objects on a road, performed with a\ncombination of two methods based on both the use of depth information and video\nanalysis of data from a stereo camera. Since neither the time of the appearance\nof an object on the road, nor its size and shape is known in advance,\nML/DL-based approaches are not applicable. The task becomes more complicated\ndue to variations in artificial illumination, inhomogeneous road surface\ntexture, and unknown character and features of the object. To solve this\nproblem we developed the depth and image fusion method that complements a\nsearch of small contrast objects by RGB-based method, and obstacle detection by\nstereo image-based approach with SLIC superpixel segmentation. We conducted\nexperiments with static and low speed obstacles in an underground parking lot\nand demonstrated the successful work of the developed technique for detecting\nand even tracking small objects, which can be parking infrastructure objects,\nthings left on the road, wheels, dropped boxes, etc.\n","authors":["Oleg Perezyabov","Mikhail Gavrilenkov","Ilya Afanasyev"],"pdf_url":"https://arxiv.org/pdf/2501.07245v1.pdf","comment":"8 pages, 15 figures"},{"id":"http://arxiv.org/abs/2501.07110v1","updated":"2025-01-13T07:51:43Z","published":"2025-01-13T07:51:43Z","title":"Dynamic Multimodal Fusion via Meta-Learning Towards Micro-Video\n  Recommendation","summary":"  Multimodal information (e.g., visual, acoustic, and textual) has been widely\nused to enhance representation learning for micro-video recommendation. For\nintegrating multimodal information into a joint representation of micro-video,\nmultimodal fusion plays a vital role in the existing micro-video recommendation\napproaches. However, the static multimodal fusion used in previous studies is\ninsufficient to model the various relationships among multimodal information of\ndifferent micro-videos. In this paper, we develop a novel meta-learning-based\nmultimodal fusion framework called Meta Multimodal Fusion (MetaMMF), which\ndynamically assigns parameters to the multimodal fusion function for each\nmicro-video during its representation learning. Specifically, MetaMMF regards\nthe multimodal fusion of each micro-video as an independent task. Based on the\nmeta information extracted from the multimodal features of the input task,\nMetaMMF parameterizes a neural network as the item-specific fusion function via\na meta learner. We perform extensive experiments on three benchmark datasets,\ndemonstrating the significant improvements over several state-of-the-art\nmultimodal recommendation models, like MMGCN, LATTICE, and InvRL. Furthermore,\nwe lighten our model by adopting canonical polyadic decomposition to improve\nthe training efficiency, and validate its effectiveness through experimental\nresults. Codes are available at https://github.com/hanliu95/MetaMMF.\n","authors":["Han Liu","Yinwei Wei","Fan Liu","Wenjie Wang","Liqiang Nie","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2501.07110v1.pdf","comment":"This paper has been accepted by ACM Transactions on Information\n  Systems"},{"id":"http://arxiv.org/abs/2406.00323v2","updated":"2025-01-13T05:39:04Z","published":"2024-06-01T06:53:03Z","title":"BeFA: A General Behavior-driven Feature Adapter for Multimedia\n  Recommendation","summary":"  Multimedia recommender systems focus on utilizing behavioral information and\ncontent information to model user preferences. Typically, it employs\npre-trained feature encoders to extract content features, then fuses them with\nbehavioral features. However, pre-trained feature encoders often extract\nfeatures from the entire content simultaneously, including excessive\npreference-irrelevant details. We speculate that it may result in the extracted\nfeatures not containing sufficient features to accurately reflect user\npreferences. To verify our hypothesis, we introduce an attribution analysis\nmethod for visually and intuitively analyzing the content features. The results\nindicate that certain products' content features exhibit the issues of\ninformation drift}and information omission,reducing the expressive ability of\nfeatures. Building upon this finding, we propose an effective and efficient\ngeneral Behavior-driven Feature Adapter (BeFA) to tackle these issues. This\nadapter reconstructs the content feature with the guidance of behavioral\ninformation, enabling content features accurately reflecting user preferences.\nExtensive experiments demonstrate the effectiveness of the adapter across all\nmultimedia recommendation methods. Our code is made publicly available on\nhttps://github.com/fqldom/BeFA.\n","authors":["Qile Fan","Penghang Yu","Zhiyi Tan","Bing-Kun Bao","Guanming Lu"],"pdf_url":"https://arxiv.org/pdf/2406.00323v2.pdf","comment":"This paper is accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2501.07711v1","updated":"2025-01-13T21:45:01Z","published":"2025-01-13T21:45:01Z","title":"Pedestrian Trajectory Prediction Based on Social Interactions Learning\n  With Random Weights","summary":"  Pedestrian trajectory prediction is a critical technology in the evolution of\nself-driving cars toward complete artificial intelligence. Over recent years,\nfocusing on the trajectories of pedestrians to model their social interactions\nhas surged with great interest in more accurate trajectory predictions.\nHowever, existing methods for modeling pedestrian social interactions rely on\npre-defined rules, struggling to capture non-explicit social interactions. In\nthis work, we propose a novel framework named DTGAN, which extends the\napplication of Generative Adversarial Networks (GANs) to graph sequence data,\nwith the primary objective of automatically capturing implicit social\ninteractions and achieving precise predictions of pedestrian trajectory. DTGAN\ninnovatively incorporates random weights within each graph to eliminate the\nneed for pre-defined interaction rules. We further enhance the performance of\nDTGAN by exploring diverse task loss functions during adversarial training,\nwhich yields improvements of 16.7\\% and 39.3\\% on metrics ADE and FDE,\nrespectively. The effectiveness and accuracy of our framework are verified on\ntwo public datasets. The experimental results show that our proposed DTGAN\nachieves superior performance and is well able to understand pedestrians'\nintentions.\n","authors":["Jiajia Xie","Sheng Zhang","Beihao Xia","Zhu Xiao","Hongbo Jiang","Siwang Zhou","Zheng Qin","Hongyang Chen"],"pdf_url":"https://arxiv.org/pdf/2501.07711v1.pdf","comment":"13 pages,7 figures,Accepted to IEEE Transactions on Multimedia (TMM)"},{"id":"http://arxiv.org/abs/2501.07650v1","updated":"2025-01-13T19:21:18Z","published":"2025-01-13T19:21:18Z","title":"An Efficient NVoD Scheme Using Implicit Error Correction and Subchannels\n  for Wireless Networks","summary":"  Implicit Error Correction (IEC) is a near Video-on-Demand (nVoD) scheme that\ntrades bandwidth utilization for initial playback delay to potentially support\nan infinite number of users. Additionally, it provides error protection without\nany further bandwidth increase by exploiting the implicit redundancy of nVoD\nprotocols, using linear combinations of the segments transmitted in a given\ntime slot. However, IEC packet loss protection is weaker at the beginning of\nthe playback due to the lack of implicit redundancy and lower decoding\nefficiency, resulting in worse subjective playback quality. In tackling this\nissue, this paper contributes with an extension of the original nVoD\narchitecture, enhancing its performance by adding a new element namely,\nsubchannels. These subdivisions of the original channels do not provide further\npacket loss protection but significantly improve the decoding efficiency, which\nin turn increases playback quality, especially at the beginning. Even for very\nhigh packet loss probabilities, subchannels are designed to obtain higher\ndecoding efficiency which results in greater packet loss protection than that\nprovided by IEC. The proposed scheme is especially useful in wireless\ncooperative networks using techniques such as network coding, as content\ntransmissions can be split into different subchannels in order to maximize\nnetwork efficiency.\n","authors":["Rafael Asorey-Cacheda","Antonio-Javier Garcia-Sanchez","Joan Garcia-Haro"],"pdf_url":"https://arxiv.org/pdf/2501.07650v1.pdf","comment":null}]},"2025-01-12T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2305.03123v4","updated":"2025-01-12T21:07:54Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v4.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2501.06932v1","updated":"2025-01-12T21:00:50Z","published":"2025-01-12T21:00:50Z","title":"Harnessing Large Language Models for Disaster Management: A Survey","summary":"  Large language models (LLMs) have revolutionized scientific research with\ntheir exceptional capabilities and transformed various fields. Among their\npractical applications, LLMs have been playing a crucial role in mitigating\nthreats to human life, infrastructure, and the environment. Despite growing\nresearch in disaster LLMs, there remains a lack of systematic review and\nin-depth analysis of LLMs for natural disaster management. To address the gap,\nthis paper presents a comprehensive survey of existing LLMs in natural disaster\nmanagement, along with a taxonomy that categorizes existing works based on\ndisaster phases and application scenarios. By collecting public datasets and\nidentifying key challenges and opportunities, this study aims to guide the\nprofessional community in developing advanced LLMs for disaster management to\nenhance the resilience against natural disasters.\n","authors":["Zhenyu Lei","Yushun Dong","Weiyu Li","Rong Ding","Qi Wang","Jundong Li"],"pdf_url":"https://arxiv.org/pdf/2501.06932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06911v1","updated":"2025-01-12T19:48:21Z","published":"2025-01-12T19:48:21Z","title":"Risk-Averse Finetuning of Large Language Models","summary":"  We consider the challenge of mitigating the generation of negative or toxic\ncontent by the Large Language Models (LLMs) in response to certain prompts. We\npropose integrating risk-averse principles into LLM fine-tuning to minimize the\noccurrence of harmful outputs, particularly rare but significant events. By\noptimizing the risk measure of Conditional Value at Risk (CVaR), our\nmethodology trains LLMs to exhibit superior performance in avoiding toxic\noutputs while maintaining effectiveness in generative tasks. Empirical\nevaluations on sentiment modification and toxicity mitigation tasks demonstrate\nthe efficacy of risk-averse reinforcement learning with human feedback (RLHF)\nin promoting a safer and more constructive online discourse environment.\n","authors":["Sapana Chaudhary","Ujwal Dinesha","Dileep Kalathil","Srinivas Shakkottai"],"pdf_url":"https://arxiv.org/pdf/2501.06911v1.pdf","comment":"Neurips 2024"},{"id":"http://arxiv.org/abs/2501.06892v1","updated":"2025-01-12T18:02:29Z","published":"2025-01-12T18:02:29Z","title":"Language Fusion for Parameter-Efficient Cross-lingual Transfer","summary":"  Limited availability of multilingual text corpora for training language\nmodels often leads to poor performance on downstream tasks due to undertrained\nrepresentation spaces for languages other than English. This\n'under-representation' has motivated recent cross-lingual transfer methods to\nleverage the English representation space by e.g. mixing English and\n'non-English' tokens at the input level or extending model parameters to\naccommodate new languages. However, these approaches often come at the cost of\nincreased computational complexity. We propose Fusion forLanguage\nRepresentations (FLARE) in adapters, a novel method that enhances\nrepresentation quality and downstream performance for languages other than\nEnglish while maintaining parameter efficiency. FLARE integrates source and\ntarget language representations within low-rank (LoRA) adapters using\nlightweight linear transformations, maintaining parameter efficiency while\nimproving transfer performance. A series of experiments across representative\ncross-lingual natural language understanding tasks, including natural language\ninference, question-answering and sentiment analysis, demonstrate FLARE's\neffectiveness. FLARE achieves performance improvements of 4.9% for Llama 3.1\nand 2.2% for Gemma~2 compared to standard LoRA fine-tuning on\nquestion-answering tasks, as measured by the exact match metric.\n","authors":["Philipp Borchert","Ivan Vulić","Marie-Francine Moens","Jochen De Weerdt"],"pdf_url":"https://arxiv.org/pdf/2501.06892v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2409.08466v2","updated":"2025-01-12T17:23:21Z","published":"2024-09-13T01:40:20Z","title":"Explaining Datasets in Words: Statistical Models with Natural Language\n  Parameters","summary":"  To make sense of massive data, we often fit simplified models and then\ninterpret the parameters; for example, we cluster the text embeddings and then\ninterpret the mean parameters of each cluster. However, these parameters are\noften high-dimensional and hard to interpret. To make model parameters directly\ninterpretable, we introduce a family of statistical models -- including\nclustering, time series, and classification models -- parameterized by natural\nlanguage predicates. For example, a cluster of text about COVID could be\nparameterized by the predicate \"discusses COVID\". To learn these statistical\nmodels effectively, we develop a model-agnostic algorithm that optimizes\ncontinuous relaxations of predicate parameters with gradient descent and\ndiscretizes them by prompting language models (LMs). Finally, we apply our\nframework to a wide range of problems: taxonomizing user chat dialogues,\ncharacterizing how they evolve across time, finding categories where one\nlanguage model is better than the other, clustering math problems based on\nsubareas, and explaining visual features in memorable images. Our framework is\nhighly versatile, applicable to both textual and visual domains, can be easily\nsteered to focus on specific properties (e.g. subareas), and explains\nsophisticated concepts that classical methods (e.g. n-gram analysis) struggle\nto produce.\n","authors":["Ruiqi Zhong","Heng Wang","Dan Klein","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2409.08466v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06873v1","updated":"2025-01-12T17:03:45Z","published":"2025-01-12T17:03:45Z","title":"Causal Claims in Economics","summary":"  We analyze over 44,000 NBER and CEPR working papers from 1980 to 2023 using a\ncustom language model to construct knowledge graphs that map economic concepts\nand their relationships. We distinguish between general claims and those\ndocumented via causal inference methods (e.g., DiD, IV, RDD, RCTs). We document\na substantial rise in the share of causal claims-from roughly 4% in 1990 to\nnearly 28% in 2020-reflecting the growing influence of the \"credibility\nrevolution.\" We find that causal narrative complexity (e.g., the depth of\ncausal chains) strongly predicts both publication in top-5 journals and higher\ncitation counts, whereas non-causal complexity tends to be uncorrelated or\nnegatively associated with these outcomes. Novelty is also pivotal for top-5\npublication, but only when grounded in credible causal methods: introducing\ngenuinely new causal edges or paths markedly increases both the likelihood of\nacceptance at leading outlets and long-run citations, while non-causal novelty\nexhibits weak or even negative effects. Papers engaging with central, widely\nrecognized concepts tend to attract more citations, highlighting a divergence\nbetween factors driving publication success and long-term academic impact.\nFinally, bridging underexplored concept pairs is rewarded primarily when\ngrounded in causal methods, yet such gap filling exhibits no consistent link\nwith future citations. Overall, our findings suggest that methodological rigor\nand causal innovation are key drivers of academic recognition, but sustained\nimpact may require balancing novel contributions with conceptual integration\ninto established economic discourse.\n","authors":["Prashant Garg","Thiemo Fetzer"],"pdf_url":"https://arxiv.org/pdf/2501.06873v1.pdf","comment":"For data, interactive tools, and additional project information,\n  visit https://www.causal.claims/. The website contains resources such as data\n  downloads, interactive author and paper-level knowledge graphs, and more"},{"id":"http://arxiv.org/abs/2501.00879v2","updated":"2025-01-12T17:03:12Z","published":"2025-01-01T15:57:34Z","title":"TrustRAG: Enhancing Robustness and Trustworthiness in RAG","summary":"  Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user queries. However, these\nsystems remain vulnerable to corpus poisoning attacks that can significantly\ndegrade LLM performance through the injection of malicious content. To address\nthese challenges, we propose TrustRAG, a robust framework that systematically\nfilters compromised and irrelevant contents before they are retrieved for\ngeneration. Our approach implements a two-stage defense mechanism: At the first\nstage, it employs K-means clustering to identify potential attack patterns in\nretrieved documents using cosine similarity and ROUGE metrics as guidance,\neffectively isolating suspicious content. Secondly, it performs a\nself-assessment which detects malicious documents and resolves discrepancies\nbetween the model's internal knowledge and external information. TrustRAG\nfunctions as a plug-and-play, training-free module that integrates seamlessly\nwith any language model, whether open or closed-source. In addition, TrustRAG\nmaintains high contextual relevance while strengthening defenses against corpus\npoisoning attacks. Through extensive experimental validation, we demonstrate\nthat TrustRAG delivers substantial improvements in retrieval accuracy,\nefficiency, and attack resistance compared to existing approaches across\nmultiple model architectures and datasets. We have made TrustRAG available as\nopen-source software at \\url{https://github.com/HuichiZhou/TrustRAG}.\n","authors":["Huichi Zhou","Kin-Hei Lee","Zhonghao Zhan","Yue Chen","Zhenhao Li","Zhaoyang Wang","Hamed Haddadi","Emine Yilmaz"],"pdf_url":"https://arxiv.org/pdf/2501.00879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09172v4","updated":"2025-01-12T16:31:19Z","published":"2024-08-17T11:33:23Z","title":"Unlocking the Power of LLM Uncertainty for Active In-Context Example\n  Selection","summary":"  Large Language Models (LLMs) have shown remarkable performance across a wide\nrange of downstream tasks. However, it is challenging for users to discern\nwhether the responses of LLM are generated with certainty or are fabricated to\nmeet user expectations. In this paper, we introduce Uncertainty Tripartite\nTesting Paradigm (Unc-TTP), a novel method for classifying LLM uncertainty by\nleveraging output inconsistency. Specifically, Unc-TTP performs three rounds of\nsampling under varying label injection interference, enumerating all possible\noutcomes, and uses the degree of output inconsistency as the indicator of the\nLLM's intrinsic uncertainty. To validate the effectiveness of this\ninconsistency-defined uncertainty, we draw inspiration from Active Learning,\ncomparing the informativeness of actively selected in-context examples. Our\nexperiments show that uncertainty examples selected via Unc-TTP are more\ninformative than certainty examples. Furthermore, the Unc-TTP-guided\nuncertainty-based active example selection strategy outperforms existing\nmethods, highlighting its effectiveness in classifying LLM uncertainty and\nenhancing in-context learning. This work not only underscores the potential of\ninconsistency-based uncertainty classification for both open- and closed-source\nLLMs but also presents a practical approach for leveraging uncertainty to\nimprove LLM performance in real-world tasks.\n","authors":["Hsiu-Yuan Huang","Zichen Wu","Yutong Yang","Junzhao Zhang","Yunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2408.09172v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06863v1","updated":"2025-01-12T16:23:18Z","published":"2025-01-12T16:23:18Z","title":"Transfer Learning of Tabular Data by Finetuning Large Language Models","summary":"  Despite the artificial intelligence (AI) revolution, deep learning has yet to\nachieve much success with tabular data due to heterogeneous feature space and\nlimited sample sizes without viable transfer learning. The new era of\ngenerative AI, powered by large language models (LLM), brings unprecedented\nlearning opportunities to diverse data and domains. This paper investigates the\neffectiveness of an LLM application programming interface (API) and transfer\nlearning of LLM in tabular data classification. LLM APIs respond to input text\nprompts with tokenized data and instructions, whereas transfer learning\nfinetunes an LLM for a target classification task. This paper proposes an\nend-to-end finetuning of LLM to demonstrate cross-data transfer learning on ten\nbenchmark data sets when large pre-trained tabular data models do not exist to\nfacilitate transfer learning. The proposed LLM finetuning method outperforms\nstate-of-the-art machine and deep learning methods on tabular data with less\nthan ten features - a standard feature size for tabular data sets. The transfer\nlearning approach uses a fraction of the computational cost of other deep\nlearning or API-based solutions while ensuring competitive or superior\nclassification performance.\n","authors":["Shourav B. Rabbani","Ibna Kowsar","Manar D. Samad"],"pdf_url":"https://arxiv.org/pdf/2501.06863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02960v2","updated":"2025-01-12T16:22:04Z","published":"2024-07-03T09:54:08Z","title":"ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary\n  LLMs on Private Datasets","summary":"  This work addresses the timely yet underexplored problem of performing\ninference and finetuning of a proprietary LLM owned by a model provider entity\non the confidential/private data of another data owner entity, in a way that\nensures the confidentiality of both the model and the data. Hereby, the\nfinetuning is conducted offsite, i.e., on the computation infrastructure of a\nthird-party cloud provider. We tackle this problem by proposing ObfuscaTune, a\nnovel, efficient and fully utility-preserving approach that combines a simple\nyet effective obfuscation technique with an efficient usage of confidential\ncomputing (only 5% of the model parameters are placed on TEE). We empirically\ndemonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models\nwith different sizes on four NLP benchmark datasets. Finally, we compare to a\nna\\\"ive version of our approach to highlight the necessity of using random\nmatrices with low condition numbers in our approach to reduce errors induced by\nthe obfuscation.\n","authors":["Ahmed Frikha","Nassim Walha","Ricardo Mendes","Krishna Kanth Nakka","Xue Jiang","Xuebing Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.02960v2.pdf","comment":"Accepted at AAAI 2025 (PPAI Workshop)"},{"id":"http://arxiv.org/abs/2501.06859v1","updated":"2025-01-12T16:17:25Z","published":"2025-01-12T16:17:25Z","title":"A Comprehensive Evaluation of Large Language Models on Mental Illnesses\n  in Arabic Context","summary":"  Mental health disorders pose a growing public health concern in the Arab\nworld, emphasizing the need for accessible diagnostic and intervention tools.\nLarge language models (LLMs) offer a promising approach, but their application\nin Arabic contexts faces challenges including limited labeled datasets,\nlinguistic complexity, and translation biases. This study comprehensively\nevaluates 8 LLMs, including general multi-lingual models, as well as bi-lingual\nones, on diverse mental health datasets (such as AraDepSu, Dreaddit, MedMCQA),\ninvestigating the impact of prompt design, language configuration (native\nArabic vs. translated English, and vice versa), and few-shot prompting on\ndiagnostic performance. We find that prompt engineering significantly\ninfluences LLM scores mainly due to reduced instruction following, with our\nstructured prompt outperforming a less structured variant on multi-class\ndatasets, with an average difference of 14.5\\%. While language influence on\nperformance was modest, model selection proved crucial: Phi-3.5 MoE excelled in\nbalanced accuracy, particularly for binary classification, while Mistral NeMo\nshowed superior performance in mean absolute error for severity prediction\ntasks. Few-shot prompting consistently improved performance, with particularly\nsubstantial gains observed for GPT-4o Mini on multi-class classification,\nboosting accuracy by an average factor of 1.58. These findings underscore the\nimportance of prompt optimization, multilingual analysis, and few-shot learning\nfor developing culturally sensitive and effective LLM-based mental health tools\nfor Arabic-speaking populations.\n","authors":["Noureldin Zahran","Aya E. Fouda","Radwa J. Hanafy","Mohammed E. Fouda"],"pdf_url":"https://arxiv.org/pdf/2501.06859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04289v5","updated":"2025-01-12T15:43:54Z","published":"2024-06-06T17:34:24Z","title":"What Languages are Easy to Language-Model? A Perspective from Learning\n  Probabilistic Regular Languages","summary":"  What can large language models learn? By definition, language models (LM) are\ndistributions over strings. Therefore, an intuitive way of addressing the above\nquestion is to formalize it as a matter of learnability of classes of\ndistributions over strings. While prior work in this direction focused on\nassessing the theoretical limits, in contrast, we seek to understand the\nempirical learnability. Unlike prior empirical work, we evaluate neural LMs on\ntheir home turf-learning probabilistic languages-rather than as classifiers of\nformal languages. In particular, we investigate the learnability of regular LMs\n(RLMs) by RNN and Transformer LMs. We empirically test the learnability of RLMs\nas a function of various complexity parameters of the RLM and the hidden state\nsize of the neural LM. We find that the RLM rank, which corresponds to the size\nof linear space spanned by the logits of its conditional distributions, and the\nexpected length of sampled strings are strong and significant predictors of\nlearnability for both RNNs and Transformers. Several other predictors also\nreach significance, but with differing patterns between RNNs and Transformers.\n","authors":["Nadav Borenstein","Anej Svete","Robin Chan","Josef Valvoda","Franz Nowak","Isabelle Augenstein","Eleanor Chodroff","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2406.04289v5.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2501.06848v1","updated":"2025-01-12T15:34:24Z","published":"2025-01-12T15:34:24Z","title":"A General Framework for Inference-time Scaling and Steering of Diffusion\n  Models","summary":"  Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we propose Feynman Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models, even\nwith off-the-shelf rewards, can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering .\n","authors":["Raghav Singhal","Zachary Horvitz","Ryan Teehan","Mengye Ren","Zhou Yu","Kathleen McKeown","Rajesh Ranganath"],"pdf_url":"https://arxiv.org/pdf/2501.06848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06842v1","updated":"2025-01-12T15:21:22Z","published":"2025-01-12T15:21:22Z","title":"SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training","summary":"  Large Language Models (LLMs) have demonstrated exceptional performance across\ndiverse tasks, yet their training remains highly resource-intensive and\nsusceptible to critical challenges such as training instability. A predominant\nsource of this instability stems from gradient and loss spikes, which disrupt\nthe learning process, often leading to costly interventions like checkpoint\nrecovery and experiment restarts, further amplifying inefficiencies. This paper\npresents a comprehensive investigation into gradient spikes observed during LLM\ntraining, revealing their prevalence across multiple architectures and\ndatasets. Our analysis shows that these spikes can be up to $1000\\times$ larger\nthan typical gradients, substantially deteriorating model performance. To\naddress this issue, we propose Spike-Aware Adam with Momentum Reset SPAM, a\nnovel optimizer designed to counteract gradient spikes through momentum reset\nand spike-aware gradient clipping. Extensive experiments, including both\npre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam\nand its variants across various tasks, including (1) LLM pre-training from 60M\nto 1B, (2) 4-bit LLM pre-training,(3) reinforcement learning, and (4) Time\nSeries Forecasting. Additionally, SPAM facilitates memory-efficient training by\nenabling sparse momentum, where only a subset of momentum terms are maintained\nand updated. When operating under memory constraints, SPAM outperforms\nstate-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini. Our\nwork underscores the importance of mitigating gradient spikes in LLM training\nand introduces an effective optimization strategy that enhances both training\nstability and resource efficiency at scale. Code is available at\nhttps://github.com/TianjinYellow/SPAM-Optimizer.git\n","authors":["Tianjin Huang","Ziquan Zhu","Gaojie Jin","Lu Liu","Zhangyang Wang","Shiwei Liu"],"pdf_url":"https://arxiv.org/pdf/2501.06842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11791v4","updated":"2025-01-12T15:17:36Z","published":"2024-01-22T09:41:05Z","title":"Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation","summary":"  Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation\nmodels using image data with only image-level supervision. Since precise\npixel-level annotations are not accessible, existing methods typically focus on\nproducing pseudo masks for training segmentation models by refining CAM-like\nheatmaps. However, the produced heatmaps may capture only the discriminative\nimage regions of object categories or the associated co-occurring backgrounds.\nTo address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS)\nframework, which learns to effectively prompt the CLIP latent space to enhance\nthe semantic alignment between the segmented regions and the target object\ncategories. More specifically, we propose Contrastive Prompt Learning and\nPrompt-guided Semantic Refinement to learn the prompts that adequately describe\nand suppress the co-occurring backgrounds associated with each object category.\nIn this way, SemPLeS can perform better semantic alignment between object\nregions and class labels, resulting in desired pseudo masks for training\nsegmentation models. The proposed SemPLeS framework achieves competitive\nperformance on standard WSSS benchmarks, PASCAL VOC 2012 and MS COCO 2014, and\nshows compatibility with other WSSS methods. Code:\nhttps://github.com/NVlabs/SemPLeS.\n","authors":["Ci-Siang Lin","Chien-Yi Wang","Yu-Chiang Frank Wang","Min-Hung Chen"],"pdf_url":"https://arxiv.org/pdf/2401.11791v4.pdf","comment":"WACV 2025. Code: https://github.com/NVlabs/SemPLeS. Project page:\n  https://projectdisr.github.io/semples/"},{"id":"http://arxiv.org/abs/2501.06834v1","updated":"2025-01-12T15:06:28Z","published":"2025-01-12T15:06:28Z","title":"LLMs Model Non-WEIRD Populations: Experiments with Synthetic Cultural\n  Agents","summary":"  Despite its importance, studying economic behavior across diverse, non-WEIRD\n(Western, Educated, Industrialized, Rich, and Democratic) populations presents\nsignificant challenges. We address this issue by introducing a novel\nmethodology that uses Large Language Models (LLMs) to create synthetic cultural\nagents (SCAs) representing these populations. We subject these SCAs to classic\nbehavioral experiments, including the dictator and ultimatum games. Our results\ndemonstrate substantial cross-cultural variability in experimental behavior.\nNotably, for populations with available data, SCAs' behaviors qualitatively\nresemble those of real human subjects. For unstudied populations, our method\ncan generate novel, testable hypotheses about economic behavior. By integrating\nAI into experimental economics, this approach offers an effective and ethical\nmethod to pilot experiments and refine protocols for hard-to-reach populations.\nOur study provides a new tool for cross-cultural economic studies and\ndemonstrates how LLMs can help experimental behavioral research.\n","authors":["Augusto Gonzalez-Bonorino","Monica Capra","Emilio Pantoja"],"pdf_url":"https://arxiv.org/pdf/2501.06834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06826v1","updated":"2025-01-12T14:39:26Z","published":"2025-01-12T14:39:26Z","title":"Correcting Annotator Bias in Training Data: Population-Aligned Instance\n  Replication (PAIR)","summary":"  Models trained on crowdsourced labels may not reflect broader population\nviews when annotator pools are not representative. Since collecting\nrepresentative labels is challenging, we propose Population-Aligned Instance\nReplication (PAIR), a method to address this bias through statistical\nadjustment. Using a simulation study of hate speech and offensive language\ndetection, we create two types of annotators with different labeling tendencies\nand generate datasets with varying proportions of the types. Models trained on\nunbalanced annotator pools show poor calibration compared to those trained on\nrepresentative data. However, PAIR, which duplicates labels from\nunderrepresented annotator groups to match population proportions,\nsignificantly reduces bias without requiring new data collection. These results\nsuggest statistical techniques from survey research can help align model\ntraining with target populations even when representative annotator pools are\nunavailable. We conclude with three practical recommendations for improving\ntraining data quality.\n","authors":["Stephanie Eckman","Bolei Ma","Christoph Kern","Rob Chew","Barbara Plank","Frauke Kreuter"],"pdf_url":"https://arxiv.org/pdf/2501.06826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06825v1","updated":"2025-01-12T14:38:51Z","published":"2025-01-12T14:38:51Z","title":"Event Argument Extraction with Enriched Prompts","summary":"  This work aims to delve deeper into prompt-based event argument extraction\n(EAE) models. We explore the impact of incorporating various types of\ninformation into the prompt on model performance, including trigger, other role\narguments for the same event, and role arguments across multiple events within\nthe same document. Further, we provide the best possible performance that the\nprompt-based EAE model can attain and demonstrate such models can be further\noptimized from the perspective of the training objective. Experiments are\ncarried out on three small language models and two large language models in\nRAMS.\n","authors":["Chen Liang"],"pdf_url":"https://arxiv.org/pdf/2501.06825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12846v2","updated":"2025-01-12T14:12:30Z","published":"2024-10-10T05:34:00Z","title":"Accurate and Regret-aware Numerical Problem Solver for Tabular Question\n  Answering","summary":"  Question answering on free-form tables (a.k.a. TableQA) is a challenging task\nbecause of the flexible structure and complex schema of tables. Recent studies\nuse Large Language Models (LLMs) for this task, exploiting their capability in\nunderstanding the questions and tabular data, which are typically given in\nnatural language and contain many textual fields, respectively. While this\napproach has shown promising results, it overlooks the challenges brought by\nnumerical values which are common in tabular data, and LLMs are known to\nstruggle with such values. We aim to address this issue, and we propose a model\nnamed TabLaP that uses LLMs as a planner rather than an answer generator. This\napproach exploits LLMs' capability in multi-step reasoning while leaving the\nactual numerical calculations to a Python interpreter for accurate calculation.\nRecognizing the inaccurate nature of LLMs, we further make a first attempt to\nquantify the trustworthiness of the answers produced by TabLaP, such that users\ncan use TabLaP in a regret-aware manner. Experimental results on two benchmark\ndatasets show that TabLaP is substantially more accurate than the\nstate-of-the-art models, improving the answer accuracy by 5.7% and 5.8% on the\ntwo datasets, respectively.\n","authors":["Yuxiang Wang","Jianzhong Qi","Junhao Gan"],"pdf_url":"https://arxiv.org/pdf/2410.12846v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03814v4","updated":"2025-01-12T13:46:49Z","published":"2024-06-06T07:39:17Z","title":"Improving Zero-Shot Chinese-English Code-Switching ASR with kNN-CTC and\n  Gated Monolingual Datastores","summary":"  The kNN-CTC model has proven to be effective for monolingual automatic speech\nrecognition (ASR). However, its direct application to multilingual scenarios\nlike code-switching, presents challenges. Although there is potential for\nperformance improvement, a kNN-CTC model utilizing a single bilingual datastore\ncan inadvertently introduce undesirable noise from the alternative language. To\naddress this, we propose a novel kNN-CTC-based code-switching ASR (CS-ASR)\nframework that employs dual monolingual datastores and a gated datastore\nselection mechanism to reduce noise interference. Our method selects the\nappropriate datastore for decoding each frame, ensuring the injection of\nlanguage-specific information into the ASR process. We apply this framework to\ncutting-edge CTC-based models, developing an advanced CS-ASR system. Extensive\nexperiments demonstrate the remarkable effectiveness of our gated datastore\nmechanism in enhancing the performance of zero-shot Chinese-English CS-ASR.\n","authors":["Jiaming Zhou","Shiwan Zhao","Hui Wang","Tian-Hao Zhang","Haoqin Sun","Xuechen Wang","Yong Qin"],"pdf_url":"https://arxiv.org/pdf/2406.03814v4.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.06810v1","updated":"2025-01-12T13:29:24Z","published":"2025-01-12T13:29:24Z","title":"Improving Cross-Lingual Phonetic Representation of Low-Resource\n  Languages Through Language Similarity Analysis","summary":"  This paper examines how linguistic similarity affects cross-lingual phonetic\nrepresentation in speech processing for low-resource languages, emphasizing\neffective source language selection. Previous cross-lingual research has used\nvarious source languages to enhance performance for the target low-resource\nlanguage without thorough consideration of selection. Our study stands out by\nproviding an in-depth analysis of language selection, supported by a practical\napproach to assess phonetic proximity among multiple language families. We\ninvestigate how within-family similarity impacts performance in multilingual\ntraining, which aids in understanding language dynamics. We also evaluate the\neffect of using phonologically similar languages, regardless of family. For the\nphoneme recognition task, utilizing phonologically similar languages\nconsistently achieves a relative improvement of 55.6% over monolingual\ntraining, even surpassing the performance of a large-scale self-supervised\nlearning model. Multilingual training within the same language family\ndemonstrates that higher phonological similarity enhances performance, while\nlower similarity results in degraded performance compared to monolingual\ntraining.\n","authors":["Minu Kim","Kangwook Jang","Hoirin Kim"],"pdf_url":"https://arxiv.org/pdf/2501.06810v1.pdf","comment":"10 pages, 5 figures, accepted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.06795v1","updated":"2025-01-12T12:32:43Z","published":"2025-01-12T12:32:43Z","title":"Bridging the Fairness Gap: Enhancing Pre-trained Models with\n  LLM-Generated Sentences","summary":"  Pre-trained language models (PLMs) are trained on data that inherently\ncontains gender biases, leading to undesirable impacts. Traditional debiasing\nmethods often rely on external corpora, which may lack quality, diversity, or\ndemographic balance, affecting the effectiveness of debiasing. With the rise of\nlarge language models and their extensive knowledge, we propose enhancing\nfairness (Fair-Gender) in PLMs by absorbing coherent, attribute-balanced, and\nsemantically rich sentences. However, these sentences cannot be directly used\nfor debiasing due to alignment issues and the risk of negative transfer. We\naddress this by applying causal analysis to estimate causal effects, filtering\nout unaligned sentences, and identifying aligned ones for incorporation into\nPLMs, thereby ensuring positive transfer. Experiments show that our approach\nsignificantly reduces gender biases in PLMs while preserving their language\nexpressiveness.\n","authors":["Liu Yu","Ludie Guo","Ping Kuang","Fan Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.06795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14486v2","updated":"2025-01-12T12:31:08Z","published":"2024-09-22T15:16:43Z","title":"Unsupervised Word Discovery: Boundary Detection with Clustering vs.\n  Dynamic Programming","summary":"  We look at the long-standing problem of segmenting unlabeled speech into\nword-like segments and clustering these into a lexicon. Several previous\nmethods use a scoring model coupled with dynamic programming to find an optimal\nsegmentation. Here we propose a much simpler strategy: we predict word\nboundaries using the dissimilarity between adjacent self-supervised features,\nthen we cluster the predicted segments to construct a lexicon. For a fair\ncomparison, we update the older ES-KMeans dynamic programming method with\nbetter features and boundary constraints. On the five-language ZeroSpeech\nbenchmarks, our simple approach gives similar state-of-the-art results compared\nto the new ES-KMeans+ method, while being almost five times faster. Project\nwebpage: https://s-malan.github.io/prom-seg-clus.\n","authors":["Simon Malan","Benjamin van Niekerk","Herman Kamper"],"pdf_url":"https://arxiv.org/pdf/2409.14486v2.pdf","comment":"Accepted at ICASSP 2025"},{"id":"http://arxiv.org/abs/2404.02882v2","updated":"2025-01-12T12:01:47Z","published":"2024-04-03T17:33:21Z","title":"Linear Attention Sequence Parallelism","summary":"  Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP.\n","authors":["Weigao Sun","Zhen Qin","Dong Li","Xuyang Shen","Yu Qiao","Yiran Zhong"],"pdf_url":"https://arxiv.org/pdf/2404.02882v2.pdf","comment":"Technical report, 20 pages"},{"id":"http://arxiv.org/abs/2501.06785v1","updated":"2025-01-12T11:46:07Z","published":"2025-01-12T11:46:07Z","title":"3DCoMPaT200: Language-Grounded Compositional Understanding of Parts and\n  Materials of 3D Shapes","summary":"  Understanding objects in 3D at the part level is essential for humans and\nrobots to navigate and interact with the environment. Current datasets for\npart-level 3D object understanding encompass a limited range of categories. For\ninstance, the ShapeNet-Part and PartNet datasets only include 16, and 24 object\ncategories respectively. The 3DCoMPaT dataset, specifically designed for\ncompositional understanding of parts and materials, contains only 42 object\ncategories. To foster richer and fine-grained part-level 3D understanding, we\nintroduce 3DCoMPaT200, a large-scale dataset tailored for compositional\nunderstanding of object parts and materials, with 200 object categories with\n$\\approx$5 times larger object vocabulary compared to 3DCoMPaT and $\\approx$ 4\ntimes larger part categories. Concretely, 3DCoMPaT200 significantly expands\nupon 3DCoMPaT, featuring 1,031 fine-grained part categories and 293 distinct\nmaterial classes for compositional application to 3D object parts.\nAdditionally, to address the complexities of compositional 3D modeling, we\npropose a novel task of Compositional Part Shape Retrieval using ULIP to\nprovide a strong 3D foundational model for 3D Compositional Understanding. This\nmethod evaluates the model shape retrieval performance given one, three, or six\nparts described in text format. These results show that the model's performance\nimproves with an increasing number of style compositions, highlighting the\ncritical role of the compositional dataset. Such results underscore the\ndataset's effectiveness in enhancing models' capability to understand complex\n3D shapes from a compositional perspective. Code and Data can be found at\nhttp://github.com/3DCoMPaT200/3DCoMPaT200\n","authors":["Mahmoud Ahmed","Xiang Li","Arpit Prajapati","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2501.06785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14526v2","updated":"2025-01-12T10:48:00Z","published":"2024-09-22T16:45:57Z","title":"What Are They Doing? Joint Audio-Speech Co-Reasoning","summary":"  In audio and speech processing, tasks usually focus on either the audio or\nspeech modality, even when both sounds and human speech are present in the same\naudio clip. Recent Auditory Large Language Models (ALLMs) have made it possible\nto process audio and speech simultaneously within a single model, leading to\nfurther considerations of joint audio-speech tasks.\n  In this paper, we establish a novel benchmark to investigate how well ALLMs\ncan perform joint audio-speech processing. Specifically, we introduce Joint\nAudio-Speech Co-Reasoning (JASCO), a novel task that unifies audio and speech\nprocessing, strictly requiring co-reasoning across both modalities. We also\nrelease a scene-reasoning dataset called \"What Are They Doing\". Additionally,\nwe provide deeper insights into the models' behaviors by analyzing their\ndependence on each modality.\n","authors":["Yingzhi Wang","Pooneh Mousavi","Artem Ploujnikov","Mirco Ravanelli"],"pdf_url":"https://arxiv.org/pdf/2409.14526v2.pdf","comment":"Accepted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.06751v1","updated":"2025-01-12T08:36:38Z","published":"2025-01-12T08:36:38Z","title":"Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models","summary":"  Text-to-image (T2I) diffusion models rely on encoded prompts to guide the\nimage generation process. Typically, these prompts are extended to a fixed\nlength by adding padding tokens before text encoding. Despite being a default\npractice, the influence of padding tokens on the image generation process has\nnot been investigated. In this work, we conduct the first in-depth analysis of\nthe role padding tokens play in T2I models. We develop two causal techniques to\nanalyze how information is encoded in the representation of tokens across\ndifferent components of the T2I pipeline. Using these techniques, we\ninvestigate when and how padding tokens impact the image generation process.\nOur findings reveal three distinct scenarios: padding tokens may affect the\nmodel's output during text encoding, during the diffusion process, or be\neffectively ignored. Moreover, we identify key relationships between these\nscenarios and the model's architecture (cross or self-attention) and its\ntraining process (frozen or trained text encoder). These insights contribute to\na deeper understanding of the mechanisms of padding tokens, potentially\ninforming future model design and training practices in T2I systems.\n","authors":["Michael Toker","Ido Galil","Hadas Orgad","Rinon Gal","Yoad Tewel","Gal Chechik","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2501.06751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06488v2","updated":"2025-01-12T08:32:52Z","published":"2024-07-09T01:27:35Z","title":"Towards Understanding Multi-Task Learning (Generalization) of LLMs via\n  Detecting and Exploring Task-Specific Neurons","summary":"  While large language models (LLMs) have demonstrated superior multi-task\ncapabilities, understanding the learning mechanisms behind this is still a\nchallenging problem. In this paper, we attempt to understand such mechanisms\nfrom the perspective of neurons. Specifically, we detect task-sensitive neurons\nin LLMs via gradient attribution on task-specific data. Through extensive\ndeactivation and fine-tuning experiments, we demonstrate that the detected\nneurons are highly correlated with the given task, which we term as\ntask-specific neurons. With these identified task-specific neurons, we delve\ninto two common problems in multi-task learning and continuous learning:\nGeneralization and Catastrophic Forgetting. We find that the overlap of\ntask-specific neurons is strongly associated with generalization and\nspecialization across tasks. Interestingly, at certain layers of LLMs, there is\na high similarity in the parameters of different task-specific neurons, and\nsuch similarity is highly correlated with the generalization performance.\nInspired by these findings, we propose a neuron-level continuous fine-tuning\nmethod that only fine-tunes the current task-specific neurons during continuous\nlearning, and extensive experiments demonstrate the effectiveness of the\nproposed method. Our study provides insights into the interpretability of LLMs\nin multi-task learning.\n","authors":["Yongqi Leng","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2407.06488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07623v3","updated":"2025-01-12T08:17:41Z","published":"2024-05-13T10:30:33Z","title":"Balancing Class Accuracies for Language Models in Inference Time via\n  Nonlinear Integer Programming","summary":"  Large language models are good knowledge bases but struggle to perform\nequally well for all classes in simple text classification tasks. This paper\ninvestigates a fundamental inference-time problem in language models:\nimbalanced class accuracies. We find what's underneath the issue is a tendency\nto over-predict some classes while under-predicting some others, for both base\nLLMs and more sophisticatedly trained, larger chat LLMs. This class accuracy\nimbalance is difficult to solve from the root via ``better'' pre-training or\nfine-tuning strategies, but we show it can be effectively mitigated via\ninference-time optimization. To this end, we conceptualize and quantify the\nover- and under-prediction issue as the Contextual Oddity Bias (COBias), and\npropose the Debiasing as Nonlinear Integer Programming (DNIP) model to correct\nin-context learned class probabilities based on minimizing COBias and\nmaximizing overall accuracy, without LLM parameter update. Considering that the\nDNIP model implicitly contains non-differentiable elements, we therefore use\nthe simulated annealing algorithm to solve. We perform extensive evaluations on\nthree LLMs across seven NLP classification tasks, in different prompting\nsettings. Results show that DNIP simultaneously achieves significant COBias\nreduction (-27%) and accuracy improvement (+12%) over the conventional ICL\napproach, suggesting that inference-time mitigation of class accuracy imbalance\nis a promising direction to push forward LLM performances.\n","authors":["Ruixi Lin","Yang You"],"pdf_url":"https://arxiv.org/pdf/2405.07623v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06741v1","updated":"2025-01-12T07:30:49Z","published":"2025-01-12T07:30:49Z","title":"Hierarchical Divide-and-Conquer for Fine-Grained Alignment in LLM-Based\n  Medical Evaluation","summary":"  In the rapidly evolving landscape of large language models (LLMs) for medical\napplications, ensuring the reliability and accuracy of these models in clinical\nsettings is paramount. Existing benchmarks often focus on fixed-format tasks\nlike multiple-choice QA, which fail to capture the complexity of real-world\nclinical diagnostics. Moreover, traditional evaluation metrics and LLM-based\nevaluators struggle with misalignment, often providing oversimplified\nassessments that do not adequately reflect human judgment. To address these\nchallenges, we introduce HDCEval, a Hierarchical Divide-and-Conquer Evaluation\nframework tailored for fine-grained alignment in medical evaluation. HDCEval is\nbuilt on a set of fine-grained medical evaluation guidelines developed in\ncollaboration with professional doctors, encompassing Patient Question\nRelevance, Medical Knowledge Correctness, and Expression. The framework\ndecomposes complex evaluation tasks into specialized subtasks, each evaluated\nby expert models trained through Attribute-Driven Token Optimization (ADTO) on\na meticulously curated preference dataset. This hierarchical approach ensures\nthat each aspect of the evaluation is handled with expert precision, leading to\na significant improvement in alignment with human evaluators.\n","authors":["Shunfan Zheng","Xiechi Zhang","Gerard de Melo","Xiaoling Wang","Linlin Wang"],"pdf_url":"https://arxiv.org/pdf/2501.06741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06736v1","updated":"2025-01-12T07:15:55Z","published":"2025-01-12T07:15:55Z","title":"ZOQO: Zero-Order Quantized Optimization","summary":"  The increasing computational and memory demands in deep learning present\nsignificant challenges, especially in resource-constrained environments. We\nintroduce a zero-order quantized optimization (ZOQO) method designed for\ntraining models with quantized parameters and operations. Our approach\nleverages zero-order approximations of the gradient sign and adapts the\nlearning process to maintain the parameters' quantization without the need for\nfull-precision gradient calculations. We demonstrate the effectiveness of ZOQO\nthrough experiments in fine-tuning of large language models and black-box\nadversarial attacks. Despite the limitations of zero-order and quantized\noperations training, our method achieves competitive performance compared to\nfull-precision methods, highlighting its potential for low-resource\nenvironments.\n","authors":["Noga Bar","Raja Giryes"],"pdf_url":"https://arxiv.org/pdf/2501.06736v1.pdf","comment":"Accepted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.06730v1","updated":"2025-01-12T06:57:06Z","published":"2025-01-12T06:57:06Z","title":"Better Prompt Compression Without Multi-Layer Perceptrons","summary":"  Prompt compression is a promising approach to speeding up language model\ninference without altering the generative model. Prior works compress prompts\ninto smaller sequences of learned tokens using an encoder that is trained as a\nLowRank Adaptation (LoRA) of the inference language model. However, we show\nthat the encoder does not need to keep the original language model's\narchitecture to achieve useful compression. We introduce the Attention-Only\nCompressor (AOC), which learns a prompt compression encoder after removing the\nmultilayer perceptron (MLP) layers in the Transformer blocks of a language\nmodel, resulting in an encoder with roughly 67% less parameters compared to the\noriginal model. Intriguingly we find that, across a range of compression ratios\nup to 480x, AOC can better regenerate prompts and outperform a baseline\ncompression encoder that is a LoRA of the inference language model without\nremoving MLP layers. These results demonstrate that the architecture of prompt\ncompression encoders does not need to be identical to that of the original\ndecoder language model, paving the way for further research into architectures\nand approaches for prompt compression.\n","authors":["Edouardo Honig","Andrew Lizarraga","Zijun Frank Zhang","Ying Nian Wu"],"pdf_url":"https://arxiv.org/pdf/2501.06730v1.pdf","comment":"7 pages, 0 figures"},{"id":"http://arxiv.org/abs/2403.10799v5","updated":"2025-01-12T06:47:39Z","published":"2024-03-16T04:12:50Z","title":"Toward Adaptive Large Language Models Structured Pruning via\n  Hybrid-grained Weight Importance Assessment","summary":"  Structured pruning for large language models (LLMs) has garnered significant\nacademic interest due to its ability to efficiently compress and accelerate\nLLMs by eliminating redundant weight groups at a coarse-grained granularity.\nCurrent structured pruning methods for LLMs typically depend on a singular\ngranularity for assessing weight importance, resulting in notable performance\ndegradation in downstream tasks. Intriguingly, our empirical investigations\nreveal that utilizing unstructured pruning, which achieves better performance\nretention by pruning weights at a finer granularity, \\emph{i.e.}, individual\nweights, yields significantly varied sparse LLM structures when juxtaposed to\nstructured pruning. This suggests that evaluating both holistic and individual\nassessment for weight importance is essential for LLM pruning. Building on this\ninsight, we introduce the Hybrid-grained Weight Importance Assessment (HyWIA),\na novel method that merges fine-grained and coarse-grained evaluations of\nweight importance for the pruning of LLMs. Leveraging an attention mechanism,\nHyWIA adaptively determines the optimal blend of granularity in weight\nimportance assessments in an end-to-end pruning manner. Extensive experiments\non LLaMA-V1/V2, Vicuna, Baichuan, and Bloom across various benchmarks\ndemonstrate the effectiveness of HyWIA in pruning LLMs. For example, HyWIA\nsurpasses the cutting-edge LLM-Pruner by an average margin of 2.82% in accuracy\nacross seven downstream tasks when pruning LLaMA-7B by 50%.\n","authors":["Jun Liu","Zhenglun Kong","Pu Zhao","Changdi Yang","Hao Tang","Xuan Shen","Geng Yuan","Wei Niu","Wenbin Zhang","Xue Lin","Dong Huang","Yanzhi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.10799v5.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2501.06728v1","updated":"2025-01-12T06:41:52Z","published":"2025-01-12T06:41:52Z","title":"Measuring the Robustness of Reference-Free Dialogue Evaluation Systems","summary":"  Advancements in dialogue systems powered by large language models (LLMs) have\noutpaced the development of reliable evaluation metrics, particularly for\ndiverse and creative responses. We present a benchmark for evaluating the\nrobustness of reference-free dialogue metrics against four categories of\nadversarial attacks: speaker tag prefixes, static responses, ungrammatical\nresponses, and repeated conversational context. We analyze metrics such as\nDialogRPT, UniEval, and PromptEval -- a prompt-based method leveraging LLMs --\nacross grounded and ungrounded datasets. By examining both their correlation\nwith human judgment and susceptibility to adversarial attacks, we find that\nthese two axes are not always aligned; metrics that appear to be equivalent\nwhen judged by traditional benchmarks may, in fact, vary in their scores of\nadversarial responses. These findings motivate the development of nuanced\nevaluation frameworks to address real-world dialogue challenges.\n","authors":["Justin Vasselli","Adam Nohejl","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2501.06728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07413v2","updated":"2025-01-12T06:07:15Z","published":"2024-08-14T09:43:32Z","title":"Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge\n  Editing for Large Language Models","summary":"  Knowledge editing aims to update outdated or incorrect knowledge in large\nlanguage models (LLMs). However, current knowledge editing methods have limited\nscalability for lifelong editing. This study explores the fundamental reason\nwhy knowledge editing fails in lifelong editing. We begin with the closed-form\nsolution derived from linear associative memory, which underpins\nstate-of-the-art knowledge editing methods. We extend the solution from single\nediting to lifelong editing, and through rigorous mathematical derivation,\nidentify an interference term in the final solution, suggesting that editing\nknowledge may impact irrelevant knowledge. Further analysis of the interference\nterm reveals a close relationship with superposition between knowledge\nrepresentations. When knowledge superposition does not exist in language\nmodels, the interference term vanishes, allowing for lossless knowledge\nediting. Experiments across numerous language models reveal that knowledge\nsuperposition is universal, exhibiting high kurtosis, zero mean, and\nheavy-tailed distributions with clear scaling laws. Ultimately, by combining\ntheory and experiments, we demonstrate that knowledge superposition is the\nfundamental reason for the failure of lifelong editing. Moreover, this is the\nfirst study to investigate knowledge editing from the perspective of\nsuperposition and provides a comprehensive observation of superposition across\nnumerous real-world language models. Code available at\nhttps://github.com/ChenhuiHu/knowledge_in_superposition.\n","authors":["Chenhui Hu","Pengfei Cao","Yubo Chen","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.07413v2.pdf","comment":"To be published in AAAI 2025"},{"id":"http://arxiv.org/abs/2410.04265v2","updated":"2025-01-12T05:32:57Z","published":"2024-10-05T18:55:01Z","title":"AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language\n  Models via Systematic Attribution of Machine Text against Web Text","summary":"  Creativity has long been considered one of the most difficult aspect of human\nintelligence for AI to mimic. However, the rise of Large Language Models\n(LLMs), like ChatGPT, has raised questions about whether AI can match or even\nsurpass human creativity. We present CREATIVITY INDEX as the first step to\nquantify the linguistic creativity of a text by reconstructing it from existing\ntext snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that\nthe seemingly remarkable creativity of LLMs may be attributable in large part\nto the creativity of human-written texts on the web. To compute CREATIVITY\nINDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming\nalgorithm that can search verbatim and near-verbatim matches of text snippets\nfrom a given document against the web. Experiments reveal that the CREATIVITY\nINDEX of professional human authors is on average 66.2% higher than that of\nLLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of\n30.1%. In addition, we find that distinguished authors like Hemingway exhibit\nmeasurably higher CREATIVITY INDEX compared to other human writers. Finally, we\ndemonstrate that CREATIVITY INDEX can be used as a surprisingly effective\ncriterion for zero-shot machine text detection, surpassing the strongest\nexisting zero-shot system, DetectGPT, by a significant margin of 30.2%, and\neven outperforming the strongest supervised system, GhostBuster, in five out of\nsix domains.\n","authors":["Ximing Lu","Melanie Sclar","Skyler Hallinan","Niloofar Mireshghallah","Jiacheng Liu","Seungju Han","Allyson Ettinger","Liwei Jiang","Khyathi Chandu","Nouha Dziri","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.04265v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07093v3","updated":"2025-01-12T05:04:20Z","published":"2023-11-13T05:45:55Z","title":"On the Effectiveness of ASR Representations in Real-world Noisy Speech\n  Emotion Recognition","summary":"  This paper proposes an efficient attempt to noisy speech emotion recognition\n(NSER). Conventional NSER approaches have proven effective in mitigating the\nimpact of artificial noise sources, such as white Gaussian noise, but are\nlimited to non-stationary noises in real-world environments due to their\ncomplexity and uncertainty. To overcome this limitation, we introduce a new\nmethod for NSER by adopting the automatic speech recognition (ASR) model as a\nnoise-robust feature extractor to eliminate non-vocal information in noisy\nspeech. We first obtain intermediate layer information from the ASR model as a\nfeature representation for emotional speech and then apply this representation\nfor the downstream NSER task. Our experimental results show that 1) the\nproposed method achieves better NSER performance compared with the conventional\nnoise reduction method, 2) outperforms self-supervised learning approaches, and\n3) even outperforms text-based approaches using ASR transcription or the ground\ntruth transcription of noisy speech.\n","authors":["Xiaohan Shi","Jiajun He","Xingfeng Li","Tomoki Toda"],"pdf_url":"https://arxiv.org/pdf/2311.07093v3.pdf","comment":"Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing"},{"id":"http://arxiv.org/abs/2501.06715v1","updated":"2025-01-12T04:49:06Z","published":"2025-01-12T04:49:06Z","title":"ZNO-Eval: Benchmarking reasoning capabilities of large language models\n  in Ukrainian","summary":"  As the usage of large language models for problems outside of simple text\nunderstanding or generation increases, assessing their abilities and\nlimitations becomes crucial. While significant progress has been made in this\narea over the last few years, most research has focused on benchmarking\nEnglish, leaving other languages underexplored. This makes evaluating the\nreasoning and robustness level of language models in Ukrainian particularly\nchallenging. The purpose of this work is to establish a comprehensive benchmark\nfor the reasoning capabilities evaluation of large language models in the\nUkrainian language. This paper presents the ZNO-Eval benchmark based on real\nexam tasks from Ukraine's standardized educational testing system: the External\nIndependent Evaluation and the National Multi-subject Test. With single-answer\noptions, multiple-choice, matching, and open-ended questions from diverse\nsubjects, including Ukrainian language, mathematics, history, and geography,\nthis dataset paves the way toward a thorough analysis of reasoning capabilities\nacross different domains and complexities. Evaluation of several well-known\nlanguage models, such as GPT-3.5-Turbo, GPT-4o, GPT-4-Turbo, Mistral Large,\nClaude 3 Opus, and Gemini-1.5 Pro on this benchmark demonstrated the\nsuperiority of GPT-4o in both common knowledge reasoning and intricate language\ntasks. At the same time, Gemini Pro and GPT-4 Turbo excelled in the arithmetic\ndomain, leading in single-answer and open-ended math problems. While all models\nwere close to max performance in text-only common knowledge tasks like history\nand geography, there still is a gap for Ukrainian language and math, thus\nhighlighting the importance of developing specialized language benchmarks for\nmore accurate assessments of model capabilities and limitations across\ndifferent languages and contexts.\n","authors":["Mykyta Syromiatnikov","Victoria Ruvinskaya","Anastasiya Troynina"],"pdf_url":"https://arxiv.org/pdf/2501.06715v1.pdf","comment":"7 pages, 5 figures. X International conference \"Informatics. Culture.\n  Technology.\" (2024)"},{"id":"http://arxiv.org/abs/2501.06704v1","updated":"2025-01-12T04:10:56Z","published":"2025-01-12T04:10:56Z","title":"Fine-tuning ChatGPT for Automatic Scoring of Written Scientific\n  Explanations in Chinese","summary":"  The development of explanations for scientific phenomena is essential in\nscience assessment, but scoring student-written explanations remains\nchallenging and resource-intensive. Large language models (LLMs) have shown\npromise in addressing this issue, particularly in alphabetic languages like\nEnglish. However, their applicability to logographic languages is less\nexplored. This study investigates the potential of fine-tuning ChatGPT, a\nleading LLM, to automatically score scientific explanations written in Chinese.\nStudent responses to seven scientific explanation tasks were collected and\nautomatically scored, with scoring accuracy examined in relation to reasoning\ncomplexity using the Kendall correlation. A qualitative analysis explored how\nlinguistic features influenced scoring accuracy. The results show that\ndomain-specific adaptation enables ChatGPT to score Chinese scientific\nexplanations with accuracy. However, scoring accuracy correlates with reasoning\ncomplexity: a negative correlation for lower-level responses and a positive one\nfor higher-level responses. The model overrates complex reasoning in low-level\nresponses with intricate sentence structures and underrates high-level\nresponses using concise causal reasoning. These correlations stem from\nlinguistic features--simplicity and clarity enhance accuracy for lower-level\nresponses, while comprehensiveness improves accuracy for higher-level ones.\nSimpler, shorter responses tend to score more accurately at lower levels,\nwhereas longer, information-rich responses yield better accuracy at higher\nlevels. These findings demonstrate the effectiveness of LLMs in automatic\nscoring within a Chinese context and emphasize the importance of linguistic\nfeatures and reasoning complexity in fine-tuning scoring models for educational\nassessments.\n","authors":["Jie Yang","Ehsan Latif","Yuze He","Xiaoming Zhai"],"pdf_url":"https://arxiv.org/pdf/2501.06704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18367v4","updated":"2025-01-12T04:01:45Z","published":"2024-12-24T11:50:18Z","title":"Towards Global AI Inclusivity: A Large-Scale Multilingual Terminology\n  Dataset (GIST)","summary":"  The field of machine translation has achieved significant advancements, yet\ndomain-specific terminology translation, particularly in AI, remains\nchallenging. We introduce GIST, a large-scale multilingual AI terminology\ndataset containing 5K terms extracted from top AI conference papers spanning\n2000 to 2023. The terms are translated into Arabic, Chinese, French, Japanese,\nand Russian using a hybrid framework that combines LLMs for extraction with\nhuman expertise for translation. The dataset's quality is benchmarked against\nexisting resources, demonstrating superior translation accuracy through\ncrowdsourced evaluation. GIST is integrated into translation workflows using\npost-translation refinement methods that require no retraining, where LLM\nprompting consistently improves BLEU and COMET scores. A web demonstration on\nthe ACL Anthology platform highlights its practical application, showcasing\nimproved accessibility for non-English speakers. This work aims to address\ncritical gaps in AI terminology resources and fosters global inclusivity and\ncollaboration in AI research.\n","authors":["Jiarui Liu","Iman Ouzzani","Wenkai Li","Lechen Zhang","Tianyue Ou","Houda Bouamor","Zhijing Jin","Mona Diab"],"pdf_url":"https://arxiv.org/pdf/2412.18367v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17596v2","updated":"2025-01-12T03:52:16Z","published":"2024-12-23T14:13:44Z","title":"LiveIdeaBench: Evaluating LLMs' Scientific Creativity and Idea\n  Generation with Minimal Context","summary":"  While Large Language Models (LLMs) have demonstrated remarkable capabilities\nin scientific tasks, existing evaluation frameworks primarily assess their\nperformance using rich contextual inputs, overlooking their ability to generate\nnovel ideas from minimal information. We introduce LiveIdeaBench, a\ncomprehensive benchmark that evaluates LLMs' scientific creativity and\ndivergent thinking capabilities using single-keyword prompts. Drawing from\nGuilford's creativity theory, our framework employs a dynamic panel of\nstate-of-the-art LLMs to assess generated ideas across four key dimensions:\noriginality, feasibility, fluency, and flexibility. Through extensive\nexperimentation with 20 leading models across 1,180 keywords spanning 18\nscientific domains, we reveal that scientific creative ability shows distinct\npatterns from general intelligence metrics. Notably, our results demonstrate\nthat models like QwQ-32B-preview achieve comparable creative performance to\ntop-tier models like o1-preview, despite significant gaps in their general\nintelligence scores. These findings highlight the importance of specialized\nevaluation frameworks for scientific creativity and suggest that the\ndevelopment of creative capabilities in LLMs may follow different trajectories\nthan traditional problem-solving abilities.\n","authors":["Kai Ruan","Xuan Wang","Jixiang Hong","Peng Wang","Yang Liu","Hao Sun"],"pdf_url":"https://arxiv.org/pdf/2412.17596v2.pdf","comment":"Updated author list, Corrected some issues and ref"},{"id":"http://arxiv.org/abs/2501.06689v1","updated":"2025-01-12T02:43:59Z","published":"2025-01-12T02:43:59Z","title":"TAPO: Task-Referenced Adaptation for Prompt Optimization","summary":"  Prompt engineering can significantly improve the performance of large\nlanguage models (LLMs), with automated prompt optimization (APO) gaining\nsignificant attention due to the time-consuming and laborious nature of manual\nprompt design. However, much of the existing work in APO overlooks\ntask-specific characteristics, resulting in prompts that lack domain\nspecificity and are not well-suited for task-specific optimization. In this\npaper, we introduce TAPO, a multitask-aware prompt optimization framework\ncomposed of three key modules. First, a task-aware metric selection module is\nproposed to enhance task-specific prompt generation capabilities. Second, we\npresent a multi-metrics evaluation module to jointly evaluate prompts from\nmultiple perspectives. Third, an evolution-based optimization framework is\nintroduced for automatic prompt refinement, which improves adaptability across\nvarious tasks. Extensive experiments on six datasets demonstrate the\neffectiveness of our approach, and our code is publicly available.\n","authors":["Wenxin Luo","Weirui Wang","Xiaopeng Li","Weibo Zhou","Pengyue Jia","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.06689v1.pdf","comment":"Accepted to ICASSP 2025"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2501.03397v3","updated":"2025-01-12T23:38:16Z","published":"2025-01-06T21:34:52Z","title":"DoubleDiffusion: Combining Heat Diffusion with Denoising Diffusion for\n  Generative Learning on 3D Meshes","summary":"  This paper proposes DoubleDiffusion, a novel framework that combines heat\ndissipation diffusion and denoising diffusion for direct generative learning on\n3D mesh surfaces. Our approach addresses the challenges of generating\ncontinuous signal distributions residing on a curve manifold surface. Unlike\nprevious methods that rely on unrolling 3D meshes into 2D or adopting field\nrepresentations, DoubleDiffusion leverages the Laplacian-Beltrami operator to\nprocess features respecting the mesh structure. This combination enables\neffective geometry-aware signal diffusion across the underlying geometry. As\nshown in Fig.1, we demonstrate that DoubleDiffusion has the ability to generate\nRGB signal distributions on complex 3D mesh surfaces and achieves per-category\nshape-conditioned texture generation across different shape geometry. Our work\ncontributes a new direction in diffusion-based generative modeling on 3D\nsurfaces, with potential applications in the field of 3D asset generation.\n","authors":["Xuyang Wang","Ziang Cheng","Zhenyu Li","Jiayu Yang","Haorui Ji","Pan Ji","Mehrtash Harandi","Richard Hartley","Hongdong Li"],"pdf_url":"https://arxiv.org/pdf/2501.03397v3.pdf","comment":"Codes: https://github.com/Wxyxixixi/DoubleDiffusion_3D_Mesh"},{"id":"http://arxiv.org/abs/2403.15442v3","updated":"2025-01-12T22:16:50Z","published":"2024-03-17T11:28:23Z","title":"Artificial Intelligence for Cochlear Implants: Review of Strategies,\n  Challenges, and Perspectives","summary":"  Automatic speech recognition (ASR) plays a pivotal role in our daily lives,\noffering utility not only for interacting with machines but also for\nfacilitating communication for individuals with partial or profound hearing\nimpairments. The process involves receiving the speech signal in analog form,\nfollowed by various signal processing algorithms to make it compatible with\ndevices of limited capacities, such as cochlear implants (CIs). Unfortunately,\nthese implants, equipped with a finite number of electrodes, often result in\nspeech distortion during synthesis. Despite efforts by researchers to enhance\nreceived speech quality using various state-of-the-art (SOTA) signal processing\ntechniques, challenges persist, especially in scenarios involving multiple\nsources of speech, environmental noise, and other adverse conditions. The\nadvent of new artificial intelligence (AI) methods has ushered in cutting-edge\nstrategies to address the limitations and difficulties associated with\ntraditional signal processing techniques dedicated to CIs. This review aims to\ncomprehensively cover advancements in CI-based ASR and speech enhancement,\namong other related aspects. The primary objective is to provide a thorough\noverview of metrics and datasets, exploring the capabilities of AI algorithms\nin this biomedical field, and summarizing and commenting on the best results\nobtained. Additionally, the review will delve into potential applications and\nsuggest future directions to bridge existing research gaps in this domain.\n","authors":["Billel Essaid","Hamza Kheddar","Noureddine Batel","Muhammad E. H. Chowdhury","Abderrahmane Lakas"],"pdf_url":"https://arxiv.org/pdf/2403.15442v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06942v1","updated":"2025-01-12T21:39:06Z","published":"2025-01-12T21:39:06Z","title":"Comparison of Autoencoders for tokenization of ASL datasets","summary":"  Generative AI, powered by large language models (LLMs), has revolutionized\napplications across text, audio, images, and video. This study focuses on\ndeveloping and evaluating encoder-decoder architectures for the American Sign\nLanguage (ASL) image dataset, consisting of 87,000 images across 29 hand sign\nclasses. Three approaches were compared: Feedforward Autoencoders,\nConvolutional Autoencoders, and Diffusion Autoencoders. The Diffusion\nAutoencoder outperformed the others, achieving the lowest mean squared error\n(MSE) and highest Mean Opinion Score (MOS) due to its probabilistic noise\nmodeling and iterative denoising capabilities. The Convolutional Autoencoder\ndemonstrated effective spatial feature extraction but lacked the robustness of\nthe diffusion process, while the Feedforward Autoencoder served as a baseline\nwith limitations in handling complex image data. Objective and subjective\nevaluations confirmed the superiority of the Diffusion Autoencoder for\nhigh-fidelity image reconstruction, emphasizing its potential in multimodal AI\napplications such as sign language recognition and generation. This work\nprovides critical insights into designing robust encoder-decoder systems to\nadvance multimodal AI capabilities.\n","authors":["Vouk Praun-Petrovic","Aadhvika Koundinya","Lavanya Prahallad"],"pdf_url":"https://arxiv.org/pdf/2501.06942v1.pdf","comment":"9 pages, 2 tables, 4 figures"},{"id":"http://arxiv.org/abs/2501.06939v1","updated":"2025-01-12T21:33:06Z","published":"2025-01-12T21:33:06Z","title":"Super-Resolution of 3D Micro-CT Images Using Generative Adversarial\n  Networks: Enhancing Resolution and Segmentation Accuracy","summary":"  We develop a procedure for substantially improving the quality of segmented\n3D micro-Computed Tomography (micro-CT) images of rocks with a Machine Learning\n(ML) Generative Model. The proposed model enhances the resolution eightfold\n(8x) and addresses segmentation inaccuracies due to the overlapping X-ray\nattenuation in micro-CT measurement for different rock minerals and phases. The\nproposed generative model is a 3D Deep Convolutional Wasserstein Generative\nAdversarial Network with Gradient Penalty (3D DC WGAN-GP). The algorithm is\ntrained on segmented 3D low-resolution micro-CT images and segmented unpaired\ncomplementary 2D high-resolution Laser Scanning Microscope (LSM) images. The\nalgorithm was demonstrated on multiple samples of Berea sandstones. We achieved\nhigh-quality super-resolved 3D images with a resolution of 0.4375 micro-m/voxel\nand accurate segmentation for constituting minerals and pore space. The\ndescribed procedure can significantly expand the modern capabilities of digital\nrock physics.\n","authors":["Evgeny Ugolkov","Xupeng He","Hyung Kwak","Hussein Hoteit"],"pdf_url":"https://arxiv.org/pdf/2501.06939v1.pdf","comment":"24 pages, 9 figures"},{"id":"http://arxiv.org/abs/2501.06938v1","updated":"2025-01-12T21:30:44Z","published":"2025-01-12T21:30:44Z","title":"Evaluating unsupervised contrastive learning framework for MRI sequences\n  classification","summary":"  The automatic identification of Magnetic Resonance Imaging (MRI) sequences\ncan streamline clinical workflows by reducing the time radiologists spend\nmanually sorting and identifying sequences, thereby enabling faster diagnosis\nand treatment planning for patients. However, the lack of standardization in\nthe parameters of MRI scans poses challenges for automated systems and\ncomplicates the generation and utilization of datasets for machine learning\nresearch. To address this issue, we propose a system for MRI sequence\nidentification using an unsupervised contrastive deep learning framework. By\ntraining a convolutional neural network based on the ResNet-18 architecture,\nour system classifies nine common MRI sequence types as a 9-class\nclassification problem. The network was trained using an in-house internal\ndataset and validated on several public datasets, including BraTS, ADNI, Fused\nRadiology-Pathology Prostate Dataset, the Breast Cancer Dataset (ACRIN), among\nothers, encompassing diverse acquisition protocols and requiring only 2D slices\nfor training. Our system achieves a classification accuracy of over 0.95 across\nthe nine most common MRI sequence types.\n","authors":["Yuli Wang","Kritika Iyer","Sep Farhand","Yoshihisa Shinagawa"],"pdf_url":"https://arxiv.org/pdf/2501.06938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06927v1","updated":"2025-01-12T20:36:39Z","published":"2025-01-12T20:36:39Z","title":"CULTURE3D: Cultural Landmarks and Terrain Dataset for 3D Applications","summary":"  In this paper, we present a large-scale fine-grained dataset using\nhigh-resolution images captured from locations worldwide. Compared to existing\ndatasets, our dataset offers a significantly larger size and includes a higher\nlevel of detail, making it uniquely suited for fine-grained 3D applications.\nNotably, our dataset is built using drone-captured aerial imagery, which\nprovides a more accurate perspective for capturing real-world site layouts and\narchitectural structures. By reconstructing environments with these detailed\nimages, our dataset supports applications such as the COLMAP format for\nGaussian Splatting and the Structure-from-Motion (SfM) method. It is compatible\nwith widely-used techniques including SLAM, Multi-View Stereo, and Neural\nRadiance Fields (NeRF), enabling accurate 3D reconstructions and point clouds.\nThis makes it a benchmark for reconstruction and segmentation tasks. The\ndataset enables seamless integration with multi-modal data, supporting a range\nof 3D applications, from architectural reconstruction to virtual tourism. Its\nflexibility promotes innovation, facilitating breakthroughs in 3D modeling and\nanalysis.\n","authors":["Xinyi Zheng","Steve Zhang","Weizhe Lin","Aaron Zhang","Walterio W. Mayol-Cuevas","Junxiao Shen"],"pdf_url":"https://arxiv.org/pdf/2501.06927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06922v1","updated":"2025-01-12T20:17:46Z","published":"2025-01-12T20:17:46Z","title":"Benchmarking YOLOv8 for Optimal Crack Detection in Civil Infrastructure","summary":"  Ensuring the structural integrity and safety of bridges is crucial for the\nreliability of transportation networks and public safety. Traditional crack\ndetection methods are increasingly being supplemented or replaced by advanced\nartificial intelligence (AI) techniques. However, most of the models rely on\ntwo-stage target detection algorithms, which pose concerns for real-time\napplications due to their lower speed. While models such as YOLO (You Only Look\nOnce) have emerged as transformative tools due to their remarkable speed and\naccuracy. However, the potential of the latest YOLOv8 framework in this domain\nremains underexplored. This study bridges that gap by rigorously evaluating\nYOLOv8's performance across five model scales (nano, small, medium, large, and\nextra-large) using a high-quality Roboflow dataset. A comprehensive\nhyperparameter optimization was performed, testing six state-of-the-art\noptimizers-Stochastic Gradient Descent, Adaptive Moment Estimation, Adam with\nDecoupled Weight Decay, Root Mean Square Propagation, Rectified Adam, and\nNesterov-accelerated Adam. Results revealed that YOLOv8, optimized with\nStochastic Gradient Descent, delivered exceptional accuracy and speed, setting\na new benchmark for real-time crack detection. Beyond its immediate\napplication, this research positions YOLOv8 as a foundational approach for\nintegrating advanced computer vision techniques into infrastructure monitoring.\nBy enabling more reliable and proactive maintenance of aging bridge networks,\nthis work paves the way for safer, more efficient transportation systems\nworldwide.\n","authors":["Woubishet Zewdu Taffese","Ritesh Sharma","Mohammad Hossein Afsharmovahed","Gunasekaran Manogaran","Genda Chen"],"pdf_url":"https://arxiv.org/pdf/2501.06922v1.pdf","comment":"Accepted at 104th TRB Annual Meeting 2025"},{"id":"http://arxiv.org/abs/2501.06918v1","updated":"2025-01-12T20:01:07Z","published":"2025-01-12T20:01:07Z","title":"Driver Age and Its Effect on Key Driving Metrics: Insights from Dynamic\n  Vehicle Data","summary":"  By 2030, the senior population aged 65 and older is expected to increase by\nover 50%, significantly raising the number of older drivers on the road.\nDrivers over 70 face higher crash death rates compared to those in their\nforties and fifties, underscoring the importance of developing more effective\nsafety interventions for this demographic. Although the impact of aging on\ndriving behavior has been studied, there is limited research on how these\nbehaviors translate into real-world driving scenarios. This study addresses\nthis need by leveraging Naturalistic Driving Data (NDD) to analyze driving\nperformance measures - specifically, speed limit adherence on interstates and\ndeceleration at stop intersections, both of which may be influenced by\nage-related declines. Using NDD, we developed Cumulative Distribution Functions\n(CDFs) to establish benchmarks for key driving behaviors among senior and young\ndrivers. Our analysis, which included anomaly detection, benchmark comparisons,\nand accuracy evaluations, revealed significant differences in driving patterns\nprimarily related to speed limit adherence at 75mph. While our approach shows\npromising potential for enhancing Advanced Driver Assistance Systems (ADAS) by\nproviding tailored interventions based on age-specific adherence to speed limit\ndriving patterns, we recognize the need for additional data to refine and\nvalidate metrics for other driving behaviors. By establishing precise\nbenchmarks for various driving performance metrics, ADAS can effectively\nidentify anomalies, such as abrupt deceleration, which may indicate impaired\ndriving or other safety concerns. This study lays a strong foundation for\nfuture research aimed at improving safety interventions through detailed\ndriving behavior analysis.\n","authors":["Aparna Joshi","Kojo Adugyamfi","Jennifer Merickel","Pujitha Gunaratne","Anuj Sharma"],"pdf_url":"https://arxiv.org/pdf/2501.06918v1.pdf","comment":"21 pages, 9 figures, 4 Tables, 104th TRB Annual Meeting 2025,\n  Washington DC"},{"id":"http://arxiv.org/abs/2501.06909v1","updated":"2025-01-12T19:45:42Z","published":"2025-01-12T19:45:42Z","title":"Local Foreground Selection aware Attentive Feature Reconstruction for\n  few-shot fine-grained plant species classification","summary":"  Plant species exhibit significant intra-class variation and minimal\ninter-class variation. To enhance classification accuracy, it is essential to\nreduce intra-class variation while maximizing inter-class variation. This paper\naddresses plant species classification using a limited number of labelled\nsamples and introduces a novel Local Foreground Selection(LFS) attention\nmechanism. LFS is a straightforward module designed to generate discriminative\nsupport and query feature maps. It operates by integrating two types of\nattention: local attention, which captures local spatial details to enhance\nfeature discrimination and increase inter-class differentiation, and foreground\nselection attention, which emphasizes the foreground plant object while\nmitigating background interference. By focusing on the foreground, the query\nand support features selectively highlight relevant feature sequences and\ndisregard less significant background sequences, thereby reducing intra-class\ndifferences. Experimental results from three plant species datasets demonstrate\nthe effectiveness of the proposed LFS attention mechanism and its complementary\nadvantages over previous feature reconstruction methods.\n","authors":["Aisha Zulfiqar","Ebroul Izquiedro"],"pdf_url":"https://arxiv.org/pdf/2501.06909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06903v1","updated":"2025-01-12T19:01:05Z","published":"2025-01-12T19:01:05Z","title":"Synthetic Prior for Few-Shot Drivable Head Avatar Inversion","summary":"  We present SynShot, a novel method for the few-shot inversion of a drivable\nhead avatar based on a synthetic prior. We tackle two major challenges. First,\ntraining a controllable 3D generative network requires a large number of\ndiverse sequences, for which pairs of images and high-quality tracked meshes\nare not always available. Second, state-of-the-art monocular avatar models\nstruggle to generalize to new views and expressions, lacking a strong prior and\noften overfitting to a specific viewpoint distribution. Inspired by machine\nlearning models trained solely on synthetic data, we propose a method that\nlearns a prior model from a large dataset of synthetic heads with diverse\nidentities, expressions, and viewpoints. With few input images, SynShot\nfine-tunes the pretrained synthetic prior to bridge the domain gap, modeling a\nphotorealistic head avatar that generalizes to novel expressions and\nviewpoints. We model the head avatar using 3D Gaussian splatting and a\nconvolutional encoder-decoder that outputs Gaussian parameters in UV texture\nspace. To account for the different modeling complexities over parts of the\nhead (e.g., skin vs hair), we embed the prior with explicit control for\nupsampling the number of per-part primitives. Compared to state-of-the-art\nmonocular methods that require thousands of real training images, SynShot\nsignificantly improves novel view and expression synthesis.\n","authors":["Wojciech Zielonka","Stephan J. Garbin","Alexandros Lattas","George Kopanas","Paulo Gotardo","Thabo Beeler","Justus Thies","Timo Bolkart"],"pdf_url":"https://arxiv.org/pdf/2501.06903v1.pdf","comment":"Website https://zielon.github.io/synshot/"},{"id":"http://arxiv.org/abs/2501.06897v1","updated":"2025-01-12T18:38:51Z","published":"2025-01-12T18:38:51Z","title":"ActiveGAMER: Active GAussian Mapping through Efficient Rendering","summary":"  We introduce ActiveGAMER, an active mapping system that utilizes 3D Gaussian\nSplatting (3DGS) to achieve high-quality, real-time scene mapping and\nexploration. Unlike traditional NeRF-based methods, which are computationally\ndemanding and restrict active mapping performance, our approach leverages the\nefficient rendering capabilities of 3DGS, allowing effective and efficient\nexploration in complex environments. The core of our system is a\nrendering-based information gain module that dynamically identifies the most\ninformative viewpoints for next-best-view planning, enhancing both geometric\nand photometric reconstruction accuracy. ActiveGAMER also integrates a\ncarefully balanced framework, combining coarse-to-fine exploration,\npost-refinement, and a global-local keyframe selection strategy to maximize\nreconstruction completeness and fidelity. Our system autonomously explores and\nreconstructs environments with state-of-the-art geometric and photometric\naccuracy and completeness, significantly surpassing existing approaches in both\naspects. Extensive evaluations on benchmark datasets such as Replica and MP3D\nhighlight ActiveGAMER's effectiveness in active mapping tasks.\n","authors":["Liyan Chen","Huangying Zhan","Kevin Chen","Xiangyu Xu","Qingan Yan","Changjiang Cai","Yi Xu"],"pdf_url":"https://arxiv.org/pdf/2501.06897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06887v1","updated":"2025-01-12T17:50:47Z","published":"2025-01-12T17:50:47Z","title":"MedGrad E-CLIP: Enhancing Trust and Transparency in AI-Driven Skin\n  Lesion Diagnosis","summary":"  As deep learning models gain attraction in medical data, ensuring transparent\nand trustworthy decision-making is essential. In skin cancer diagnosis, while\nadvancements in lesion detection and classification have improved accuracy, the\nblack-box nature of these methods poses challenges in understanding their\ndecision processes, leading to trust issues among physicians. This study\nleverages the CLIP (Contrastive Language-Image Pretraining) model, trained on\ndifferent skin lesion datasets, to capture meaningful relationships between\nvisual features and diagnostic criteria terms. To further enhance transparency,\nwe propose a method called MedGrad E-CLIP, which builds on gradient-based\nE-CLIP by incorporating a weighted entropy mechanism designed for complex\nmedical imaging like skin lesions. This approach highlights critical image\nregions linked to specific diagnostic descriptions. The developed integrated\npipeline not only classifies skin lesions by matching corresponding\ndescriptions but also adds an essential layer of explainability developed\nespecially for medical data. By visually explaining how different features in\nan image relates to diagnostic criteria, this approach demonstrates the\npotential of advanced vision-language models in medical image analysis,\nultimately improving transparency, robustness, and trust in AI-driven\ndiagnostic systems.\n","authors":["Sadia Kamal","Tim Oates"],"pdf_url":"https://arxiv.org/pdf/2501.06887v1.pdf","comment":"Accepted to 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision Workshops (WACVW)"},{"id":"http://arxiv.org/abs/2501.06884v1","updated":"2025-01-12T17:41:23Z","published":"2025-01-12T17:41:23Z","title":"Transforming Vision Transformer: Towards Efficient Multi-Task\n  Asynchronous Learning","summary":"  Multi-Task Learning (MTL) for Vision Transformer aims at enhancing the model\ncapability by tackling multiple tasks simultaneously. Most recent works have\npredominantly focused on designing Mixture-of-Experts (MoE) structures and in\ntegrating Low-Rank Adaptation (LoRA) to efficiently perform multi-task\nlearning. However, their rigid combination hampers both the optimization of MoE\nand the ef fectiveness of reparameterization of LoRA, leading to sub-optimal\nperformance and low inference speed. In this work, we propose a novel approach\ndubbed Efficient Multi-Task Learning (EMTAL) by transforming a pre-trained\nVision Transformer into an efficient multi-task learner during training, and\nreparameterizing the learned structure for efficient inference. Specifically,\nwe firstly develop the MoEfied LoRA structure, which decomposes the pre-trained\nTransformer into a low-rank MoE structure and employ LoRA to fine-tune the\nparameters. Subsequently, we take into account the intrinsic asynchronous\nnature of multi-task learning and devise a learning Quality Retaining (QR)\noptimization mechanism, by leveraging the historical high-quality class logits\nto prevent a well-trained task from performance degradation. Finally, we design\na router fading strategy to integrate the learned parameters into the original\nTransformer, archiving efficient inference. Extensive experiments on public\nbenchmarks demonstrate the superiority of our method, compared to the\nstate-of-the-art multi-task learning approaches.\n","authors":["Hanwen Zhong","Jiaxin Chen","Yutong Zhang","Di Huang","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2501.06884v1.pdf","comment":"Accepted by the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2501.06880v1","updated":"2025-01-12T17:28:09Z","published":"2025-01-12T17:28:09Z","title":"Real-Time Neural-Enhancement for Online Cloud Gaming","summary":"  Online Cloud gaming demands real-time, high-quality video transmission across\nvariable wide-area networks (WANs). Neural-enhanced video transmission\nalgorithms employing super-resolution (SR) for video quality enhancement have\neffectively challenged WAN environments. However, these SR-based methods\nrequire intensive fine-tuning for the whole video, making it infeasible in\ndiverse online cloud gaming. To address this, we introduce River, a cloud\ngaming delivery framework designed based on the observation that video segment\nfeatures in cloud gaming are typically repetitive and redundant. This permits a\nsignificant opportunity to reuse fine-tuned SR models, reducing the fine-tuning\nlatency of minutes to query latency of milliseconds. To enable the idea, we\ndesign a practical system that addresses several challenges, such as model\norganization, online model scheduler, and transfer strategy. River first builds\na content-aware encoder that fine-tunes SR models for diverse video segments\nand stores them in a lookup table. When delivering cloud gaming video streams\nonline, River checks the video features and retrieves the most relevant SR\nmodels to enhance the frame quality. Meanwhile, if no existing SR model\nperforms well enough for some video segments, River will further fine-tune new\nmodels and update the lookup table. Finally, to avoid the overhead of streaming\nmodel weight to the clients, River designs a prefetching strategy that predicts\nthe models with the highest possibility of being retrieved. Our evaluation\nbased on real video game streaming demonstrates River can reduce redundant\ntraining overhead by 44% and improve the Peak-Signal-to-Noise-Ratio by 1.81dB\ncompared to the SOTA solutions. Practical deployment shows River meets\nreal-time requirements, achieving approximately 720p 20fps on mobile devices.\n","authors":["Shan Jiang","Zhenhua Han","Haisheng Tan","Xinyang Jiang","Yifan Yang","Xiaoxi Zhang","Hongqiu Ni","Yuqing Yang","Xiang-Yang Li"],"pdf_url":"https://arxiv.org/pdf/2501.06880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06879v1","updated":"2025-01-12T17:26:24Z","published":"2025-01-12T17:26:24Z","title":"Defect Detection Network In PCB Circuit Devices Based on GAN Enhanced\n  YOLOv11","summary":"  This study proposes an advanced method for surface defect detection in\nprinted circuit boards (PCBs) using an improved YOLOv11 model enhanced with a\ngenerative adversarial network (GAN). The approach focuses on identifying six\ncommon defect types: missing hole, rat bite, open circuit, short circuit, burr,\nand virtual welding. By employing GAN to generate synthetic defect images, the\ndataset is augmented with diverse and realistic patterns, improving the model's\nability to generalize, particularly for complex and infrequent defects like\nburrs. The enhanced YOLOv11 model is evaluated on a PCB defect dataset,\ndemonstrating significant improvements in accuracy, recall, and robustness,\nespecially when dealing with defects in complex environments or small targets.\nThis research contributes to the broader field of electronic design automation\n(EDA), where efficient defect detection is a crucial step in ensuring\nhigh-quality PCB manufacturing. By integrating advanced deep learning\ntechniques, this approach enhances the automation and precision of defect\ndetection, reducing reliance on manual inspection and accelerating\ndesign-to-production workflows. The findings underscore the importance of\nincorporating GAN-based data augmentation and optimized detection architectures\nin EDA processes, providing valuable insights for improving reliability and\nefficiency in PCB defect detection within industrial applications.\n","authors":["Jiayi Huang","Feiyun Zhao","Lieyang Chen"],"pdf_url":"https://arxiv.org/pdf/2501.06879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06878v1","updated":"2025-01-12T17:24:51Z","published":"2025-01-12T17:24:51Z","title":"Uncertainty-Aware Online Extrinsic Calibration: A Conformal Prediction\n  Approach","summary":"  Accurate sensor calibration is crucial for autonomous systems, yet its\nuncertainty quantification remains underexplored. We present the first approach\nto integrate uncertainty awareness into online extrinsic calibration, combining\nMonte Carlo Dropout with Conformal Prediction to generate prediction intervals\nwith a guaranteed level of coverage. Our method proposes a framework to enhance\nexisting calibration models with uncertainty quantification, compatible with\nvarious network architectures. Validated on KITTI (RGB Camera-LiDAR) and DSEC\n(Event Camera-LiDAR) datasets, we demonstrate effectiveness across different\nvisual sensor types, measuring performance with adapted metrics to evaluate the\nefficiency and reliability of the intervals. By providing calibration\nparameters with quantifiable confidence measures, we offer insights into the\nreliability of calibration estimates, which can greatly improve the robustness\nof sensor fusion in dynamic environments and usefully serve the Computer Vision\ncommunity.\n","authors":["Mathieu Cocheteux","Julien Moreau","Franck Davoine"],"pdf_url":"https://arxiv.org/pdf/2501.06878v1.pdf","comment":"Accepted for publication at WACV 2025"},{"id":"http://arxiv.org/abs/2411.17922v3","updated":"2025-01-12T16:50:07Z","published":"2024-11-26T22:31:09Z","title":"Exploring Superpixel Segmentation Methods in the Context of Citizen\n  Science and Deforestation Detection","summary":"  Tropical forests play an essential role in the planet's ecosystem, making the\nconservation of these biomes a worldwide priority. However, ongoing\ndeforestation and degradation pose a significant threat to their existence,\nnecessitating effective monitoring and the proposal of actions to mitigate the\ndamage caused by these processes. In this regard, initiatives range from\ngovernment and private sector monitoring programs to solutions based on citizen\nscience campaigns, for example. Particularly in the context of citizen science\ncampaigns, the segmentation of remote sensing images to identify deforested\nareas and subsequently submit them to analysis by non-specialized volunteers is\nnecessary. Thus, segmentation using superpixel-based techniques proves to be a\nviable solution for this important task. Therefore, this paper presents an\nanalysis of 22 superpixel-based segmentation methods applied to remote sensing\nimages, aiming to identify which of them are more suitable for generating\nsegments for citizen science campaigns. The results reveal that seven of the\nsegmentation methods outperformed the baseline method (SLIC) currently employed\nin the ForestEyes citizen science project, indicating an opportunity for\nimprovement in this important stage of campaign development.\n","authors":["Hugo Resende","Isabela Borlido","Victor Sundermann","Eduardo B. Neto","Silvio Jamil F. Guimarães","Fabio Faria","Alvaro Luiz Fazenda"],"pdf_url":"https://arxiv.org/pdf/2411.17922v3.pdf","comment":"This paper is under review"},{"id":"http://arxiv.org/abs/2501.06869v1","updated":"2025-01-12T16:39:13Z","published":"2025-01-12T16:39:13Z","title":"A Foundational Generative Model for Breast Ultrasound Image Analysis","summary":"  Foundational models have emerged as powerful tools for addressing various\ntasks in clinical settings. However, their potential development to breast\nultrasound analysis remains untapped. In this paper, we present BUSGen, the\nfirst foundational generative model specifically designed for breast ultrasound\nimage analysis. Pretrained on over 3.5 million breast ultrasound images, BUSGen\nhas acquired extensive knowledge of breast structures, pathological features,\nand clinical variations. With few-shot adaptation, BUSGen can generate\nrepositories of realistic and informative task-specific data, facilitating the\ndevelopment of models for a wide range of downstream tasks. Extensive\nexperiments highlight BUSGen's exceptional adaptability, significantly\nexceeding real-data-trained foundational models in breast cancer screening,\ndiagnosis, and prognosis. In breast cancer early diagnosis, our approach\noutperformed all board-certified radiologists (n=9), achieving an average\nsensitivity improvement of 16.5% (P-value<0.0001). Additionally, we\ncharacterized the scaling effect of using generated data which was as effective\nas the collected real-world data for training diagnostic models. Moreover,\nextensive experiments demonstrated that our approach improved the\ngeneralization ability of downstream models. Importantly, BUSGen protected\npatient privacy by enabling fully de-identified data sharing, making progress\nforward in secure medical data utilization. An online demo of BUSGen is\navailable at https://aibus.bio.\n","authors":["Haojun Yu","Youcheng Li","Nan Zhang","Zihan Niu","Xuantong Gong","Yanwen Luo","Haotian Ye","Siyu He","Quanlin Wu","Wangyan Qin","Mengyuan Zhou","Jie Han","Jia Tao","Ziwei Zhao","Di Dai","Di He","Dong Wang","Binghui Tang","Ling Huo","James Zou","Qingli Zhu","Yong Wang","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2501.06869v1.pdf","comment":"Peking University; Stanford University; Peking University Cancer\n  Hospital & Institute; Peking Union Medical College Hospital; Cancer Hospital,\n  Chinese Academy of Medical Sciences"},{"id":"http://arxiv.org/abs/2501.06862v1","updated":"2025-01-12T16:22:17Z","published":"2025-01-12T16:22:17Z","title":"LarvSeg: Exploring Image Classification Data For Large Vocabulary\n  Semantic Segmentation via Category-wise Attentive Classifier","summary":"  Scaling up the vocabulary of semantic segmentation models is extremely\nchallenging because annotating large-scale mask labels is labour-intensive and\ntime-consuming. Recently, language-guided segmentation models have been\nproposed to address this challenge. However, their performance drops\nsignificantly when applied to out-of-distribution categories. In this paper, we\npropose a new large vocabulary semantic segmentation framework, called LarvSeg.\nDifferent from previous works, LarvSeg leverages image classification data to\nscale the vocabulary of semantic segmentation models as large-vocabulary\nclassification datasets usually contain balanced categories and are much easier\nto obtain. However, for classification tasks, the category is image-level,\nwhile for segmentation we need to predict the label at pixel level. To address\nthis issue, we first propose a general baseline framework to incorporate\nimage-level supervision into the training process of a pixel-level segmentation\nmodel, making the trained network perform semantic segmentation on newly\nintroduced categories in the classification data. We then observe that a model\ntrained on segmentation data can group pixel features of categories beyond the\ntraining vocabulary. Inspired by this finding, we design a category-wise\nattentive classifier to apply supervision to the precise regions of\ncorresponding categories to improve the model performance. Extensive\nexperiments demonstrate that LarvSeg significantly improves the large\nvocabulary semantic segmentation performance, especially in the categories\nwithout mask labels. For the first time, we provide a 21K-category semantic\nsegmentation model with the help of ImageNet21K. The code is available at\nhttps://github.com/HaojunYu1998/large_voc_seg.\n","authors":["Haojun Yu","Di Dai","Ziwei Zhao","Di He","Han Hu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2501.06862v1.pdf","comment":"PRCV 2024"},{"id":"http://arxiv.org/abs/2501.02198v2","updated":"2025-01-12T15:42:20Z","published":"2025-01-04T05:20:53Z","title":"Fresh-CL: Feature Realignment through Experts on Hypersphere in\n  Continual Learning","summary":"  Continual Learning enables models to learn and adapt to new tasks while\nretaining prior knowledge. Introducing new tasks, however, can naturally lead\nto feature entanglement across tasks, limiting the model's capability to\ndistinguish between new domain data. In this work, we propose a method called\nFeature Realignment through Experts on hyperSpHere in Continual Learning\n(Fresh-CL). By leveraging predefined and fixed simplex equiangular tight frame\n(ETF) classifiers on a hypersphere, our model improves feature separation both\nintra and inter tasks. However, the projection to a simplex ETF shifts with new\ntasks, disrupting structured feature representation of previous tasks and\ndegrading performance. Therefore, we propose a dynamic extension of ETF through\nmixture of experts, enabling adaptive projections onto diverse subspaces to\nenhance feature representation. Experiments on 11 datasets demonstrate a 2%\nimprovement in accuracy compared to the strongest baseline, particularly in\nfine-grained datasets, confirming the efficacy of combining ETF and MoE to\nimprove feature distinction in continual learning scenarios.\n","authors":["Zhongyi Zhou","Yaxin Peng","Pin Yi","Minjie Zhu","Chaomin Shen"],"pdf_url":"https://arxiv.org/pdf/2501.02198v2.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.06848v1","updated":"2025-01-12T15:34:24Z","published":"2025-01-12T15:34:24Z","title":"A General Framework for Inference-time Scaling and Steering of Diffusion\n  Models","summary":"  Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we propose Feynman Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models, even\nwith off-the-shelf rewards, can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering .\n","authors":["Raghav Singhal","Zachary Horvitz","Ryan Teehan","Mengye Ren","Zhou Yu","Kathleen McKeown","Rajesh Ranganath"],"pdf_url":"https://arxiv.org/pdf/2501.06848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04016v2","updated":"2025-01-12T15:24:23Z","published":"2024-07-04T15:46:01Z","title":"Mitigating Low-Frequency Bias: Feature Recalibration and Frequency\n  Attention Regularization for Adversarial Robustness","summary":"  Ensuring the robustness of deep neural networks against adversarial attacks\nremains a fundamental challenge in computer vision. While adversarial training\n(AT) has emerged as a promising defense strategy, our analysis reveals a\ncritical limitation: AT-trained models exhibit a bias toward low-frequency\nfeatures while neglecting high-frequency components. This bias is particularly\nconcerning as each frequency component carries distinct and crucial\ninformation: low-frequency features encode fundamental structural patterns,\nwhile high-frequency features capture intricate details and textures. To\naddress this limitation, we propose High-Frequency Feature Disentanglement and\nRecalibration (HFDR), a novel module that strategically separates and\nrecalibrates frequency-specific features to capture latent semantic cues. We\nfurther introduce frequency attention regularization to harmonize feature\nextraction across the frequency spectrum and mitigate the inherent\nlow-frequency bias of AT. Extensive experiments demonstrate our method's\nsuperior performance against white-box attacks and transfer attacks, while\nexhibiting strong generalization capabilities across diverse scenarios.\n","authors":["Kejia Zhang","Juanjuan Weng","Yuanzheng Cai","Zhiming Luo","Shaozi Li"],"pdf_url":"https://arxiv.org/pdf/2407.04016v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06841v1","updated":"2025-01-12T15:18:31Z","published":"2025-01-12T15:18:31Z","title":"Faithful Counterfactual Visual Explanations (FCVE)","summary":"  Deep learning models in computer vision have made remarkable progress, but\ntheir lack of transparency and interpretability remains a challenge. The\ndevelopment of explainable AI can enhance the understanding and performance of\nthese models. However, existing techniques often struggle to provide convincing\nexplanations that non-experts easily understand, and they cannot accurately\nidentify models' intrinsic decision-making processes. To address these\nchallenges, we propose to develop a counterfactual explanation (CE) model that\nbalances plausibility and faithfulness. This model generates easy-to-understand\nvisual explanations by making minimum changes necessary in images without\naltering the pixel data. Instead, the proposed method identifies internal\nconcepts and filters learned by models and leverages them to produce plausible\ncounterfactual explanations. The provided explanations reflect the internal\ndecision-making process of the model, thus ensuring faithfulness to the model.\n","authors":["Bismillah Khan","Syed Ali Tariq","Tehseen Zia","Muhammad Ahsan","David Windridge"],"pdf_url":"https://arxiv.org/pdf/2501.06841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03880v2","updated":"2025-01-12T15:18:28Z","published":"2025-01-07T15:43:36Z","title":"SELMA3D challenge: Self-supervised learning for 3D light-sheet\n  microscopy image segmentation","summary":"  Recent innovations in light sheet microscopy, paired with developments in\ntissue clearing techniques, enable the 3D imaging of large mammalian tissues\nwith cellular resolution. Combined with the progress in large-scale data\nanalysis, driven by deep learning, these innovations empower researchers to\nrapidly investigate the morphological and functional properties of diverse\nbiological samples. Segmentation, a crucial preliminary step in the analysis\nprocess, can be automated using domain-specific deep learning models with\nexpert-level performance. However, these models exhibit high sensitivity to\ndomain shifts, leading to a significant drop in accuracy when applied to data\noutside their training distribution. To address this limitation, and inspired\nby the recent success of self-supervised learning in training generalizable\nmodels, we organized the SELMA3D Challenge during the MICCAI 2024 conference.\nSELMA3D provides a vast collection of light-sheet images from cleared mice and\nhuman brains, comprising 35 large 3D images-each with over 1000^3 voxels-and\n315 annotated small patches for finetuning, preliminary testing and final\ntesting. The dataset encompasses diverse biological structures, including\nvessel-like and spot-like structures. Five teams participated in all phases of\nthe challenge, and their proposed methods are reviewed in this paper.\nQuantitative and qualitative results from most participating teams demonstrate\nthat self-supervised learning on large datasets improves segmentation model\nperformance and generalization. We will continue to support and extend SELMA3D\nas an inaugural MICCAI challenge focused on self-supervised learning for 3D\nmicroscopy image segmentation.\n","authors":["Ying Chen","Rami Al-Maskari","Izabela Horvath","Mayar Ali","Luciano Hoher","Kaiyuan Yang","Zengming Lin","Zhiwei Zhai","Mengzhe Shen","Dejin Xun","Yi Wang","Tony Xu","Maged Goubran","Yunheng Wu","Kensaku Mori","Johannes C. Paetzold","Ali Erturk"],"pdf_url":"https://arxiv.org/pdf/2501.03880v2.pdf","comment":"2st version"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2501.06956v1","updated":"2025-01-12T22:25:46Z","published":"2025-01-12T22:25:46Z","title":"Patent Novelty Assessment Accelerating Innovation and Patent Prosecution","summary":"  In the rapidly evolving landscape of technological innovation, safeguarding\nintellectual property rights through patents is crucial for fostering progress\nand stimulating research and development investments. This report introduces a\nground-breaking Patent Novelty Assessment and Claim Generation System,\nmeticulously crafted to dissect the inventive aspects of intellectual property\nand simplify access to extensive patent claim data. Addressing a crucial gap in\nacademic institutions, our system provides college students and researchers\nwith an intuitive platform to navigate and grasp the intricacies of patent\nclaims, particularly tailored for the nuances of Chinese patents. Unlike\nconventional analysis systems, our initiative harnesses a proprietary Chinese\nAPI to ensure unparalleled precision and relevance. The primary challenge lies\nin the complexity of accessing and comprehending diverse patent claims,\ninhibiting effective innovation upon existing ideas. Our solution aims to\novercome these barriers by offering a bespoke approach that seamlessly\nretrieves comprehensive claim information, finely tuned to the specifics of the\nChinese patent landscape. By equipping users with efficient access to\ncomprehensive patent claim information, our transformative platform seeks to\nignite informed exploration and innovation in the ever-evolving domain of\nintellectual property. Its envisioned impact transcends individual colleges,\nnurturing an environment conducive to research and development while deepening\nthe understanding of patented concepts within the academic community.\n","authors":["Kapil Kashyap","Sean Fargose","Gandhar Dhonde","Aditya Mishra"],"pdf_url":"https://arxiv.org/pdf/2501.06956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06873v1","updated":"2025-01-12T17:03:45Z","published":"2025-01-12T17:03:45Z","title":"Causal Claims in Economics","summary":"  We analyze over 44,000 NBER and CEPR working papers from 1980 to 2023 using a\ncustom language model to construct knowledge graphs that map economic concepts\nand their relationships. We distinguish between general claims and those\ndocumented via causal inference methods (e.g., DiD, IV, RDD, RCTs). We document\na substantial rise in the share of causal claims-from roughly 4% in 1990 to\nnearly 28% in 2020-reflecting the growing influence of the \"credibility\nrevolution.\" We find that causal narrative complexity (e.g., the depth of\ncausal chains) strongly predicts both publication in top-5 journals and higher\ncitation counts, whereas non-causal complexity tends to be uncorrelated or\nnegatively associated with these outcomes. Novelty is also pivotal for top-5\npublication, but only when grounded in credible causal methods: introducing\ngenuinely new causal edges or paths markedly increases both the likelihood of\nacceptance at leading outlets and long-run citations, while non-causal novelty\nexhibits weak or even negative effects. Papers engaging with central, widely\nrecognized concepts tend to attract more citations, highlighting a divergence\nbetween factors driving publication success and long-term academic impact.\nFinally, bridging underexplored concept pairs is rewarded primarily when\ngrounded in causal methods, yet such gap filling exhibits no consistent link\nwith future citations. Overall, our findings suggest that methodological rigor\nand causal innovation are key drivers of academic recognition, but sustained\nimpact may require balancing novel contributions with conceptual integration\ninto established economic discourse.\n","authors":["Prashant Garg","Thiemo Fetzer"],"pdf_url":"https://arxiv.org/pdf/2501.06873v1.pdf","comment":"For data, interactive tools, and additional project information,\n  visit https://www.causal.claims/. The website contains resources such as data\n  downloads, interactive author and paper-level knowledge graphs, and more"},{"id":"http://arxiv.org/abs/2501.06833v1","updated":"2025-01-12T15:00:10Z","published":"2025-01-12T15:00:10Z","title":"Unveiling Temporal Trends in 19th Century Literature: An Information\n  Retrieval Approach","summary":"  In English literature, the 19th century witnessed a significant transition in\nstyles, themes, and genres. Consequently, the novels from this period display\nremarkable diversity. This paper explores these variations by examining the\nevolution of term usage in 19th century English novels through the lens of\ninformation retrieval. By applying a query expansion-based approach to a\ndecade-segmented collection of fiction from the British Library, we examine how\nrelated terms vary over time. Our analysis employs multiple standard metrics\nincluding Kendall's tau, Jaccard similarity, and Jensen-Shannon divergence to\nassess overlaps and shifts in expanded query term sets. Our results indicate a\nsignificant degree of divergence in the related terms across decades as\nselected by the query expansion technique, suggesting substantial linguistic\nand conceptual changes throughout the 19th century novels.\n","authors":["Suchana Datta","Dwaipayan Roy","Derek Greene","Gerardine Meaney"],"pdf_url":"https://arxiv.org/pdf/2501.06833v1.pdf","comment":"Accepted at JCDL 2024"},{"id":"http://arxiv.org/abs/2501.06699v1","updated":"2025-01-12T03:32:12Z","published":"2025-01-12T03:32:12Z","title":"Large Language Models, Knowledge Graphs and Search Engines: A Crossroads\n  for Answering Users' Questions","summary":"  Much has been discussed about how Large Language Models, Knowledge Graphs and\nSearch Engines can be combined in a synergistic manner. A dimension largely\nabsent from current academic discourse is the user perspective. In particular,\nthere remain many open questions regarding how best to address the diverse\ninformation needs of users, incorporating varying facets and levels of\ndifficulty. This paper introduces a taxonomy of user information needs, which\nguides us to study the pros, cons and possible synergies of Large Language\nModels, Knowledge Graphs and Search Engines. From this study, we derive a\nroadmap for future research.\n","authors":["Aidan Hogan","Xin Luna Dong","Denny Vrandečić","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2501.06699v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2312.04356v3","updated":"2025-01-12T23:49:20Z","published":"2023-12-07T15:23:07Z","title":"NeuJeans: Private Neural Network Inference with Joint Optimization of\n  Convolution and FHE Bootstrapping","summary":"  Fully homomorphic encryption (FHE) is a promising cryptographic primitive for\nrealizing private neural network inference (PI) services by allowing a client\nto fully offload the inference task to a cloud server while keeping the client\ndata oblivious to the server. This work proposes NeuJeans, an FHE-based\nsolution for the PI of deep convolutional neural networks (CNNs). NeuJeans\ntackles the critical problem of the enormous computational cost for the FHE\nevaluation of CNNs. We introduce a novel encoding method called\nCoefficients-in-Slot (CinS) encoding, which enables multiple convolutions in\none HE multiplication without costly slot permutations. We further observe that\nCinS encoding is obtained by conducting the first several steps of the Discrete\nFourier Transform (DFT) on a ciphertext in conventional Slot encoding. This\nproperty enables us to save the conversion between CinS and Slot encodings as\nbootstrapping a ciphertext starts with DFT. Exploiting this, we devise\noptimized execution flows for various two-dimensional convolution (conv2d)\noperations and apply them to end-to-end CNN implementations. NeuJeans\naccelerates the performance of conv2d-activation sequences by up to 5.68 times\ncompared to state-of-the-art FHE-based PI work and performs the PI of a CNN at\nthe scale of ImageNet within a mere few seconds.\n","authors":["Jae Hyung Ju","Jaiyoung Park","Jongmin Kim","Minsik Kang","Donghwan Kim","Jung Hee Cheon","Jung Ho Ahn"],"pdf_url":"https://arxiv.org/pdf/2312.04356v3.pdf","comment":"15 pages, 6 figures, published at ACM 2024"},{"id":"http://arxiv.org/abs/2501.06965v1","updated":"2025-01-12T22:49:41Z","published":"2025-01-12T22:49:41Z","title":"Kolmogorov-Arnold Recurrent Network for Short Term Load Forecasting\n  Across Diverse Consumers","summary":"  Load forecasting plays a crucial role in energy management, directly\nimpacting grid stability, operational efficiency, cost reduction, and\nenvironmental sustainability. Traditional Vanilla Recurrent Neural Networks\n(RNNs) face issues such as vanishing and exploding gradients, whereas\nsophisticated RNNs such as LSTMs have shown considerable success in this\ndomain. However, these models often struggle to accurately capture complex and\nsudden variations in energy consumption, and their applicability is typically\nlimited to specific consumer types, such as offices or schools. To address\nthese challenges, this paper proposes the Kolmogorov-Arnold Recurrent Network\n(KARN), a novel load forecasting approach that combines the flexibility of\nKolmogorov-Arnold Networks with RNN's temporal modeling capabilities. KARN\nutilizes learnable temporal spline functions and edge-based activations to\nbetter model non-linear relationships in load data, making it adaptable across\na diverse range of consumer types. The proposed KARN model was rigorously\nevaluated on a variety of real-world datasets, including student residences,\ndetached homes, a home with electric vehicle charging, a townhouse, and\nindustrial buildings. Across all these consumer categories, KARN consistently\noutperformed traditional Vanilla RNNs, while it surpassed LSTM and Gated\nRecurrent Units (GRUs) in six buildings. The results demonstrate KARN's\nsuperior accuracy and applicability, making it a promising tool for enhancing\nload forecasting in diverse energy management scenarios.\n","authors":["Muhammad Umair Danish","Katarina Grolinger"],"pdf_url":"https://arxiv.org/pdf/2501.06965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06962v1","updated":"2025-01-12T22:48:04Z","published":"2025-01-12T22:48:04Z","title":"Compact Bayesian Neural Networks via pruned MCMC sampling","summary":"  Bayesian Neural Networks (BNNs) offer robust uncertainty quantification in\nmodel predictions, but training them presents a significant computational\nchallenge. This is mainly due to the problem of sampling multimodal posterior\ndistributions using Markov Chain Monte Carlo (MCMC) sampling and variational\ninference algorithms. Moreover, the number of model parameters scales\nexponentially with additional hidden layers, neurons, and features in the\ndataset. Typically, a significant portion of these densely connected parameters\nare redundant and pruning a neural network not only improves portability but\nalso has the potential for better generalisation capabilities. In this study,\nwe address some of the challenges by leveraging MCMC sampling with network\npruning to obtain compact probabilistic models having removed redundant\nparameters. We sample the posterior distribution of model parameters (weights\nand biases) and prune weights with low importance, resulting in a compact\nmodel. We ensure that the compact BNN retains its ability to estimate\nuncertainty via the posterior distribution while retaining the model training\nand generalisation performance accuracy by adapting post-pruning resampling. We\nevaluate the effectiveness of our MCMC pruning strategy on selected benchmark\ndatasets for regression and classification problems through empirical result\nanalysis. We also consider two coral reef drill-core lithology classification\ndatasets to test the robustness of the pruning model in complex real-world\ndatasets. We further investigate if refining compact BNN can retain any loss of\nperformance. Our results demonstrate the feasibility of training and pruning\nBNNs using MCMC whilst retaining generalisation performance with over 75%\nreduction in network size. This paves the way for developing compact BNN models\nthat provide uncertainty estimates for real-world applications.\n","authors":["Ratneel Deo","Scott Sisson","Jody M. Webster","Rohitash Chandra"],"pdf_url":"https://arxiv.org/pdf/2501.06962v1.pdf","comment":"22 pages, 11 figures"},{"id":"http://arxiv.org/abs/2501.06959v1","updated":"2025-01-12T22:39:58Z","published":"2025-01-12T22:39:58Z","title":"Sanidha: A Studio Quality Multi-Modal Dataset for Carnatic Music","summary":"  Music source separation demixes a piece of music into its individual sound\nsources (vocals, percussion, melodic instruments, etc.), a task with no simple\nmathematical solution. It requires deep learning methods involving training on\nlarge datasets of isolated music stems. The most commonly available datasets\nare made from commercial Western music, limiting the models' applications to\nnon-Western genres like Carnatic music. Carnatic music is a live tradition,\nwith the available multi-track recordings containing overlapping sounds and\nbleeds between the sources. This poses a challenge to commercially available\nsource separation models like Spleeter and Hybrid Demucs. In this work, we\nintroduce 'Sanidha', the first open-source novel dataset for Carnatic music,\noffering studio-quality, multi-track recordings with minimal to no overlap or\nbleed. Along with the audio files, we provide high-definition videos of the\nartists' performances. Additionally, we fine-tuned Spleeter, one of the most\ncommonly used source separation models, on our dataset and observed improved\nSDR performance compared to fine-tuning on a pre-existing Carnatic multi-track\ndataset. The outputs of the fine-tuned model with 'Sanidha' are evaluated\nthrough a listening study.\n","authors":["Venkatakrishnan Vaidyanathapuram Krishnan","Noel Alben","Anish Nair","Nathaniel Condit-Schultz"],"pdf_url":"https://arxiv.org/pdf/2501.06959v1.pdf","comment":"Accepted to the 25th International Society for Music Information\n  Retrieval Conference (ISMIR 2024)"},{"id":"http://arxiv.org/abs/2403.00853v3","updated":"2025-01-12T22:29:40Z","published":"2024-02-29T18:03:03Z","title":"Parallel Momentum Methods Under Biased Gradient Estimations","summary":"  Parallel stochastic gradient methods are gaining prominence in solving\nlarge-scale machine learning problems that involve data distributed across\nmultiple nodes. However, obtaining unbiased stochastic gradients, which have\nbeen the focus of most theoretical research, is challenging in many distributed\nmachine learning applications. The gradient estimations easily become biased,\nfor example, when gradients are compressed or clipped, when data is shuffled,\nand in meta-learning and reinforcement learning. In this work, we establish\nworst-case bounds on parallel momentum methods under biased gradient estimation\non both general non-convex and $\\mu$-PL problems. Our analysis covers general\ndistributed optimization problems, and we work out the implications for special\ncases where gradient estimates are biased, i.e. in meta-learning and when the\ngradients are compressed or clipped. Our numerical experiments verify our\ntheoretical findings and show faster convergence performance of momentum\nmethods than traditional biased gradient descent.\n","authors":["Ali Beikmohammadi","Sarit Khirirat","Sindri Magnússon"],"pdf_url":"https://arxiv.org/pdf/2403.00853v3.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2501.06954v1","updated":"2025-01-12T22:21:06Z","published":"2025-01-12T22:21:06Z","title":"A Hessian-informed hyperparameter optimization for differential learning\n  rate","summary":"  Differential learning rate (DLR), a technique that applies different learning\nrates to different model parameters, has been widely used in deep learning and\nachieved empirical success via its various forms. For example,\nparameter-efficient fine-tuning (PEFT) applies zero learning rates to most\nparameters so as to significantly save the computational cost.\n  At the core, DLR leverages the observation that different parameters can have\ndifferent loss curvature, which is hard to characterize in general. We propose\nthe Hessian-informed differential learning rate (Hi-DLR), an efficient approach\nthat solves the hyperparameter optimization (HPO) of learning rates and\ncaptures the loss curvature for any model and optimizer adaptively. Given a\nproper grouping of parameters, we empirically demonstrate that Hi-DLR can\nimprove the convergence by dynamically determining the learning rates during\nthe training. Furthermore, we can quantify the influence of different\nparameters and freeze the less-contributing parameters, which leads to a new\nPEFT that automatically adapts to various tasks and models. Additionally,\nHi-DLR also exhibits comparable performance on various full model training\ntasks.\n","authors":["Shiyun Xu","Zhiqi Bu","Yiliang Zhang","Ian Barnett"],"pdf_url":"https://arxiv.org/pdf/2501.06954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06942v1","updated":"2025-01-12T21:39:06Z","published":"2025-01-12T21:39:06Z","title":"Comparison of Autoencoders for tokenization of ASL datasets","summary":"  Generative AI, powered by large language models (LLMs), has revolutionized\napplications across text, audio, images, and video. This study focuses on\ndeveloping and evaluating encoder-decoder architectures for the American Sign\nLanguage (ASL) image dataset, consisting of 87,000 images across 29 hand sign\nclasses. Three approaches were compared: Feedforward Autoencoders,\nConvolutional Autoencoders, and Diffusion Autoencoders. The Diffusion\nAutoencoder outperformed the others, achieving the lowest mean squared error\n(MSE) and highest Mean Opinion Score (MOS) due to its probabilistic noise\nmodeling and iterative denoising capabilities. The Convolutional Autoencoder\ndemonstrated effective spatial feature extraction but lacked the robustness of\nthe diffusion process, while the Feedforward Autoencoder served as a baseline\nwith limitations in handling complex image data. Objective and subjective\nevaluations confirmed the superiority of the Diffusion Autoencoder for\nhigh-fidelity image reconstruction, emphasizing its potential in multimodal AI\napplications such as sign language recognition and generation. This work\nprovides critical insights into designing robust encoder-decoder systems to\nadvance multimodal AI capabilities.\n","authors":["Vouk Praun-Petrovic","Aadhvika Koundinya","Lavanya Prahallad"],"pdf_url":"https://arxiv.org/pdf/2501.06942v1.pdf","comment":"9 pages, 2 tables, 4 figures"},{"id":"http://arxiv.org/abs/2501.06939v1","updated":"2025-01-12T21:33:06Z","published":"2025-01-12T21:33:06Z","title":"Super-Resolution of 3D Micro-CT Images Using Generative Adversarial\n  Networks: Enhancing Resolution and Segmentation Accuracy","summary":"  We develop a procedure for substantially improving the quality of segmented\n3D micro-Computed Tomography (micro-CT) images of rocks with a Machine Learning\n(ML) Generative Model. The proposed model enhances the resolution eightfold\n(8x) and addresses segmentation inaccuracies due to the overlapping X-ray\nattenuation in micro-CT measurement for different rock minerals and phases. The\nproposed generative model is a 3D Deep Convolutional Wasserstein Generative\nAdversarial Network with Gradient Penalty (3D DC WGAN-GP). The algorithm is\ntrained on segmented 3D low-resolution micro-CT images and segmented unpaired\ncomplementary 2D high-resolution Laser Scanning Microscope (LSM) images. The\nalgorithm was demonstrated on multiple samples of Berea sandstones. We achieved\nhigh-quality super-resolved 3D images with a resolution of 0.4375 micro-m/voxel\nand accurate segmentation for constituting minerals and pore space. The\ndescribed procedure can significantly expand the modern capabilities of digital\nrock physics.\n","authors":["Evgeny Ugolkov","Xupeng He","Hyung Kwak","Hussein Hoteit"],"pdf_url":"https://arxiv.org/pdf/2501.06939v1.pdf","comment":"24 pages, 9 figures"},{"id":"http://arxiv.org/abs/2311.02798v3","updated":"2025-01-12T21:23:17Z","published":"2023-11-05T23:47:52Z","title":"Multi-channel learning for integrating structural hierarchies into\n  context-dependent molecular representation","summary":"  Reliable molecular property prediction is essential for various scientific\nendeavors and industrial applications, such as drug discovery. However, the\ndata scarcity, combined with the highly non-linear causal relationships between\nphysicochemical and biological properties and conventional molecular\nfeaturization schemes, complicates the development of robust molecular machine\nlearning models. Self-supervised learning (SSL) has emerged as a popular\nsolution, utilizing large-scale, unannotated molecular data to learn a\nfoundational representation of chemical space that might be advantageous for\ndownstream tasks. Yet, existing molecular SSL methods largely overlook chemical\nknowledge, including molecular structure similarity, scaffold composition, and\nthe context-dependent aspects of molecular properties when operating over the\nchemical space. They also struggle to learn the subtle variations in\nstructure-activity relationship. This paper introduces a novel pre-training\nframework that learns robust and generalizable chemical knowledge. It leverages\nthe structural hierarchy within the molecule, embeds them through distinct\npre-training tasks across channels, and aggregates channel information in a\ntask-specific manner during fine-tuning. Our approach demonstrates competitive\nperformance across various molecular property benchmarks and offers strong\nadvantages in particularly challenging yet ubiquitous scenarios like activity\ncliffs.\n","authors":["Yue Wan","Jialu Wu","Tingjun Hou","Chang-Yu Hsieh","Xiaowei Jia"],"pdf_url":"https://arxiv.org/pdf/2311.02798v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03123v4","updated":"2025-01-12T21:07:54Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v4.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2501.06934v1","updated":"2025-01-12T21:06:38Z","published":"2025-01-12T21:06:38Z","title":"A group-theoretic framework for machine learning in hyperbolic spaces","summary":"  Embedding the data in hyperbolic spaces can preserve complex relationships in\nvery few dimensions, thus enabling compact models and improving efficiency of\nmachine learning (ML) algorithms. The underlying idea is that hyperbolic\nrepresentations can prevent the loss of important structural information for\ncertain ubiquitous types of data. However, further advances in hyperbolic ML\nrequire more principled mathematical approaches and adequate geometric methods.\nThe present study aims at enhancing mathematical foundations of hyperbolic ML\nby combining group-theoretic and conformal-geometric arguments with\noptimization and statistical techniques. Precisely, we introduce the notion of\nthe mean (barycenter) and the novel family of probability distributions on\nhyperbolic balls. We further propose efficient optimization algorithms for\ncomputation of the barycenter and for maximum likelihood estimation. One can\nbuild upon basic concepts presented here in order to design more demanding\nalgorithms and implement hyperbolic deep learning pipelines.\n","authors":["Vladimir Jaćimović"],"pdf_url":"https://arxiv.org/pdf/2501.06934v1.pdf","comment":"22 pages, 4 figures"},{"id":"http://arxiv.org/abs/2501.06933v1","updated":"2025-01-12T21:02:20Z","published":"2025-01-12T21:02:20Z","title":"Neural equilibria for long-term prediction of nonlinear conservation\n  laws","summary":"  We introduce Neural Discrete Equilibrium (NeurDE), a machine learning (ML)\napproach for long-term forecasting of flow phenomena that relies on a \"lifting\"\nof physical conservation laws into the framework of kinetic theory. The kinetic\nformulation provides an excellent structure for ML algorithms by separating\nnonlinear, non-local physics into a nonlinear but local relaxation to\nequilibrium and a linear non-local transport. This separation allows the ML to\nfocus on the local nonlinear components while addressing the simpler linear\ntransport with efficient classical numerical algorithms. To accomplish this, we\ndesign an operator network that maps macroscopic observables to equilibrium\nstates in a manner that maximizes entropy, yielding expressive BGK-type\ncollisions. By incorporating our surrogate equilibrium into the lattice\nBoltzmann (LB) algorithm, we achieve accurate flow forecasts for a wide range\nof challenging flows. We show that NeurDE enables accurate prediction of\ncompressible flows, including supersonic flows, while tracking shocks over\nhundreds of time steps, using a small velocity lattice-a heretofore\nunattainable feat without expensive numerical root finding.\n","authors":["J. Antonio Lara Benitez","Junyi Guo","Kareem Hegazy","Ivan Dokmanić","Michael W. Mahoney","Maarten V. de Hoop"],"pdf_url":"https://arxiv.org/pdf/2501.06933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14518v2","updated":"2025-01-12T21:01:00Z","published":"2024-08-26T00:13:14Z","title":"A Survey on Reinforcement Learning Applications in SLAM","summary":"  The emergence of mobile robotics, particularly in the automotive industry,\nintroduces a promising era of enriched user experiences and adept handling of\ncomplex navigation challenges. The realization of these advancements\nnecessitates a focused technological effort and the successful execution of\nnumerous intricate tasks, particularly in the critical domain of Simultaneous\nLocalization and Mapping (SLAM). Various artificial intelligence (AI)\nmethodologies, such as deep learning and reinforcement learning, present viable\nsolutions to address the challenges in SLAM. This study specifically explores\nthe application of reinforcement learning in the context of SLAM. By enabling\nthe agent (the robot) to iteratively interact with and receive feedback from\nits environment, reinforcement learning facilitates the acquisition of\nnavigation and mapping skills, thereby enhancing the robot's decision-making\ncapabilities. This approach offers several advantages, including improved\nnavigation proficiency, increased resilience, reduced dependence on sensor\nprecision, and refinement of the decision-making process. The findings of this\nstudy, which provide an overview of reinforcement learning's utilization in\nSLAM, reveal significant advancements in the field. The investigation also\nhighlights the evolution and innovative integration of these techniques.\n","authors":["Mohammad Dehghani Tezerjani","Mohammad Khoshnazar","Mohammadhamed Tangestanizadeh","Arman Kiani","Qing Yang"],"pdf_url":"https://arxiv.org/pdf/2408.14518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06932v1","updated":"2025-01-12T21:00:50Z","published":"2025-01-12T21:00:50Z","title":"Harnessing Large Language Models for Disaster Management: A Survey","summary":"  Large language models (LLMs) have revolutionized scientific research with\ntheir exceptional capabilities and transformed various fields. Among their\npractical applications, LLMs have been playing a crucial role in mitigating\nthreats to human life, infrastructure, and the environment. Despite growing\nresearch in disaster LLMs, there remains a lack of systematic review and\nin-depth analysis of LLMs for natural disaster management. To address the gap,\nthis paper presents a comprehensive survey of existing LLMs in natural disaster\nmanagement, along with a taxonomy that categorizes existing works based on\ndisaster phases and application scenarios. By collecting public datasets and\nidentifying key challenges and opportunities, this study aims to guide the\nprofessional community in developing advanced LLMs for disaster management to\nenhance the resilience against natural disasters.\n","authors":["Zhenyu Lei","Yushun Dong","Weiyu Li","Rong Ding","Qi Wang","Jundong Li"],"pdf_url":"https://arxiv.org/pdf/2501.06932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06926v1","updated":"2025-01-12T20:35:28Z","published":"2025-01-12T20:35:28Z","title":"Automatic Double Reinforcement Learning in Semiparametric Markov\n  Decision Processes with Applications to Long-Term Causal Inference","summary":"  Double reinforcement learning (DRL) enables statistically efficient inference\non the value of a policy in a nonparametric Markov Decision Process (MDP) given\ntrajectories generated by another policy. However, this approach necessarily\nrequires stringent overlap between the state distributions, which is often\nviolated in practice. To relax this requirement and extend DRL, we study\nefficient inference on linear functionals of the $Q$-function (of which policy\nvalue is a special case) in infinite-horizon, time-invariant MDPs under\nsemiparametric restrictions on the $Q$-function. These restrictions can reduce\nthe overlap requirement and lower the efficiency bound, yielding more precise\nestimates. As an important example, we study the evaluation of long-term value\nunder domain adaptation, given a few short trajectories from the new domain and\nrestrictions on the difference between the domains. This can be used for\nlong-term causal inference. Our method combines flexible estimates of the\n$Q$-function and the Riesz representer of the functional of interest (e.g., the\nstationary state density ratio for policy value) and is automatic in that we do\nnot need to know the form of the latter - only the functional we care about. To\naddress potential model misspecification bias, we extend the adaptive debiased\nmachine learning (ADML) framework of \\citet{van2023adaptive} to construct\nnonparametrically valid and superefficient estimators that adapt to the\nfunctional form of the $Q$-function. As a special case, we propose a novel\nadaptive debiased plug-in estimator that uses isotonic-calibrated fitted\n$Q$-iteration - a new calibration algorithm for MDPs - to circumvent the\ncomputational challenges of estimating debiasing nuisances from min-max\nobjectives.\n","authors":["Lars van der Laan","David Hubbard","Allen Tran","Nathan Kallus","Aurélien Bibaut"],"pdf_url":"https://arxiv.org/pdf/2501.06926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06925v1","updated":"2025-01-12T20:34:26Z","published":"2025-01-12T20:34:26Z","title":"A Hybrid Virtual Element Method and Deep Learning Approach for Solving\n  One-Dimensional Euler-Bernoulli Beams","summary":"  A hybrid framework integrating the Virtual Element Method (VEM) with deep\nlearning is presented as an initial step toward developing efficient and\nflexible numerical models for one-dimensional Euler-Bernoulli beams. The\nprimary aim is to explore a data-driven surrogate model capable of predicting\ndisplacement fields across varying material and geometric parameters while\nmaintaining computational efficiency. Building upon VEM's ability to handle\nhigher-order polynomials and non-conforming discretizations, the method offers\na robust numerical foundation for structural mechanics. A neural network\narchitecture is introduced to separately process nodal and material-specific\ndata, effectively capturing complex interactions with minimal reliance on large\ndatasets. To address challenges in training, the model incorporates Sobolev\ntraining and GradNorm techniques, ensuring balanced loss contributions and\nenhanced generalization. While this framework is in its early stages, it\ndemonstrates the potential for further refinement and development into a\nscalable alternative to traditional methods. The proposed approach lays the\ngroundwork for advancing numerical and data-driven techniques in beam modeling,\noffering a foundation for future research in structural mechanics.\n","authors":["Paulo Akira F. Enabe","Rodrigo Provasi"],"pdf_url":"https://arxiv.org/pdf/2501.06925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06923v1","updated":"2025-01-12T20:23:46Z","published":"2025-01-12T20:23:46Z","title":"Optimal Online Bookmaking for Binary Games","summary":"  In online betting, the bookmaker can update the payoffs it offers on a\nparticular event many times before the event takes place, and the updated\npayoffs may depend on the bets accumulated thus far. We study the problem of\nbookmaking with the goal of maximizing the return in the worst-case, with\nrespect to the gamblers' behavior and the event's outcome. We formalize this\nproblem as the \\emph{Optimal Online Bookmaking game}, and provide the exact\nsolution for the binary case. To this end, we develop the optimal bookmaking\nstrategy, which relies on a new technique called bi-balancing trees, that\nassures that the house loss is the same for all \\emph{decisive} betting\nsequences, where the gambler bets all its money on a single outcome in each\nround.\n","authors":["Alankrita Bhatt","Or Ordentlich","Oron Sabag"],"pdf_url":"https://arxiv.org/pdf/2501.06923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06916v1","updated":"2025-01-12T19:59:33Z","published":"2025-01-12T19:59:33Z","title":"Black-box optimization and quantum annealing for filtering out\n  mislabeled training instances","summary":"  This study proposes an approach for removing mislabeled instances from\ncontaminated training datasets by combining surrogate model-based black-box\noptimization (BBO) with postprocessing and quantum annealing. Mislabeled\ntraining instances, a common issue in real-world datasets, often degrade model\ngeneralization, necessitating robust and efficient noise-removal strategies.\nThe proposed method evaluates filtered training subsets based on validation\nloss, iteratively refines loss estimates through surrogate model-based BBO with\npostprocessing, and leverages quantum annealing to efficiently sample diverse\ntraining subsets with low validation error. Experiments on a noisy majority bit\ntask demonstrate the method's ability to prioritize the removal of high-risk\nmislabeled instances. Integrating D-Wave's clique sampler running on a physical\nquantum annealer achieves faster optimization and higher-quality training\nsubsets compared to OpenJij's simulated quantum annealing sampler or Neal's\nsimulated annealing sampler, offering a scalable framework for enhancing\ndataset quality. This work highlights the effectiveness of the proposed method\nfor supervised learning tasks, with future directions including its application\nto unsupervised learning, real-world datasets, and large-scale implementations.\n","authors":["Makoto Otsuka","Kento Kodama","Keisuke Morita","Masayuki Ohzeki"],"pdf_url":"https://arxiv.org/pdf/2501.06916v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2308.12636v4","updated":"2025-01-12T16:26:00Z","published":"2023-08-24T08:22:21Z","title":"Exploring Transferability of Multimodal Adversarial Samples for\n  Vision-Language Pre-training Models with Contrastive Learning","summary":"  The integration of visual and textual data in Vision-Language Pre-training\n(VLP) models is crucial for enhancing vision-language understanding. However,\nthe adversarial robustness of these models, especially in the alignment of\nimage-text features, has not yet been sufficiently explored. In this paper, we\nintroduce a novel gradient-based multimodal adversarial attack method,\nunderpinned by contrastive learning, to improve the transferability of\nmultimodal adversarial samples in VLP models. This method concurrently\ngenerates adversarial texts and images within imperceptive perturbation,\nemploying both image-text and intra-modal contrastive loss. We evaluate the\neffectiveness of our approach on image-text retrieval and visual entailment\ntasks, using publicly available datasets in a black-box setting. Extensive\nexperiments indicate a significant advancement over existing single-modal\ntransfer-based adversarial attack methods and current multimodal adversarial\nattack approaches.\n","authors":["Youze Wang","Wenbo Hu","Yinpeng Dong","Hanwang Zhang","Hang Su","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2308.12636v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08505v4","updated":"2025-01-12T03:44:16Z","published":"2024-03-13T13:12:57Z","title":"CAMSIC: Content-aware Masked Image Modeling Transformer for Stereo Image\n  Compression","summary":"  Existing learning-based stereo image codec adopt sophisticated transformation\nwith simple entropy models derived from single image codecs to encode latent\nrepresentations. However, those entropy models struggle to effectively capture\nthe spatial-disparity characteristics inherent in stereo images, which leads to\nsuboptimal rate-distortion results. In this paper, we propose a stereo image\ncompression framework, named CAMSIC. CAMSIC independently transforms each image\nto latent representation and employs a powerful decoder-free Transformer\nentropy model to capture both spatial and disparity dependencies, by\nintroducing a novel content-aware masked image modeling (MIM) technique. Our\ncontent-aware MIM facilitates efficient bidirectional interaction between prior\ninformation and estimated tokens, which naturally obviates the need for an\nextra Transformer decoder. Experiments show that our stereo image codec\nachieves state-of-the-art rate-distortion performance on two stereo image\ndatasets Cityscapes and InStereo2K with fast encoding and decoding speed. Code\nis available at https://github.com/Xinjie-Q/CAMSIC.\n","authors":["Xinjie Zhang","Shenyuan Gao","Zhening Liu","Jiawei Shao","Xingtong Ge","Dailan He","Tongda Xu","Yan Wang","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08505v4.pdf","comment":"Accepted by AAAI 2025"}]},"2025-01-11T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2501.06663v1","updated":"2025-01-11T23:29:51Z","published":"2025-01-11T23:29:51Z","title":"Ultra Memory-Efficient On-FPGA Training of Transformers via\n  Tensor-Compressed Optimization","summary":"  Transformer models have achieved state-of-the-art performance across a wide\nrange of machine learning tasks. There is growing interest in training\ntransformers on resource-constrained edge devices due to considerations such as\nprivacy, domain adaptation, and on-device scientific machine learning. However,\nthe significant computational and memory demands required for transformer\ntraining often exceed the capabilities of an edge device. Leveraging low-rank\ntensor compression, this paper presents the first on-FPGA accelerator for\nend-to-end transformer training. On the algorithm side, we present a\nbi-directional contraction flow for tensorized transformer training,\nsignificantly reducing the computational FLOPS and intra-layer memory costs\ncompared to existing tensor operations. On the hardware side, we store all\nhighly compressed model parameters and gradient information on chip, creating\nan on-chip-memory-only framework for each stage in training. This reduces\noff-chip communication and minimizes latency and energy costs. Additionally, we\nimplement custom computing kernels for each training stage and employ\nintra-layer parallelism and pipe-lining to further enhance run-time and memory\nefficiency. Through experiments on transformer models within $36.7$ to $93.5$\nMB using FP-32 data formats on the ATIS dataset, our tensorized FPGA\naccelerator could conduct single-batch end-to-end training on the AMD Alevo U50\nFPGA, with a memory budget of less than $6$-MB BRAM and $22.5$-MB URAM.\nCompared to uncompressed training on the NVIDIA RTX 3090 GPU, our on-FPGA\ntraining achieves a memory reduction of $30\\times$ to $51\\times$. Our FPGA\naccelerator also achieves up to $3.6\\times$ less energy cost per epoch compared\nwith tensor Transformer training on an NVIDIA RTX 3090 GPU.\n","authors":["Jiayi Tian","Jinming Lu","Hai Li","Xiangwei Wang"," Cong"," Hao","Ian Young","Zheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.06663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06662v1","updated":"2025-01-11T23:28:50Z","published":"2025-01-11T23:28:50Z","title":"The Magnitude of Categories of Texts Enriched by Language Models","summary":"  The purpose of this article is twofold. Firstly, we use the next-token\nprobabilities given by a language model to explicitly define a\n$[0,1]$-enrichment of a category of texts in natural language, in the sense of\nBradley, Terilla, and Vlassopoulos. We consider explicitly the terminating\nconditions for text generation and determine when the enrichment itself can be\ninterpreted as a probability over texts. Secondly, we compute the M\\\"obius\nfunction and the magnitude of an associated generalized metric space\n$\\mathcal{M}$ of texts using a combinatorial version of these quantities\nrecently introduced by Vigneaux. The magnitude function $f(t)$ of $\\mathcal{M}$\nis a sum over texts $x$ (prompts) of the Tsallis $t$-entropies of the\nnext-token probability distributions $p(-|x)$ plus the cardinality of the\nmodel's possible outputs. The derivative of $f$ at $t=1$ recovers a sum of\nShannon entropies, which justifies seeing magnitude as a partition function.\nFollowing Leinster and Schulman, we also express the magnitude function of\n$\\mathcal M$ as an Euler characteristic of magnitude homology and provide an\nexplicit description of the zeroeth and first magnitude homology groups.\n","authors":["Tai-Danae Bradley","Juan Pablo Vigneaux"],"pdf_url":"https://arxiv.org/pdf/2501.06662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06645v1","updated":"2025-01-11T21:41:27Z","published":"2025-01-11T21:41:27Z","title":"FocalPO: Enhancing Preference Optimizing by Focusing on Correct\n  Preference Rankings","summary":"  Efficient preference optimization algorithms such as Direct Preference\nOptimization (DPO) have become a popular approach in aligning large language\nmodels (LLMs) with human preferences. These algorithms implicitly treat the LLM\nas a reward model, and focus on training it to correct misranked preference\npairs. However, recent work~\\citep{chen2024preference} empirically finds that\nDPO training \\textit{rarely improves these misranked preference pairs}, despite\nits gradient emphasizing on these cases. We introduce FocalPO, a DPO variant\nthat instead \\textit{down-weighs} misranked preference pairs and prioritizes\nenhancing the model's understanding of pairs that it can already rank\ncorrectly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this\nby adding a modulating factor to dynamically scale DPO loss. Our experiment\ndemonstrates that FocalPO surpasses DPO and its variants on popular benchmarks\nlike Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B.\nAdditionally, we empirically reveals how FocalPO affects training on correct\nand incorrect sample groups, further underscoring its effectiveness.\n","authors":["Tong Liu","Xiao Yu","Wenxuan Zhou","Jindong Gu","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2501.06645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06638v1","updated":"2025-01-11T21:03:22Z","published":"2025-01-11T21:03:22Z","title":"Scaling Down Semantic Leakage: Investigating Associative Bias in Smaller\n  Language Models","summary":"  Semantic leakage is a phenomenon recently introduced by Gonen et al. (2024).\nIt refers to a situation in which associations learnt from the training data\nemerge in language model generations in an unexpected and sometimes undesired\nway. Prior work has focused on leakage in large language models (7B+\nparameters). In this study, I use Qwen2.5 model family to explore whether\nsmaller models, ranging from 500M to 7B parameters, demonstrate less semantic\nleakage due to their limited capacity for capturing complex associations.\nBuilding on the previous dataset from Gonen et al. (2024), I introduce a new\ndataset of color-focused prompts, categorized into specific types of semantic\nassociations, to systematically evaluate the models' performance. Results\nindicate that smaller models exhibit less semantic leakage overall, although\nthis trend is not strictly linear, with medium-sized models sometimes\nsurpassing larger ones in leaking behavior. The dataset, the model generations,\nand the evaluation code are publicly available at\nhttps://github.com/smilni/semantic_leakage_project.\n","authors":["Veronika Smilga"],"pdf_url":"https://arxiv.org/pdf/2501.06638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06636v1","updated":"2025-01-11T20:55:35Z","published":"2025-01-11T20:55:35Z","title":"Dual use issues in the field of Natural Language Generation","summary":"  This report documents the results of a recent survey in the SIGGEN community,\nfocusing on Dual Use issues in Natural Language Generation (NLG). SIGGEN is the\nSpecial Interest Group (SIG) of the Association for Computational Linguistics\n(ACL) for researchers working on NLG. The survey was prompted by the ACL\nexecutive board, which asked all SIGs to provide an overview of dual use issues\nwithin their respective subfields. The survey was sent out in October 2024 and\nthe results were processed in January 2025. With 23 respondents, the survey is\npresumably not representative of all SIGGEN members, but at least this document\noffers a helpful resource for future discussions.\n  This report is open to feedback from the SIGGEN community. Let me know if you\nhave any questions or comments!\n","authors":["Emiel van Miltenburg"],"pdf_url":"https://arxiv.org/pdf/2501.06636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06597v1","updated":"2025-01-11T17:45:13Z","published":"2025-01-11T17:45:13Z","title":"EmoXpt: Analyzing Emotional Variances in Human Comments and\n  LLM-Generated Responses","summary":"  The widespread adoption of generative AI has generated diverse opinions, with\nindividuals expressing both support and criticism of its applications. This\nstudy investigates the emotional dynamics surrounding generative AI by\nanalyzing human tweets referencing terms such as ChatGPT, OpenAI, Copilot, and\nLLMs. To further understand the emotional intelligence of ChatGPT, we examine\nits responses to selected tweets, highlighting differences in sentiment between\nhuman comments and LLM-generated responses. We introduce EmoXpt, a sentiment\nanalysis framework designed to assess both human perspectives on generative AI\nand the sentiment embedded in ChatGPT's responses. Unlike prior studies that\nfocus exclusively on human sentiment, EmoXpt uniquely evaluates the emotional\nexpression of ChatGPT. Experimental results demonstrate that LLM-generated\nresponses are notably more efficient, cohesive, and consistently positive than\nhuman responses.\n","authors":["Shireesh Reddy Pyreddy","Tarannum Shaila Zaman"],"pdf_url":"https://arxiv.org/pdf/2501.06597v1.pdf","comment":"7 pages, 10 figures, 5 tables. This paper has been accepted and\n  presented at the 2025 IEEE 15th Annual Computing and Communication Workshop\n  and Conference (CCWC)"},{"id":"http://arxiv.org/abs/2501.06590v1","updated":"2025-01-11T17:10:30Z","published":"2025-01-11T17:10:30Z","title":"ChemAgent: Self-updating Library in Large Language Models Improves\n  Chemical Reasoning","summary":"  Chemical reasoning usually involves complex, multi-step processes that demand\nprecise calculations, where even minor errors can lead to cascading failures.\nFurthermore, large language models (LLMs) encounter difficulties handling\ndomain-specific formulas, executing reasoning steps accurately, and integrating\ncode effectively when tackling chemical reasoning tasks. To address these\nchallenges, we present ChemAgent, a novel framework designed to improve the\nperformance of LLMs through a dynamic, self-updating library. This library is\ndeveloped by decomposing chemical tasks into sub-tasks and compiling these\nsub-tasks into a structured collection that can be referenced for future\nqueries. Then, when presented with a new problem, ChemAgent retrieves and\nrefines pertinent information from the library, which we call memory,\nfacilitating effective task decomposition and the generation of solutions. Our\nmethod designs three types of memory and a library-enhanced reasoning\ncomponent, enabling LLMs to improve over time through experience. Experimental\nresults on four chemical reasoning datasets from SciBench demonstrate that\nChemAgent achieves performance gains of up to 46% (GPT-4), significantly\noutperforming existing methods. Our findings suggest substantial potential for\nfuture applications, including tasks such as drug discovery and materials\nscience. Our code can be found at https://github.com/gersteinlab/chemagent\n","authors":["Xiangru Tang","Tianyu Hu","Muyang Ye","Yanjun Shao","Xunjian Yin","Siru Ouyang","Wangchunshu Zhou","Pan Lu","Zhuosheng Zhang","Yilun Zhao","Arman Cohan","Mark Gerstein"],"pdf_url":"https://arxiv.org/pdf/2501.06590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06589v1","updated":"2025-01-11T17:06:30Z","published":"2025-01-11T17:06:30Z","title":"Ladder-residual: parallelism-aware architecture for accelerating large\n  model inference with communication overlapping","summary":"  Large language model inference is both memory-intensive and time-consuming,\noften requiring distributed algorithms to efficiently scale. Various model\nparallelism strategies are used in multi-gpu training and inference to\npartition computation across multiple devices, reducing memory load and\ncomputation time. However, using model parallelism necessitates communication\nof information between GPUs, which has been a major bottleneck and limits the\ngains obtained by scaling up the number of devices. We introduce Ladder\nResidual, a simple architectural modification applicable to all residual-based\nmodels that enables straightforward overlapping that effectively hides the\nlatency of communication. Our insight is that in addition to systems\noptimization, one can also redesign the model architecture to decouple\ncommunication from computation. While Ladder Residual can allow\ncommunication-computation decoupling in conventional parallelism patterns, we\nfocus on Tensor Parallelism in this paper, which is particularly bottlenecked\nby its heavy communication. For a Transformer model with 70B parameters,\napplying Ladder Residual to all its layers can achieve 30% end-to-end wall\nclock speed up at inference time with TP sharding over 8 devices. We refer the\nresulting Transformer model as the Ladder Transformer. We train a 1B and 3B\nLadder Transformer from scratch and observe comparable performance to a\nstandard dense transformer baseline. We also show that it is possible to\nconvert parts of the Llama-3.1 8B model to our Ladder Residual architecture\nwith minimal accuracy degradation by only retraining for 3B tokens.\n","authors":["Muru Zhang","Mayank Mishra","Zhongzhu Zhou","William Brandon","Jue Wang","Yoon Kim","Jonathan Ragan-Kelley","Shuaiwen Leon Song","Ben Athiwaratkun","Tri Dao"],"pdf_url":"https://arxiv.org/pdf/2501.06589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06582v1","updated":"2025-01-11T16:37:49Z","published":"2025-01-11T16:37:49Z","title":"ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting","summary":"  Information retrieval, specifically contract clause retrieval, is\nfoundational to contract drafting because lawyers rarely draft contracts from\nscratch; instead, they locate and revise the most relevant precedent. We\nintroduce the Atticus Clause Retrieval Dataset (ACORD), the first retrieval\nbenchmark for contract drafting fully annotated by experts. ACORD focuses on\ncomplex contract clauses such as Limitation of Liability, Indemnification,\nChange of Control, and Most Favored Nation. It includes 114 queries and over\n126,000 query-clause pairs, each ranked on a scale from 1 to 5 stars. The task\nis to find the most relevant precedent clauses to a query. The bi-encoder\nretriever paired with pointwise LLMs re-rankers shows promising results.\nHowever, substantial improvements are still needed to effectively manage the\ncomplex legal work typically undertaken by lawyers. As the first retrieval\nbenchmark for contract drafting annotated by experts, ACORD can serve as a\nvaluable IR benchmark for the NLP community.\n","authors":["Steven H. Wang","Maksim Zubkov","Kexin Fan","Sarah Harrell","Yuyang Sun","Wei Chen","Andreas Plesner","Roger Wattenhofer"],"pdf_url":"https://arxiv.org/pdf/2501.06582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10950v2","updated":"2025-01-11T15:51:41Z","published":"2024-11-17T03:32:50Z","title":"Understanding Multimodal LLMs: the Mechanistic Interpretability of Llava\n  in Visual Question Answering","summary":"  Understanding the mechanisms behind Large Language Models (LLMs) is crucial\nfor designing improved models and strategies. While recent studies have yielded\nvaluable insights into the mechanisms of textual LLMs, the mechanisms of\nMulti-modal Large Language Models (MLLMs) remain underexplored. In this paper,\nwe apply mechanistic interpretability methods to analyze the visual question\nanswering (VQA) mechanisms in the first MLLM, Llava. We compare the mechanisms\nbetween VQA and textual QA (TQA) in color answering tasks and find that: a) VQA\nexhibits a mechanism similar to the in-context learning mechanism observed in\nTQA; b) the visual features exhibit significant interpretability when\nprojecting the visual embeddings into the embedding space; and c) Llava\nenhances the existing capabilities of the corresponding textual LLM Vicuna\nduring visual instruction tuning. Based on these findings, we develop an\ninterpretability tool to help users and researchers identify important visual\nlocations for final predictions, aiding in the understanding of visual\nhallucination. Our method demonstrates faster and more effective results\ncompared to existing interpretability approaches. Code:\n\\url{https://github.com/zepingyu0512/llava-mechanism}\n","authors":["Zeping Yu","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2411.10950v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2409.14750v2","updated":"2025-01-11T15:12:37Z","published":"2024-09-23T06:56:51Z","title":"FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional\n  Referring Expression Comprehension","summary":"  Referring Expression Comprehension (REC) is a crucial cross-modal task that\nobjectively evaluates the capabilities of language understanding, image\ncomprehension, and language-to-image grounding. Consequently, it serves as an\nideal testing ground for Multi-modal Large Language Models (MLLMs). In pursuit\nof this goal, we have established a new REC dataset characterized by two key\nfeatures: Firstly, it is designed with controllable varying levels of\ndifficulty, necessitating multi-level fine-grained reasoning across object\ncategories, attributes, and multi-hop relationships. Secondly, it includes\nnegative text and images created through fine-grained editing and generation\nbased on existing data, thereby testing the model's ability to correctly reject\nscenarios where the target object is not visible in the image--an essential\naspect often overlooked in existing datasets and approaches. Utilizing this\nhigh-quality dataset, we conducted comprehensive evaluations of both\nstate-of-the-art specialist models and MLLMs. Our findings indicate that there\nremains a significant gap in achieving satisfactory grounding performance. We\nanticipate that our dataset will inspire new approaches to enhance visual\nreasoning and develop more advanced cross-modal interaction strategies,\nultimately unlocking the full potential of MLLMs. Our code and the datasets are\navailable at https://github.com/liujunzhuo/FineCops-Ref.\n","authors":["Junzhuo Liu","Xuzheng Yang","Weiwei Li","Peng Wang"],"pdf_url":"https://arxiv.org/pdf/2409.14750v2.pdf","comment":"18 pages, EMNLP 2024 main"},{"id":"http://arxiv.org/abs/2501.06564v1","updated":"2025-01-11T15:02:49Z","published":"2025-01-11T15:02:49Z","title":"Natural Language Processing and Deep Learning Models to Classify Phase\n  of Flight in Aviation Safety Occurrences","summary":"  The air transport system recognizes the criticality of safety, as even minor\nanomalies can have severe consequences. Reporting accidents and incidents play\na vital role in identifying their causes and proposing safety recommendations.\nHowever, the narratives describing pre-accident events are presented in\nunstructured text that is not easily understood by computer systems.\nClassifying and categorizing safety occurrences based on these narratives can\nsupport informed decision-making by aviation industry stakeholders. In this\nstudy, researchers applied natural language processing (NLP) and artificial\nintelligence (AI) models to process text narratives to classify the flight\nphases of safety occurrences. The classification performance of two deep\nlearning models, ResNet and sRNN was evaluated, using an initial dataset of\n27,000 safety occurrence reports from the NTSB. The results demonstrated good\nperformance, with both models achieving an accuracy exceeding 68%, well above\nthe random guess rate of 14% for a seven-class classification problem. The\nmodels also exhibited high precision, recall, and F1 scores. The sRNN model\ngreatly outperformed the simplified ResNet model architecture used in this\nstudy. These findings indicate that NLP and deep learning models can infer the\nflight phase from raw text narratives, enabling effective analysis of safety\noccurrences.\n","authors":["Aziida Nanyonga","Hassan Wasswa","Oleksandra Molloy","Ugur Turhan","Graham Wild"],"pdf_url":"https://arxiv.org/pdf/2501.06564v1.pdf","comment":"NLP, Aviation reports, Text analysis, Deep learning algorithms,\n  Flight phase classification"},{"id":"http://arxiv.org/abs/2501.06557v1","updated":"2025-01-11T14:33:57Z","published":"2025-01-11T14:33:57Z","title":"A Survey on Spoken Italian Datasets and Corpora","summary":"  Spoken language datasets are vital for advancing linguistic research, Natural\nLanguage Processing, and speech technology. However, resources dedicated to\nItalian, a linguistically rich and diverse Romance language, remain\nunderexplored compared to major languages like English or Mandarin. This survey\nprovides a comprehensive analysis of 66 spoken Italian datasets, highlighting\ntheir characteristics, methodologies, and applications. The datasets are\ncategorized by speech type, source and context, and demographic and linguistic\nfeatures, with a focus on their utility in fields such as Automatic Speech\nRecognition, emotion detection, and education. Challenges related to dataset\nscarcity, representativeness, and accessibility are discussed alongside\nrecommendations for enhancing dataset creation and utilization. The full\ndataset inventory is publicly accessible via GitHub and archived on Zenodo,\nserving as a valuable resource for researchers and developers. By addressing\ncurrent gaps and proposing future directions, this work aims to support the\nadvancement of Italian speech technologies and linguistic research.\n","authors":["Marco Giordano","Claudia Rinaldi"],"pdf_url":"https://arxiv.org/pdf/2501.06557v1.pdf","comment":"submitted to IEEE Access Journal in Dec 2024"},{"id":"http://arxiv.org/abs/2412.14965v2","updated":"2025-01-11T14:08:22Z","published":"2024-12-19T15:44:04Z","title":"Movie2Story: A framework for understanding videos and telling stories in\n  the form of novel text","summary":"  In recent years, large-scale models have achieved significant advancements,\naccompanied by the emergence of numerous high-quality benchmarks for evaluating\nvarious aspects of their comprehension abilities. However, most existing\nbenchmarks primarily focus on spatial understanding in static image tasks.\nWhile some benchmarks extend evaluations to temporal tasks, they fall short in\nassessing text generation under complex contexts involving long videos and rich\nauxiliary information. To address this limitation, we propose a novel\nbenchmark: the Multi-modal Story Generation Benchmark (MSBench), designed to\nevaluate text generation capabilities in scenarios enriched with auxiliary\ninformation. Our work introduces an innovative automatic dataset generation\nmethod to ensure the availability of accurate auxiliary information. On one\nhand, we leverage existing datasets and apply automated processes to generate\nnew evaluation datasets, significantly reducing manual efforts. On the other\nhand, we refine auxiliary data through systematic filtering and utilize\nstate-of-the-art models to ensure the fairness and accuracy of the ground-truth\ndatasets. Our experiments reveal that current Multi-modal Large Language Models\n(MLLMs) perform suboptimally under the proposed evaluation metrics,\nhighlighting significant gaps in their capabilities. To address these\nchallenges, we propose a novel model architecture and methodology to better\nhandle the overall process, demonstrating improvements on our benchmark.\n","authors":["Kangning Li","Zheyang Jia","Anyu Ying"],"pdf_url":"https://arxiv.org/pdf/2412.14965v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20597v3","updated":"2025-01-11T13:49:03Z","published":"2024-12-29T22:02:00Z","title":"GliLem: Leveraging GliNER for Contextualized Lemmatization in Estonian","summary":"  We present GliLem -- a novel hybrid lemmatization system for Estonian that\nenhances the highly accurate rule-based morphological analyzer Vabamorf with an\nexternal disambiguation module based on GliNER -- an open vocabulary NER model\nthat is able to match text spans with text labels in natural language. We\nleverage the flexibility of a pre-trained GliNER model to improve the\nlemmatization accuracy of Vabamorf by 10% compared to its original\ndisambiguation module and achieve an improvement over the token\nclassification-based baseline. To measure the impact of improvements in\nlemmatization accuracy on the information retrieval downstream task, we first\ncreated an information retrieval dataset for Estonian by automatically\ntranslating the DBpedia-Entity dataset from English. We benchmark several token\nnormalization approaches, including lemmatization, on the created dataset using\nthe BM25 algorithm. We observe a substantial improvement in IR metrics when\nusing lemmatization over simplistic stemming. The benefits of improving lemma\ndisambiguation accuracy manifest in small but consistent improvement in the IR\nrecall measure, especially in the setting of high k.\n","authors":["Aleksei Dorkin","Kairit Sirts"],"pdf_url":"https://arxiv.org/pdf/2412.20597v3.pdf","comment":"Accepted to NoDaLiDa/Baltic-HLT 2025. Minor presentation and\n  formatting fixes"},{"id":"http://arxiv.org/abs/2409.10673v2","updated":"2025-01-11T13:11:03Z","published":"2024-09-16T19:14:35Z","title":"A Bayesian Interpretation of Adaptive Low-Rank Adaptation","summary":"  Motivated by the sensitivity-based importance score of the adaptive low-rank\nadaptation (AdaLoRA), we utilize more theoretically supported metrics,\nincluding the signal-to-noise ratio (SNR), along with the Improved Variational\nOnline Newton (IVON) optimizer, for adaptive parameter budget allocation. The\nresulting Bayesian counterpart not only has matched or surpassed the\nperformance of using the sensitivity-based importance metric but is also a\nfaster alternative to AdaLoRA with Adam. Our theoretical analysis reveals a\nsignificant connection between the two metrics, providing a Bayesian\nperspective on the efficacy of sensitivity as an importance score. Furthermore,\nour findings suggest that the magnitude, rather than the variance, is the\nprimary indicator of the importance of parameters.\n","authors":["Haolin Chen","Philip N. Garner"],"pdf_url":"https://arxiv.org/pdf/2409.10673v2.pdf","comment":"ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.06536v1","updated":"2025-01-11T12:56:27Z","published":"2025-01-11T12:56:27Z","title":"Dispersion Measures as Predictors of Lexical Decision Time, Word\n  Familiarity, and Lexical Complexity","summary":"  Various measures of dispersion have been proposed to paint a fuller picture\nof a word's distribution in a corpus, but only little has been done to validate\nthem externally. We evaluate a wide range of dispersion measures as predictors\nof lexical decision time, word familiarity, and lexical complexity in five\ndiverse languages. We find that the logarithm of range is not only a better\npredictor than log-frequency across all tasks and languages, but that it is\nalso the most powerful additional variable to log-frequency, consistently\noutperforming the more complex dispersion measures. We discuss the effects of\ncorpus part granularity and logarithmic transformation, shedding light on\ncontradictory results of previous studies.\n","authors":["Adam Nohejl","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2501.06536v1.pdf","comment":"Pre-print, to be presented at the NLP Meeting 2025 (www.anlp.jp -\n  NON-REVIEWED)"},{"id":"http://arxiv.org/abs/2501.06521v1","updated":"2025-01-11T12:08:15Z","published":"2025-01-11T12:08:15Z","title":"Fine-tuning Large Language Models for Improving Factuality in Legal\n  Question Answering","summary":"  Hallucination, or the generation of incorrect or fabricated information,\nremains a critical challenge in large language models (LLMs), particularly in\nhigh-stake domains such as legal question answering (QA). In order to mitigate\nthe hallucination rate in legal QA, we first introduce a benchmark called\nLegalHalBench and three automatic metrics to evaluate the common hallucinations\nwhen LLMs answer legal questions. We then propose a hallucination mitigation\nmethod that integrates behavior cloning and a novel Hard Sample-aware Iterative\nDirect Preference Optimization (HIPO). We conduct extensive real-data\nexperiments to validate the effectiveness of our approach. Our results\ndemonstrate remarkable improvements in various metrics, including the newly\nproposed Non-Hallucinated Statute Rate, Statute Relevance Rate, Legal Claim\nTruthfulness, as well as traditional metrics such as METEOR, BERTScore,\nROUGE-L, and win rates.\n","authors":["Yinghao Hu","Leilei Gan","Wenyi Xiao","Kun Kuang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2501.06521v1.pdf","comment":"18 pages, 8 figures, to be published in COLING 2025"},{"id":"http://arxiv.org/abs/2411.16527v2","updated":"2025-01-11T11:46:51Z","published":"2024-11-25T16:14:45Z","title":"Profiling Bias in LLMs: Stereotype Dimensions in Contextual Word\n  Embeddings","summary":"  Large language models (LLMs) are the foundation of the current successes of\nartificial intelligence (AI), however, they are unavoidably biased. To\neffectively communicate the risks and encourage mitigation efforts these models\nneed adequate and intuitive descriptions of their discriminatory properties,\nappropriate for all audiences of AI. We suggest bias profiles with respect to\nstereotype dimensions based on dictionaries from social psychology research.\nAlong these dimensions we investigate gender bias in contextual embeddings,\nacross contexts and layers, and generate stereotype profiles for twelve\ndifferent LLMs, demonstrating their intuition and use case for exposing and\nvisualizing bias.\n","authors":["Carolin M. Schuster","Maria-Alexandra Dinisor","Shashwat Ghatiwala","Georg Groh"],"pdf_url":"https://arxiv.org/pdf/2411.16527v2.pdf","comment":"Accepted to NoDaLiDa/Baltic-HLT 2025"},{"id":"http://arxiv.org/abs/2501.06497v1","updated":"2025-01-11T10:22:04Z","published":"2025-01-11T10:22:04Z","title":"PASS: Presentation Automation for Slide Generation and Speech","summary":"  In today's fast-paced world, effective presentations have become an essential\ntool for communication in both online and offline meetings. The crafting of a\ncompelling presentation requires significant time and effort, from gathering\nkey insights to designing slides that convey information clearly and concisely.\nHowever, despite the wealth of resources available, people often find\nthemselves manually extracting crucial points, analyzing data, and organizing\ncontent in a way that ensures clarity and impact. Furthermore, a successful\npresentation goes beyond just the slides; it demands rehearsal and the ability\nto weave a captivating narrative to fully engage the audience. Although there\nhas been some exploration of automating document-to-slide generation, existing\nresearch is largely centered on converting research papers. In addition,\nautomation of the delivery of these presentations has yet to be addressed. We\nintroduce PASS, a pipeline used to generate slides from general Word documents,\ngoing beyond just research papers, which also automates the oral delivery of\nthe generated slides. PASS analyzes user documents to create a dynamic,\nengaging presentation with an AI-generated voice. Additionally, we developed an\nLLM-based evaluation metric to assess our pipeline across three critical\ndimensions of presentations: relevance, coherence, and redundancy. The data and\ncodes are available at https://github.com/AggarwalTushar/PASS.\n","authors":["Tushar Aggarwal","Aarohi Bhand"],"pdf_url":"https://arxiv.org/pdf/2501.06497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13469v2","updated":"2025-01-11T10:20:26Z","published":"2024-06-19T11:50:09Z","title":"Encoder vs Decoder: Comparative Analysis of Encoder and Decoder Language\n  Models on Multilingual NLU Tasks","summary":"  This paper explores the performance of encoder and decoder language models on\nmultilingual Natural Language Understanding (NLU) tasks, with a broad focus on\nGermanic languages. Building upon the ScandEval benchmark, initially restricted\nto evaluating encoder models, we extend the evaluation framework to include\ndecoder models. We introduce a method for evaluating decoder models on NLU\ntasks and apply it to the languages Danish, Swedish, Norwegian, Icelandic,\nFaroese, German, Dutch, and English. Through a series of experiments and\nanalyses, we also address research questions regarding the comparative\nperformance of encoder and decoder models, the impact of NLU task types, and\nthe variation across language resources. Our findings reveal that encoder\nmodels can achieve significantly better NLU performance than decoder models\ndespite having orders of magnitude fewer parameters. Additionally, we\ninvestigate the correlation between decoders and task performance via a UMAP\nanalysis, shedding light on the unique capabilities of decoder and encoder\nmodels. This study contributes to a deeper understanding of language model\nparadigms in NLU tasks and provides valuable insights for model selection and\nevaluation in multilingual settings.\n","authors":["Dan Saattrup Nielsen","Kenneth Enevoldsen","Peter Schneider-Kamp"],"pdf_url":"https://arxiv.org/pdf/2406.13469v2.pdf","comment":"NoDaLiDa 2025 camera ready version, including appendices"},{"id":"http://arxiv.org/abs/2501.06496v1","updated":"2025-01-11T10:11:19Z","published":"2025-01-11T10:11:19Z","title":"Analyzing the Role of Context in Forecasting with Large Language Models","summary":"  This study evaluates the forecasting performance of recent language models\n(LLMs) on binary forecasting questions. We first introduce a novel dataset of\nover 600 binary forecasting questions, augmented with related news articles and\ntheir concise question-related summaries. We then explore the impact of input\nprompts with varying level of context on forecasting performance. The results\nindicate that incorporating news articles significantly improves performance,\nwhile using few-shot examples leads to a decline in accuracy. We find that\nlarger models consistently outperform smaller models, highlighting the\npotential of LLMs in enhancing automated forecasting.\n","authors":["Gerrit Mutschlechner","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2501.06496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06490v1","updated":"2025-01-11T09:23:55Z","published":"2025-01-11T09:23:55Z","title":"Sequential Classification of Aviation Safety Occurrences with Natural\n  Language Processing","summary":"  Safety is a critical aspect of the air transport system given even slight\noperational anomalies can result in serious consequences. To reduce the chances\nof aviation safety occurrences, accidents and incidents are reported to\nestablish the root cause, propose safety recommendations etc. However, analysis\nnarratives of the pre-accident events are presented using human-understandable,\nraw, unstructured, text that a computer system cannot understand. The ability\nto classify and categorise safety occurrences from their textual narratives\nwould help aviation industry stakeholders make informed safety-critical\ndecisions. To classify and categorise safety occurrences, we applied natural\nlanguage processing (NLP) and AI (Artificial Intelligence) models to process\ntext narratives. The study aimed to answer the question. How well can the\ndamage level caused to the aircraft in a safety occurrence be inferred from the\ntext narrative using natural language processing. The classification\nperformance of various deep learning models including LSTM, BLSTM, GRU, sRNN,\nand combinations of these models including LSTM and GRU, BLSTM+GRU, sRNN and\nLSTM, sRNN and BLSTM, sRNN and GRU, sRNN and BLSTM and GRU, and sRNN and LSTM\nand GRU was evaluated on a set of 27,000 safety occurrence reports from the\nNTSB. The results of this study indicate that all models investigated performed\ncompetitively well recording an accuracy of over 87.9% which is well above the\nrandom guess of 25% for a four-class classification problem. Also, the models\nrecorded high precision, recall, and F1 scores above 80%, 88%, and 85%,\nrespectively. sRNN slightly outperformed other single models in terms of recall\n(90%) and accuracy (90%) while LSTM reported slightly better performance in\nterms of precision (87%).\n","authors":["Aziida Nanyonga","Hassan Wasswa","Ugur Turhan","Oleksandra Molloy","Graham Wild"],"pdf_url":"https://arxiv.org/pdf/2501.06490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06478v1","updated":"2025-01-11T08:11:09Z","published":"2025-01-11T08:11:09Z","title":"Speech Recognition for Automatically Assessing Afrikaans and isiXhosa\n  Preschool Oral Narratives","summary":"  We develop automatic speech recognition (ASR) systems for stories told by\nAfrikaans and isiXhosa preschool children. Oral narratives provide a way to\nassess children's language development before they learn to read. We consider a\nrange of prior child-speech ASR strategies to determine which is best suited to\nthis unique setting. Using Whisper and only 5 minutes of transcribed in-domain\nchild speech, we find that additional in-domain adult data (adult speech\nmatching the story domain) provides the biggest improvement, especially when\ncoupled with voice conversion. Semi-supervised learning also helps for both\nlanguages, while parameter-efficient fine-tuning helps on Afrikaans but not on\nisiXhosa (which is under-represented in the Whisper model). Few child-speech\nstudies look at non-English data, and even fewer at the preschool ages of 4 and\n5. Our work therefore represents a unique validation of a wide range of\nprevious child-speech ASR strategies in an under-explored setting.\n","authors":["Christiaan Jacobs","Annelien Smith","Daleen Klop","Ondřej Klejch","Febe de Wet","Herman Kamper"],"pdf_url":"https://arxiv.org/pdf/2501.06478v1.pdf","comment":"Accepted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2407.08195v2","updated":"2025-01-11T07:53:25Z","published":"2024-07-11T05:33:19Z","title":"A Text-to-Game Engine for UGC-Based Role-Playing Games","summary":"  The transition from professionally generated content (PGC) to user-generated\ncontent (UGC) has reshaped various media formats, encompassing formats such as\ntext and video. With rapid advancements in generative AI, a similar\ntransformation is set to redefine the gaming industry, particularly within the\ndomain of role-playing games (RPGs). This paper introduces a novel framework\nfor a text-to-game engine that leverages foundation models to transform simple\ntextual inputs into intricate, multi-modal RPG experiences. The engine\ndynamically generates game narratives, integrating text, visuals, and\nmechanics, while adapting characters, environments, and gameplay in realtime\nbased on player interactions. To evaluate and demonstrate the feasibility and\nversatility of this framework, we developed the 'Zagii' game engine. Zagii has\nsuccessfully powered hundreds of RPG games across diverse genres and\nfacilitated tens of thousands of online gameplay sessions, showcasing its\nscalability and adaptability. These results highlight the framework's\neffectiveness and its potential to foster a more open and democratized approach\nto game development. Our work underscores the transformative role of generative\nAI in reshaping the gaming lifecycle and advancing the boundaries of\ninteractive entertainment.\n","authors":["Lei Zhang","Xuezheng Peng","Shuyi Yang","Feiyang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.08195v2.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2501.06468v1","updated":"2025-01-11T07:47:31Z","published":"2025-01-11T07:47:31Z","title":"First Token Probability Guided RAG for Telecom Question Answering","summary":"  Large Language Models (LLMs) have garnered significant attention for their\nimpressive general-purpose capabilities. For applications requiring intricate\ndomain knowledge, Retrieval-Augmented Generation (RAG) has shown a distinct\nadvantage in incorporating domain-specific information into LLMs. However,\nexisting RAG research has not fully addressed the challenges of Multiple Choice\nQuestion Answering (MCQA) in telecommunications, particularly in terms of\nretrieval quality and mitigating hallucinations. To tackle these challenges, we\npropose a novel first token probability guided RAG framework. This framework\nleverages confidence scores to optimize key hyperparameters, such as chunk\nnumber and chunk window size, while dynamically adjusting the context. Our\nmethod starts by retrieving the most relevant chunks and generates a single\ntoken as the potential answer. The probabilities of all options are then\nnormalized to serve as confidence scores, which guide the dynamic adjustment of\nthe context. By iteratively optimizing the hyperparameters based on these\nconfidence scores, we can continuously improve RAG performance. We conducted\nexperiments to validate the effectiveness of our framework, demonstrating its\npotential to enhance accuracy in domain-specific MCQA tasks.\n","authors":["Tingwei Chen","Jiayi Chen","Zijian Zhao","Haolong Chen","Liang Zhang","Guangxu Zhu"],"pdf_url":"https://arxiv.org/pdf/2501.06468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06467v1","updated":"2025-01-11T07:43:18Z","published":"2025-01-11T07:43:18Z","title":"Retrieval-Augmented Dialogue Knowledge Aggregation for Expressive\n  Conversational Speech Synthesis","summary":"  Conversational speech synthesis (CSS) aims to take the current dialogue (CD)\nhistory as a reference to synthesize expressive speech that aligns with the\nconversational style. Unlike CD, stored dialogue (SD) contains preserved\ndialogue fragments from earlier stages of user-agent interaction, which include\nstyle expression knowledge relevant to scenarios similar to those in CD. Note\nthat this knowledge plays a significant role in enabling the agent to\nsynthesize expressive conversational speech that generates empathetic feedback.\nHowever, prior research has overlooked this aspect. To address this issue, we\npropose a novel Retrieval-Augmented Dialogue Knowledge Aggregation scheme for\nexpressive CSS, termed RADKA-CSS, which includes three main components: 1) To\neffectively retrieve dialogues from SD that are similar to CD in terms of both\nsemantic and style. First, we build a stored dialogue semantic-style database\n(SDSSD) which includes the text and audio samples. Then, we design a\nmulti-attribute retrieval scheme to match the dialogue semantic and style\nvectors of the CD with the stored dialogue semantic and style vectors in the\nSDSSD, retrieving the most similar dialogues. 2) To effectively utilize the\nstyle knowledge from CD and SD, we propose adopting the multi-granularity graph\nstructure to encode the dialogue and introducing a multi-source style knowledge\naggregation mechanism. 3) Finally, the aggregated style knowledge are fed into\nthe speech synthesizer to help the agent synthesize expressive speech that\naligns with the conversational style. We conducted a comprehensive and in-depth\nexperiment based on the DailyTalk dataset, which is a benchmarking dataset for\nthe CSS task.\n  Both objective and subjective evaluations demonstrate that RADKA-CSS\noutperforms baseline models in expressiveness rendering. Code and audio samples\ncan be found at: https://github.com/Coder-jzq/RADKA-CSS.\n","authors":["Rui Liu","Zhenqi Jia","Feilong Bao","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2501.06467v1.pdf","comment":"Accepted by Information Fusion 2025"},{"id":"http://arxiv.org/abs/2501.06465v1","updated":"2025-01-11T07:35:51Z","published":"2025-01-11T07:35:51Z","title":"MedCT: A Clinical Terminology Graph for Generative AI Applications in\n  Healthcare","summary":"  We introduce the world's first clinical terminology for the Chinese\nhealthcare community, namely MedCT, accompanied by a clinical foundation model\nMedBERT and an entity linking model MedLink. The MedCT system enables\nstandardized and programmable representation of Chinese clinical data,\nsuccessively stimulating the development of new medicines, treatment pathways,\nand better patient outcomes for the populous Chinese community. Moreover, the\nMedCT knowledge graph provides a principled mechanism to minimize the\nhallucination problem of large language models (LLMs), therefore achieving\nsignificant levels of accuracy and safety in LLM-based clinical applications.\nBy leveraging the LLMs' emergent capabilities of generativeness and\nexpressiveness, we were able to rapidly built a production-quality terminology\nsystem and deployed to real-world clinical field within three months, while\nclassical terminologies like SNOMED CT have gone through more than twenty years\ndevelopment. Our experiments show that the MedCT system achieves\nstate-of-the-art (SOTA) performance in semantic matching and entity linking\ntasks, not only for Chinese but also for English. We also conducted a\nlongitudinal field experiment by applying MedCT and LLMs in a representative\nspectrum of clinical tasks, including electronic health record (EHR)\nauto-generation and medical document search for diagnostic decision making. Our\nstudy shows a multitude of values of MedCT for clinical workflows and patient\noutcomes, especially in the new genre of clinical LLM applications. We present\nour approach in sufficient engineering detail, such that implementing a\nclinical terminology for other non-English societies should be readily\nreproducible. We openly release our terminology, models and algorithms, along\nwith real-world clinical datasets for the development.\n","authors":["Ye Chen","Dongdong Huang","Haoyun Xu","Cong Fu","Lin Sheng","Qingli Zhou","Yuqiang Shen","Kai Wang"],"pdf_url":"https://arxiv.org/pdf/2501.06465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03240v2","updated":"2025-01-11T07:24:54Z","published":"2024-10-04T09:04:20Z","title":"Beyond Film Subtitles: Is YouTube the Best Approximation of Spoken\n  Vocabulary?","summary":"  Word frequency is a key variable in psycholinguistics, useful for modeling\nhuman familiarity with words even in the era of large language models (LLMs).\nFrequency in film subtitles has proved to be a particularly good approximation\nof everyday language exposure. For many languages, however, film subtitles are\nnot easily available, or are overwhelmingly translated from English. We\ndemonstrate that frequencies extracted from carefully processed YouTube\nsubtitles provide an approximation comparable to, and often better than, the\nbest currently available resources. Moreover, they are available for languages\nfor which a high-quality subtitle or speech corpus does not exist. We use\nYouTube subtitles to construct frequency norms for five diverse languages,\nChinese, English, Indonesian, Japanese, and Spanish, and evaluate their\ncorrelation with lexical decision time, word familiarity, and lexical\ncomplexity. In addition to being strongly correlated with two psycholinguistic\nvariables, a simple linear regression on the new frequencies achieves a new\nhigh score on a lexical complexity prediction task in English and Japanese,\nsurpassing both models trained on film subtitle frequencies and the LLM GPT-4.\nOur code, the frequency lists, fastText word embeddings, and statistical\nlanguage models are freely available at https://github.com/naist-nlp/tubelex.\n","authors":["Adam Nohejl","Frederikus Hudi","Eunike Andriani Kardinata","Shintaro Ozaki","Maria Angelica Riera Machin","Hongyu Sun","Justin Vasselli","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2410.03240v2.pdf","comment":"Accepted to COLING 2025. 9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2501.06458v1","updated":"2025-01-11T07:10:23Z","published":"2025-01-11T07:10:23Z","title":"O1 Replication Journey -- Part 3: Inference-time Scaling for Medical\n  Reasoning","summary":"  Building upon our previous investigations of O1 replication (Part 1: Journey\nLearning [Qin et al., 2024] and Part 2: Distillation [Huang et al., 2024]),\nthis work explores the potential of inference-time scaling in large language\nmodels (LLMs) for medical reasoning tasks, ranging from diagnostic\ndecision-making to treatment planning. Through extensive experiments on medical\nbenchmarks of varying complexity (MedQA, Medbullets, and JAMA Clinical\nChallenges), our investigation reveals several key insights: (1) Increasing\ninference time does lead to improved performance. With a modest training set of\n500 samples, our model yields substantial performance improvements of 6%-11%.\n(2) Task complexity directly correlates with the required length of reasoning\nchains, confirming the necessity of extended thought processes for challenging\nproblems. (3) The differential diagnoses generated by our model adhere to the\nprinciples of the hypothetico-deductive method, producing a list of potential\nconditions that may explain a patient's symptoms and systematically narrowing\nthese possibilities by evaluating the evidence. These findings demonstrate the\npromising synergy between inference-time scaling and journey learning in\nadvancing LLMs' real-world clinical reasoning capabilities.\n","authors":["Zhongzhen Huang","Gui Geng","Shengyi Hua","Zhen Huang","Haoyang Zou","Shaoting Zhang","Pengfei Liu","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.06458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06434v1","updated":"2025-01-11T04:31:18Z","published":"2025-01-11T04:31:18Z","title":"Synthetic Feature Augmentation Improves Generalization Performance of\n  Language Models","summary":"  Training and fine-tuning deep learning models, especially large language\nmodels (LLMs), on limited and imbalanced datasets poses substantial challenges.\nThese issues often result in poor generalization, where models overfit to\ndominant classes and underperform on minority classes, leading to biased\npredictions and reduced robustness in real-world applications. To overcome\nthese challenges, we propose augmenting features in the embedding space by\ngenerating synthetic samples using a range of techniques. By upsampling\nunderrepresented classes, this method improves model performance and alleviates\ndata imbalance. We validate the effectiveness of this approach across multiple\nopen-source text classification benchmarks, demonstrating its potential to\nenhance model robustness and generalization in imbalanced data scenarios.\n","authors":["Ashok Choudhary","Cornelius Thiels","Hojjat Salehinejad"],"pdf_url":"https://arxiv.org/pdf/2501.06434v1.pdf","comment":"Accepted for presentation at IEEE SSCI 2025"},{"id":"http://arxiv.org/abs/2501.01034v2","updated":"2025-01-11T03:47:08Z","published":"2025-01-02T03:28:52Z","title":"Advancing Singlish Understanding: Bridging the Gap with Datasets and\n  Multimodal Models","summary":"  Singlish, a Creole language rooted in English, is a key focus in linguistic\nresearch within multilingual and multicultural contexts. However, its spoken\nform remains underexplored, limiting insights into its linguistic structure and\napplications. To address this gap, we standardize and annotate the largest\nspoken Singlish corpus, introducing the Multitask National Speech Corpus\n(MNSC). These datasets support diverse tasks, including Automatic Speech\nRecognition (ASR), Spoken Question Answering (SQA), Spoken Dialogue\nSummarization (SDS), and Paralinguistic Question Answering (PQA). We release\nstandardized splits and a human-verified test set to facilitate further\nresearch. Additionally, we propose SingAudioLLM, a multi-task multimodal model\nleveraging multimodal large language models to handle these tasks concurrently.\nExperiments reveal our models adaptability to Singlish context, achieving\nstate-of-the-art performance and outperforming prior models by 10-30% in\ncomparison with other AudioLLMs and cascaded solutions.\n","authors":["Bin Wang","Xunlong Zou","Shuo Sun","Wenyu Zhang","Yingxu He","Zhuohan Liu","Chengwei Wei","Nancy F. Chen","AiTi Aw"],"pdf_url":"https://arxiv.org/pdf/2501.01034v2.pdf","comment":"Open-Source: https://github.com/AudioLLMs/Singlish"},{"id":"http://arxiv.org/abs/2501.06425v1","updated":"2025-01-11T03:37:10Z","published":"2025-01-11T03:37:10Z","title":"Tensor Product Attention Is All You Need","summary":"  Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.\n","authors":["Yifan Zhang","Yifeng Liu","Huizhuo Yuan","Zhen Qin","Yang Yuan","Quanquan Gu","Andrew Chi-Chih Yao"],"pdf_url":"https://arxiv.org/pdf/2501.06425v1.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2411.06989v2","updated":"2025-01-11T03:06:07Z","published":"2024-11-11T13:48:01Z","title":"The Backpropagation of the Wave Network","summary":"  This paper provides an in-depth analysis of Wave Network, a novel token\nrepresentation method derived from the Wave Network, designed to capture both\nglobal and local semantics of input text through wave-inspired complex vectors.\nIn complex vector token representation, each token is represented with a\nmagnitude component, capturing the global semantics of the entire input text,\nand a phase component, encoding the relationships between individual tokens and\nthe global semantics. Building on prior research that demonstrated the\neffectiveness of wave-like operations, such as interference and modulation,\nduring forward propagation, this study investigates the convergence behavior,\nbackpropagation characteristics, and embedding independence within the\nToken2Wave framework. A detailed computational complexity analysis shows that\nToken2Wave can significantly reduce video memory usage and training time\ncompared to BERT. Gradient comparisons for the [CLS] token, total input text,\nand classifier parameters further highlight Token2Wave's unique\ncharacteristics. This research offers new insights into wave-based token\nrepresentations, demonstrating their potential to enable efficient and\ncomputationally friendly language model architectures.\n","authors":["Xin Zhang","Victor S. Sheng"],"pdf_url":"https://arxiv.org/pdf/2411.06989v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17812v3","updated":"2025-01-11T00:02:54Z","published":"2024-02-27T14:51:11Z","title":"DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping\n  Backward Propagation","summary":"  Large language models (LLMs) have achieved significant success across various\ndomains. However, training these LLMs typically involves substantial memory and\ncomputational costs during both forward and backward propagation. While\nparameter-efficient fine-tuning (PEFT) considerably reduces the training memory\nassociated with parameters, it does not address the significant computational\ncosts and activation memory. In this paper, we propose Dropping Backward\nPropagation (DropBP), a novel approach designed to reduce computational costs\nand activation memory while maintaining accuracy. DropBP randomly drops layers\nduring backward propagation, which is essentially equivalent to training\nshallow submodules generated by undropped layers and residual connections.\nAdditionally, DropBP calculates the sensitivity of each layer to assign an\nappropriate drop rate, thereby stabilizing the training process. DropBP is not\nonly applicable to full fine-tuning but can also be orthogonally integrated\nwith all types of PEFT by dropping layers during backward propagation.\nSpecifically, DropBP can reduce training time by 44% with comparable accuracy\nto the baseline, accelerate convergence to the same perplexity by 1.5x, and\nenable training with a sequence length 6.2x larger on a single NVIDIA-A100 GPU.\nFurthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100\nGPU and 117% on an Intel Gaudi2 HPU. The code is available at\nhttps://github.com/WooSunghyeon/dropbp.\n","authors":["Sunghyeon Woo","Baeseong Park","Byeongwook Kim","Minjung Jo","Se Jung Kwon","Dongsuk Jeon","Dongsoo Lee"],"pdf_url":"https://arxiv.org/pdf/2402.17812v3.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2501.06581v1","updated":"2025-01-11T16:34:10Z","published":"2025-01-11T16:34:10Z","title":"Recommending the right academic programs: An interest mining approach\n  using BERTopic","summary":"  Prospective students face the challenging task of selecting a university\nprogram that will shape their academic and professional careers. For\ndecision-makers and support services, it is often time-consuming and extremely\ndifficult to match personal interests with suitable programs due to the vast\nand complex catalogue information available. This paper presents the first\ninformation system that provides students with efficient recommendations based\non both program content and personal preferences. BERTopic, a powerful topic\nmodeling algorithm, is used that leverages text embedding techniques to\ngenerate topic representations. It enables us to mine interest topics from all\ncourse descriptions, representing the full body of knowledge taught at the\ninstitution. Underpinned by the student's individual choice of topics, a\nshortlist of the most relevant programs is computed through statistical\nbacktracking in the knowledge map, a novel characterization of the\nprogram-course relationship. This approach can be applied to a wide range of\neducational settings, including professional and vocational training. A case\nstudy at a post-secondary school with 80 programs and over 5,000 courses shows\nthat the system provides immediate and effective decision support. The\npresented interest topics are meaningful, leading to positive effects such as\nserendipity, personalization, and fairness, as revealed by a qualitative study\ninvolving 65 students. Over 98% of users indicated that the recommendations\naligned with their interests, and about 94% stated they would use the tool in\nthe future. Quantitative analysis shows the system can be configured to ensure\nfairness, achieving 98% program coverage while maintaining a personalization\nscore of 0.77. These findings suggest that this real-time, user-centered,\ndata-driven system could improve the program selection process.\n","authors":["Alessandro Hill","Kalen Goo","Puneet Agarwal"],"pdf_url":"https://arxiv.org/pdf/2501.06581v1.pdf","comment":"Accepted at Data Mining and Knowledge Discovery (Springer)"},{"id":"http://arxiv.org/abs/2402.01339v2","updated":"2025-01-11T15:07:26Z","published":"2024-02-02T11:52:07Z","title":"Improving Sequential Recommendations with LLMs","summary":"  The sequential recommendation problem has attracted considerable research\nattention in the past few years, leading to the rise of numerous recommendation\nmodels. In this work, we explore how Large Language Models (LLMs), which are\nnowadays introducing disruptive effects in many AI-based applications, can be\nused to build or improve sequential recommendation approaches. Specifically, we\ndesign three orthogonal approaches and hybrids of those to leverage the power\nof LLMs in different ways. In addition, we investigate the potential of each\napproach by focusing on its comprising technical aspects and determining an\narray of alternative choices for each one. We conduct extensive experiments on\nthree datasets and explore a large variety of configurations, including\ndifferent language models and baseline recommendation models, to obtain a\ncomprehensive picture of the performance of each approach. Among other\nobservations, we highlight that initializing state-of-the-art sequential\nrecommendation models such as BERT4Rec or SASRec with embeddings obtained from\nan LLM can lead to substantial performance gains in terms of accuracy.\nFurthermore, we find that fine-tuning an LLM for recommendation tasks enables\nit to learn not only the tasks, but also concepts of a domain to some extent.\nWe also show that fine-tuning OpenAI GPT leads to considerably better\nperformance than fine-tuning Google PaLM 2. Overall, our extensive experiments\nindicate a huge potential value of leveraging LLMs in future recommendation\napproaches. We publicly share the code and data of our experiments to ensure\nreproducibility.\n","authors":["Artun Boz","Wouter Zorgdrager","Zoe Kotti","Jesse Harte","Panos Louridas","Dietmar Jannach","Vassilios Karakoidas","Marios Fragkoulis"],"pdf_url":"https://arxiv.org/pdf/2402.01339v2.pdf","comment":"35 pages, 12 figures, 7 tables"},{"id":"http://arxiv.org/abs/2501.06496v1","updated":"2025-01-11T10:11:19Z","published":"2025-01-11T10:11:19Z","title":"Analyzing the Role of Context in Forecasting with Large Language Models","summary":"  This study evaluates the forecasting performance of recent language models\n(LLMs) on binary forecasting questions. We first introduce a novel dataset of\nover 600 binary forecasting questions, augmented with related news articles and\ntheir concise question-related summaries. We then explore the impact of input\nprompts with varying level of context on forecasting performance. The results\nindicate that incorporating news articles significantly improves performance,\nwhile using few-shot examples leads to a decline in accuracy. We find that\nlarger models consistently outperform smaller models, highlighting the\npotential of LLMs in enhancing automated forecasting.\n","authors":["Gerrit Mutschlechner","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2501.06496v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2501.03939v2","updated":"2025-01-11T14:21:37Z","published":"2025-01-07T17:00:35Z","title":"Visual question answering: from early developments to recent advances --\n  a survey","summary":"  Visual Question Answering (VQA) is an evolving research field aimed at\nenabling machines to answer questions about visual content by integrating image\nand language processing techniques such as feature extraction, object\ndetection, text embedding, natural language understanding, and language\ngeneration. With the growth of multimodal data research, VQA has gained\nsignificant attention due to its broad applications, including interactive\neducational tools, medical image diagnosis, customer service, entertainment,\nand social media captioning. Additionally, VQA plays a vital role in assisting\nvisually impaired individuals by generating descriptive content from images.\nThis survey introduces a taxonomy of VQA architectures, categorizing them based\non design choices and key components to facilitate comparative analysis and\nevaluation. We review major VQA approaches, focusing on deep learning-based\nmethods, and explore the emerging field of Large Visual Language Models (LVLMs)\nthat have demonstrated success in multimodal tasks like VQA. The paper further\nexamines available datasets and evaluation metrics essential for measuring VQA\nsystem performance, followed by an exploration of real-world VQA applications.\nFinally, we highlight ongoing challenges and future directions in VQA research,\npresenting open questions and potential areas for further development. This\nsurvey serves as a comprehensive resource for researchers and practitioners\ninterested in the latest advancements and future\n","authors":["Ngoc Dung Huynh","Mohamed Reda Bouadjenek","Sunil Aryal","Imran Razzak","Hakim Hacid"],"pdf_url":"https://arxiv.org/pdf/2501.03939v2.pdf","comment":"20 papers"},{"id":"http://arxiv.org/abs/2501.06488v1","updated":"2025-01-11T09:12:43Z","published":"2025-01-11T09:12:43Z","title":"NVS-SQA: Exploring Self-Supervised Quality Representation Learning for\n  Neurally Synthesized Scenes without References","summary":"  Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting,\neffectively creates photorealistic scenes from sparse viewpoints, typically\nevaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However,\nthese full-reference methods, which compare synthesized views to reference\nviews, may not fully capture the perceptual quality of neurally synthesized\nscenes (NSS), particularly due to the limited availability of dense reference\nviews. Furthermore, the challenges in acquiring human perceptual labels hinder\nthe creation of extensive labeled datasets, risking model overfitting and\nreduced generalizability. To address these issues, we propose NVS-SQA, a NSS\nquality assessment method to learn no-reference quality representations through\nself-supervision without reliance on human labels. Traditional self-supervised\nlearning predominantly relies on the \"same instance, similar representation\"\nassumption and extensive datasets. However, given that these conditions do not\napply in NSS quality assessment, we employ heuristic cues and quality scores as\nlearning objectives, along with a specialized contrastive pair preparation\nprocess to improve the effectiveness and efficiency of learning. The results\nshow that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e.,\non average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second\nbest) and even exceeds 16 full-reference methods across all evaluation metrics\n(i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).\n","authors":["Qiang Qu","Yiran Shen","Xiaoming Chen","Yuk Ying Chung","Weidong Cai","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2501.06488v1.pdf","comment":null}]},"2025-01-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2501.08328v1","updated":"2025-01-14T18:59:03Z","published":"2025-01-14T18:59:03Z","title":"PokerBench: Training Large Language Models to become Professional Poker\n  Players","summary":"  We introduce PokerBench - a benchmark for evaluating the poker-playing\nabilities of large language models (LLMs). As LLMs excel in traditional NLP\ntasks, their application to complex, strategic games like poker poses a new\nchallenge. Poker, an incomplete information game, demands a multitude of skills\nsuch as mathematics, reasoning, planning, strategy, and a deep understanding of\ngame theory and human psychology. This makes Poker the ideal next frontier for\nlarge language models. PokerBench consists of a comprehensive compilation of\n11,000 most important scenarios, split between pre-flop and post-flop play,\ndeveloped in collaboration with trained poker players. We evaluate prominent\nmodels including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models,\nfinding that all state-of-the-art LLMs underperform in playing optimal poker.\nHowever, after fine-tuning, these models show marked improvements. We validate\nPokerBench by having models with different scores compete with each other,\ndemonstrating that higher scores on PokerBench lead to higher win rates in\nactual poker games. Through gameplay between our fine-tuned model and GPT-4, we\nalso identify limitations of simple supervised fine-tuning for learning optimal\nplaying strategy, suggesting the need for more advanced methodologies for\neffectively training language models to excel in games. PokerBench thus\npresents a unique benchmark for a quick and reliable evaluation of the\npoker-playing ability of LLMs as well as a comprehensive benchmark to study the\nprogress of LLMs in complex game-playing scenarios. The dataset and code will\nbe made available at: \\url{https://github.com/pokerllm/pokerbench}.\n","authors":["Richard Zhuang","Akshat Gupta","Richard Yang","Aniket Rahane","Zhengyu Li","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2501.08328v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2405.06685v2","updated":"2025-01-14T18:58:42Z","published":"2024-05-06T12:54:41Z","title":"Multigenre AI-powered Story Composition","summary":"  This paper shows how to construct genre patterns, whose purpose is to guide\ninteractive story composition in a way that enforces thematic consistency. To\nstart the discussion we argue, based on previous seminal works, for the\nexistence of five fundamental genres, namely comedy, romance - in the sense of\nepic plots, flourishing since the twelfth century -, tragedy, satire, and\nmystery. To construct the patterns, a simple two-phase process is employed:\nfirst retrieving examples that match our genre characterizations, and then\napplying a form of most specific generalization to the groups of examples in\norder to find their commonalities. In both phases, AI agents are instrumental,\nwith our PatternTeller prototype being called to operate the story composition\nprocess, offering the opportunity to generate stories from a given premise of\nthe user, to be developed under the guidance of the chosen pattern and trying\nto accommodate the user's suggestions along the composition stages.\n","authors":["Edirlei Soares de Lima","Margot M. E. Neggers","Antonio L. Furtado"],"pdf_url":"https://arxiv.org/pdf/2405.06685v2.pdf","comment":"Added publication details to references that were published after the\n  submission of the previous version (references [18] and [19])"},{"id":"http://arxiv.org/abs/2501.08322v1","updated":"2025-01-14T18:55:35Z","published":"2025-01-14T18:55:35Z","title":"Exploring Robustness of Multilingual LLMs on Real-World Noisy Data","summary":"  Large Language Models (LLMs) are trained on Web data that might contain\nspelling errors made by humans. But do they become robust to similar real-world\nnoise? In this paper, we investigate the effect of real-world spelling mistakes\non the performance of 9 language models, with parameters ranging from 0.2B to\n13B, in 3 different NLP tasks, namely Natural Language Inference (NLI), Name\nEntity Recognition (NER), and Intent Classification (IC). We perform our\nexperiments on 6 different languages and build a dictionary of real-world noise\nfor them using the Wikipedia edit history. We show that the performance gap of\nthe studied models on the clean and noisy test data averaged across all the\ndatasets and languages ranges from 2.3 to 4.3 absolute percentage points. In\naddition, mT5 models, in general, show more robustness compared to BLOOM,\nFalcon, and BERT-like models. In particular, mT5 (13B), was the most robust on\naverage overall, across the 3 tasks, and in 4 of the 6 languages.\n","authors":["Amirhossein Aliakbarzadeh","Lucie Flek","Akbar Karimi"],"pdf_url":"https://arxiv.org/pdf/2501.08322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08319v1","updated":"2025-01-14T18:53:00Z","published":"2025-01-14T18:53:00Z","title":"Enhancing Automated Interpretability with Output-Centric Feature\n  Descriptions","summary":"  Automated interpretability pipelines generate natural language descriptions\nfor the concepts represented by features in large language models (LLMs), such\nas plants or the first word in a sentence. These descriptions are derived using\ninputs that activate the feature, which may be a dimension or a direction in\nthe model's representation space. However, identifying activating inputs is\ncostly, and the mechanistic role of a feature in model behavior is determined\nboth by how inputs cause a feature to activate and by how feature activation\naffects outputs. Using steering evaluations, we reveal that current pipelines\nprovide descriptions that fail to capture the causal effect of the feature on\noutputs. To fix this, we propose efficient, output-centric methods for\nautomatically generating feature descriptions. These methods use the tokens\nweighted higher after feature stimulation or the highest weight tokens after\napplying the vocabulary \"unembedding\" head directly to the feature. Our\noutput-centric descriptions better capture the causal effect of a feature on\nmodel outputs than input-centric descriptions, but combining the two leads to\nthe best performance on both input and output evaluations. Lastly, we show that\noutput-centric descriptions can be used to find inputs that activate features\npreviously thought to be \"dead\".\n","authors":["Yoav Gur-Arieh","Roy Mayan","Chen Agassy","Atticus Geiger","Mor Geva"],"pdf_url":"https://arxiv.org/pdf/2501.08319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08313v1","updated":"2025-01-14T18:50:05Z","published":"2025-01-14T18:50:05Z","title":"MiniMax-01: Scaling Foundation Models with Lightning Attention","summary":"  We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01,\nwhich are comparable to top-tier models while offering superior capabilities in\nprocessing longer contexts. The core lies in lightning attention and its\nefficient scaling. To maximize computational capacity, we integrate it with\nMixture of Experts (MoE), creating a model with 32 experts and 456 billion\ntotal parameters, of which 45.9 billion are activated for each token. We\ndevelop an optimized parallel strategy and highly efficient\ncomputation-communication overlap techniques for MoE and lightning attention.\nThis approach enables us to conduct efficient training and inference on models\nwith hundreds of billions of parameters across contexts spanning millions of\ntokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens\nduring training and extrapolate to 4 million tokens during inference at an\naffordable cost. Our vision-language model, MiniMax-VL-01 is built through\ncontinued training with 512 billion vision-language tokens. Experiments on both\nstandard and in-house benchmarks show that our models match the performance of\nstate-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32\ntimes longer context window. We publicly release MiniMax-01 at\nhttps://github.com/MiniMax-AI.\n","authors":[" MiniMax","Aonian Li","Bangwei Gong","Bo Yang","Boji Shan","Chang Liu","Cheng Zhu","Chunhao Zhang","Congchao Guo","Da Chen","Dong Li","Enwei Jiao","Gengxin Li","Guojun Zhang","Haohai Sun","Houze Dong","Jiadai Zhu","Jiaqi Zhuang","Jiayuan Song","Jin Zhu","Jingtao Han","Jingyang Li","Junbin Xie","Junhao Xu","Junjie Yan","Kaishun Zhang","Kecheng Xiao","Kexi Kang","Le Han","Leyang Wang","Lianfei Yu","Liheng Feng","Lin Zheng","Linbo Chai","Long Xing","Meizhi Ju","Mingyuan Chi","Mozhi Zhang","Peikai Huang","Pengcheng Niu","Pengfei Li","Pengyu Zhao","Qi Yang","Qidi Xu","Qiexiang Wang","Qin Wang","Qiuhui Li","Ruitao Leng","Shengmin Shi","Shuqi Yu","Sichen Li","Songquan Zhu","Tao Huang","Tianrun Liang","Weigao Sun","Weixuan Sun","Weiyu Cheng","Wenkai Li","Xiangjun Song","Xiao Su","Xiaodong Han","Xinjie Zhang","Xinzhu Hou","Xu Min","Xun Zou","Xuyang Shen","Yan Gong","Yingjie Zhu","Yipeng Zhou","Yiran Zhong","Yongyi Hu","Yuanxiang Fan","Yue Yu","Yufeng Yang","Yuhao Li","Yunan Huang","Yunji Li","Yunpeng Huang","Yunzhi Xu","Yuxin Mao","Zehan Li","Zekang Li","Zewei Tao","Zewen Ying","Zhaoyang Cong","Zhen Qin","Zhenhua Fan","Zhihang Yu","Zhuo Jiang","Zijia Wu"],"pdf_url":"https://arxiv.org/pdf/2501.08313v1.pdf","comment":"A technical report from MiniMax. The authors are listed in\n  alphabetical order. We open-sourced our MiniMax-01 at\n  https://github.com/MiniMax-AI"},{"id":"http://arxiv.org/abs/2501.08312v1","updated":"2025-01-14T18:50:00Z","published":"2025-01-14T18:50:00Z","title":"Everybody Likes to Sleep: A Computer-Assisted Comparison of Object\n  Naming Data from 30 Languages","summary":"  Object naming - the act of identifying an object with a word or a phrase - is\na fundamental skill in interpersonal communication, relevant to many\ndisciplines, such as psycholinguistics, cognitive linguistics, or language and\nvision research. Object naming datasets, which consist of concept lists with\npicture pairings, are used to gain insights into how humans access and select\nnames for objects in their surroundings and to study the cognitive processes\ninvolved in converting visual stimuli into semantic concepts. Unfortunately,\nobject naming datasets often lack transparency and have a highly idiosyncratic\nstructure. Our study tries to make current object naming data transparent and\ncomparable by using a multilingual, computer-assisted approach that links\nindividual items of object naming lists to unified concepts. Our current sample\nlinks 17 object naming datasets that cover 30 languages from 10 different\nlanguage families. We illustrate how the comparative dataset can be explored by\nsearching for concepts that recur across the majority of datasets and comparing\nthe conceptual spaces of covered object naming datasets with classical basic\nvocabulary lists from historical linguistics and linguistic typology. Our\nfindings can serve as a basis for enhancing cross-linguistic object naming\nresearch and as a guideline for future studies dealing with object naming\ntasks.\n","authors":["Alžběta Kučerová","Johann-Mattis List"],"pdf_url":"https://arxiv.org/pdf/2501.08312v1.pdf","comment":"To appear in the Proceedings of the Global WordNet Conference 2025"},{"id":"http://arxiv.org/abs/2501.08296v1","updated":"2025-01-14T18:25:07Z","published":"2025-01-14T18:25:07Z","title":"A Survey on Pedophile Attribution Techniques for Online Platforms","summary":"  Reliance on anonymity in social media has increased its popularity on these\nplatforms among all ages. The availability of public Wi-Fi networks has\nfacilitated a vast variety of online content, including social media\napplications. Although anonymity and ease of access can be a convenient means\nof communication for their users, it is difficult to manage and protect its\nvulnerable users against sexual predators. Using an automated identification\nsystem that can attribute predators to their text would make the solution more\nattainable. In this survey, we provide a review of the methods of pedophile\nattribution used in social media platforms. We examine the effect of the size\nof the suspect set and the length of the text on the task of attribution.\nMoreover, we review the most-used datasets, features, classification techniques\nand performance measures for attributing sexual predators. We found that few\nstudies have proposed tools to mitigate the risk of online sexual predators,\nbut none of them can provide suspect attribution. Finally, we list several open\nresearch problems.\n","authors":["Hiba Fallatah","Ching Suen","Olga Ormandjieva"],"pdf_url":"https://arxiv.org/pdf/2501.08296v1.pdf","comment":"17 pages, 3 figures"},{"id":"http://arxiv.org/abs/2501.08292v1","updated":"2025-01-14T18:13:08Z","published":"2025-01-14T18:13:08Z","title":"HALoGEN: Fantastic LLM Hallucinations and Where to Find Them","summary":"  Despite their impressive ability to generate high-quality and fluent text,\ngenerative large language models (LLMs) also produce hallucinations: statements\nthat are misaligned with established world knowledge or provided input context.\nHowever, measuring hallucination can be challenging, as having humans verify\nmodel generations on-the-fly is both expensive and time-consuming. In this\nwork, we release HALoGEN, a comprehensive hallucination benchmark consisting\nof: (1) 10,923 prompts for generative models spanning nine domains including\nprogramming, scientific attribution, and summarization, and (2) automatic\nhigh-precision verifiers for each use case that decompose LLM generations into\natomic units, and verify each unit against a high-quality knowledge source. We\nuse this framework to evaluate ~150,000 generations from 14 language models,\nfinding that even the best-performing models are riddled with hallucinations\n(sometimes up to 86% of generated atomic facts depending on the domain). We\nfurther define a novel error classification for LLM hallucinations based on\nwhether they likely stem from incorrect recollection of training data (Type A\nerrors), or incorrect knowledge in training data (Type B errors), or are\nfabrication (Type C errors). We hope our framework provides a foundation to\nenable the principled study of why generative models hallucinate, and advances\nthe development of trustworthy large language models.\n","authors":["Abhilasha Ravichander","Shrusti Ghela","David Wadden","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2501.08292v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2501.08284v1","updated":"2025-01-14T18:00:07Z","published":"2025-01-14T18:00:07Z","title":"AfriHate: A Multilingual Collection of Hate Speech and Abusive Language\n  Datasets for African Languages","summary":"  Hate speech and abusive language are global phenomena that need\nsocio-cultural background knowledge to be understood, identified, and\nmoderated. However, in many regions of the Global South, there have been\nseveral documented occurrences of (1) absence of moderation and (2) censorship\ndue to the reliance on keyword spotting out of context. Further, high-profile\nindividuals have frequently been at the center of the moderation process, while\nlarge and targeted hate speech campaigns against minorities have been\noverlooked. These limitations are mainly due to the lack of high-quality data\nin the local languages and the failure to include local communities in the\ncollection, annotation, and moderation processes. To address this issue, we\npresent AfriHate: a multilingual collection of hate speech and abusive language\ndatasets in 15 African languages. Each instance in AfriHate is annotated by\nnative speakers familiar with the local culture. We report the challenges\nrelated to the construction of the datasets and present various classification\nbaseline results with and without using LLMs. The datasets, individual\nannotations, and hate speech and offensive language lexicons are available on\nhttps://github.com/AfriHate/AfriHate\n","authors":["Shamsuddeen Hassan Muhammad","Idris Abdulmumin","Abinew Ali Ayele","David Ifeoluwa Adelani","Ibrahim Said Ahmad","Saminu Mohammad Aliyu","Nelson Odhiambo Onyango","Lilian D. A. Wanzare","Samuel Rutunda","Lukman Jibril Aliyu","Esubalew Alemneh","Oumaima Hourrane","Hagos Tesfahun Gebremichael","Elyas Abdi Ismail","Meriem Beloucif","Ebrahim Chekol Jibril","Andiswa Bukula","Rooweither Mabuya","Salomey Osei","Abigail Oppong","Tadesse Destaw Belay","Tadesse Kebede Guge","Tesfa Tegegne Asfaw","Chiamaka Ijeoma Chukwuneke","Paul Röttger","Seid Muhie Yimam","Nedjma Ousidhoum"],"pdf_url":"https://arxiv.org/pdf/2501.08284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08276v1","updated":"2025-01-14T17:50:06Z","published":"2025-01-14T17:50:06Z","title":"Exploring Robustness of LLMs to Sociodemographically-Conditioned\n  Paraphrasing","summary":"  Large Language Models (LLMs) have shown impressive performance in various NLP\ntasks. However, there are concerns about their reliability in different domains\nof linguistic variations. Many works have proposed robustness evaluation\nmeasures for local adversarial attacks, but we need globally robust models\nunbiased to different language styles. We take a broader approach to explore a\nwider range of variations across sociodemographic dimensions to perform\nstructured reliability tests on the reasoning capacity of language models. We\nextend the SocialIQA dataset to create diverse paraphrased sets conditioned on\nsociodemographic styles. The assessment aims to provide a deeper understanding\nof LLMs in (a) their capability of generating demographic paraphrases with\nengineered prompts and (b) their reasoning capabilities in real-world, complex\nlanguage scenarios. We also explore measures such as perplexity,\nexplainability, and ATOMIC performance of paraphrases for fine-grained\nreliability analysis of LLMs on these sets. We find that demographic-specific\nparaphrasing significantly impacts the performance of language models,\nindicating that the subtleties of language variations remain a significant\nchallenge. The code and dataset will be made available for reproducibility and\nfuture research.\n","authors":["Pulkit Arora","Akbar Karimi","Lucie Flek"],"pdf_url":"https://arxiv.org/pdf/2501.08276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08271v1","updated":"2025-01-14T17:37:40Z","published":"2025-01-14T17:37:40Z","title":"Comparative Analysis of Efficient Adapter-Based Fine-Tuning of\n  State-of-the-Art Transformer Models","summary":"  In this work, we investigate the efficacy of various adapter architectures on\nsupervised binary classification tasks from the SuperGLUE benchmark as well as\na supervised multi-class news category classification task from Kaggle.\nSpecifically, we compare classification performance and time complexity of\nthree transformer models, namely DistilBERT, ELECTRA, and BART, using\nconventional fine-tuning as well as nine state-of-the-art (SoTA) adapter\narchitectures. Our analysis reveals performance differences across adapter\narchitectures, highlighting their ability to achieve comparable or better\nperformance relative to fine-tuning at a fraction of the training time. Similar\nresults are observed on the new classification task, further supporting our\nfindings and demonstrating adapters as efficient and flexible alternatives to\nfine-tuning. This study provides valuable insights and guidelines for selecting\nand implementing adapters in diverse natural language processing (NLP)\napplications.\n","authors":["Saad Mashkoor Siddiqui","Mohammad Ali Sheikh","Muhammad Aleem","Kajol R Singh"],"pdf_url":"https://arxiv.org/pdf/2501.08271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02748v3","updated":"2025-01-14T17:20:04Z","published":"2024-10-03T17:57:01Z","title":"CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation","summary":"  Existing automatic prompt engineering methods are typically designed for\ndiscriminative tasks, where new task prompts are iteratively refined with\nlimited feedback from a single metric reflecting a single aspect. However,\nthese approaches are suboptimal for generative tasks, which require more\nnuanced guidance beyond a single numeric metric to improve the prompt and\noptimize multiple aspects of the generated text. To address these challenges,\nwe propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt\nOptimization (CriSPO) approach. CriSPO introduces a critique-suggestion module\nas its core component. This module spontaneously discovers aspects, and\ncompares generated and reference texts across these aspects, providing specific\nsuggestions for prompt modification. These clear critiques and actionable\nsuggestions guide a receptive optimizer module to make more substantial\nchanges, exploring a broader and more effective search space. To further\nimprove CriSPO with multi-metric optimization, we introduce an Automatic Suffix\nTuning (AST) extension to enhance the performance of task prompts across\nmultiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4\nsummarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE score\nimprovement on summarization and substantial improvement of various metrics on\nQA. Code available at https://github.com/amazon-science/crispo\n","authors":["Han He","Qianchu Liu","Lei Xu","Chaitanya Shivade","Yi Zhang","Sundararajan Srinivasan","Katrin Kirchhoff"],"pdf_url":"https://arxiv.org/pdf/2410.02748v3.pdf","comment":"Accepted to AAAI-2025"},{"id":"http://arxiv.org/abs/2409.12746v2","updated":"2025-01-14T16:41:28Z","published":"2024-09-19T13:13:07Z","title":"Bilingual Evaluation of Language Models on General Knowledge in\n  University Entrance Exams with Minimal Contamination","summary":"  In this article we present UNED-ACCESS 2024, a bilingual dataset that\nconsists of 1003 multiple-choice questions of university entrance level exams\nin Spanish and English. Questions are originally formulated in Spanish and\ntranslated manually into English, and have not ever been publicly released. A\nselection of current open-source and proprietary models are evaluated in a\nuniform zero-shot experimental setting both on the UNED-ACCESS 2024 dataset and\non an equivalent subset of MMLU questions. Results show that (i) reasoning\nquestions are challenging for models, (ii) smaller models perform worse than\nlarger models and degrade faster in Spanish than in English and (iii) the\nperformance gap between languages is negligible for the best models and grows\nup to 37% for smaller models. Model ranking on UNED-ACCESS 2024 is almost\nidentical in English and Spanish, and has also a high correlation (0.98\nPearson) with ranking on MMLU, suggesting that a small dataset is sufficiently\ndiverse and representative to measure performance by discipline.\n","authors":["Eva Sánchez Salido","Roser Morante","Julio Gonzalo","Guillermo Marco","Jorge Carrillo-de-Albornoz","Laura Plaza","Enrique Amigó","Andrés Fernández","Alejandro Benito-Santos","Adrián Ghajari Espinosa","Victor Fresno"],"pdf_url":"https://arxiv.org/pdf/2409.12746v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08248v1","updated":"2025-01-14T16:38:33Z","published":"2025-01-14T16:38:33Z","title":"Eliciting In-context Retrieval and Reasoning for Long-context Large\n  Language Models","summary":"  Recent advancements in long-context language models (LCLMs) promise to\ntransform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With\ntheir expanded context windows, LCLMs can process entire knowledge bases and\nperform retrieval and reasoning directly -- a capability we define as\nIn-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like\nLOFT often overestimate LCLM performance by providing overly simplified\ncontexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs\nin more realistic scenarios by including confounding passages retrieved with\nstrong retrievers. We then propose three methods to enhance LCLM performance:\n(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which\nuses attention heads to filter and de-noise long contexts during decoding, and\n(3) joint retrieval head training alongside the generation head. Our evaluation\nof five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with\nour best approach applied to Mistral-7B: +17 and +15 points by Exact Match on\nLOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised\nfine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks\ndespite being a much smaller model.\n","authors":["Yifu Qiu","Varun Embar","Yizhe Zhang","Navdeep Jaitly","Shay B. Cohen","Benjamin Han"],"pdf_url":"https://arxiv.org/pdf/2501.08248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14831v3","updated":"2025-01-14T16:17:49Z","published":"2024-05-23T17:47:55Z","title":"HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language\n  Models","summary":"  In order to thrive in hostile and ever-changing natural environments,\nmammalian brains evolved to store large amounts of knowledge about the world\nand continually integrate new information while avoiding catastrophic\nforgetting. Despite the impressive accomplishments, large language models\n(LLMs), even with retrieval-augmented generation (RAG), still struggle to\nefficiently and effectively integrate a large amount of new experiences after\npre-training. In this work, we introduce HippoRAG, a novel retrieval framework\ninspired by the hippocampal indexing theory of human long-term memory to enable\ndeeper and more efficient knowledge integration over new experiences. HippoRAG\nsynergistically orchestrates LLMs, knowledge graphs, and the Personalized\nPageRank algorithm to mimic the different roles of neocortex and hippocampus in\nhuman memory. We compare HippoRAG with existing RAG methods on multi-hop\nquestion answering and show that our method outperforms the state-of-the-art\nmethods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves\ncomparable or better performance than iterative retrieval like IRCoT while\nbeing 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into\nIRCoT brings further substantial gains. Finally, we show that our method can\ntackle new types of scenarios that are out of reach of existing methods. Code\nand data are available at https://github.com/OSU-NLP-Group/HippoRAG.\n","authors":["Bernal Jiménez Gutiérrez","Yiheng Shu","Yu Gu","Michihiro Yasunaga","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2405.14831v3.pdf","comment":"NeurIPS 2024. Code and data:\n  https://github.com/OSU-NLP-Group/HippoRAG"},{"id":"http://arxiv.org/abs/2307.09059v3","updated":"2025-01-14T16:11:11Z","published":"2023-07-18T08:23:46Z","title":"Text-guided Image Restoration and Semantic Enhancement for Text-to-Image\n  Person Retrieval","summary":"  The goal of Text-to-Image Person Retrieval (TIPR) is to retrieve specific\nperson images according to the given textual descriptions. A primary challenge\nin this task is bridging the substantial representational gap between visual\nand textual modalities. The prevailing methods map texts and images into\nunified embedding space for matching, while the intricate semantic\ncorrespondences between texts and images are still not effectively constructed.\nTo address this issue, we propose a novel TIPR framework to build fine-grained\ninteractions and alignment between person images and the corresponding texts.\nSpecifically, via fine-tuning the Contrastive Language-Image Pre-training\n(CLIP) model, a visual-textual dual encoder is firstly constructed, to\npreliminarily align the image and text features. Secondly, a Text-guided Image\nRestoration (TIR) auxiliary task is proposed to map abstract textual entities\nto specific image regions, improving the alignment between local textual and\nvisual embeddings. Additionally, a cross-modal triplet loss is presented to\nhandle hard samples, and further enhance the model's discriminability for minor\ndifferences. Moreover, a pruning-based text data augmentation approach is\nproposed to enhance focus on essential elements in descriptions, thereby\navoiding excessive model attention to less significant information. The\nexperimental results show our proposed method outperforms state-of-the-art\nmethods on three popular benchmark datasets, and the code will be made publicly\navailable at https://github.com/Delong-liu-bupt/SEN.\n","authors":["Delong Liu","Haiwen Li","Zhicheng Zhao","Yuan Dong","Nikolaos V. Boulgouris"],"pdf_url":"https://arxiv.org/pdf/2307.09059v3.pdf","comment":"The paper was withdrawn due to a dispute among the authors regarding\n  the content of the article"},{"id":"http://arxiv.org/abs/2411.14012v2","updated":"2025-01-14T15:58:02Z","published":"2024-11-21T10:54:35Z","title":"Logic Augmented Generation","summary":"  Semantic Knowledge Graphs (SKG) face challenges with scalability,\nflexibility, contextual understanding, and handling unstructured or ambiguous\ninformation. However, they offer formal and structured knowledge enabling\nhighly interpretable and reliable results by means of reasoning and querying.\nLarge Language Models (LLMs) overcome those limitations making them suitable in\nopen-ended tasks and unstructured environments. Nevertheless, LLMs are neither\ninterpretable nor reliable. To solve the dichotomy between LLMs and SKGs we\nenvision Logic Augmented Generation (LAG) that combines the benefits of the two\nworlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate\npotentially infinite relations and tacit knowledge on-demand. SKGs are key for\ninjecting a discrete heuristic dimension with clear logical and factual\nboundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,\nmedical diagnostics and climate projections. Understanding the properties and\nlimitations of LAG, which are still mostly unknown, is of utmost importance for\nenabling a variety of tasks involving tacit knowledge in order to provide\ninterpretable and effective results.\n","authors":["Aldo Gangemi","Andrea Giovanni Nuzzolese"],"pdf_url":"https://arxiv.org/pdf/2411.14012v2.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2501.08208v1","updated":"2025-01-14T15:46:39Z","published":"2025-01-14T15:46:39Z","title":"ASTRID -- An Automated and Scalable TRIaD for the Evaluation of\n  RAG-based Clinical Question Answering Systems","summary":"  Large Language Models (LLMs) have shown impressive potential in clinical\nquestion answering (QA), with Retrieval Augmented Generation (RAG) emerging as\na leading approach for ensuring the factual accuracy of model responses.\nHowever, current automated RAG metrics perform poorly in clinical and\nconversational use cases. Using clinical human evaluations of responses is\nexpensive, unscalable, and not conducive to the continuous iterative\ndevelopment of RAG systems. To address these challenges, we introduce ASTRID -\nan Automated and Scalable TRIaD for evaluating clinical QA systems leveraging\nRAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy\n(RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is\ndesigned to better capture the faithfulness of a model's response to the\nknowledge base without penalising conversational elements. To validate our\ntriad, we curate a dataset of over 200 real-world patient questions posed to an\nLLM-based QA agent during surgical follow-up for cataract surgery - the highest\nvolume operation in the world - augmented with clinician-selected questions for\nemergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate\nthat CF can predict human ratings of faithfulness better than existing\ndefinitions for conversational use cases. Furthermore, we show that evaluation\nusing our triad consisting of CF, RA, and CR exhibits alignment with clinician\nassessment for inappropriate, harmful, or unhelpful responses. Finally, using\nnine different LLMs, we demonstrate that the three metrics can closely agree\nwith human evaluations, highlighting the potential of these metrics for use in\nLLM-driven automated evaluation pipelines. We also publish the prompts and\ndatasets for these experiments, providing valuable resources for further\nresearch and development.\n","authors":["Mohita Chowdhury","Yajie Vera He","Aisling Higham","Ernest Lim"],"pdf_url":"https://arxiv.org/pdf/2501.08208v1.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2501.08203v1","updated":"2025-01-14T15:38:41Z","published":"2025-01-14T15:38:41Z","title":"ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math\n  Problem Solving","summary":"  While Large Language Models (LLMs) have shown impressive capabilities in math\nproblem-solving tasks, their robustness to noisy inputs is not well-studied. In\nthis work, we propose ArithmAttack to examine how robust the LLMs are when they\nencounter noisy prompts that contain extra noise in the form of punctuation\nmarks. While being easy to implement, ArithmAttack does not cause any\ninformation loss since words are not added or deleted from the context. We\nevaluate the robustness of seven LLMs, including LLama3, Mistral, and\nMathstral, on noisy GSM8K and MultiArith datasets. Our experiments suggest that\nall the studied models show vulnerability to such noise, with more noise\nleading to poorer performances.\n","authors":["Zain Ul Abedin","Shahzeb Qamar","Lucie Flek","Akbar Karimi"],"pdf_url":"https://arxiv.org/pdf/2501.08203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03565v3","updated":"2025-01-14T15:30:50Z","published":"2024-04-04T16:20:34Z","title":"Personalized LLM Response Generation with Parameterized Memory Injection","summary":"  Large Language Models (LLMs) have exhibited remarkable proficiency in\ncomprehending and generating natural language. On the other hand, personalized\nLLM response generation holds the potential to offer substantial benefits for\nindividuals in critical areas such as medical. Existing research has explored\nmemory-augmented methods to prompt the LLM with pre-stored user-specific\nknowledge for personalized response generation in terms of new queries. We\ncontend that such paradigm is unable to perceive fine-granularity information.\nIn this study, we propose a novel \\textbf{M}emory-\\textbf{i}njected approach\nusing parameter-efficient fine-tuning (PEFT) and along with a Bayesian\nOptimisation searching strategy to achieve \\textbf{L}LM\n\\textbf{P}ersonalization(\\textbf{MiLP}).\n","authors":["Kai Zhang","Yejin Kim","Xiaozhong Liu"],"pdf_url":"https://arxiv.org/pdf/2404.03565v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08200v1","updated":"2025-01-14T15:27:01Z","published":"2025-01-14T15:27:01Z","title":"CWEval: Outcome-driven Evaluation on Functionality and Security of LLM\n  Code Generation","summary":"  Large Language Models (LLMs) have significantly aided developers by\ngenerating or assisting in code writing, enhancing productivity across various\ntasks. While identifying incorrect code is often straightforward, detecting\nvulnerabilities in functionally correct code is more challenging, especially\nfor developers with limited security knowledge, which poses considerable\nsecurity risks of using LLM-generated code and underscores the need for robust\nevaluation benchmarks that assess both functional correctness and security.\nCurrent benchmarks like CyberSecEval and SecurityEval attempt to solve it but\nare hindered by unclear and impractical specifications, failing to assess both\nfunctionality and security accurately. To tackle these deficiencies, we\nintroduce CWEval, a novel outcome-driven evaluation framework designed to\nenhance the evaluation of secure code generation by LLMs. This framework not\nonly assesses code functionality but also its security simultaneously with\nhigh-quality task specifications and outcome-driven test oracles which provides\nhigh accuracy. Coupled with CWEval-bench, a multilingual, security-critical\ncoding benchmark, CWEval provides a rigorous empirical security evaluation on\nLLM-generated code, overcoming previous benchmarks' shortcomings. Through our\nevaluations, CWEval reveals a notable portion of functional but insecure code\nproduced by LLMs, and shows a serious inaccuracy of previous evaluations,\nultimately contributing significantly to the field of secure code generation.\nWe open-source our artifact at: https://github.com/Co1lin/CWEval .\n","authors":["Jinjun Peng","Leyi Cui","Kele Huang","Junfeng Yang","Baishakhi Ray"],"pdf_url":"https://arxiv.org/pdf/2501.08200v1.pdf","comment":"to be published in LLM4Code 2025"},{"id":"http://arxiv.org/abs/2501.08197v1","updated":"2025-01-14T15:22:47Z","published":"2025-01-14T15:22:47Z","title":"OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for\n  LLM Training","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir success heavily relies on the quality of pretraining corpora. For Chinese\nLLMs, the scarcity of high-quality Chinese datasets presents a significant\nchallenge, often limiting their performance. To address this issue, we propose\nthe OpenCSG Chinese Corpus, a series of high-quality datasets specifically\ndesigned for LLM pretraining, post-training, and fine-tuning. This corpus\nincludes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, and\nSmoltalk-chinese, each with distinct characteristics: Fineweb-edu datasets\nfocus on filtered, high-quality content derived from diverse Chinese web\nsources; Cosmopedia-chinese provides synthetic, textbook-style data for\nknowledge-intensive training; and Smoltalk-chinese emphasizes stylistic and\ndiverse chat-format data. The OpenCSG Chinese Corpus is characterized by its\nhigh-quality text, diverse coverage across domains, and scalable, reproducible\ndata curation processes. Additionally, we conducted extensive experimental\nanalyses, including evaluations on smaller parameter models, which demonstrated\nsignificant performance improvements in tasks such as C-Eval, showcasing the\neffectiveness of the corpus for training Chinese LLMs.\n","authors":["Yijiong Yu","Ziyun Dai","Zekun Wang","Wei Wang","Ran Chen","Ji Pei"],"pdf_url":"https://arxiv.org/pdf/2501.08197v1.pdf","comment":"The datasets are available on\n  https://huggingface.co/collections/opencsg/chinese-fineweb-66cfed105f502ece8f29643e\n  ; The code is on https://github.com/yuyijiong/fineweb-edu-chinese"},{"id":"http://arxiv.org/abs/2501.01028v3","updated":"2025-01-14T15:19:52Z","published":"2025-01-02T03:17:51Z","title":"KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model","summary":"  As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters.\n","authors":["Xinshuo Hu","Zifei Shan","Xinping Zhao","Zetian Sun","Zhenyu Liu","Dongfang Li","Shaolin Ye","Xinyuan Wei","Qian Chen","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.01028v3.pdf","comment":"Technical Report. 23 pages, 6 figures, 10 tables"},{"id":"http://arxiv.org/abs/2501.08187v1","updated":"2025-01-14T15:12:19Z","published":"2025-01-14T15:12:19Z","title":"A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction\n  Following","summary":"  Large language models excel at interpreting complex natural language\ninstructions, enabling them to perform a wide range of tasks. In the life\nsciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language\nof cellular biology\", capturing intricate gene expression patterns at the\nsingle-cell level. However, interacting with this \"language\" through\nconventional tools is often inefficient and unintuitive, posing challenges for\nresearchers. To address these limitations, we present InstructCell, a\nmulti-modal AI copilot that leverages natural language as a medium for more\ndirect and flexible single-cell analysis. We construct a comprehensive\nmulti-modal instruction dataset that pairs text-based instructions with\nscRNA-seq profiles from diverse tissues and species. Building on this, we\ndevelop a multi-modal cell language architecture capable of simultaneously\ninterpreting and processing both modalities. InstructCell empowers researchers\nto accomplish critical tasks-such as cell type annotation, conditional\npseudo-cell generation, and drug sensitivity prediction-using straightforward\nnatural language commands. Extensive evaluations demonstrate that InstructCell\nconsistently meets or exceeds the performance of existing single-cell\nfoundation models, while adapting to diverse experimental conditions. More\nimportantly, InstructCell provides an accessible and intuitive tool for\nexploring complex single-cell data, lowering technical barriers and enabling\ndeeper biological insights.\n","authors":["Yin Fang","Xinle Deng","Kangwei Liu","Ningyu Zhang","Jingyang Qian","Penghui Yang","Xiaohui Fan","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2501.08187v1.pdf","comment":"37 pages; 13 figures; Code: https://github.com/zjunlp/Instructcell;\n  Models: https://huggingface.co/zjunlp/Instructcell-chat,\n  https://huggingface.co/zjunlp/InstructCell-instruct"},{"id":"http://arxiv.org/abs/2501.07572v2","updated":"2025-01-14T15:06:56Z","published":"2025-01-13T18:58:07Z","title":"WebWalker: Benchmarking LLMs in Web Traversal","summary":"  Retrieval-augmented generation (RAG) demonstrates remarkable performance\nacross tasks in open-domain question-answering. However, traditional search\nengines may retrieve shallow content, limiting the ability of LLMs to handle\ncomplex, multi-layered information. To address it, we introduce WebWalkerQA, a\nbenchmark designed to assess the ability of LLMs to perform web traversal. It\nevaluates the capacity of LLMs to traverse a website's subpages to extract\nhigh-quality data systematically. We propose WebWalker, which is a multi-agent\nframework that mimics human-like web navigation through an explore-critic\nparadigm. Extensive experimental results show that WebWalkerQA is challenging\nand demonstrates the effectiveness of RAG combined with WebWalker, through the\nhorizontal and vertical integration in real-world scenarios.\n","authors":["Jialong Wu","Wenbiao Yin","Yong Jiang","Zhenglin Wang","Zekun Xi","Runnan Fang","Linhai Zhang","Yulan He","Deyu Zhou","Pengjun Xie","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2501.07572v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08167v1","updated":"2025-01-14T14:49:14Z","published":"2025-01-14T14:49:14Z","title":"Potential and Perils of Large Language Models as Judges of Unstructured\n  Textual Data","summary":"  Rapid advancements in large language models have unlocked remarkable\ncapabilities when it comes to processing and summarizing unstructured text\ndata. This has implications for the analysis of rich, open-ended datasets, such\nas survey responses, where LLMs hold the promise of efficiently distilling key\nthemes and sentiments. However, as organizations increasingly turn to these\npowerful AI systems to make sense of textual feedback, a critical question\narises, can we trust LLMs to accurately represent the perspectives contained\nwithin these text based datasets? While LLMs excel at generating human-like\nsummaries, there is a risk that their outputs may inadvertently diverge from\nthe true substance of the original responses. Discrepancies between the\nLLM-generated outputs and the actual themes present in the data could lead to\nflawed decision-making, with far-reaching consequences for organizations. This\nresearch investigates the effectiveness of LLMs as judge models to evaluate the\nthematic alignment of summaries generated by other LLMs. We utilized an\nAnthropic Claude model to generate thematic summaries from open-ended survey\nresponses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as\nLLM judges. The LLM-as-judge approach was compared to human evaluations using\nCohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable\nalternative to traditional human centric evaluation methods. Our findings\nreveal that while LLMs as judges offer a scalable solution comparable to human\nraters, humans may still excel at detecting subtle, context-specific nuances.\nThis research contributes to the growing body of knowledge on AI assisted text\nanalysis. We discuss limitations and provide recommendations for future\nresearch, emphasizing the need for careful consideration when generalizing LLM\njudge models across various contexts and use cases.\n","authors":["Rewina Bedemariam","Natalie Perez","Sreyoshi Bhaduri","Satya Kapoor","Alex Gil","Elizabeth Conjar","Ikkei Itoku","David Theil","Aman Chadha","Naumaan Nayyar"],"pdf_url":"https://arxiv.org/pdf/2501.08167v1.pdf","comment":"11 pages, 1 appendix"},{"id":"http://arxiv.org/abs/2408.16779v2","updated":"2025-01-14T14:26:03Z","published":"2024-08-15T16:41:00Z","title":"Inductive Learning of Logical Theories with LLMs: An Expressivity-Graded\n  Analysis","summary":"  This work presents a novel systematic methodology to analyse the capabilities\nand limitations of Large Language Models (LLMs) with feedback from a formal\ninference engine, on logic theory induction. The analysis is complexity-graded\nw.r.t. rule dependency structure, allowing quantification of specific inference\nchallenges on LLM performance. Integrating LLMs with formal methods is a\npromising frontier in the Natural Language Processing field, as an important\navenue for improving model inference control and explainability. In particular,\ninductive learning over complex sets of facts and rules, poses unique\nchallenges for current autoregressive models, as they lack explicit symbolic\ngrounding. While they can be complemented by formal systems, the properties\ndelivered by LLMs regarding inductive learning, are not well understood and\nquantified. Empirical results indicate that the largest LLMs can achieve\ncompetitive results against a SOTA Inductive Logic Programming (ILP) system\nbaseline, but also that tracking long predicate relationship chains is a more\ndifficult obstacle than theory complexity for LLMs.\n","authors":["João Pedro Gandarela","Danilo S. Carvalho","André Freitas"],"pdf_url":"https://arxiv.org/pdf/2408.16779v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08145v1","updated":"2025-01-14T14:23:18Z","published":"2025-01-14T14:23:18Z","title":"Refusal Behavior in Large Language Models: A Nonlinear Perspective","summary":"  Refusal behavior in large language models (LLMs) enables them to decline\nresponding to harmful, unethical, or inappropriate prompts, ensuring alignment\nwith ethical standards. This paper investigates refusal behavior across six\nLLMs from three architectural families. We challenge the assumption of refusal\nas a linear phenomenon by employing dimensionality reduction techniques,\nincluding PCA, t-SNE, and UMAP. Our results reveal that refusal mechanisms\nexhibit nonlinear, multidimensional characteristics that vary by model\narchitecture and layer. These findings highlight the need for nonlinear\ninterpretability to improve alignment research and inform safer AI deployment\nstrategies.\n","authors":["Fabian Hildebrandt","Andreas Maier","Patrick Krauss","Achim Schilling"],"pdf_url":"https://arxiv.org/pdf/2501.08145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13612v2","updated":"2025-01-14T14:16:45Z","published":"2024-12-18T08:42:25Z","title":"Are LLMs Good Literature Review Writers? Evaluating the Literature\n  Review Writing Ability of Large Language Models","summary":"  The literature review is a crucial form of academic writing that involves\ncomplex processes of literature collection, organization, and summarization.\nThe emergence of large language models (LLMs) has introduced promising tools to\nautomate these processes. However, their actual capabilities in writing\ncomprehensive literature reviews remain underexplored, such as whether they can\ngenerate accurate and reliable references. To address this gap, we propose a\nframework to assess the literature review writing ability of LLMs\nautomatically. We evaluate the performance of LLMs across three tasks:\ngenerating references, writing abstracts, and writing literature reviews. We\nemploy external tools for a multidimensional evaluation, which includes\nassessing hallucination rates in references, semantic coverage, and factual\nconsistency with human-written context. By analyzing the experimental results,\nwe find that, despite advancements, even the most sophisticated models still\ncannot avoid generating hallucinated references. Additionally, different models\nexhibit varying performance in literature review writing across different\ndisciplines.\n","authors":["Xuemei Tang","Xufeng Duan","Zhenguang G. Cai"],"pdf_url":"https://arxiv.org/pdf/2412.13612v2.pdf","comment":"12 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2501.08120v1","updated":"2025-01-14T13:52:41Z","published":"2025-01-14T13:52:41Z","title":"In-situ graph reasoning and knowledge expansion using Graph-PReFLexOR","summary":"  The pursuit of automated scientific discovery has fueled progress from\nsymbolic logic to modern AI, forging new frontiers in reasoning and pattern\nrecognition. Transformers function as potential systems, where every possible\nrelationship remains latent potentiality until tasks impose constraints, akin\nto measurement. Yet, refining their sampling requires more than probabilistic\nselection: solutions must conform to specific structures or rules, ensuring\nconsistency and the invocation of general principles. We present\nGraph-PReFLexOR (Graph-based Preference-based Recursive Language Modeling for\nExploratory Optimization of Reasoning), a framework that combines graph\nreasoning with symbolic abstraction to dynamically expand domain knowledge.\nInspired by reinforcement learning, Graph-PReFLexOR defines reasoning as a\nstructured mapping, where tasks yield knowledge graphs, abstract patterns, and\nultimately, final answers. Inspired by category theory, it encodes concepts as\nnodes and their relationships as edges, supporting hierarchical inference and\nadaptive learning through isomorphic representations. Demonstrations include\nhypothesis generation, materials design, and creative reasoning, such as\ndiscovering relationships between mythological concepts like 'thin places' with\nmaterials science. We propose a 'knowledge garden growth' strategy that\nintegrates insights across domains, promoting interdisciplinary connections.\nResults with a 3-billion-parameter Graph-PReFLexOR model show superior\nreasoning depth and adaptability, underscoring the potential for transparent,\nmultidisciplinary AI-driven discovery. It lays the groundwork for general\nautonomous reasoning solutions.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2501.08120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08102v1","updated":"2025-01-14T13:19:47Z","published":"2025-01-14T13:19:47Z","title":"Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media","summary":"  Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design.\n","authors":["Wenlu Fan","Yuqi Zhu","Chenyang Wang","Bin Wang","Wentao Xu"],"pdf_url":"https://arxiv.org/pdf/2501.08102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11531v2","updated":"2025-01-14T12:56:34Z","published":"2024-11-18T12:40:51Z","title":"Addressing Hallucinations in Language Models with Knowledge Graph\n  Embeddings as an Additional Modality","summary":"  In this paper we present an approach to reduce hallucinations in Large\nLanguage Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional\nmodality. Our method involves transforming input text into a set of KG\nembeddings and using an adapter to integrate these embeddings into the language\nmodel space, without relying on external retrieval processes.\n  To facilitate this, we created WikiEntities, a dataset containing over 3\nmillion Wikipedia texts annotated with entities from Wikidata and their\ncorresponding embeddings from PyTorch-BigGraph. This dataset serves as a\nvaluable resource for training Entity Linking models and adapting the described\nmethod to various LLMs using specialized adapters.\n  Our method does not require fine-tuning of the language models themselves;\ninstead, we only train the adapter. This ensures that the model's performance\non other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA\n2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and\ndemonstrated that our approach improves performance on the HaluEval, True-False\nbenchmarks and FEVER dataset. The results indicate that incorporating KGs as a\nnew modality can effectively reduce hallucinations and improve the factual\naccuracy of language models, all without the need for external retrieval.\n","authors":["Viktoriia Chekalina","Anton Razzhigaev","Elizaveta Goncharova","Andrey Kuznetsov"],"pdf_url":"https://arxiv.org/pdf/2411.11531v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02953v4","updated":"2025-01-14T12:55:27Z","published":"2023-10-04T16:44:23Z","title":"JsonTuning: Towards Generalizable, Robust, and Controllable Instruction\n  Tuning","summary":"  Instruction tuning is vital for enhancing the performance of large language\nmodels (LLMs), but existing text-to-text methods, referred to as TextTuning,\nstruggle with issues such as generalization, robustness, and controllability\ndue to their lack of explicit task structures. We introduce JsonTuning, a\nstructure-to-structure approach that uses JSON structures to represent tasks.\nThis method improves generalization by clarifying task elements and their\nrelations, boosts robustness by minimizing ambiguity, and enhances\ncontrollability by allowing precise control over outputs. We conduct an\nextensive comparative analysis between JsonTuning and TextTuning using various\nlanguage models and benchmarks. Our findings reveal that JsonTuning\nconsistently surpasses TextTuning in terms of performance, robustness, and\ncontrollability across different scenarios. By overcoming the limitations of\nTextTuning, JsonTuning demonstrates significant potential for developing more\neffective and reliable LLMs capable of handling diverse scenarios.\n","authors":["Chang Gao","Wenxuan Zhang","Guizhen Chen","Wai Lam"],"pdf_url":"https://arxiv.org/pdf/2310.02953v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08085v1","updated":"2025-01-14T12:54:19Z","published":"2025-01-14T12:54:19Z","title":"Dynamic Multimodal Sentiment Analysis: Leveraging Cross-Modal Attention\n  for Enabled Classification","summary":"  This paper explores the development of a multimodal sentiment analysis model\nthat integrates text, audio, and visual data to enhance sentiment\nclassification. The goal is to improve emotion detection by capturing the\ncomplex interactions between these modalities, thereby enabling more accurate\nand nuanced sentiment interpretation. The study evaluates three feature fusion\nstrategies -- late stage fusion, early stage fusion, and multi-headed attention\n-- within a transformer-based architecture. Experiments were conducted using\nthe CMU-MOSEI dataset, which includes synchronized text, audio, and visual\ninputs labeled with sentiment scores. Results show that early stage fusion\nsignificantly outperforms late stage fusion, achieving an accuracy of 71.87\\%,\nwhile the multi-headed attention approach offers marginal improvement, reaching\n72.39\\%. The findings suggest that integrating modalities early in the process\nenhances sentiment classification, while attention mechanisms may have limited\nimpact within the current framework. Future work will focus on refining feature\nfusion techniques, incorporating temporal data, and exploring dynamic feature\nweighting to further improve model performance.\n","authors":["Hui Lee","Singh Suniljit","Yong Siang Ong"],"pdf_url":"https://arxiv.org/pdf/2501.08085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08057v1","updated":"2025-01-14T12:12:06Z","published":"2025-01-14T12:12:06Z","title":"Optimizing Speech Multi-View Feature Fusion through Conditional\n  Computation","summary":"  Recent advancements have highlighted the efficacy of self-supervised learning\n(SSL) features in various speech-related tasks, providing lightweight and\nversatile multi-view speech representations. However, our study reveals that\nwhile SSL features expedite model convergence, they conflict with traditional\nspectral features like FBanks in terms of update directions. In response, we\npropose a novel generalized feature fusion framework grounded in conditional\ncomputation, featuring a gradient-sensitive gating network and a multi-stage\ndropout strategy. This framework mitigates feature conflicts and bolsters model\nrobustness to multi-view input features. By integrating SSL and spectral\nfeatures, our approach accelerates convergence and maintains performance on par\nwith spectral models across multiple speech translation tasks on the MUSTC\ndataset.\n","authors":["Weiqiao Shan","Yuhao Zhang","Yuchen Han","Bei Li","Xiaofeng Zhao","Yuang Li","Min Zhang","Hao Yang","Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2501.08057v1.pdf","comment":"ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.04987v2","updated":"2025-01-14T12:06:33Z","published":"2025-01-09T06:00:27Z","title":"TreeKV: Smooth Key-Value Cache Compression with Tree Structures","summary":"  Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.\n","authors":["Ziwei He","Jian Yuan","Haoli Bai","Jingwen Leng","Bo Jiang"],"pdf_url":"https://arxiv.org/pdf/2501.04987v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08053v1","updated":"2025-01-14T12:01:54Z","published":"2025-01-14T12:01:54Z","title":"Exploring Narrative Clustering in Large Language Models: A Layerwise\n  Analysis of BERT","summary":"  This study investigates the internal mechanisms of BERT, a transformer-based\nlarge language model, with a focus on its ability to cluster narrative content\nand authorial style across its layers. Using a dataset of narratives developed\nvia GPT-4, featuring diverse semantic content and stylistic variations, we\nanalyze BERT's layerwise activations to uncover patterns of localized neural\nprocessing. Through dimensionality reduction techniques such as Principal\nComponent Analysis (PCA) and Multidimensional Scaling (MDS), we reveal that\nBERT exhibits strong clustering based on narrative content in its later layers,\nwith progressively compact and distinct clusters. While strong stylistic\nclustering might occur when narratives are rephrased into different text types\n(e.g., fables, sci-fi, kids' stories), minimal clustering is observed for\nauthorial style specific to individual writers. These findings highlight BERT's\nprioritization of semantic content over stylistic features, offering insights\ninto its representational capabilities and processing hierarchy. This study\ncontributes to understanding how transformer models like BERT encode linguistic\ninformation, paving the way for future interdisciplinary research in artificial\nintelligence and cognitive neuroscience.\n","authors":["Awritrojit Banerjee","Achim Schilling","Patrick Krauss"],"pdf_url":"https://arxiv.org/pdf/2501.08053v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2408.03062,\n  arXiv:2408.04270, arXiv:2307.01577"},{"id":"http://arxiv.org/abs/2501.08035v1","updated":"2025-01-14T11:39:55Z","published":"2025-01-14T11:39:55Z","title":"READ: Reinforcement-based Adversarial Learning for Text Classification\n  with Limited Labeled Data","summary":"  Pre-trained transformer models such as BERT have shown massive gains across\nmany text classification tasks. However, these models usually need enormous\nlabeled data to achieve impressive performances. Obtaining labeled data is\noften expensive and time-consuming, whereas collecting unlabeled data using\nsome heuristics is relatively much cheaper for any task. Therefore, this paper\nproposes a method that encapsulates reinforcement learning-based text\ngeneration and semi-supervised adversarial learning approaches in a novel way\nto improve the model's performance. Our method READ, Reinforcement-based\nAdversarial learning, utilizes an unlabeled dataset to generate diverse\nsynthetic text through reinforcement learning, improving the model's\ngeneralization capability using adversarial learning. Our experimental results\nshow that READ outperforms the existing state-of-art methods on multiple\ndatasets.\n","authors":["Rohit Sharma","Shanu Kumar","Avinash Kumar"],"pdf_url":"https://arxiv.org/pdf/2501.08035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01347v4","updated":"2025-01-14T11:36:42Z","published":"2025-01-02T16:54:08Z","title":"AdaptVC: High Quality Voice Conversion with Adaptive Learning","summary":"  The goal of voice conversion is to transform the speech of a source speaker\nto sound like that of a reference speaker while preserving the original\ncontent. A key challenge is to extract disentangled linguistic content from the\nsource and voice style from the reference. While existing approaches leverage\nvarious methods to isolate the two, a generalization still requires further\nattention, especially for robustness in zero-shot scenarios. In this paper, we\nachieve successful disentanglement of content and speaker features by tuning\nself-supervised speech features with adapters. The adapters are trained to\ndynamically encode nuanced features from rich self-supervised features, and the\ndecoder fuses them to produce speech that accurately resembles the reference\nwith minimal loss of content. Moreover, we leverage a conditional flow matching\ndecoder with cross-attention speaker conditioning to further boost the\nsynthesis quality and efficiency. Subjective and objective evaluations in a\nzero-shot scenario demonstrate that the proposed method outperforms existing\nmodels in speech quality and similarity to the reference speech.\n","authors":["Jaehun Kim","Ji-Hoon Kim","Yeunju Choi","Tan Dat Nguyen","Seongkyu Mun","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2501.01347v4.pdf","comment":"ICASSP 2025; demo available https://mm.kaist.ac.kr/projects/AdaptVC"},{"id":"http://arxiv.org/abs/2408.07583v2","updated":"2025-01-14T10:52:15Z","published":"2024-08-14T14:28:11Z","title":"Transformers and Large Language Models for Efficient Intrusion Detection\n  Systems: A Comprehensive Survey","summary":"  With significant advancements in Transformers LLMs, NLP has extended its\nreach into many research fields due to its enhanced capabilities in text\ngeneration and user interaction. One field benefiting greatly from these\nadvancements is cybersecurity. In cybersecurity, many parameters that need to\nbe protected and exchanged between senders and receivers are in the form of\ntext and tabular data, making NLP a valuable tool in enhancing the security\nmeasures of communication protocols. This survey paper provides a comprehensive\nanalysis of the utilization of Transformers and LLMs in cyber-threat detection\nsystems. The methodology of paper selection and bibliometric analysis is\noutlined to establish a rigorous framework for evaluating existing research.\nThe fundamentals of Transformers are discussed, including background\ninformation on various cyber-attacks and datasets commonly used in this field.\nThe survey explores the application of Transformers in IDSs, focusing on\ndifferent architectures such as Attention-based models, LLMs like BERT and GPT,\nCNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.\nFurthermore, it explores the diverse environments and applications where\nTransformers and LLMs-based IDS have been implemented, including computer\nnetworks, IoT devices, critical infrastructure protection, cloud computing,\nSDN, as well as in autonomous vehicles. The paper also addresses research\nchallenges and future directions in this area, identifying key issues such as\ninterpretability, scalability, and adaptability to evolving threats, and more.\nFinally, the conclusion summarizes the findings and highlights the significance\nof Transformers and LLMs in enhancing cyber-threat detection capabilities,\nwhile also outlining potential avenues for further research and development.\n","authors":["Hamza Kheddar"],"pdf_url":"https://arxiv.org/pdf/2408.07583v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2405.04760 by other authors"},{"id":"http://arxiv.org/abs/2501.08008v1","updated":"2025-01-14T10:51:31Z","published":"2025-01-14T10:51:31Z","title":"TriAdaptLoRA: Brain-Inspired Triangular Adaptive Low-Rank Adaptation for\n  Parameter-Efficient Fine-Tuning","summary":"  The fine-tuning of Large Language Models (LLMs) is pivotal for achieving\noptimal performance across diverse downstream tasks. However, while full\nfine-tuning delivers superior results, it entails significant computational and\nresource costs. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,\naddress these challenges by reducing the number of trainable parameters, but\nthey often struggle with rank adjustment efficiency and task-specific\nadaptability. We propose Triangular Adaptive Low-Rank Adaptation\n(TriAdaptLoRA), a novel PEFT framework inspired by neuroscience principles,\nwhich dynamically optimizes the allocation of trainable parameters.\nTriAdaptLoRA introduces three key innovations: 1) a triangular split of\ntransformation matrices into lower and upper triangular components to maximize\nparameter utilization, 2) a parameter importance metric based on normalized\nFrobenius norms for efficient adaptation, and 3) an adaptive rank-growth\nstrategy governed by dynamic thresholds, allowing flexible parameter allocation\nacross training steps. Experiments conducted on a variety of natural language\nunderstanding and generation tasks demonstrate that TriAdaptLoRA consistently\noutperforms existing PEFT methods. It achieves superior performance, enhanced\nstability, and reduced computational overhead, particularly under linear\nthreshold-driven rank growth. These results highlight its efficacy as a\nscalable and resource-efficient solution for fine-tuning LLMs.\n","authors":["Yao Liang","Yuwei Wang","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2501.08008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08003v1","updated":"2025-01-14T10:47:33Z","published":"2025-01-14T10:47:33Z","title":"Formalising lexical and syntactic diversity for data sampling in French","summary":"  Diversity is an important property of datasets and sampling data for\ndiversity is useful in dataset creation. Finding the optimally diverse sample\nis expensive, we therefore present a heuristic significantly increasing\ndiversity relative to random sampling. We also explore whether different kinds\nof diversity -- lexical and syntactic -- correlate, with the purpose of\nsampling for expensive syntactic diversity through inexpensive lexical\ndiversity. We find that correlations fluctuate with different datasets and\nversions of diversity measures. This shows that an arbitrarily chosen measure\nmay fall short of capturing diversity-related properties of datasets.\n","authors":["Louis Estève","Manon Scholivet","Agata Savary"],"pdf_url":"https://arxiv.org/pdf/2501.08003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11005v2","updated":"2025-01-14T09:52:50Z","published":"2024-10-14T18:44:23Z","title":"One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of\n  Large Language Models in Reasoning Tasks","summary":"  Language is not monolithic. While benchmarks, including those designed for\nmultiple languages, are often used as proxies to evaluate the performance of\nLarge Language Models (LLMs), they tend to overlook the nuances of\nwithin-language variation, and thus fail to model the experience of speakers of\nnon-standard dialects. Focusing on African American Vernacular English (AAVE),\nwe present the first study aimed at objectively assessing the fairness and\nrobustness of LLMs in handling dialects in canonical reasoning tasks, including\nalgorithm, math, logic, and integrated reasoning. We introduce \\textbf{ReDial}\n(\\textbf{Re}asoning with \\textbf{Dial}ect Queries), a benchmark containing\n1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE\nspeakers, including experts with computer science backgrounds, to rewrite seven\npopular benchmarks, such as HumanEval and GSM8K. With ReDial, we evaluate\nwidely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model\nfamilies. Our findings reveal that \\textbf{almost all of these widely used\nmodels show significant brittleness and unfairness to queries in AAVE}. Our\nwork establishes a systematic and objective framework for analyzing LLM bias in\ndialectal queries. Moreover, it highlights how mainstream LLMs provide unfair\nservice to dialect speakers in reasoning tasks, laying a critical foundation\nfor relevant future research. Code and data can be accessed at\nhttps://github.com/fangru-lin/redial_dialect_robustness_fairness.\n","authors":["Fangru Lin","Shaoguang Mao","Emanuele La Malfa","Valentin Hofmann","Adrian de Wynter","Xun Wang","Si-Qing Chen","Michael Wooldridge","Janet B. Pierrehumbert","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.11005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07947v1","updated":"2025-01-14T09:00:45Z","published":"2025-01-14T09:00:45Z","title":"\"Wait, did you mean the doctor?\": Collecting a Dialogue Corpus for\n  Topical Analysis","summary":"  Dialogue is at the core of human behaviour and being able to identify the\ntopic at hand is crucial to take part in conversation. Yet, there are few\naccounts of the topical organisation in casual dialogue and of how people\nrecognise the current topic in the literature. Moreover, analysing topics in\ndialogue requires conversations long enough to contain several topics and types\nof topic shifts. Such data is complicated to collect and annotate. In this\npaper we present a dialogue collection experiment which aims to build a corpus\nsuitable for topical analysis. We will carry out the collection with a\nmessaging tool we developed.\n","authors":["Amandine Decker","Vincent Tourneur","Maxime Amblard","Ellen Breitholtz"],"pdf_url":"https://arxiv.org/pdf/2501.07947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07927v1","updated":"2025-01-14T08:30:49Z","published":"2025-01-14T08:30:49Z","title":"Gandalf the Red: Adaptive Security for LLMs","summary":"  Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and rigorously expresses the\nsecurity-utility in an optimizable form. We further address the shortcomings in\nexisting evaluations by introducing Gandalf, a crowd-sourced, gamified\nred-teaming platform designed to generate realistic, adaptive attack datasets.\nUsing Gandalf, we collect and release a dataset of 279k prompt attacks.\nComplemented by benign user data, our analysis reveals the interplay between\nsecurity and utility, showing that defenses integrated in the LLM (e.g., system\nprompts) can degrade usability even without blocking requests. We demonstrate\nthat restricted application domains, defense-in-depth, and adaptive defenses\nare effective strategies for building secure and useful LLM applications. Code\nis available at\n\\href{https://github.com/lakeraai/dsec-gandalf}{\\texttt{https://github.com/lakeraai/dsec-gandalf}}.\n","authors":["Niklas Pfister","Václav Volhejn","Manuel Knott","Santiago Arias","Julia Bazińska","Mykhailo Bichurin","Alan Commike","Janet Darling","Peter Dienes","Matthew Fiedler","David Haber","Matthias Kraft","Marco Lancini","Max Mathys","Damián Pascual-Ortiz","Jakub Podolak","Adrià Romero-López","Kyriacos Shiarlis","Andreas Signer","Zsolt Terek","Athanasios Theocharis","Daniel Timbrell","Samuel Trautwein","Samuel Watts","Natalie Wu","Mateo Rojas-Carulla"],"pdf_url":"https://arxiv.org/pdf/2501.07927v1.pdf","comment":"Niklas Pfister, V\\'aclav Volhejn and Manuel Knott contributed equally"},{"id":"http://arxiv.org/abs/2501.07924v1","updated":"2025-01-14T08:23:15Z","published":"2025-01-14T08:23:15Z","title":"Exploring Aviation Incident Narratives Using Topic Modeling and\n  Clustering Techniques","summary":"  Aviation safety is a global concern, requiring detailed investigations into\nincidents to understand contributing factors comprehensively. This study uses\nthe National Transportation Safety Board (NTSB) dataset. It applies advanced\nnatural language processing (NLP) techniques, including Latent Dirichlet\nAllocation (LDA), Non-Negative Matrix Factorization (NMF), Latent Semantic\nAnalysis (LSA), Probabilistic Latent Semantic Analysis (pLSA), and K-means\nclustering. The main objectives are identifying latent themes, exploring\nsemantic relationships, assessing probabilistic connections, and cluster\nincidents based on shared characteristics. This research contributes to\naviation safety by providing insights into incident narratives and\ndemonstrating the versatility of NLP and topic modelling techniques in\nextracting valuable information from complex datasets. The results, including\ntopics identified from various techniques, provide an understanding of\nrecurring themes. Comparative analysis reveals that LDA performed best with a\ncoherence value of 0.597, pLSA of 0.583, LSA of 0.542, and NMF of 0.437.\nK-means clustering further reveals commonalities and unique insights into\nincident narratives. In conclusion, this study uncovers latent patterns and\nthematic structures within incident narratives, offering a comparative analysis\nof multiple-topic modelling techniques. Future research avenues include\nexploring temporal patterns, incorporating additional datasets, and developing\npredictive models for early identification of safety issues. This research lays\nthe groundwork for enhancing the understanding and improvement of aviation\nsafety by utilising the wealth of information embedded in incident narratives.\n","authors":["Aziida Nanyonga","Hassan Wasswa","Ugur Turhan","Keith Joiner","Graham Wild"],"pdf_url":"https://arxiv.org/pdf/2501.07924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07923v1","updated":"2025-01-14T08:18:41Z","published":"2025-01-14T08:18:41Z","title":"Aviation Safety Enhancement via NLP & Deep Learning: Classifying Flight\n  Phases in ATSB Safety Reports","summary":"  Aviation safety is paramount, demanding precise analysis of safety\noccurrences during different flight phases. This study employs Natural Language\nProcessing (NLP) and Deep Learning models, including LSTM, CNN, Bidirectional\nLSTM (BLSTM), and simple Recurrent Neural Networks (sRNN), to classify flight\nphases in safety reports from the Australian Transport Safety Bureau (ATSB).\nThe models exhibited high accuracy, precision, recall, and F1 scores, with LSTM\nachieving the highest performance of 87%, 88%, 87%, and 88%, respectively. This\nperformance highlights their effectiveness in automating safety occurrence\nanalysis. The integration of NLP and Deep Learning technologies promises\ntransformative enhancements in aviation safety analysis, enabling targeted\nsafety measures and streamlined report handling.\n","authors":["Aziida Nanyonga","Hassan Wasswa","Graham Wild"],"pdf_url":"https://arxiv.org/pdf/2501.07923v1.pdf","comment":"NLP, Aviation Safety, ATSB, Deep learning, Flight phase. arXiv admin\n  note: substantial text overlap with arXiv:2501.01694"},{"id":"http://arxiv.org/abs/2403.10568v3","updated":"2025-01-14T08:01:17Z","published":"2024-03-14T17:47:10Z","title":"MoPE: Mixture of Prompt Experts for Parameter-Efficient and Scalable\n  Multimodal Fusion","summary":"  Despite the demonstrated parameter efficiency of prompt-based multimodal\nfusion methods, their limited adaptivity and expressiveness often result in\nsuboptimal performance compared to other tuning approaches. In this paper, we\nintroduce the Mixture of Prompt Experts (MoPE), the first technique designed to\novercome these limitations by decomposing standard prompts to capture\ninstance-level features adaptively. Building on this decomposition, MoPE\nenhances prompt fusion's expressiveness by leveraging multimodal pairing priors\nto route the most effective prompt for each instance dynamically. Compared to\nvanilla prompting, our MoPE-based fusion method exhibits greater\nexpressiveness, scaling more effectively with the training data and the overall\nnumber of trainable parameters. We also investigate regularization terms for\nexpert routing, which lead to emergent expert specialization with enhanced\nadaptiveness and interpretablity. Extensive experiments across six multimodal\ndatasets spanning four modalities demonstrate state-of-the-art performance for\nprompt fusion, matching or even surpassing the performance of fine-tuning while\nrequiring only 0.8% of the trainable parameters. Project homepage:\nhttps://github.com/songrise/MoPE\n","authors":["Ruixiang Jiang","Lingbo Liu","Changwen Chen"],"pdf_url":"https://arxiv.org/pdf/2403.10568v3.pdf","comment":"Under Review, Extended version of arxiv:2312.03734"},{"id":"http://arxiv.org/abs/2411.07240v2","updated":"2025-01-14T07:57:26Z","published":"2024-11-11T18:59:02Z","title":"UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts","summary":"  The evaluation of mathematical reasoning capabilities is essential for\nadvancing Artificial General Intelligence (AGI). While Large Language Models\n(LLMs) have shown impressive performance in solving mathematical problems,\nexisting benchmarks such as GSM8K and MATH present limitations, including\nnarrow problem definitions with specific numbers and reliance on predetermined\nrules that hinder accurate assessments of reasoning and generality. This paper\nintroduces the UTMath Benchmark, a robust evaluation framework designed to\nassess LLMs through extensive unit tests, with a focus on both the accuracy and\ngenerality of model responses. It comprises 1,053 cutting-edge problems\nspanning nine mathematical domains, with an average of 68 test cases per\nproblem. UTMath is highly challenging, with the best-performing model, o1-mini,\nsolving only 32.57\\% of the problems, followed by o1-preview at 27.16\\%, and\nGPT-4o at 26.93\\%. Furthermore, we present the Reasoning-to-Coding of Thoughts\n(RCoT) approach, which encourages LLMs to engage in explicit reasoning prior to\ncode generation, thereby facilitating the production of more sophisticated\nsolutions and enhancing overall performance and efficiency. Additionally, we\nalso release the UTMath-Train training dataset (more than 70k samples), to\nsupport the community in further exploring mathematical reasoning. Our\nbenchmark can be accessed via the following link:\nhttps://github.com/UTMathGroup/UTMath\n","authors":["Bo Yang","Qingping Yang","Yingwei Ma","Runtao Liu"],"pdf_url":"https://arxiv.org/pdf/2411.07240v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07890v1","updated":"2025-01-14T06:59:51Z","published":"2025-01-14T06:59:51Z","title":"GRAPHMOE: Amplifying Cognitive Depth of Mixture-of-Experts Network via\n  Introducing Self-Rethinking Mechanism","summary":"  Traditional Mixture-of-Experts (MoE) networks benefit from utilizing multiple\nsmaller expert models as opposed to a single large network. However, these\nexperts typically operate independently, leaving a question open about whether\ninterconnecting these models could enhance the performance of MoE networks. In\nresponse, we introduce GRAPHMOE, a novel method aimed at augmenting the\ncognitive depth of language models via a self-rethinking mechanism constructed\non Pseudo GraphMoE networks. GRAPHMOE employs a recurrent routing strategy to\nsimulate iterative thinking steps, thereby facilitating the flow of information\namong expert nodes. We implement the GRAPHMOE architecture using Low-Rank\nAdaptation techniques (LoRA) and conduct extensive experiments on various\nbenchmark datasets. The experimental results reveal that GRAPHMOE outperforms\nother LoRA based models, achieving state-of-the-art (SOTA) performance.\nAdditionally, this study explores a novel recurrent routing strategy that may\ninspire further advancements in enhancing the reasoning capabilities of\nlanguage models.\n","authors":["Chen Tang","Bo Lv","Zifan Zheng","Bohao Yang","Kun Zhao","Ning Liao","Xiaoxing Wang","Feiyu Xiong","Zhiyu Li","Nayu Liu","Jingchi Jiang"],"pdf_url":"https://arxiv.org/pdf/2501.07890v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2501.07886v1","updated":"2025-01-14T06:54:17Z","published":"2025-01-14T06:54:17Z","title":"Iterative Label Refinement Matters More than Preference Optimization\n  under Weak Supervision","summary":"  Language model (LM) post-training relies on two stages of human supervision:\ntask demonstrations for supervised finetuning (SFT), followed by preference\ncomparisons for reinforcement learning from human feedback (RLHF). As LMs\nbecome more capable, the tasks they are given become harder to supervise. Will\npost-training remain effective under unreliable supervision? To test this, we\nsimulate unreliable demonstrations and comparison feedback using small LMs and\ntime-constrained humans. We find that in the presence of unreliable\nsupervision, SFT still retains some effectiveness, but DPO (a common RLHF\nalgorithm) fails to improve the model beyond SFT. To address this, we propose\niterative label refinement (ILR) as an alternative to RLHF. ILR improves the\nSFT data by using comparison feedback to decide whether human demonstrations\nshould be replaced by model-generated alternatives, then retrains the model via\nSFT on the updated data. SFT+ILR outperforms SFT+DPO on several tasks with\nunreliable supervision (math, coding, and safe instruction-following). Our\nfindings suggest that as LMs are used for complex tasks where human supervision\nis unreliable, RLHF may no longer be the best use of human comparison feedback;\ninstead, it is better to direct feedback towards improving the training data\nrather than continually training the model. Our code and data are available at\nhttps://github.com/helloelwin/iterative-label-refinement.\n","authors":["Yaowen Ye","Cassidy Laidlaw","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2501.07886v1.pdf","comment":"22 pages, 10 figures"},{"id":"http://arxiv.org/abs/2501.07171v2","updated":"2025-01-14T06:46:14Z","published":"2025-01-13T09:58:03Z","title":"BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and\n  Vision-Language Models Derived from Scientific Literature","summary":"  The development of vision-language models (VLMs) is driven by large-scale and\ndiverse multimodal datasets. However, progress toward generalist biomedical\nVLMs is limited by the lack of annotated, publicly accessible datasets across\nbiology and medicine. Existing efforts are restricted to narrow domains,\nmissing the full diversity of biomedical knowledge encoded in scientific\nliterature. To address this gap, we introduce BIOMEDICA, a scalable,\nopen-source framework to extract, annotate, and serialize the entirety of the\nPubMed Central Open Access subset into an easy-to-use, publicly accessible\ndataset. Our framework produces a comprehensive archive with over 24 million\nunique image-text pairs from over 6 million articles. Metadata and\nexpert-guided annotations are also provided. We demonstrate the utility and\naccessibility of our resource by releasing BMCA-CLIP, a suite of CLIP-style\nmodels continuously pre-trained on the BIOMEDICA dataset via streaming,\neliminating the need to download 27 TB of data locally. On average, our models\nachieve state-of-the-art performance across 40 tasks - spanning pathology,\nradiology, ophthalmology, dermatology, surgery, molecular biology,\nparasitology, and cell biology - excelling in zero-shot classification with a\n6.56% average improvement (as high as 29.8% and 17.5% in dermatology and\nophthalmology, respectively), and stronger image-text retrieval, all while\nusing 10x less compute. To foster reproducibility and collaboration, we release\nour codebase and dataset for the broader research community.\n","authors":["Alejandro Lozano","Min Woo Sun","James Burgess","Liangyu Chen","Jeffrey J Nirschl","Jeffrey Gu","Ivan Lopez","Josiah Aklilu","Austin Wolfgang Katzer","Collin Chiu","Anita Rau","Xiaohan Wang","Yuhui Zhang","Alfred Seunghoon Song","Robert Tibshirani","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2501.07171v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03852v3","updated":"2025-01-14T06:40:36Z","published":"2023-09-07T17:07:36Z","title":"FLM-101B: An Open LLM and How to Train It with $100K Budget","summary":"  Large language models (LLMs) are considered important approaches towards\nfoundational machine intelligence, achieving remarkable success in Natural\nLanguage Processing and multimodal tasks, among others. However, the carbon\nfootprints and financial costs originating from heavy pre-training computation\nis a non-negligible issue. Progressive training methods, inspired by the\nneurogenesis process that grows neural structures, have shown potential to\naccelerate LLM pre-training. However, the algorithms, implementation, and\npractices for progressively training LLMs beyond 100B parameters remain\nunderexplored. In this paper, we show that our model, namely FLM-101B, trained\nwith our growth strategy under a budget of \\$100K, reaches 80\\% of the\nbaselines' performances with only 10\\% of their floating-point operations. We\nbelieve that further studies on progressive training will benefit the community\nby cutting down the costs and promoting green AI. The checkpoint of FLM-101B is\nreleased at https://huggingface.co/CofeAI/FLM-101B.\n","authors":["Xiang Li","Yiqun Yao","Xin Jiang","Xuezhi Fang","Xuying Meng","Siqi Fan","Peng Han","Jing Li","Li Du","Bowen Qin","Zheng Zhang","Aixin Sun","Yequan Wang"],"pdf_url":"https://arxiv.org/pdf/2309.03852v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07875v1","updated":"2025-01-14T06:33:40Z","published":"2025-01-14T06:33:40Z","title":"Continual Learning with Embedding Layer Surgery and Task-wise Beam\n  Search using Whisper","summary":"  Current Multilingual ASR models only support a fraction of the world's\nlanguages. Continual Learning (CL) aims to tackle this problem by adding new\nlanguages to pre-trained models while avoiding the loss of performance on\nexisting languages, also known as Catastrophic Forgetting (CF). However,\nexisting CL methods overlook the adaptation of the token embedding lookup table\nat the decoder, despite its significant contribution to CF. We propose\nEmbedding Layer Surgery where separate copies of the token embeddings are\ncreated for each new languages, and one of the copies is selected to replace\nthe old languages embeddings when transcribing the corresponding new language.\nUnfortunately, this approach means LID errors also cause incorrect ASR\nembedding selection. Our Task-wise Beam Search allows self-correction for such\nmistakes. By adapting Whisper to 10 hours of data for each of 10 unseen\nlanguages from Common Voice, results show that our method reduces the Average\nWER (AWER) of pre-trained languages from 14.2% to 11.9% compared with\nExperience Replay, without compromising the AWER of the unseen languages.\n","authors":["Chin Yuen Kwok","Jia Qi Yip","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2501.07875v1.pdf","comment":"Published in 2024 IEEE Spoken Language Technology Workshop"},{"id":"http://arxiv.org/abs/2412.09012v2","updated":"2025-01-14T06:06:54Z","published":"2024-12-12T07:23:52Z","title":"What Makes Cryptic Crosswords Challenging for LLMs?","summary":"  Cryptic crosswords are puzzles that rely on general knowledge and the\nsolver's ability to manipulate language on different levels, dealing with\nvarious types of wordplay. Previous research suggests that solving such puzzles\nis challenging even for modern NLP models, including Large Language Models\n(LLMs). However, there is little to no research on the reasons for their poor\nperformance on this task. In this paper, we establish the benchmark results for\nthree popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance\non this task is still significantly below that of humans. We also investigate\nwhy these models struggle to achieve superior performance. We release our code\nand introduced datasets at\nhttps://github.com/bodasadallah/decrypting-crosswords.\n","authors":["Abdelrahman Sadallah","Daria Kotova","Ekaterina Kochmar"],"pdf_url":"https://arxiv.org/pdf/2412.09012v2.pdf","comment":"COLING 2025. arXiv admin note: text overlap with arXiv:2403.12094"},{"id":"http://arxiv.org/abs/2501.07861v1","updated":"2025-01-14T05:56:26Z","published":"2025-01-14T05:56:26Z","title":"ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process\n  Rewarding","summary":"  Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs)\nhold promise in knowledge-intensive tasks but face limitations in complex\nmulti-step reasoning. While recent methods have integrated RAG with\nchain-of-thought reasoning or test-time search using Process Reward Models\n(PRMs), these approaches encounter challenges such as a lack of explanations,\nbias in PRM training data, early-step bias in PRM scores, and insufficient\npost-training optimization of reasoning potential. To address these issues, we\npropose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding\n(ReARTeR), a framework that enhances RAG systems' reasoning capabilities\nthrough post-training and test-time scaling. At test time, ReARTeR introduces\nTrustworthy Process Rewarding via a Process Reward Model for accurate scalar\nscoring and a Process Explanation Model (PEM) for generating natural language\nexplanations, enabling step refinement. During post-training, it utilizes Monte\nCarlo Tree Search guided by Trustworthy Process Rewarding to collect\nhigh-quality step-level preference data, optimized through Iterative Preference\nOptimization. ReARTeR addresses three core challenges: (1) misalignment between\nPRM and PEM, tackled through off-policy preference learning; (2) bias in PRM\ntraining data, mitigated by balanced annotation methods and stronger\nannotations for challenging examples; and (3) early-step bias in PRM, resolved\nthrough a temporal-difference-based look-ahead search strategy. Experimental\nresults on multi-step reasoning benchmarks demonstrate significant\nimprovements, underscoring ReARTeR's potential to advance the reasoning\ncapabilities of RAG systems.\n","authors":["Zhongxiang Sun","Qipeng Wang","Weijie Yu","Xiaoxue Zang","Kai Zheng","Jun Xu","Xiao Zhang","Song Yang","Han Li"],"pdf_url":"https://arxiv.org/pdf/2501.07861v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.19255v2","updated":"2025-01-14T05:48:07Z","published":"2024-12-26T15:45:45Z","title":"Multi-matrix Factorization Attention","summary":"  We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.\n","authors":["Jingcheng Hu","Houyi Li","Yinmin Zhang","Zili Wang","Shuigeng Zhou","Xiangyu Zhang","Heung-Yeung Shum","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.19255v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07853v1","updated":"2025-01-14T05:41:09Z","published":"2025-01-14T05:41:09Z","title":"Optimizing Language Models for Grammatical Acceptability: A Comparative\n  Study of Fine-Tuning Techniques","summary":"  This study explores the fine-tuning (FT) of the Open Pre-trained Transformer\n(OPT-125M) for grammatical acceptability tasks using the CoLA dataset. By\ncomparing Vanilla-Fine-Tuning (VFT), Pattern-Based-Fine-Tuning (PBFT), and\nParameter-Efficient Fine-Tuning techniques (PEFT) like Low-Rank Adaptation\n(LoRA), we demonstrate significant improvements in computational efficiency\nwhile maintaining high accuracy. Our experiments reveal that while VFT achieves\nthe highest accuracy (81.2%), LoRA enhancing FT by reducing memory usage and\niteration time by more than 50%, and increases accuracy in PBFT case. Context\nDistillation (CD), though computationally efficient, underperformed with\naccuracy around 31%. Our findings contribute to democratizing access to large\nlanguage models (LLM) by reducing computational barriers.\n","authors":["Shobhit Ratan","Farley Knight","Ghada Jerfel","Sze Chung Ho"],"pdf_url":"https://arxiv.org/pdf/2501.07853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05692v2","updated":"2025-01-14T05:39:40Z","published":"2024-04-08T17:18:04Z","title":"Evaluating Mathematical Reasoning Beyond Accuracy","summary":"  The leaderboard of Large Language Models (LLMs) in mathematical tasks has\nbeen continuously updated. However, the majority of evaluations focus solely on\nthe final results, neglecting the quality of the intermediate steps. This\noversight can mask underlying problems, such as logical errors or unnecessary\nsteps in the reasoning process. To measure reasoning beyond final-answer\naccuracy, we introduce ReasonEval, a new methodology for evaluating the quality\nof reasoning steps. ReasonEval employs validity and redundancy to characterize\nthe reasoning quality, as well as accompanying LLMs to assess them\nautomatically. We explore different design options for the LLM-based evaluators\nand empirically demonstrate that ReasonEval, when instantiated with base models\npossessing strong mathematical knowledge and trained with high-quality labeled\ndata, consistently outperforms baseline methods in the meta-evaluation\ndatasets. We also highlight the strong generalization capabilities of\nReasonEval. By utilizing ReasonEval to evaluate LLMs specialized in math, we\nfind that an increase in final-answer accuracy does not necessarily guarantee\nan improvement in the overall quality of the reasoning steps for challenging\nmathematical problems. Additionally, we observe that ReasonEval can play a\nsignificant role in data selection. We open-source the best-performing model,\nmeta-evaluation script, and all evaluation results to facilitate future\nresearch.\n","authors":["Shijie Xia","Xuefeng Li","Yixin Liu","Tongshuang Wu","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2404.05692v2.pdf","comment":"v2 is the AAAI 2025 camera ready version. Project site with code:\n  https://github.com/GAIR-NLP/ReasonEval"},{"id":"http://arxiv.org/abs/2411.15640v3","updated":"2025-01-14T05:35:08Z","published":"2024-11-23T19:43:02Z","title":"AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering\n  Benchmark Dataset","summary":"  Recent advancements in large language model(LLM) performance on medical\nmultiple choice question (MCQ) benchmarks have stimulated interest from\nhealthcare providers and patients globally. Particularly in low-and\nmiddle-income countries (LMICs) facing acute physician shortages and lack of\nspecialists, LLMs offer a potentially scalable pathway to enhance healthcare\naccess and reduce costs. However, their effectiveness in the Global South,\nespecially across the African continent, remains to be established. In this\nwork, we introduce AfriMed-QA, the first large scale Pan-African English\nmulti-specialty medical Question-Answering (QA) dataset, 15,000 questions (open\nand closed-ended) sourced from over 60 medical schools across 16 countries,\ncovering 32 medical specialties. We further evaluate 30 LLMs across multiple\naxes including correctness and demographic bias. Our findings show significant\nperformance variation across specialties and geographies, MCQ performance\nclearly lags USMLE (MedQA). We find that biomedical LLMs underperform general\nmodels and smaller edge-friendly LLMs struggle to achieve a passing score.\nInterestingly, human evaluations show a consistent consumer preference for LLM\nanswers and explanations when compared with clinician answers.\n","authors":["Tobi Olatunji","Charles Nimo","Abraham Owodunni","Tassallah Abdullahi","Emmanuel Ayodele","Mardhiyah Sanni","Chinemelu Aka","Folafunmi Omofoye","Foutse Yuehgoh","Timothy Faniran","Bonaventure F. P. Dossou","Moshood Yekini","Jonas Kemp","Katherine Heller","Jude Chidubem Omeke","Chidi Asuzu MD","Naome A. Etori","Aimérou Ndiaye","Ifeoma Okoh","Evans Doe Ocansey","Wendy Kinara","Michael Best","Irfan Essa","Stephen Edward Moore","Chris Fourie","Mercy Nyamewaa Asiedu"],"pdf_url":"https://arxiv.org/pdf/2411.15640v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07845v1","updated":"2025-01-14T05:18:20Z","published":"2025-01-14T05:18:20Z","title":"Reasoning with Graphs: Structuring Implicit Knowledge to Enhance LLMs\n  Reasoning","summary":"  Large language models (LLMs) have demonstrated remarkable success across a\nwide range of tasks; however, they still encounter challenges in reasoning\ntasks that require understanding and inferring relationships between distinct\npieces of information within text sequences. This challenge is particularly\npronounced in tasks involving multi-step processes, such as logical reasoning\nand multi-hop question answering, where understanding implicit relationships\nbetween entities and leveraging multi-hop connections in the given context are\ncrucial. Graphs, as fundamental data structures, explicitly represent pairwise\nrelationships between entities, thereby offering the potential to enhance LLMs'\nreasoning capabilities. External graphs have proven effective in supporting\nLLMs across multiple tasks. However, in many reasoning tasks, no pre-existing\ngraph structure is provided. Can we structure implicit knowledge derived from\ncontext into graphs to assist LLMs in reasoning? In this paper, we propose\nReasoning with Graphs (RwG) by first constructing explicit graphs from the\ncontext and then leveraging these graphs to enhance LLM reasoning performance\non reasoning tasks. Extensive experiments demonstrate the effectiveness of the\nproposed method in improving both logical reasoning and multi-hop question\nanswering tasks.\n","authors":["Haoyu Han","Yaochen Xie","Hui Liu","Xianfeng Tang","Sreyashi Nag","William Headden","Hui Liu","Yang Li","Chen Luo","Shuiwang Ji","Qi He","Jiliang Tang"],"pdf_url":"https://arxiv.org/pdf/2501.07845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02950v2","updated":"2025-01-14T05:03:19Z","published":"2024-06-05T05:18:20Z","title":"Joint Beam Search Integrating CTC, Attention, and Transducer Decoders","summary":"  End-to-end automatic speech recognition (E2E-ASR) can be classified by its\ndecoder architectures, such as connectionist temporal classification (CTC),\nrecurrent neural network transducer (RNN-T), attention-based encoder-decoder,\nand Mask-CTC models. Each decoder architecture has advantages and\ndisadvantages, leading practitioners to switch between these different models\ndepending on application requirements. Instead of building separate models, we\npropose a joint modeling scheme where four decoders (CTC, RNN-T, attention, and\nMask-CTC) share the same encoder -- we refer to this as 4D modeling. The 4D\nmodel is trained jointly, which will bring model regularization and maximize\nthe model robustness thanks to their complementary properties. To efficiently\ntrain the 4D model, we introduce a two-stage training strategy that stabilizes\nthe joint training. In addition, we propose three novel joint beam search\nalgorithms by combining three decoders (CTC, RNN-T, and attention) to further\nimprove performance. These three beam search algorithms differ in which decoder\nis used as the primary decoder. We carefully evaluate the performance and\ncomputational tradeoffs associated with each algorithm. Experimental results\ndemonstrate that the jointly trained 4D model outperforms the E2E-ASR models\ntrained with only one individual decoder. Furthermore, we demonstrate that the\nproposed joint beam search algorithm outperforms the previously proposed\nCTC/attention decoding.\n","authors":["Yui Sudo","Muhammad Shakeel","Yosuke Fukumoto","Brian Yan","Jiatong Shi","Yifan Peng","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2406.02950v2.pdf","comment":"accepted to IEEE/ACM Transactions on Audio Speech and Language\n  Processing"},{"id":"http://arxiv.org/abs/2408.11869v3","updated":"2025-01-14T04:25:23Z","published":"2024-08-19T02:27:00Z","title":"ELDER: Enhancing Lifelong Model Editing with Mixture-of-LoRA","summary":"  Large language models (LLMs) require model editing to efficiently update\nspecific knowledge within them and avoid factual errors. Most model editing\nmethods are solely designed for single-time use and result in a significant\nforgetting effect in lifelong editing scenarios, where sequential edits are\nconducted over time. Previous approaches manage sequential edits by freezing\noriginal parameters and discretely allocating new parameters for each knowledge\nupdate. However, these methods lack robustness to minor input variations due to\nthe discrete mapping between data and parameters. To overcome this challenge,\nwe propose ELDER, a novel approach to create a continuous association between\ndata and adapters. ELDER integrates multiple LoRAs through a router network and\nis trained to establish a smooth data-adapter association, thereby enhancing\nthe edit robustness and generalization of semantically equivalent inputs. To\nensure inputs containing the same knowledge will be processed by the same\nLoRAs, we design a novel loss to guide the model link LoRA allocations with\nedit knowledge. Furthermore, we propose a deferral mechanism to retain the\noriginal LLM capabilities post-edit. Extensive experiments on GPT-2 XL and\nLLaMA2-7B demonstrate that ELDER effectively edits models in the lifelong\nsetting, outperforming eight baselines while exhibiting strong scalability and\npreserving LLMs' general abilities on downstream tasks. Our code is available\nat https://github.com/JiaangL/ELDER.\n","authors":["Jiaang Li","Quan Wang","Zhongnan Wang","Yongdong Zhang","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2408.11869v3.pdf","comment":"Accepted by AAAI-25"},{"id":"http://arxiv.org/abs/2410.12476v2","updated":"2025-01-14T04:19:49Z","published":"2024-10-16T11:46:32Z","title":"Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial\n  Generation","summary":"  Machine learning (ML) exhibits promise in the clinical domain. However, it is\nconstrained by data scarcity and ethical considerations, as the generation of\nclinical trials presents significant challenges due to stringent privacy\nregulations, high costs, and the extended duration required for conducting\nstudies with human participants. Despite the advancements of large language\nmodels (LLMs) in general generation tasks, their potential in facilitating the\ngeneration of synthetic clinical trials is under-explored. To address this gap,\nwe introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs\nto generate artificial yet realistic and diverse clinical trials with binary\nsuccess/failure labels. Experiments conducted on real clinical trials from the\n\\url{ClinicalTrials.gov} database demonstrate that our synthetic data can\neffectively augment real datasets. Furthermore, by fine-tuning a pre-trained\nmodel as a binary classifier on synthetic clinical trial datasets, we\ndemonstrate that this augmentation enhances model training for downstream tasks\nsuch as trial outcome prediction. Our findings suggest that LLMs for synthetic\nclinical trial generation hold promise for accelerating clinical research and\nupholding ethical standards for patient privacy. The code is publicly available\nat\nhttps://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.\n","authors":["Zerui Xu","Fang Wu","Yuanyuan Zhang","Yue Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.12476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07824v1","updated":"2025-01-14T03:59:48Z","published":"2025-01-14T03:59:48Z","title":"Real-time Verification and Refinement of Language Model Text Generation","summary":"  Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods.\n","authors":["Joonho Ko","Jinheon Baek","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2501.07824v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2501.07818v1","updated":"2025-01-14T03:43:23Z","published":"2025-01-14T03:43:23Z","title":"A Multi-Encoder Frozen-Decoder Approach for Fine-Tuning Large Language\n  Models","summary":"  Among parameter-efficient fine-tuning methods, freezing has emerged as a\npopular strategy for speeding up training, reducing catastrophic forgetting,\nand improving downstream performance. We investigate the impact of freezing the\ndecoder in a multi-task setup comprising diverse natural language tasks, aiming\nto reduce deployment overhead and enhance portability to novel tasks. Our\nexperiments, conducted by fine-tuning both individual and multi-task setups on\nthe AlexaTM model, reveal that freezing decoders is highly effective for tasks\nwith natural language outputs and mitigates catastrophic forgetting in\nmultilingual tasks. However, we find that pairing frozen decoders with a larger\nmodel can effectively maintain or even enhance performance in structured and QA\ntasks, making it a viable strategy for a broader range of task types.\n","authors":["Kaustubh D. Dhole"],"pdf_url":"https://arxiv.org/pdf/2501.07818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00090v2","updated":"2025-01-14T03:27:10Z","published":"2024-11-27T12:34:45Z","title":"Energy-Efficient Split Learning for Fine-Tuning Large Language Models in\n  Edge Networks","summary":"  In this letter, we propose an energy-efficient split learning (SL) framework\nfor fine-tuning large language models (LLMs) using geo-distributed personal\ndata at the network edge, where LLMs are split and alternately across massive\nmobile devices and an edge server. Considering the device heterogeneity and\nchannel dynamics in edge networks, a \\underline{C}ut l\\underline{A}yer and\ncomputing \\underline{R}esource \\underline{D}ecision (CARD) algorithm is\ndeveloped to minimize training delay and energy consumption. Simulation results\ndemonstrate that the proposed approach reduces the average training delay and\nserver's energy consumption by 70.8% and 53.1%, compared to the benchmarks,\nrespectively.\n","authors":["Zuguang Li","Shaohua Wu","Liang Li","Songge Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.00090v2.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.07815v1","updated":"2025-01-14T03:26:43Z","published":"2025-01-14T03:26:43Z","title":"Agent-Centric Projection of Prompting Techniques and Implications for\n  Synthetic Training Data for Large Language Models","summary":"  Recent advances in prompting techniques and multi-agent systems for Large\nLanguage Models (LLMs) have produced increasingly complex approaches. However,\nwe lack a framework for characterizing and comparing prompting techniques or\nunderstanding their relationship to multi-agent LLM systems. This position\npaper introduces and explains the concepts of linear contexts (a single,\ncontinuous sequence of interactions) and non-linear contexts (branching or\nmulti-path) in LLM systems. These concepts enable the development of an\nagent-centric projection of prompting techniques, a framework that can reveal\ndeep connections between prompting strategies and multi-agent systems. We\npropose three conjectures based on this framework: (1) results from non-linear\nprompting techniques can predict outcomes in equivalent multi-agent systems,\n(2) multi-agent system architectures can be replicated through single-LLM\nprompting techniques that simulate equivalent interaction patterns, and (3)\nthese equivalences suggest novel approaches for generating synthetic training\ndata. We argue that this perspective enables systematic cross-pollination of\nresearch findings between prompting and multi-agent domains, while providing\nnew directions for improving both the design and training of future LLM\nsystems.\n","authors":["Dhruv Dhamani","Mary Lou Maher"],"pdf_url":"https://arxiv.org/pdf/2501.07815v1.pdf","comment":"8 pages, 5 figures. Accepted at ICAART 2025. Derived from an early\n  draft at 2312.17601. arXiv admin note: substantial text overlap with\n  arXiv:2312.17601"},{"id":"http://arxiv.org/abs/2501.07813v1","updated":"2025-01-14T03:25:26Z","published":"2025-01-14T03:25:26Z","title":"Talk to Right Specialists: Routing and Planning in Multi-agent System\n  for Question Answering","summary":"  Leveraging large language models (LLMs), an agent can utilize\nretrieval-augmented generation (RAG) techniques to integrate external knowledge\nand increase the reliability of its responses. Current RAG-based agents\nintegrate single, domain-specific knowledge sources, limiting their ability and\nleading to hallucinated or inaccurate responses when addressing cross-domain\nqueries. Integrating multiple knowledge bases into a unified RAG-based agent\nraises significant challenges, including increased retrieval overhead and data\nsovereignty when sensitive data is involved. In this work, we propose RopMura,\na novel multi-agent system that addresses these limitations by incorporating\nhighly efficient routing and planning mechanisms. RopMura features two key\ncomponents: a router that intelligently selects the most relevant agents based\non knowledge boundaries and a planner that decomposes complex multi-hop queries\ninto manageable steps, allowing for coordinating cross-domain responses.\nExperimental results demonstrate that RopMura effectively handles both\nsingle-hop and multi-hop queries, with the routing mechanism enabling precise\nanswers for single-hop queries and the combined routing and planning mechanisms\nachieving accurate, multi-step resolutions for complex queries.\n","authors":["Feijie Wu","Zitao Li","Fei Wei","Yaliang Li","Bolin Ding","Jing Gao"],"pdf_url":"https://arxiv.org/pdf/2501.07813v1.pdf","comment":"Work In Progress"},{"id":"http://arxiv.org/abs/2411.17075v5","updated":"2025-01-14T03:05:10Z","published":"2024-11-26T03:27:43Z","title":"Don't Command, Cultivate: An Exploratory Study of System-2 Alignment","summary":"  The o1 system card identifies the o1 models as the most robust within OpenAI,\nwith their defining characteristic being the progression from rapid, intuitive\nthinking to slower, more deliberate reasoning. This observation motivated us to\ninvestigate the influence of System-2 thinking patterns on model safety. In our\npreliminary research, we conducted safety evaluations of the o1 model,\nincluding complex jailbreak attack scenarios using adversarial natural language\nprompts and mathematical encoding prompts. Our findings indicate that the o1\nmodel demonstrates relatively improved safety performance; however, it still\nexhibits vulnerabilities, particularly against jailbreak attacks employing\nmathematical encoding. Through detailed case analysis, we identified specific\npatterns in the o1 model's responses. We also explored the alignment of\nSystem-2 safety in open-source models using prompt engineering and supervised\nfine-tuning techniques. Experimental results show that some simple methods to\nencourage the model to carefully scrutinize user requests are beneficial for\nmodel safety. Additionally, we proposed a implementation plan for process\nsupervision to enhance safety alignment. The implementation details and\nexperimental results will be provided in future versions.\n","authors":["Yuhang Wang","Yuxiang Zhang","Yanxu Zhu","Xinyan Wen","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2411.17075v5.pdf","comment":"In this version, the DPO and reinforcement learning methods have been\n  added"},{"id":"http://arxiv.org/abs/2501.06252v2","updated":"2025-01-14T02:52:26Z","published":"2025-01-09T01:19:21Z","title":"$\\text{Transformer}^2$: Self-adaptive LLMs","summary":"  Self-adaptive large language models (LLMs) aim to solve the challenges posed\nby traditional fine-tuning methods, which are often computationally intensive\nand static in their ability to handle diverse tasks. We introduce\n$\\text{Transformer}^2$, a novel self-adaptation framework that adapts LLMs for\nunseen tasks in real-time by selectively adjusting only the singular components\nof their weight matrices. During inference, $\\text{Transformer}^2$ employs a\ntwo-pass mechanism: first, a dispatch system identifies the task properties,\nand then task-specific \"expert\" vectors, trained using reinforcement learning,\nare dynamically mixed to obtain targeted behavior for the incoming prompt. Our\nmethod outperforms ubiquitous approaches such as LoRA, with fewer parameters\nand greater efficiency. $\\text{Transformer}^2$ demonstrates versatility across\ndifferent LLM architectures and modalities, including vision-language tasks.\n$\\text{Transformer}^2$ represents a significant leap forward, offering a\nscalable, efficient solution for enhancing the adaptability and task-specific\nperformance of LLMs, paving the way for truly dynamic, self-organizing AI\nsystems.\n","authors":["Qi Sun","Edoardo Cetin","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2501.06252v2.pdf","comment":"18 panges, 11 figures, 9 tables"},{"id":"http://arxiv.org/abs/2407.02772v2","updated":"2025-01-14T02:30:09Z","published":"2024-07-03T03:01:43Z","title":"Gradient descent with generalized Newton's method","summary":"  We propose the generalized Newton's method (GeN) -- a Hessian-informed\napproach that applies to any optimizer such as SGD and Adam, and covers the\nNewton-Raphson method as a sub-case. Our method automatically and dynamically\nselects the learning rate that accelerates the convergence, without the\nintensive tuning of the learning rate scheduler. In practice, our method is\neasily implementable, since it only requires additional forward passes with\nalmost zero computational overhead (in terms of training time and memory cost),\nif the overhead is amortized over many iterations. We present extensive\nexperiments on language and vision tasks (e.g. GPT and ResNet) to showcase that\nGeN optimizers match the state-of-the-art performance, which was achieved with\ncarefully tuned learning rate schedulers.\n","authors":["Zhiqi Bu","Shiyun Xu"],"pdf_url":"https://arxiv.org/pdf/2407.02772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07783v1","updated":"2025-01-14T01:57:41Z","published":"2025-01-14T01:57:41Z","title":"Parameter-Inverted Image Pyramid Networks for Visual Perception and\n  Multimodal Understanding","summary":"  Image pyramids are widely adopted in top-performing methods to obtain\nmulti-scale features for precise visual perception and understanding. However,\ncurrent image pyramids use the same large-scale model to process multiple\nresolutions of images, leading to significant computational cost. To address\nthis challenge, we propose a novel network architecture, called\nParameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses\npretrained models (ViTs or CNNs) as branches to process multi-scale images,\nwhere images of higher resolutions are processed by smaller network branches to\nbalance computational cost and performance. To integrate information from\ndifferent spatial scales, we further propose a novel cross-branch feature\ninteraction mechanism. To validate PIIP, we apply it to various perception\nmodels and a representative multimodal large language model called LLaVA, and\nconduct extensive experiments on various tasks such as object detection,\nsegmentation, image classification and multimodal understanding. PIIP achieves\nsuperior performance compared to single-branch and existing multi-resolution\napproaches with lower computational cost. When applied to InternViT-6B, a\nlarge-scale vision foundation model, PIIP can improve its performance by 1%-2%\non detection and segmentation with only 40%-60% of the original computation,\nfinally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For\nmultimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and\n74.5% on MMBench with only 2.8M training data. Our code is released at\nhttps://github.com/OpenGVLab/PIIP.\n","authors":["Zhaokai Wang","Xizhou Zhu","Xue Yang","Gen Luo","Hao Li","Changyao Tian","Wenhan Dou","Junqi Ge","Lewei Lu","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2501.07783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19018v2","updated":"2025-01-14T01:42:40Z","published":"2024-12-26T01:56:42Z","title":"Let the Rule Speak: Enhancing In-context Learning Debiasing with\n  Interpretability","summary":"  In-context learning, which allows large language models to perform diverse\ntasks with a few demonstrations, is found to have imbalanced per-class\nprediction accuracy on multi-class text classification. Although notable output\ncorrection methods have been developed to tackle the issue and simultaneously\nimprove downstream prediction accuracy, they may fail to answer the core\ninterpretability challenges: why and which certain classes need corrections,\nand more importantly, a tailored correction for per-sample, per-class's\nprobability. To address such interpretability gaps, we first find that the\nimbalance arises from certain classes consistently receiving high ICL output\nprobabilities, whereas others receiving lower or mixed ranges, so the former is\nmore frequently chosen, resulting in higher accuracy; more crucially, we find\nthat these ranges have significantly varying degrees of influence on the\naccuracy bias, highlighting the need for precise, interpretable probability\ncorrections by range. Motivated by this, we propose FuRud, a Fuzzy Rule\nOptimization based Debiasing method, that (1) detects which classes need\ncorrections, and (2) for each correction-needed class, detects its probability\nranges and applies asymmetric amplifications or reductions to correct them\ninterpretably. Notably, across seven benchmark datasets, FuRud reduces the\npairwise class accuracy bias (COBias) by more than half (56%), while achieving\na relative increase of 21% in accuracy, outperforming state-of-the-art\ndebiasing methods. Moreover, FuRud can optimize downstream tasks with as few as\n10 optimization examples. Furthermore, FuRud can work for prompt formats that\nlead to highly skewed predictions. For example, FuRud greatly improves ICL\noutputs which use letter options, with 44% relative accuracy increase and 54%\nrelative COBias reduction.\n","authors":["Ruixi Lin","Yang You"],"pdf_url":"https://arxiv.org/pdf/2412.19018v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01313v8","updated":"2025-01-14T01:28:03Z","published":"2023-02-02T18:39:30Z","title":"Double Equivariance for Inductive Link Prediction for Both New Nodes and\n  New Relation Types","summary":"  The task of fully inductive link prediction in knowledge graphs has gained\nsignificant attention, with various graph neural networks being proposed to\naddress it. This task presents greater challenges than traditional inductive\nlink prediction tasks with only new nodes, as models must be capable of\nzero-shot generalization to both unseen nodes and unseen relation types in the\ninference graph. Despite the development of novel models, a unifying\ntheoretical understanding of their success remains elusive, and the limitations\nof these methods are not well-studied. In this work, we introduce the concept\nof double permutation-equivariant representations and demonstrate its necessity\nfor effective performance in this task. We show that many existing models,\ndespite their diverse architectural designs, conform to this framework.\nHowever, we also identify inherent limitations in double\npermutation-equivariant representations, which restrict these models's ability\nto learn effectively on datasets with varying characteristics. Our findings\nsuggest that while double equivariance is necessary for meta-learning across\nknowledge graphs from different domains, it is not sufficient. There remains a\nfundamental gap between double permutation-equivariant models and the concept\nof foundation models designed to learn patterns across all domains.\n","authors":["Jincheng Zhou","Yucheng Zhang","Jianfei Gao","Yangze Zhou","Bruno Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2302.01313v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09008v2","updated":"2025-01-14T01:21:55Z","published":"2024-06-13T11:19:50Z","title":"LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large\n  Language Models","summary":"  Topic modeling has been a widely used tool for unsupervised text analysis.\nHowever, comprehensive evaluations of a topic model remain challenging.\nExisting evaluation methods are either less comparable across different models\n(e.g., perplexity) or focus on only one specific aspect of a model (e.g., topic\nquality or document representation quality) at a time, which is insufficient to\nreflect the overall model performance. In this paper, we propose WALM (Word\nAgreement with Language Model), a new evaluation method for topic modeling that\nconsiders the semantic quality of document representations and topics in a\njoint manner, leveraging the power of Large Language Models (LLMs). With\nextensive experiments involving different types of topic models, WALM is shown\nto align with human judgment and can serve as a complementary evaluation method\nto the existing ones, bringing a new perspective to topic modeling. Our\nsoftware package is available at\nhttps://github.com/Xiaohao-Yang/Topic_Model_Evaluation.\n","authors":["Xiaohao Yang","He Zhao","Dinh Phung","Wray Buntine","Lan Du"],"pdf_url":"https://arxiv.org/pdf/2406.09008v2.pdf","comment":"Forthcoming in Transactions of the Association for Computational\n  Linguistics (TACL) published by MIT Press"},{"id":"http://arxiv.org/abs/2501.07766v1","updated":"2025-01-14T00:47:24Z","published":"2025-01-14T00:47:24Z","title":"Large Language Models for Knowledge Graph Embedding Techniques, Methods,\n  and Challenges: A Survey","summary":"  Large Language Models (LLMs) have attracted a lot of attention in various\nfields due to their superior performance, aiming to train hundreds of millions\nor more parameters on large amounts of text data to understand and generate\nnatural language. As the superior performance of LLMs becomes apparent, they\nare increasingly being applied to knowledge graph embedding (KGE) related tasks\nto improve the processing results. As a deep learning model in the field of\nNatural Language Processing (NLP), it learns a large amount of textual data to\npredict the next word or generate content related to a given text. However,\nLLMs have recently been invoked to varying degrees in different types of KGE\nrelated scenarios such as multi-modal KGE and open KGE according to their task\ncharacteristics. In this paper, we investigate a wide range of approaches for\nperforming LLMs-related tasks in different types of KGE scenarios. To better\ncompare the various approaches, we summarize each KGE scenario in a\nclassification. In addition to the categorization methods, we provide a tabular\noverview of the methods and their source code links for a more direct\ncomparison. In the article we also discuss the applications in which the\nmethods are mainly used and suggest several forward-looking directions for the\ndevelopment of this new research area.\n","authors":["Bingchen Liu","Xin Li"],"pdf_url":"https://arxiv.org/pdf/2501.07766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08496v1","updated":"2025-01-14T23:59:23Z","published":"2025-01-14T23:59:23Z","title":"Quantifying the Importance of Data Alignment in Downstream Model\n  Performance","summary":"  Contrary to the conventional emphasis on dataset size, we explore the role of\ndata alignment -- an often overlooked aspect of data quality -- in training\ncapable Large Language Models (LLMs). To do so, we use the Task2Vec-based\nalignment coefficient, a quantitative measure of the similarity between two\ndatasets, to quantify the impact of alignment between training data and\nevaluation data on downstream performance. In particular, we conduct controlled\n\\textit{interventional} experiments for two settings: 1. the impact of\nincreased alignment coefficients between various pre-training (pt) against\nevaluation datasets, and 2. the impact of increased alignment coefficients\nbetween domain specific fine-tuning (ft) against domain specific evaluation.\nThe domain specific task we explore is Autoformalization -- the machine\ntranslation task between natural language and code for formal verification. In\nboth settings, we find a strong, predictable negative correlation between the\nalignment coefficient of a model's training and evaluation data and the model's\nloss/perplexity on the respective downstream task. These findings suggest a\nre-evaluation of LLM training approaches, demonstrating the relevance of data\nalignment compared to data quantity, especially in specialized downstream tasks\nsuch as Autoformalization.\n","authors":["Krrish Chawla","Aryan Sahai","Mario DePavia","Sudharsan Sundar","Brando Miranda"],"pdf_url":"https://arxiv.org/pdf/2501.08496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08474v1","updated":"2025-01-14T22:38:55Z","published":"2025-01-14T22:38:55Z","title":"The Theater Stage as Laboratory: Review of Real-Time Comedy LLM Systems\n  for Live Performance","summary":"  In this position paper, we review the eclectic recent history of academic and\nartistic works involving computational systems for humor generation, and focus\nspecifically on live performance. We make the case that AI comedy should be\nevaluated in live conditions, in front of audiences sharing either physical or\nonline spaces, and under real-time constraints. We further suggest that\nimprovised comedy is therefore the perfect substrate for deploying and\nassessing computational humor systems. Using examples of successful AI-infused\nshows, we demonstrate that live performance raises three sets of challenges for\ncomputational humor generation: 1) questions around robotic embodiment,\nanthropomorphism and competition between humans and machines, 2) questions\naround comedic timing and the nature of audience interaction, and 3) questions\nabout the human interpretation of seemingly absurd AI-generated humor. We argue\nthat these questions impact the choice of methodologies for evaluating\ncomputational humor, as any such method needs to work around the constraints of\nlive audiences and performance spaces. These interrogations also highlight\ndifferent types of collaborative relationship of human comedians towards AI\ntools.\n","authors":["Piotr Wojciech Mirowski","Boyd Branch","Kory Wallace Mathewson"],"pdf_url":"https://arxiv.org/pdf/2501.08474v1.pdf","comment":"8 pages, 1st Workshop on Computational Humor (CHum), COLING 2025"},{"id":"http://arxiv.org/abs/2501.08468v1","updated":"2025-01-14T22:27:48Z","published":"2025-01-14T22:27:48Z","title":"Selective Attention Merging for low resource tasks: A case study of\n  Child ASR","summary":"  While Speech Foundation Models (SFMs) excel in various speech tasks, their\nperformance for low-resource tasks such as child Automatic Speech Recognition\n(ASR) is hampered by limited pretraining data. To address this, we explore\ndifferent model merging techniques to leverage knowledge from models trained on\nlarger, more diverse speech corpora. This paper also introduces Selective\nAttention (SA) Merge, a novel method that selectively merges task vectors from\nattention matrices to enhance SFM performance on low-resource tasks.\nExperiments on the MyST database show significant reductions in relative word\nerror rate of up to 14%, outperforming existing model merging and data\naugmentation techniques. By combining data augmentation techniques with SA\nMerge, we achieve a new state-of-the-art WER of 8.69 on the MyST database for\nthe Whisper-small model, highlighting the potential of SA Merge for improving\nlow-resource ASR.\n","authors":["Natarajan Balaji Shankar","Zilai Wang","Eray Eren","Abeer Alwan"],"pdf_url":"https://arxiv.org/pdf/2501.08468v1.pdf","comment":"To appear in ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.08460v1","updated":"2025-01-14T22:09:06Z","published":"2025-01-14T22:09:06Z","title":"Towards Zero-Shot & Explainable Video Description by Reasoning over\n  Graphs of Events in Space and Time","summary":"  In the current era of Machine Learning, Transformers have become the de facto\napproach across a variety of domains, such as computer vision and natural\nlanguage processing. Transformer-based solutions are the backbone of current\nstate-of-the-art methods for language generation, image and video\nclassification, segmentation, action and object recognition, among many others.\nInterestingly enough, while these state-of-the-art methods produce impressive\nresults in their respective domains, the problem of understanding the\nrelationship between vision and language is still beyond our reach. In this\nwork, we propose a common ground between vision and language based on events in\nspace and time in an explainable and programmatic way, to connect\nlearning-based vision and language state of the art models and provide a\nsolution to the long standing problem of describing videos in natural language.\nWe validate that our algorithmic approach is able to generate coherent, rich\nand relevant textual descriptions on videos collected from a variety of\ndatasets, using both standard metrics (e.g. Bleu, ROUGE) and the modern\nLLM-as-a-Jury approach.\n","authors":["Mihai Masala","Marius Leordeanu"],"pdf_url":"https://arxiv.org/pdf/2501.08460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11186v4","updated":"2025-01-14T22:07:53Z","published":"2024-07-15T19:17:31Z","title":"Empowering Persian LLMs for Instruction Following: A Novel Dataset and\n  Training Approach","summary":"  Instruction-tuned large language models have demonstrated remarkable\ncapabilities in following human instructions across various domains. However,\ntheir proficiency remains notably deficient in many low-resource languages. To\naddress this challenge, we begin by introducing FarsInstruct a comprehensive\ninstruction dataset designed to enhance the instruction following ability of\nlarge language models specifically for the Persian language a significant yet\nunderrepresented language globally. FarsInstruct encompasses a wide range of\ntask types and datasets, each containing a mix of straightforward to complex\nmanual written instructions, as well as translations from the Public Pool of\nPrompts, ensuring a rich linguistic and cultural representation. Furthermore,\nwe introduce Co-CoLA, a framework designed to enhance the multi-task\nadaptability of LoRA-tuned models. Through extensive experimental analyses, our\nstudy showcases the effectiveness of the FarsInstruct dataset coupled with\ntraining by the Co-CoLA framework, in improving the performance of large\nlanguage models within the Persian context. As of the current writing,\nFarsInstruct comprises 197 templates across 21 distinct datasets, and we intend\nto update it consistently, thus augmenting its applicability.\n","authors":["Hojjat Mokhtarabadi","Ziba Zamani","Abbas Maazallahi","Mohammad Hossein Manshaei"],"pdf_url":"https://arxiv.org/pdf/2407.11186v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08457v1","updated":"2025-01-14T22:02:38Z","published":"2025-01-14T22:02:38Z","title":"Large Language Models For Text Classification: Case Study And\n  Comprehensive Review","summary":"  Unlocking the potential of Large Language Models (LLMs) in data\nclassification represents a promising frontier in natural language processing.\nIn this work, we evaluate the performance of different LLMs in comparison with\nstate-of-the-art deep-learning and machine-learning models, in two different\nclassification scenarios: i) the classification of employees' working locations\nbased on job reviews posted online (multiclass classification), and 2) the\nclassification of news articles as fake or not (binary classification). Our\nanalysis encompasses a diverse range of language models differentiating in\nsize, quantization, and architecture. We explore the impact of alternative\nprompting techniques and evaluate the models based on the weighted F1-score.\nAlso, we examine the trade-off between performance (F1-score) and time\n(inference response time) for each language model to provide a more nuanced\nunderstanding of each model's practical applicability. Our work reveals\nsignificant variations in model responses based on the prompting strategies. We\nfind that LLMs, particularly Llama3 and GPT-4, can outperform traditional\nmethods in complex classification tasks, such as multiclass classification,\nthough at the cost of longer inference times. In contrast, simpler ML models\noffer better performance-to-time trade-offs in simpler binary classification\ntasks.\n","authors":["Arina Kostina","Marios D. Dikaiakos","Dimosthenis Stefanidis","George Pallis"],"pdf_url":"https://arxiv.org/pdf/2501.08457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03205v3","updated":"2025-01-14T21:58:47Z","published":"2024-12-04T10:44:50Z","title":"U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills\n  in LLMs","summary":"  The current evaluation of mathematical skills in LLMs is limited, as existing\nbenchmarks are either relatively small, primarily focus on elementary and\nhigh-school problems, or lack diversity in topics. Additionally, the inclusion\nof visual elements in tasks remains largely under-explored.\n  To address these gaps, we introduce U-MATH, a novel benchmark of 1,100\nunpublished open-ended university-level problems sourced from teaching\nmaterials. It is balanced across six core subjects, with 20% of multimodal\nproblems. Given the open-ended nature of U-MATH problems, we employ an LLM to\njudge the correctness of generated solutions. To this end, we release\n$\\mu$-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.\n  The evaluation of general domain, math-specific, and multimodal LLMs\nhighlights the challenges presented by U-MATH. Our findings reveal that LLMs\nachieve a maximum accuracy of only 63% on text-based tasks, with even lower 45%\non visual problems. The solution assessment proves challenging for LLMs, with\nthe best LLM judge having an F1-score of 80% on $\\mu$-MATH.\n","authors":["Konstantin Chernyshev","Vitaliy Polshkov","Ekaterina Artemova","Alex Myasnikov","Vlad Stepanov","Alexei Miasnikov","Sergei Tilga"],"pdf_url":"https://arxiv.org/pdf/2412.03205v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08454v1","updated":"2025-01-14T21:55:37Z","published":"2025-01-14T21:55:37Z","title":"Tag&Tab: Pretraining Data Detection in Large Language Models Using\n  Keyword-Based Membership Inference Attack","summary":"  Large language models (LLMs) have become essential digital task assistance\ntools. Their training relies heavily on the collection of vast amounts of data,\nwhich may include copyright-protected or sensitive information. Recent studies\non the detection of pretraining data in LLMs have primarily focused on\nsentence-level or paragraph-level membership inference attacks (MIAs), usually\ninvolving probability analysis of the target model prediction tokens. However,\nthe proposed methods often demonstrate poor performance, specifically in terms\nof accuracy, failing to account for the semantic importance of textual content\nand word significance. To address these shortcomings, we propose Tag&Tab, a\nnovel approach for detecting data that has been used as part of the LLM\npretraining. Our method leverages advanced natural language processing (NLP)\ntechniques to tag keywords in the input text - a process we term Tagging. Then,\nthe LLM is used to obtain the probabilities of these keywords and calculate\ntheir average log-likelihood to determine input text membership, a process we\nrefer to as Tabbing. Our experiments on three benchmark datasets (BookMIA,\nMIMIR, and the Pile) and several open-source LLMs of varying sizes demonstrate\nan average increase in the AUC scores ranging from 4.1% to 12.1% over\nstate-of-the-art methods. Tag&Tab not only sets a new standard for data leakage\ndetection in LLMs, but its outstanding performance is a testament to the\nimportance of words in MIAs on LLMs.\n","authors":["Sagiv Antebi","Edan Habler","Asaf Shabtai","Yuval Elovici"],"pdf_url":"https://arxiv.org/pdf/2501.08454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08442v1","updated":"2025-01-14T21:21:39Z","published":"2025-01-14T21:21:39Z","title":"Jochre 3 and the Yiddish OCR corpus","summary":"  We describe the construction of a publicly available Yiddish OCR Corpus, and\ndescribe and evaluate the open source OCR tool suite Jochre 3, including an\nAlto editor for corpus annotation, OCR software for Alto OCR layer generation,\nand a customizable OCR search engine. The current version of the Yiddish OCR\ncorpus contains 658 pages, 186K tokens and 840K glyphs. The Jochre 3 OCR tool\nuses various fine-tuned YOLOv8 models for top-down page layout analysis, and a\ncustom CNN network for glyph recognition. It attains a CER of 1.5% on our test\ncorpus, far out-performing all other existing public models for Yiddish. We\nanalyzed the full 660M word Yiddish Book Center with Jochre 3 OCR, and the new\nOCR is searchable through the Yiddish Book Center OCR search engine.\n","authors":["Assaf Urieli","Amber Clooney","Michelle Sigiel","Grisha Leyfer"],"pdf_url":"https://arxiv.org/pdf/2501.08442v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2501.08441v1","updated":"2025-01-14T21:10:08Z","published":"2025-01-14T21:10:08Z","title":"Religious Bias Landscape in Language and Text-to-Image Models: Analysis,\n  Detection, and Debiasing Strategies","summary":"  Note: This paper includes examples of potentially offensive content related\nto religious bias, presented solely for academic purposes. The widespread\nadoption of language models highlights the need for critical examinations of\ntheir inherent biases, particularly concerning religion. This study\nsystematically investigates religious bias in both language models and\ntext-to-image generation models, analyzing both open-source and closed-source\nsystems. We construct approximately 400 unique, naturally occurring prompts to\nprobe language models for religious bias across diverse tasks, including mask\nfilling, prompt completion, and image generation. Our experiments reveal\nconcerning instances of underlying stereotypes and biases associated\ndisproportionately with certain religions. Additionally, we explore\ncross-domain biases, examining how religious bias intersects with demographic\nfactors such as gender, age, and nationality. This study further evaluates the\neffectiveness of targeted debiasing techniques by employing corrective prompts\ndesigned to mitigate the identified biases. Our findings demonstrate that\nlanguage models continue to exhibit significant biases in both text and image\ngeneration tasks, emphasizing the urgent need to develop fairer language models\nto achieve global acceptability.\n","authors":["Ajwad Abrar","Nafisa Tabassum Oeshy","Mohsinul Kabir","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2501.08441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.02056v2","updated":"2025-01-14T21:02:56Z","published":"2024-12-03T00:28:31Z","title":"A Multi-way Parallel Named Entity Annotated Corpus for English, Tamil\n  and Sinhala","summary":"  This paper presents a multi-way parallel English-Tamil-Sinhala corpus\nannotated with Named Entities (NEs), where Sinhala and Tamil are low-resource\nlanguages. Using pre-trained multilingual Language Models (mLMs), we establish\nnew benchmark Named Entity Recognition (NER) results on this dataset for\nSinhala and Tamil. We also carry out a detailed investigation on the NER\ncapabilities of different types of mLMs. Finally, we demonstrate the utility of\nour NER system on a low-resource Neural Machine Translation (NMT) task. Our\ndataset is publicly released: https://github.com/suralk/multiNER.\n","authors":["Surangika Ranathunga","Asanka Ranasinghea","Janaka Shamala","Ayodya Dandeniyaa","Rashmi Galappaththia","Malithi Samaraweeraa"],"pdf_url":"https://arxiv.org/pdf/2412.02056v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08421v1","updated":"2025-01-14T20:24:12Z","published":"2025-01-14T20:24:12Z","title":"SEAL: Speaker Error Correction using Acoustic-conditioned Large Language\n  Models","summary":"  Speaker Diarization (SD) is a crucial component of modern end-to-end ASR\npipelines. Traditional SD systems, which are typically audio-based and operate\nindependently of ASR, often introduce speaker errors, particularly during\nspeaker transitions and overlapping speech. Recently, language models including\nfine-tuned large language models (LLMs) have shown to be effective as a\nsecond-pass speaker error corrector by leveraging lexical context in the\ntranscribed output. In this work, we introduce a novel acoustic conditioning\napproach to provide more fine-grained information from the acoustic diarizer to\nthe LLM. We also show that a simpler constrained decoding strategy reduces LLM\nhallucinations, while avoiding complicated post-processing. Our approach\nsignificantly reduces the speaker error rates by 24-43% across Fisher,\nCallhome, and RT03-CTS datasets, compared to the first-pass Acoustic SD.\n","authors":["Anurag Kumar","Rohit Paturi","Amber Afshan","Sundararajan Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2501.08421v1.pdf","comment":"Accepted at ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.08413v1","updated":"2025-01-14T20:08:16Z","published":"2025-01-14T20:08:16Z","title":"Ensemble of Large Language Models for Curated Labeling and Rating of\n  Free-text Data","summary":"  Free-text responses are commonly collected in psychological studies,\nproviding rich qualitative insights that quantitative measures may not capture.\nLabeling curated topics of research interest in free-text data by multiple\ntrained human coders is typically labor-intensive and time-consuming. Though\nlarge language models (LLMs) excel in language processing, LLM-assisted\nlabeling techniques relying on closed-source LLMs cannot be directly applied to\nfree-text data, without explicit consent for external use.\n  In this study, we propose a framework of assembling locally-deployable LLMs\nto enhance the labeling of predetermined topics in free-text data under privacy\nconstraints. Analogous to annotation by multiple human raters, this framework\nleverages the heterogeneity of diverse open-source LLMs. The ensemble approach\nseeks a balance between the agreement and disagreement across LLMs, guided by a\nrelevancy scoring methodology that utilizes embedding distances between topic\ndescriptions and LLMs' reasoning. We evaluated the ensemble approach using both\npublicly accessible Reddit data from eating disorder related forums, and\nfree-text responses from eating disorder patients, both complemented by human\nannotations.\n  We found that: (1) there is heterogeneity in the performance of labeling\namong same-sized LLMs, with some showing low sensitivity but high precision,\nwhile others exhibit high sensitivity but low precision. (2) Compared to\nindividual LLMs, the ensemble of LLMs achieved the highest accuracy and optimal\nprecision-sensitivity trade-off in predicting human annotations. (3) The\nrelevancy scores across LLMs showed greater agreement than dichotomous labels,\nindicating that the relevancy scoring method effectively mitigates the\nheterogeneity in LLMs' labeling.\n","authors":["Jiaxing Qiu","Dongliang Guo","Papini Natalie","Peace Noelle","Levinson Cheri","Teague R. Henry"],"pdf_url":"https://arxiv.org/pdf/2501.08413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08406v1","updated":"2025-01-14T19:53:58Z","published":"2025-01-14T19:53:58Z","title":"OptiChat: Bridging Optimization Models and Practitioners with Large\n  Language Models","summary":"  Optimization models have been applied to solve a wide variety of\ndecision-making problems. These models are usually developed by optimization\nexperts but are used by practitioners without optimization expertise in various\napplication domains. As a result, practitioners often struggle to interact with\nand draw useful conclusions from optimization models independently. To fill\nthis gap, we introduce OptiChat, a natural language dialogue system designed to\nhelp practitioners interpret model formulation, diagnose infeasibility, analyze\nsensitivity, retrieve information, evaluate modifications, and provide\ncounterfactual explanations. By augmenting large language models (LLMs) with\nfunctional calls and code generation tailored for optimization models, we\nenable seamless interaction and minimize the risk of hallucinations in\nOptiChat. We develop a new dataset to evaluate OptiChat's performance in\nexplaining optimization models. Experiments demonstrate that OptiChat\neffectively bridges the gap between optimization models and practitioners,\ndelivering autonomous, accurate, and instant responses.\n","authors":["Hao Chen","Gonzalo Esteban Constante-Flores","Krishna Sri Ipsit Mantri","Sai Madhukiran Kompalli","Akshdeep Singh Ahluwalia","Can Li"],"pdf_url":"https://arxiv.org/pdf/2501.08406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08365v1","updated":"2025-01-14T17:18:05Z","published":"2025-01-14T17:18:05Z","title":"Towards Best Practices for Open Datasets for LLM Training","summary":"  Many AI companies are training their large language models (LLMs) on data\nwithout the permission of the copyright owners. The permissibility of doing so\nvaries by jurisdiction: in countries like the EU and Japan, this is allowed\nunder certain restrictions, while in the United States, the legal landscape is\nmore ambiguous. Regardless of the legal status, concerns from creative\nproducers have led to several high-profile copyright lawsuits, and the threat\nof litigation is commonly cited as a reason for the recent trend towards\nminimizing the information shared about training datasets by both corporate and\npublic interest actors. This trend in limiting data information causes harm by\nhindering transparency, accountability, and innovation in the broader ecosystem\nby denying researchers, auditors, and impacted individuals access to the\ninformation needed to understand AI models.\n  While this could be mitigated by training language models on open access and\npublic domain data, at the time of writing, there are no such models (trained\nat a meaningful scale) due to the substantial technical and sociological\nchallenges in assembling the necessary corpus. These challenges include\nincomplete and unreliable metadata, the cost and complexity of digitizing\nphysical records, and the diverse set of legal and technical skills required to\nensure relevance and responsibility in a quickly changing landscape. Building\ntowards a future where AI systems can be trained on openly licensed data that\nis responsibly curated and governed requires collaboration across legal,\ntechnical, and policy domains, along with investments in metadata standards,\ndigitization, and fostering a culture of openness.\n","authors":["Stefan Baack","Stella Biderman","Kasia Odrozek","Aviya Skowron","Ayah Bdeir","Jillian Bommarito","Jennifer Ding","Maximilian Gahntz","Paul Keller","Pierre-Carl Langlais","Greg Lindahl","Sebastian Majstorovic","Nik Marda","Guilherme Penedo","Maarten Van Segbroeck","Jennifer Wang","Leandro von Werra","Mitchell Baker","Julie Belião","Kasia Chmielinski","Marzieh Fadaee","Lisa Gutermuth","Hynek Kydlíček","Greg Leppert","EM Lewis-Jong","Solana Larsen","Shayne Longpre","Angela Oduor Lungati","Cullen Miller","Victor Miller","Max Ryabinin","Kathleen Siminyu","Andrew Strait","Mark Surman","Anna Tumadóttir","Maurice Weber","Rebecca Weiss","Lee White","Thomas Wolf"],"pdf_url":"https://arxiv.org/pdf/2501.08365v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2501.08333v1","updated":"2025-01-14T18:59:59Z","published":"2025-01-14T18:59:59Z","title":"DAViD: Modeling Dynamic Affordance of 3D Objects using Pre-trained Video\n  Diffusion Models","summary":"  Understanding the ability of humans to use objects is crucial for AI to\nimprove daily life. Existing studies for learning such ability focus on\nhuman-object patterns (e.g., contact, spatial relation, orientation) in static\nsituations, and learning Human-Object Interaction (HOI) patterns over time\n(i.e., movement of human and object) is relatively less explored. In this\npaper, we introduce a novel type of affordance named Dynamic Affordance. For a\ngiven input 3D object mesh, we learn dynamic affordance which models the\ndistribution of both (1) human motion and (2) human-guided object pose during\ninteractions. As a core idea, we present a method to learn the 3D dynamic\naffordance from synthetically generated 2D videos, leveraging a pre-trained\nvideo diffusion model. Specifically, we propose a pipeline that first generates\n2D HOI videos from the 3D object and then lifts them into 3D to generate 4D HOI\nsamples. Once we generate diverse 4D HOI samples on various target objects, we\ntrain our DAViD, where we present a method based on the Low-Rank Adaptation\n(LoRA) module for pre-trained human motion diffusion model (MDM) and an object\npose diffusion model with human pose guidance. Our motion diffusion model is\nextended for multi-object interactions, demonstrating the advantage of our\npipeline with LoRA for combining the concepts of object usage. Through\nextensive experiments, we demonstrate our DAViD outperforms the baselines in\ngenerating human motion with HOIs.\n","authors":["Hyeonwoo Kim","Sangwon Beak","Hanbyul Joo"],"pdf_url":"https://arxiv.org/pdf/2501.08333v1.pdf","comment":"Project Page: https://snuvclab.github.io/david/"},{"id":"http://arxiv.org/abs/2501.08332v1","updated":"2025-01-14T18:59:55Z","published":"2025-01-14T18:59:55Z","title":"MangaNinja: Line Art Colorization with Precise Reference Following","summary":"  Derived from diffusion models, MangaNinjia specializes in the task of\nreference-guided line art colorization. We incorporate two thoughtful designs\nto ensure precise character detail transcription, including a patch shuffling\nmodule to facilitate correspondence learning between the reference color image\nand the target line art, and a point-driven control scheme to enable\nfine-grained color matching. Experiments on a self-collected benchmark\ndemonstrate the superiority of our model over current solutions in terms of\nprecise colorization. We further showcase the potential of the proposed\ninteractive point control in handling challenging cases, cross-character\ncolorization, multi-reference harmonization, beyond the reach of existing\nalgorithms.\n","authors":["Zhiheng Liu","Ka Leong Cheng","Xi Chen","Jie Xiao","Hao Ouyang","Kai Zhu","Yu Liu","Yujun Shen","Qifeng Chen","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2501.08332v1.pdf","comment":"Project page and code: https://johanan528.github.io/MangaNinjia/"},{"id":"http://arxiv.org/abs/2501.08331v1","updated":"2025-01-14T18:59:10Z","published":"2025-01-14T18:59:10Z","title":"Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using\n  Real-Time Warped Noise","summary":"  Generative modeling aims to transform random noise into structured outputs.\nIn this work, we enhance video diffusion models by allowing motion control via\nstructured latent noise sampling. This is achieved by just a change in data: we\npre-process training videos to yield structured noise. Consequently, our method\nis agnostic to diffusion model design, requiring no changes to model\narchitectures or training pipelines. Specifically, we propose a novel noise\nwarping algorithm, fast enough to run in real time, that replaces random\ntemporal Gaussianity with correlated warped noise derived from optical flow\nfields, while preserving the spatial Gaussianity. The efficiency of our\nalgorithm enables us to fine-tune modern video diffusion base models using\nwarped noise with minimal overhead, and provide a one-stop solution for a wide\nrange of user-friendly motion control: local object motion control, global\ncamera movement control, and motion transfer. The harmonization between\ntemporal coherence and spatial Gaussianity in our warped noise leads to\neffective motion control while maintaining per-frame pixel quality. Extensive\nexperiments and user studies demonstrate the advantages of our method, making\nit a robust and scalable approach for controlling motion in video diffusion\nmodels. Video results are available on our webpage:\nhttps://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow/; source\ncode and model checkpoints are available on GitHub:\nhttps://github.com/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow.\n","authors":["Ryan Burgert","Yuancheng Xu","Wenqi Xian","Oliver Pilarski","Pascal Clausen","Mingming He","Li Ma","Yitong Deng","Lingxiao Li","Mohsen Mousavi","Michael Ryoo","Paul Debevec","Ning Yu"],"pdf_url":"https://arxiv.org/pdf/2501.08331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08329v1","updated":"2025-01-14T18:59:05Z","published":"2025-01-14T18:59:05Z","title":"Predicting 4D Hand Trajectory from Monocular Videos","summary":"  We present HaPTIC, an approach that infers coherent 4D hand trajectories from\nmonocular videos. Current video-based hand pose reconstruction methods\nprimarily focus on improving frame-wise 3D pose using adjacent frames rather\nthan studying consistent 4D hand trajectories in space. Despite the additional\ntemporal cues, they generally underperform compared to image-based methods due\nto the scarcity of annotated video data. To address these issues, we repurpose\na state-of-the-art image-based transformer to take in multiple frames and\ndirectly predict a coherent trajectory. We introduce two types of lightweight\nattention layers: cross-view self-attention to fuse temporal information, and\nglobal cross-attention to bring in larger spatial context. Our method infers 4D\nhand trajectories similar to the ground truth while maintaining strong 2D\nreprojection alignment. We apply the method to both egocentric and allocentric\nvideos. It significantly outperforms existing methods in global trajectory\naccuracy while being comparable to the state-of-the-art in single-image pose\nestimation. Project website: https://judyye.github.io/haptic-www\n","authors":["Yufei Ye","Yao Feng","Omid Taheri","Haiwen Feng","Shubham Tulsiani","Michael J. Black"],"pdf_url":"https://arxiv.org/pdf/2501.08329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08326v1","updated":"2025-01-14T18:58:04Z","published":"2025-01-14T18:58:04Z","title":"Omni-RGPT: Unifying Image and Video Region-level Understanding via Token\n  Marks","summary":"  We present Omni-RGPT, a multimodal large language model designed to\nfacilitate region-level comprehension for both images and videos. To achieve\nconsistent region representation across spatio-temporal dimensions, we\nintroduce Token Mark, a set of tokens highlighting the target regions within\nthe visual feature space. These tokens are directly embedded into spatial\nregions using region prompts (e.g., boxes or masks) and simultaneously\nincorporated into the text prompt to specify the target, establishing a direct\nconnection between visual and text tokens. To further support robust video\nunderstanding without requiring tracklets, we introduce an auxiliary task that\nguides Token Mark by leveraging the consistency of the tokens, enabling stable\nregion interpretation across the video. Additionally, we introduce a\nlarge-scale region-level video instruction dataset (RegVID-300k). Omni-RGPT\nachieves state-of-the-art results on image and video-based commonsense\nreasoning benchmarks while showing strong performance in captioning and\nreferring expression comprehension tasks.\n","authors":["Miran Heo","Min-Hung Chen","De-An Huang","Sifei Liu","Subhashree Radhakrishnan","Seon Joo Kim","Yu-Chiang Frank Wang","Ryo Hachiuma"],"pdf_url":"https://arxiv.org/pdf/2501.08326v1.pdf","comment":"Project page: https://miranheo.github.io/omni-rgpt/"},{"id":"http://arxiv.org/abs/2501.08325v1","updated":"2025-01-14T18:57:21Z","published":"2025-01-14T18:57:21Z","title":"GameFactory: Creating New Games with Generative Interactive Videos","summary":"  Generative game engines have the potential to revolutionize game development\nby autonomously creating new content and reducing manual workload. However,\nexisting video-based game generation methods fail to address the critical\nchallenge of scene generalization, limiting their applicability to existing\ngames with fixed styles and scenes. In this paper, we present GameFactory, a\nframework focused on exploring scene generalization in game video generation.\nTo enable the creation of entirely new and diverse games, we leverage\npre-trained video diffusion models trained on open-domain video data. To bridge\nthe domain gap between open-domain priors and small-scale game dataset, we\npropose a multi-phase training strategy that decouples game style learning from\naction control, preserving open-domain generalization while achieving action\ncontrollability. Using Minecraft as our data source, we release GF-Minecraft, a\nhigh-quality and diversity action-annotated video dataset for research.\nFurthermore, we extend our framework to enable autoregressive\naction-controllable game video generation, allowing the production of\nunlimited-length interactive game videos. Experimental results demonstrate that\nGameFactory effectively generates open-domain, diverse, and action-controllable\ngame videos, representing a significant step forward in AI-driven game\ngeneration. Our dataset and project page are publicly available at\n\\url{https://vvictoryuki.github.io/gamefactory/}.\n","authors":["Jiwen Yu","Yiran Qin","Xintao Wang","Pengfei Wan","Di Zhang","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08316v1","updated":"2025-01-14T18:51:48Z","published":"2025-01-14T18:51:48Z","title":"Diffusion Adversarial Post-Training for One-Step Video Generation","summary":"  The diffusion models are widely used for image and video generation, but\ntheir iterative generation process is slow and expansive. While existing\ndistillation approaches have demonstrated the potential for one-step generation\nin the image domain, they still suffer from significant quality degradation. In\nthis work, we propose Adversarial Post-Training (APT) against real data\nfollowing diffusion pre-training for one-step video generation. To improve the\ntraining stability and quality, we introduce several improvements to the model\narchitecture and training procedures, along with an approximated R1\nregularization objective. Empirically, our experiments show that our\nadversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720,\n24fps videos in real time using a single forward evaluation step. Additionally,\nour model is capable of generating 1024px images in a single step, achieving\nquality comparable to state-of-the-art methods.\n","authors":["Shanchuan Lin","Xin Xia","Yuxi Ren","Ceyuan Yang","Xuefeng Xiao","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2501.08316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07169v3","updated":"2025-01-14T18:51:43Z","published":"2024-12-10T04:03:46Z","title":"Rate-In: Information-Driven Adaptive Dropout Rates for Improved\n  Inference-Time Uncertainty Estimation","summary":"  Accurate uncertainty estimation is crucial for deploying neural networks in\nrisk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a\nwidely used technique for approximating predictive uncertainty by performing\nstochastic forward passes with dropout during inference. However, using static\ndropout rates across all layers and inputs can lead to suboptimal uncertainty\nestimates, as it fails to adapt to the varying characteristics of individual\ninputs and network layers. Existing approaches optimize dropout rates during\ntraining using labeled data, resulting in fixed inference-time parameters that\ncannot adjust to new data distributions, compromising uncertainty estimates in\nMonte Carlo simulations.\n  In this paper, we propose Rate-In, an algorithm that dynamically adjusts\ndropout rates during inference by quantifying the information loss induced by\ndropout in each layer's feature maps. By treating dropout as controlled noise\ninjection and leveraging information-theoretic principles, Rate-In adapts\ndropout rates per layer and per input instance without requiring ground truth\nlabels. By quantifying the functional information loss in feature maps, we\nadaptively tune dropout rates to maintain perceptual quality across diverse\nmedical imaging tasks and architectural configurations. Our extensive empirical\nstudy on synthetic data and real-world medical imaging tasks demonstrates that\nRate-In improves calibration and sharpens uncertainty estimates compared to\nfixed or heuristic dropout rates without compromising predictive performance.\nRate-In offers a practical, unsupervised, inference-time approach to optimizing\ndropout for more reliable predictive uncertainty estimation in critical\napplications.\n","authors":["Tal Zeevi","Ravid Shwartz-Ziv","Yann LeCun","Lawrence H. Staib","John A. Onofrey"],"pdf_url":"https://arxiv.org/pdf/2412.07169v3.pdf","comment":"Updated author affiliation"},{"id":"http://arxiv.org/abs/2501.08313v1","updated":"2025-01-14T18:50:05Z","published":"2025-01-14T18:50:05Z","title":"MiniMax-01: Scaling Foundation Models with Lightning Attention","summary":"  We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01,\nwhich are comparable to top-tier models while offering superior capabilities in\nprocessing longer contexts. The core lies in lightning attention and its\nefficient scaling. To maximize computational capacity, we integrate it with\nMixture of Experts (MoE), creating a model with 32 experts and 456 billion\ntotal parameters, of which 45.9 billion are activated for each token. We\ndevelop an optimized parallel strategy and highly efficient\ncomputation-communication overlap techniques for MoE and lightning attention.\nThis approach enables us to conduct efficient training and inference on models\nwith hundreds of billions of parameters across contexts spanning millions of\ntokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens\nduring training and extrapolate to 4 million tokens during inference at an\naffordable cost. Our vision-language model, MiniMax-VL-01 is built through\ncontinued training with 512 billion vision-language tokens. Experiments on both\nstandard and in-house benchmarks show that our models match the performance of\nstate-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32\ntimes longer context window. We publicly release MiniMax-01 at\nhttps://github.com/MiniMax-AI.\n","authors":[" MiniMax","Aonian Li","Bangwei Gong","Bo Yang","Boji Shan","Chang Liu","Cheng Zhu","Chunhao Zhang","Congchao Guo","Da Chen","Dong Li","Enwei Jiao","Gengxin Li","Guojun Zhang","Haohai Sun","Houze Dong","Jiadai Zhu","Jiaqi Zhuang","Jiayuan Song","Jin Zhu","Jingtao Han","Jingyang Li","Junbin Xie","Junhao Xu","Junjie Yan","Kaishun Zhang","Kecheng Xiao","Kexi Kang","Le Han","Leyang Wang","Lianfei Yu","Liheng Feng","Lin Zheng","Linbo Chai","Long Xing","Meizhi Ju","Mingyuan Chi","Mozhi Zhang","Peikai Huang","Pengcheng Niu","Pengfei Li","Pengyu Zhao","Qi Yang","Qidi Xu","Qiexiang Wang","Qin Wang","Qiuhui Li","Ruitao Leng","Shengmin Shi","Shuqi Yu","Sichen Li","Songquan Zhu","Tao Huang","Tianrun Liang","Weigao Sun","Weixuan Sun","Weiyu Cheng","Wenkai Li","Xiangjun Song","Xiao Su","Xiaodong Han","Xinjie Zhang","Xinzhu Hou","Xu Min","Xun Zou","Xuyang Shen","Yan Gong","Yingjie Zhu","Yipeng Zhou","Yiran Zhong","Yongyi Hu","Yuanxiang Fan","Yue Yu","Yufeng Yang","Yuhao Li","Yunan Huang","Yunji Li","Yunpeng Huang","Yunzhi Xu","Yuxin Mao","Zehan Li","Zekang Li","Zewei Tao","Zewen Ying","Zhaoyang Cong","Zhen Qin","Zhenhua Fan","Zhihang Yu","Zhuo Jiang","Zijia Wu"],"pdf_url":"https://arxiv.org/pdf/2501.08313v1.pdf","comment":"A technical report from MiniMax. The authors are listed in\n  alphabetical order. We open-sourced our MiniMax-01 at\n  https://github.com/MiniMax-AI"},{"id":"http://arxiv.org/abs/2501.08303v1","updated":"2025-01-14T18:34:14Z","published":"2025-01-14T18:34:14Z","title":"Advancing Semantic Future Prediction through Multimodal Visual Sequence\n  Transformers","summary":"  Semantic future prediction is important for autonomous systems navigating\ndynamic environments. This paper introduces FUTURIST, a method for multimodal\nfuture semantic prediction that uses a unified and efficient visual sequence\ntransformer architecture. Our approach incorporates a multimodal masked visual\nmodeling objective and a novel masking mechanism designed for multimodal\ntraining. This allows the model to effectively integrate visible information\nfrom various modalities, improving prediction accuracy. Additionally, we\npropose a VAE-free hierarchical tokenization process, which reduces\ncomputational complexity, streamlines the training pipeline, and enables\nend-to-end training with high-resolution, multimodal inputs. We validate\nFUTURIST on the Cityscapes dataset, demonstrating state-of-the-art performance\nin future semantic segmentation for both short- and mid-term forecasting. We\nprovide the implementation code at https://github.com/Sta8is/FUTURIST .\n","authors":["Efstathios Karypidis","Ioannis Kakogeorgiou","Spyros Gidaris","Nikos Komodakis"],"pdf_url":"https://arxiv.org/pdf/2501.08303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08295v1","updated":"2025-01-14T18:22:21Z","published":"2025-01-14T18:22:21Z","title":"LayerAnimate: Layer-specific Control for Animation","summary":"  Animated video separates foreground and background elements into layers, with\ndistinct processes for sketching, refining, coloring, and in-betweening.\nExisting video generation methods typically treat animation as a monolithic\ndata domain, lacking fine-grained control over individual layers. In this\npaper, we introduce LayerAnimate, a novel architectural approach that enhances\nfine-grained control over individual animation layers within a video diffusion\nmodel, allowing users to independently manipulate foreground and background\nelements in distinct layers. To address the challenge of limited layer-specific\ndata, we propose a data curation pipeline that features automated element\nsegmentation, motion-state hierarchical merging, and motion coherence\nrefinement. Through quantitative and qualitative comparisons, and user study,\nwe demonstrate that LayerAnimate outperforms current methods in terms of\nanimation quality, control precision, and usability, making it an ideal tool\nfor both professional animators and amateur enthusiasts. This framework opens\nup new possibilities for layer-specific animation applications and creative\nflexibility. Our code is available at https://layeranimate.github.io.\n","authors":["Yuxue Yang","Lue Fan","Zuzen Lin","Feng Wang","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.08295v1.pdf","comment":"Project page: https://layeranimate.github.io"},{"id":"http://arxiv.org/abs/2407.04545v2","updated":"2025-01-14T18:20:45Z","published":"2024-07-05T14:30:24Z","title":"Gaussian Eigen Models for Human Heads","summary":"  Current personalized neural head avatars face a trade-off: lightweight models\nlack detail and realism, while high-quality, animatable avatars require\nsignificant computational resources, making them unsuitable for commodity\ndevices. To address this gap, we introduce Gaussian Eigen Models (GEM), which\nprovide high-quality, lightweight, and easily controllable head avatars. GEM\nutilizes 3D Gaussian primitives for representing the appearance combined with\nGaussian splatting for rendering. Building on the success of mesh-based 3D\nmorphable face models (3DMM), we define GEM as an ensemble of linear eigenbases\nfor representing the head appearance of a specific subject. In particular, we\nconstruct linear bases to represent the position, scale, rotation, and opacity\nof the 3D Gaussians. This allows us to efficiently generate Gaussian primitives\nof a specific head shape by a linear combination of the basis vectors, only\nrequiring a low-dimensional parameter vector that contains the respective\ncoefficients. We propose to construct these linear bases (GEM) by distilling\nhigh-quality compute-intense CNN-based Gaussian avatar models that can generate\nexpression-dependent appearance changes like wrinkles. These high-quality\nmodels are trained on multi-view videos of a subject and are distilled using a\nseries of principal component analyses. Once we have obtained the bases that\nrepresent the animatable appearance space of a specific human, we learn a\nregressor that takes a single RGB image as input and predicts the\nlow-dimensional parameter vector that corresponds to the shown facial\nexpression. In a series of experiments, we compare GEM's self-reenactment and\ncross-person reenactment results to state-of-the-art 3D avatar methods,\ndemonstrating GEM's higher visual quality and better generalization to new\nexpressions.\n","authors":["Wojciech Zielonka","Timo Bolkart","Thabo Beeler","Justus Thies"],"pdf_url":"https://arxiv.org/pdf/2407.04545v2.pdf","comment":"https://zielon.github.io/gem/"},{"id":"http://arxiv.org/abs/2410.24031v2","updated":"2025-01-14T18:03:42Z","published":"2024-10-31T15:29:51Z","title":"A Multi-Modal Approach for Face Anti-Spoofing in Non-Calibrated Systems\n  using Disparity Maps","summary":"  Face recognition technologies are increasingly used in various applications,\nyet they are vulnerable to face spoofing attacks. These spoofing attacks often\ninvolve unique 3D structures, such as printed papers or mobile device screens.\nAlthough stereo-depth cameras can detect such attacks effectively, their\nhigh-cost limits their widespread adoption. Conversely, two-sensor systems\nwithout extrinsic calibration offer a cost-effective alternative but are unable\nto calculate depth using stereo techniques. In this work, we propose a method\nto overcome this challenge by leveraging facial attributes to derive disparity\ninformation and estimate relative depth for anti-spoofing purposes, using\nnon-calibrated systems. We introduce a multi-modal anti-spoofing model, coined\nDisparity Model, that incorporates created disparity maps as a third modality\nalongside the two original sensor modalities. We demonstrate the effectiveness\nof the Disparity Model in countering various spoof attacks using a\ncomprehensive dataset collected from the Intel RealSense ID Solution F455. Our\nmethod outperformed existing methods in the literature, achieving an Equal\nError Rate (EER) of 1.71% and a False Negative Rate (FNR) of 2.77% at a False\nPositive Rate (FPR) of 1%. These errors are lower by 2.45% and 7.94% than the\nerrors of the best comparison method, respectively. Additionally, we introduce\na model ensemble that addresses 3D spoof attacks as well, achieving an EER of\n2.04% and an FNR of 3.83% at an FPR of 1%. Overall, our work provides a\nstate-of-the-art solution for the challenging task of anti-spoofing in\nnon-calibrated systems that lack depth information.\n","authors":["Ariel Larey","Eyal Rond","Omer Achrack"],"pdf_url":"https://arxiv.org/pdf/2410.24031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08286v1","updated":"2025-01-14T18:01:15Z","published":"2025-01-14T18:01:15Z","title":"VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large\n  Scenes","summary":"  VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM framework\ndesigned for large scenes. The framework comprises four main components: VIO\nFront End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIO\nFront End, RGB frames are processed through dense bundle adjustment and\nuncertainty estimation to extract scene geometry and poses. Based on this\noutput, the mapping module incrementally constructs and maintains a 2D Gaussian\nmap. Key components of the 2D Gaussian Map include a Sample-based Rasterizer,\nScore Manager, and Pose Refinement, which collectively improve mapping speed\nand localization accuracy. This enables the SLAM system to handle large-scale\nurban environments with up to 50 million Gaussian ellipsoids. To ensure global\nconsistency in large-scale scenes, we design a Loop Closure module, which\ninnovatively leverages the Novel View Synthesis (NVS) capabilities of Gaussian\nSplatting for loop closure detection and correction of the Gaussian map.\nAdditionally, we propose a Dynamic Eraser to address the inevitable presence of\ndynamic objects in real-world outdoor scenes. Extensive evaluations in indoor\nand outdoor environments demonstrate that our approach achieves localization\nperformance on par with Visual-Inertial Odometry while surpassing recent\nGS/NeRF SLAM methods. It also significantly outperforms all existing methods in\nterms of mapping and rendering quality. Furthermore, we developed a mobile app\nand verified that our framework can generate high-quality Gaussian maps in real\ntime using only a smartphone camera and a low-frequency IMU sensor. To the best\nof our knowledge, VINGS-Mono is the first monocular Gaussian SLAM method\ncapable of operating in outdoor environments and supporting kilometer-scale\nlarge scenes.\n","authors":["Ke Wu","Zicheng Zhang","Muer Tie","Ziqing Ai","Zhongxue Gan","Wenchao Ding"],"pdf_url":"https://arxiv.org/pdf/2501.08286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08285v1","updated":"2025-01-14T18:00:41Z","published":"2025-01-14T18:00:41Z","title":"Can Bayesian Neural Networks Explicitly Model Input Uncertainty?","summary":"  Inputs to machine learning models can have associated noise or uncertainties,\nbut they are often ignored and not modelled. It is unknown if Bayesian Neural\nNetworks and their approximations are able to consider uncertainty in their\ninputs. In this paper we build a two input Bayesian Neural Network (mean and\nstandard deviation) and evaluate its capabilities for input uncertainty\nestimation across different methods like Ensembles, MC-Dropout, and Flipout.\nOur results indicate that only some uncertainty estimation methods for\napproximate Bayesian NNs can model input uncertainty, in particular Ensembles\nand Flipout.\n","authors":["Matias Valdenegro-Toro","Marco Zullich"],"pdf_url":"https://arxiv.org/pdf/2501.08285v1.pdf","comment":"12 pages, 11 figures, VISAPP 2025 camera ready"},{"id":"http://arxiv.org/abs/2501.08282v1","updated":"2025-01-14T17:58:12Z","published":"2025-01-14T17:58:12Z","title":"LLaVA-ST: A Multimodal Large Language Model for Fine-Grained\n  Spatial-Temporal Understanding","summary":"  Recent advancements in multimodal large language models (MLLMs) have shown\npromising results, yet existing approaches struggle to effectively handle both\ntemporal and spatial localization simultaneously. This challenge stems from two\nkey issues: first, incorporating spatial-temporal localization introduces a\nvast number of coordinate combinations, complicating the alignment of\nlinguistic and visual coordinate representations; second, encoding fine-grained\ntemporal and spatial information during video feature compression is inherently\ndifficult. To address these issues, we propose LLaVA-ST, a MLLM for\nfine-grained spatial-temporal multimodal understanding. In LLaVA-ST, we propose\nLanguage-Aligned Positional Embedding, which embeds the textual coordinate\nspecial token into the visual space, simplifying the alignment of fine-grained\nspatial-temporal correspondences. Additionally, we design the Spatial-Temporal\nPacker, which decouples the feature compression of temporal and spatial\nresolutions into two distinct point-to-region attention processing streams.\nFurthermore, we propose ST-Align dataset with 4.3M training samples for\nfine-grained spatial-temporal multimodal understanding. With ST-align, we\npresent a progressive training pipeline that aligns the visual and textual\nfeature through sequential coarse-to-fine stages.Additionally, we introduce an\nST-Align benchmark to evaluate spatial-temporal interleaved fine-grained\nunderstanding tasks, which include Spatial-Temporal Video Grounding (STVG) ,\nEvent Localization and Captioning (ELC) and Spatial Video Grounding (SVG).\nLLaVA-ST achieves outstanding performance on 11 benchmarks requiring\nfine-grained temporal, spatial, or spatial-temporal interleaving multimodal\nunderstanding. Our code, data and benchmark will be released at Our code, data\nand benchmark will be released at https://github.com/appletea233/LLaVA-ST .\n","authors":["Hongyu Li","Jinyu Chen","Ziyu Wei","Shaofei Huang","Tianrui Hui","Jialin Gao","Xiaoming Wei","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08279v1","updated":"2025-01-14T17:55:12Z","published":"2025-01-14T17:55:12Z","title":"SmartEraser: Remove Anything from Images using Masked-Region Guidance","summary":"  Object removal has so far been dominated by the mask-and-inpaint paradigm,\nwhere the masked region is excluded from the input, leaving models relying on\nunmasked areas to inpaint the missing region. However, this approach lacks\ncontextual information for the masked area, often resulting in unstable\nperformance. In this work, we introduce SmartEraser, built with a new removing\nparadigm called Masked-Region Guidance. This paradigm retains the masked region\nin the input, using it as guidance for the removal process. It offers several\ndistinct advantages: (a) it guides the model to accurately identify the object\nto be removed, preventing its regeneration in the output; (b) since the user\nmask often extends beyond the object itself, it aids in preserving the\nsurrounding context in the final result. Leveraging this new paradigm, we\npresent Syn4Removal, a large-scale object removal dataset, where instance\nsegmentation data is used to copy and paste objects onto images as removal\ntargets, with the original images serving as ground truths. Experimental\nresults demonstrate that SmartEraser significantly outperforms existing\nmethods, achieving superior performance in object removal, especially in\ncomplex scenes with intricate compositions.\n","authors":["Longtao Jiang","Zhendong Wang","Jianmin Bao","Wengang Zhou","Dongdong Chen","Lei Shi","Dong Chen","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2501.08279v1.pdf","comment":"Project at: https://longtaojiang.github.io/smarteraser.github.io/"},{"id":"http://arxiv.org/abs/2406.08476v2","updated":"2025-01-14T17:46:01Z","published":"2024-06-12T17:59:04Z","title":"RMem: Restricted Memory Banks Improve Video Object Segmentation","summary":"  With recent video object segmentation (VOS) benchmarks evolving to\nchallenging scenarios, we revisit a simple but overlooked strategy: restricting\nthe size of memory banks. This diverges from the prevalent practice of\nexpanding memory banks to accommodate extensive historical information. Our\nspecially designed \"memory deciphering\" study offers a pivotal insight\nunderpinning such a strategy: expanding memory banks, while seemingly\nbeneficial, actually increases the difficulty for VOS modules to decode\nrelevant features due to the confusion from redundant information. By\nrestricting memory banks to a limited number of essential frames, we achieve a\nnotable improvement in VOS accuracy. This process balances the importance and\nfreshness of frames to maintain an informative memory bank within a bounded\ncapacity. Additionally, restricted memory banks reduce the training-inference\ndiscrepancy in memory lengths compared with continuous expansion. This fosters\nnew opportunities in temporal reasoning and enables us to introduce the\npreviously overlooked \"temporal positional embedding.\" Finally, our insights\nare embodied in \"RMem\" (\"R\" for restricted), a simple yet effective VOS\nmodification that excels at challenging VOS scenarios and establishes new state\nof the art for object state changes (on the VOST dataset) and long videos (on\nthe Long Videos dataset). Our code and demo are available at\nhttps://restricted-memory.github.io/.\n","authors":["Junbao Zhou","Ziqi Pang","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.08476v2.pdf","comment":"CVPR 2024, Project Page: https://restricted-memory.github.io/"},{"id":"http://arxiv.org/abs/2409.07571v3","updated":"2025-01-14T17:33:46Z","published":"2024-09-11T18:58:16Z","title":"FaVoR: Features via Voxel Rendering for Camera Relocalization","summary":"  Camera relocalization methods range from dense image alignment to direct\ncamera pose regression from a query image. Among these, sparse feature matching\nstands out as an efficient, versatile, and generally lightweight approach with\nnumerous applications. However, feature-based methods often struggle with\nsignificant viewpoint and appearance changes, leading to matching failures and\ninaccurate pose estimates. To overcome this limitation, we propose a novel\napproach that leverages a globally sparse yet locally dense 3D representation\nof 2D features. By tracking and triangulating landmarks over a sequence of\nframes, we construct a sparse voxel map optimized to render image patch\ndescriptors observed during tracking. Given an initial pose estimate, we first\nsynthesize descriptors from the voxels using volumetric rendering and then\nperform feature matching to estimate the camera pose. This methodology enables\nthe generation of descriptors for unseen views, enhancing robustness to view\nchanges. We extensively evaluate our method on the 7-Scenes and Cambridge\nLandmarks datasets. Our results show that our method significantly outperforms\nexisting state-of-the-art feature representation techniques in indoor\nenvironments, achieving up to a 39% improvement in median translation error.\nAdditionally, our approach yields comparable results to other methods for\noutdoor scenarios while maintaining lower memory and computational costs.\n","authors":["Vincenzo Polizzi","Marco Cannici","Davide Scaramuzza","Jonathan Kelly"],"pdf_url":"https://arxiv.org/pdf/2409.07571v3.pdf","comment":"Accepted to the IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV), Tucson, Arizona, US, Feb 28-Mar 4, 2025"},{"id":"http://arxiv.org/abs/2501.06693v2","updated":"2025-01-14T17:29:06Z","published":"2025-01-12T03:01:15Z","title":"Vid2Sim: Realistic and Interactive Simulation from Video for Urban\n  Navigation","summary":"  Sim-to-real gap has long posed a significant challenge for robot learning in\nsimulation, preventing the deployment of learned models in the real world.\nPrevious work has primarily focused on domain randomization and system\nidentification to mitigate this gap. However, these methods are often limited\nby the inherent constraints of the simulation and graphics engines. In this\nwork, we propose Vid2Sim, a novel framework that effectively bridges the\nsim2real gap through a scalable and cost-efficient real2sim pipeline for neural\n3D scene reconstruction and simulation. Given a monocular video as input,\nVid2Sim can generate photorealistic and physically interactable 3D simulation\nenvironments to enable the reinforcement learning of visual navigation agents\nin complex urban environments. Extensive experiments demonstrate that Vid2Sim\nsignificantly improves the performance of urban navigation in the digital twins\nand real world by 31.2% and 68.3% in success rate compared with agents trained\nwith prior simulation methods.\n","authors":["Ziyang Xie","Zhizheng Liu","Zhenghao Peng","Wayne Wu","Bolei Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.06693v2.pdf","comment":"Project page: https://metadriverse.github.io/vid2sim/"},{"id":"http://arxiv.org/abs/2501.08266v1","updated":"2025-01-14T17:26:02Z","published":"2025-01-14T17:26:02Z","title":"AI Driven Water Segmentation with deep learning models for Enhanced\n  Flood Monitoring","summary":"  Flooding is a major natural hazard causing significant fatalities and\neconomic losses annually, with increasing frequency due to climate change.\nRapid and accurate flood detection and monitoring are crucial for mitigating\nthese impacts. This study compares the performance of three deep learning\nmodels UNet, ResNet, and DeepLabv3 for pixelwise water segmentation to aid in\nflood detection, utilizing images from drones, in field observations, and\nsocial media. This study involves creating a new dataset that augments\nwellknown benchmark datasets with flood-specific images, enhancing the\nrobustness of the models. The UNet, ResNet, and DeepLab v3 architectures are\ntested to determine their effectiveness in various environmental conditions and\ngeographical locations, and the strengths and limitations of each model are\nalso discussed here, providing insights into their applicability in different\nscenarios by predicting image segmentation masks. This fully automated approach\nallows these models to isolate flooded areas in images, significantly reducing\nprocessing time compared to traditional semi-automated methods. The outcome of\nthis study is to predict segmented masks for each image effected by a flood\ndisaster and the validation accuracy of these models. This methodology\nfacilitates timely and continuous flood monitoring, providing vital data for\nemergency response teams to reduce loss of life and economic damages. It offers\na significant reduction in the time required to generate flood maps, cutting\ndown the manual processing time. Additionally, we present avenues for future\nresearch, including the integration of multimodal data sources and the\ndevelopment of robust deep learning architectures tailored specifically for\nflood detection tasks. Overall, our work contributes to the advancement of\nflood management strategies through innovative use of deep learning\ntechnologies.\n","authors":["Sanjida Afrin Mou","Tasfia Noor Chowdhury","Adib Ibn Mannan","Sadia Nourin Mim","Lubana Tarannum","Tasrin Noman","Jamal Uddin Ahamed"],"pdf_url":"https://arxiv.org/pdf/2501.08266v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.08258v1","updated":"2025-01-14T17:10:02Z","published":"2025-01-14T17:10:02Z","title":"Towards an End-to-End (E2E) Adversarial Learning and Application in the\n  Physical World","summary":"  The traditional learning process of patch-based adversarial attacks,\nconducted in the digital domain and then applied in the physical domain (e.g.,\nvia printed stickers), may suffer from reduced performance due to adversarial\npatches' limited transferability from the digital domain to the physical\ndomain. Given that previous studies have considered using projectors to apply\nadversarial attacks, we raise the following question: can adversarial learning\n(i.e., patch generation) be performed entirely in the physical domain with a\nprojector? In this work, we propose the Physical-domain Adversarial Patch\nLearning Augmentation (PAPLA) framework, a novel end-to-end (E2E) framework\nthat converts adversarial learning from the digital domain to the physical\ndomain using a projector. We evaluate PAPLA across multiple scenarios,\nincluding controlled laboratory settings and realistic outdoor environments,\ndemonstrating its ability to ensure attack success compared to conventional\ndigital learning-physical application (DL-PA) methods. We also analyze the\nimpact of environmental factors, such as projection surface color, projector\nstrength, ambient light, distance, and angle of the target object relative to\nthe camera, on the effectiveness of projected patches. Finally, we demonstrate\nthe feasibility of the attack against a parked car and a stop sign in a\nreal-world outdoor environment. Our results show that under specific\nconditions, E2E adversarial learning in the physical domain eliminates the\ntransferability issue and ensures evasion by object detectors. Finally, we\nprovide insights into the challenges and opportunities of applying adversarial\nlearning in the physical domain and explain where such an approach is more\neffective than using a sticker.\n","authors":["Dudi Biton","Jacob Shams","Koda Satoru","Asaf Shabtai","Yuval Elovici","Ben Nassi"],"pdf_url":"https://arxiv.org/pdf/2501.08258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10733v4","updated":"2025-01-14T16:47:44Z","published":"2024-10-14T17:15:07Z","title":"Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models","summary":"  We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit.\n","authors":["Junyu Chen","Han Cai","Junsong Chen","Enze Xie","Shang Yang","Haotian Tang","Muyang Li","Yao Lu","Song Han"],"pdf_url":"https://arxiv.org/pdf/2410.10733v4.pdf","comment":"Preprint. First two authors contributed equally to this work. Update:\n  add USiT (UViT+SiT sampler) results"},{"id":"http://arxiv.org/abs/2405.20299v4","updated":"2025-01-14T16:38:36Z","published":"2024-05-30T17:46:23Z","title":"Scaling White-Box Transformers for Vision","summary":"  CRATE, a white-box transformer architecture designed to learn compressed and\nsparse representations, offers an intriguing alternative to standard vision\ntransformers (ViTs) due to its inherent mathematical interpretability. Despite\nextensive investigations into the scaling behaviors of language and vision\ntransformers, the scalability of CRATE remains an open question which this\npaper aims to address. Specifically, we propose CRATE-$\\alpha$, featuring\nstrategic yet minimal modifications to the sparse coding block in the CRATE\narchitecture design, and a light training recipe designed to improve the\nscalability of CRATE. Through extensive experiments, we demonstrate that\nCRATE-$\\alpha$ can effectively scale with larger model sizes and datasets. For\nexample, our CRATE-$\\alpha$-B substantially outperforms the prior best CRATE-B\nmodel accuracy on ImageNet classification by 3.7%, achieving an accuracy of\n83.2%. Meanwhile, when scaling further, our CRATE-$\\alpha$-L obtains an\nImageNet classification accuracy of 85.1%. More notably, these model\nperformance improvements are achieved while preserving, and potentially even\nenhancing the interpretability of learned CRATE models, as we demonstrate\nthrough showing that the learned token representations of increasingly larger\ntrained CRATE-$\\alpha$ models yield increasingly higher-quality unsupervised\nobject segmentation of images. The project page is\nhttps://rayjryang.github.io/CRATE-alpha/.\n","authors":["Jinrui Yang","Xianhang Li","Druv Pai","Yuyin Zhou","Yi Ma","Yaodong Yu","Cihang Xie"],"pdf_url":"https://arxiv.org/pdf/2405.20299v4.pdf","comment":"project page: https://rayjryang.github.io/CRATE-alpha/"},{"id":"http://arxiv.org/abs/2501.08245v1","updated":"2025-01-14T16:31:01Z","published":"2025-01-14T16:31:01Z","title":"Continual Deep Active Learning for Medical Imaging: Replay-Base\n  Architecture for Context Adaptation","summary":"  Deep Learning for medical imaging faces challenges in adapting and\ngeneralizing to new contexts. Additionally, it often lacks sufficient labeled\ndata for specific tasks requiring significant annotation effort. Continual\nLearning (CL) tackles adaptability and generalizability by enabling lifelong\nlearning from a data stream while mitigating forgetting of previously learned\nknowledge. Active Learning (AL) reduces the number of required annotations for\neffective training. This work explores both approaches (CAL) to develop a novel\nframework for robust medical image analysis. Based on the automatic recognition\nof shifts in image characteristics, Replay-Base Architecture for Context\nAdaptation (RBACA) employs a CL rehearsal method to continually learn from\ndiverse contexts, and an AL component to select the most informative instances\nfor annotation. A novel approach to evaluate CAL methods is established using a\ndefined metric denominated IL-Score, which allows for the simultaneous\nassessment of transfer learning, forgetting, and final model performance. We\nshow that RBACA works in domain and class-incremental learning scenarios, by\nassessing its IL-Score on the segmentation and diagnosis of cardiac images. The\nresults show that RBACA outperforms a baseline framework without CAL, and a\nstate-of-the-art CAL method across various memory sizes and annotation budgets.\nOur code is available in https://github.com/RuiDaniel/RBACA .\n","authors":["Rui Daniel","M. Rita Verdelho","Catarina Barata","Carlos Santiago"],"pdf_url":"https://arxiv.org/pdf/2501.08245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08241v1","updated":"2025-01-14T16:28:02Z","published":"2025-01-14T16:28:02Z","title":"A Feature-Level Ensemble Model for COVID-19 Identification in CXR Images\n  using Choquet Integral and Differential Evolution Optimization","summary":"  The COVID-19 pandemic has profoundly impacted billions globally. It\nchallenges public health and healthcare systems due to its rapid spread and\nsevere respiratory effects. An effective strategy to mitigate the COVID-19\npandemic involves integrating testing to identify infected individuals. While\nRT-PCR is considered the gold standard for diagnosing COVID-19, it has some\nlimitations such as the risk of false negatives. To address this problem, this\npaper introduces a novel Deep Learning Diagnosis System that integrates\npre-trained Deep Convolutional Neural Networks (DCNNs) within an ensemble\nlearning framework to achieve precise identification of COVID-19 cases from\nChest X-ray (CXR) images. We combine feature vectors from the final hidden\nlayers of pre-trained DCNNs using the Choquet integral to capture interactions\nbetween different DCNNs that a linear approach cannot. We employed\nSugeno-$\\lambda$ measure theory to derive fuzzy measures for subsets of\nnetworks to enable aggregation. We utilized Differential Evolution to estimate\nfuzzy densities. We developed a TensorFlow-based layer for Choquet operation to\nfacilitate efficient aggregation, due to the intricacies involved in\naggregating feature vectors. Experimental results on the COVIDx dataset show\nthat our ensemble model achieved 98\\% accuracy in three-class classification\nand 99.50\\% in binary classification, outperforming its components-DenseNet-201\n(97\\% for three-class, 98.75\\% for binary), Inception-v3 (96.25\\% for\nthree-class, 98.50\\% for binary), and Xception (94.50\\% for three-class, 98\\%\nfor binary)-and surpassing many previous methods.\n","authors":["Amir Reza Takhsha","Maryam Rastgarpour","Mozhgan Naderi"],"pdf_url":"https://arxiv.org/pdf/2501.08241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10729v2","updated":"2025-01-14T16:17:00Z","published":"2024-06-15T20:04:06Z","title":"A Comprehensive Survey of Foundation Models in Medicine","summary":"  Foundation models (FMs) are large-scale deep learning models that are\ndeveloped using large datasets and self-supervised learning methods. These\nmodels serve as a base for different downstream tasks, including healthcare.\nFMs have been adopted with great success across various domains within\nhealthcare. Existing healthcare-based surveys have not yet included all of\nthese domains. Therefore, we provide a detailed survey of FMs in healthcare. We\nfocus on the history, learning strategies, flagship models, applications, and\nchallenges of FMs. We explore how FMs such as the BERT and GPT families are\nreshaping various healthcare domains, including clinical large language models,\nmedical image analysis, and omics. Furthermore, we provide a detailed taxonomy\nof healthcare applications facilitated by FMs, such as clinical NLP, medical\ncomputer vision, graph learning, and other biology-related tasks. Despite the\npromising opportunities FMs provide, they also have several associated\nchallenges, which are explained in detail. We also outline open research issues\nand potential lessons learned to provide researchers and practitioners with\ninsights into the capabilities of FMs in healthcare to advance their deployment\nand mitigate associated risks.\n","authors":["Wasif Khan","Seowung Leem","Kyle B. See","Joshua K. Wong","Shaoting Zhang","Ruogu Fang"],"pdf_url":"https://arxiv.org/pdf/2406.10729v2.pdf","comment":"Currently under review in IEEE REVIEWS IN BIOMEDICAL ENGINEERING"},{"id":"http://arxiv.org/abs/2307.09059v3","updated":"2025-01-14T16:11:11Z","published":"2023-07-18T08:23:46Z","title":"Text-guided Image Restoration and Semantic Enhancement for Text-to-Image\n  Person Retrieval","summary":"  The goal of Text-to-Image Person Retrieval (TIPR) is to retrieve specific\nperson images according to the given textual descriptions. A primary challenge\nin this task is bridging the substantial representational gap between visual\nand textual modalities. The prevailing methods map texts and images into\nunified embedding space for matching, while the intricate semantic\ncorrespondences between texts and images are still not effectively constructed.\nTo address this issue, we propose a novel TIPR framework to build fine-grained\ninteractions and alignment between person images and the corresponding texts.\nSpecifically, via fine-tuning the Contrastive Language-Image Pre-training\n(CLIP) model, a visual-textual dual encoder is firstly constructed, to\npreliminarily align the image and text features. Secondly, a Text-guided Image\nRestoration (TIR) auxiliary task is proposed to map abstract textual entities\nto specific image regions, improving the alignment between local textual and\nvisual embeddings. Additionally, a cross-modal triplet loss is presented to\nhandle hard samples, and further enhance the model's discriminability for minor\ndifferences. Moreover, a pruning-based text data augmentation approach is\nproposed to enhance focus on essential elements in descriptions, thereby\navoiding excessive model attention to less significant information. The\nexperimental results show our proposed method outperforms state-of-the-art\nmethods on three popular benchmark datasets, and the code will be made publicly\navailable at https://github.com/Delong-liu-bupt/SEN.\n","authors":["Delong Liu","Haiwen Li","Zhicheng Zhao","Yuan Dong","Nikolaos V. Boulgouris"],"pdf_url":"https://arxiv.org/pdf/2307.09059v3.pdf","comment":"The paper was withdrawn due to a dispute among the authors regarding\n  the content of the article"},{"id":"http://arxiv.org/abs/2501.08226v1","updated":"2025-01-14T16:10:25Z","published":"2025-01-14T16:10:25Z","title":"Efficient Deep Learning-based Forward Solvers for Brain Tumor Growth\n  Models","summary":"  Glioblastoma, a highly aggressive brain tumor, poses major challenges due to\nits poor prognosis and high morbidity rates. Partial differential\nequation-based models offer promising potential to enhance therapeutic outcomes\nby simulating patient-specific tumor behavior for improved radiotherapy\nplanning. However, model calibration remains a bottleneck due to the high\ncomputational demands of optimization methods like Monte Carlo sampling and\nevolutionary algorithms. To address this, we recently introduced an approach\nleveraging a neural forward solver with gradient-based optimization to\nsignificantly reduce calibration time. This approach requires a highly accurate\nand fully differentiable forward model. We investigate multiple architectures,\nincluding (i) an enhanced TumorSurrogate, (ii) a modified nnU-Net, and (iii) a\n3D Vision Transformer (ViT). The optimized TumorSurrogate achieved the best\noverall results, excelling in both tumor outline matching and voxel-level\nprediction of tumor cell concentration. It halved the MSE relative to the\nbaseline model and achieved the highest Dice score across all tumor cell\nconcentration thresholds. Our study demonstrates significant enhancement in\nforward solver performance and outlines important future research directions.\n","authors":["Zeineb Haouari","Jonas Weidner","Ivan Ezhov","Aswathi Varma","Daniel Rueckert","Bjoern Menze","Benedikt Wiestler"],"pdf_url":"https://arxiv.org/pdf/2501.08226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08225v1","updated":"2025-01-14T16:09:16Z","published":"2025-01-14T16:09:16Z","title":"FramePainter: Endowing Interactive Image Editing with Video Diffusion\n  Priors","summary":"  Interactive image editing allows users to modify images through visual\ninteraction operations such as drawing, clicking, and dragging. Existing\nmethods construct such supervision signals from videos, as they capture how\nobjects change with various physical interactions. However, these models are\nusually built upon text-to-image diffusion models, so necessitate (i) massive\ntraining samples and (ii) an additional reference encoder to learn real-world\ndynamics and visual consistency. In this paper, we reformulate this task as an\nimage-to-video generation problem, so that inherit powerful video diffusion\npriors to reduce training costs and ensure temporal consistency. Specifically,\nwe introduce FramePainter as an efficient instantiation of this formulation.\nInitialized with Stable Video Diffusion, it only uses a lightweight sparse\ncontrol encoder to inject editing signals. Considering the limitations of\ntemporal attention in handling large motion between two frames, we further\npropose matching attention to enlarge the receptive field while encouraging\ndense correspondence between edited and source image tokens. We highlight the\neffectiveness and efficiency of FramePainter across various of editing signals:\nit domainantly outperforms previous state-of-the-art methods with far less\ntraining data, achieving highly seamless and coherent editing of images, \\eg,\nautomatically adjust the reflection of the cup. Moreover, FramePainter also\nexhibits exceptional generalization in scenarios not present in real-world\nvideos, \\eg, transform the clownfish into shark-like shape. Our code will be\navailable at https://github.com/YBYBZhang/FramePainter.\n","authors":["Yabo Zhang","Xinpeng Zhou","Yihan Zeng","Hang Xu","Hui Li","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2501.08225v1.pdf","comment":"Code: https://github.com/YBYBZhang/FramePainter"},{"id":"http://arxiv.org/abs/2408.12454v3","updated":"2025-01-14T15:35:55Z","published":"2024-08-22T14:52:53Z","title":"Relaxed Rotational Equivariance via $G$-Biases in Vision","summary":"  Group Equivariant Convolution (GConv) can capture rotational equivariance\nfrom original data. It assumes uniform and strict rotational equivariance\nacross all features as the transformations under the specific group. However,\nthe presentation or distribution of real-world data rarely conforms to strict\nrotational equivariance, commonly referred to as Rotational Symmetry-Breaking\n(RSB) in the system or dataset, making GConv unable to adapt effectively to\nthis phenomenon. Motivated by this, we propose a simple but highly effective\nmethod to address this problem, which utilizes a set of learnable biases called\n$G$-Biases under the group order to break strict group constraints and then\nachieve a Relaxed Rotational Equivariant Convolution (RREConv). To validate the\nefficiency of RREConv, we conduct extensive ablation experiments on the\ndiscrete rotational group $\\mathcal{C}_n$. Experiments demonstrate that the\nproposed RREConv-based methods achieve excellent performance compared to\nexisting GConv-based methods in both classification and 2D object detection\ntasks on the natural image datasets.\n","authors":["Zhiqiang Wu","Yingjie Liu","Licheng Sun","Jian Yang","Hanlin Dong","Shing-Ho J. Lin","Xuan Tang","Jinpeng Mi","Bo Jin","Xian Wei"],"pdf_url":"https://arxiv.org/pdf/2408.12454v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08199v1","updated":"2025-01-14T15:23:36Z","published":"2025-01-14T15:23:36Z","title":"EmoNeXt: an Adapted ConvNeXt for Facial Emotion Recognition","summary":"  Facial expressions play a crucial role in human communication serving as a\npowerful and impactful means to express a wide range of emotions. With\nadvancements in artificial intelligence and computer vision, deep neural\nnetworks have emerged as effective tools for facial emotion recognition. In\nthis paper, we propose EmoNeXt, a novel deep learning framework for facial\nexpression recognition based on an adapted ConvNeXt architecture network. We\nintegrate a Spatial Transformer Network (STN) to focus on feature-rich regions\nof the face and Squeeze-and-Excitation blocks to capture channel-wise\ndependencies. Moreover, we introduce a self-attention regularization term,\nencouraging the model to generate compact feature vectors. We demonstrate the\nsuperiority of our model over existing state-of-the-art deep learning models on\nthe FER2013 dataset regarding emotion classification accuracy.\n","authors":["Yassine El Boudouri","Amine Bohi"],"pdf_url":"https://arxiv.org/pdf/2501.08199v1.pdf","comment":"6 pages, 5 figures and 2 tables. 2023 IEEE 25th International\n  Workshop on Multimedia Signal Processing (MMSP), Poitiers, France"},{"id":"http://arxiv.org/abs/2501.08195v1","updated":"2025-01-14T15:18:28Z","published":"2025-01-14T15:18:28Z","title":"Self-supervised Deep Hyperspectral Inpainting with the Plug and Play and\n  Deep Image Prior Models","summary":"  Hyperspectral images are typically composed of hundreds of narrow and\ncontiguous spectral bands, each containing information regarding the material\ncomposition of the imaged scene. However, these images can be affected by\nvarious sources of noise, distortions, or data loss, which can significantly\ndegrade their quality and usefulness. This paper introduces a convergent\nguaranteed algorithm, LRS-PnP-DIP(1-Lip), which successfully addresses the\ninstability issue of DHP that has been reported before. The proposed algorithm\nextends the successful joint low-rank and sparse model to further exploit the\nunderlying data structures beyond the conventional and sometimes restrictive\nunions of subspace models. A stability analysis guarantees the convergence of\nthe proposed algorithm under mild assumptions , which is crucial for its\napplication in real-world scenarios. Extensive experiments demonstrate that the\nproposed solution consistently delivers visually and quantitatively superior\ninpainting results, establishing state-of-the-art performance.\n","authors":["Shuo Li","Mehrdad Yaghoobi"],"pdf_url":"https://arxiv.org/pdf/2501.08195v1.pdf","comment":"31 pages, 9 Figures, 7 Tables. arXiv admin note: text overlap with\n  arXiv:2306.08128"},{"id":"http://arxiv.org/abs/2501.08188v1","updated":"2025-01-14T15:13:00Z","published":"2025-01-14T15:13:00Z","title":"A Critical Synthesis of Uncertainty Quantification and Foundation Models\n  in Monocular Depth Estimation","summary":"  While recent foundation models have enabled significant breakthroughs in\nmonocular depth estimation, a clear path towards safe and reliable deployment\nin the real-world remains elusive. Metric depth estimation, which involves\npredicting absolute distances, poses particular challenges, as even the most\nadvanced foundation models remain prone to critical errors. Since quantifying\nthe uncertainty has emerged as a promising endeavor to address these\nlimitations and enable trustworthy deployment, we fuse five different\nuncertainty quantification methods with the current state-of-the-art\nDepthAnythingV2 foundation model. To cover a wide range of metric depth\ndomains, we evaluate their performance on four diverse datasets. Our findings\nidentify fine-tuning with the Gaussian Negative Log-Likelihood Loss (GNLL) as a\nparticularly promising approach, offering reliable uncertainty estimates while\nmaintaining predictive performance and computational efficiency on par with the\nbaseline, encompassing both training and inference time. By fusing uncertainty\nquantification and foundation models within the context of monocular depth\nestimation, this paper lays a critical foundation for future research aimed at\nimproving not only model performance but also its explainability. Extending\nthis critical synthesis of uncertainty quantification and foundation models\ninto other crucial tasks, such as semantic segmentation and pose estimation,\npresents exciting opportunities for safer and more reliable machine vision\nsystems.\n","authors":["Steven Landgraf","Rongjun Qin","Markus Ulrich"],"pdf_url":"https://arxiv.org/pdf/2501.08188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08182v1","updated":"2025-01-14T15:08:56Z","published":"2025-01-14T15:08:56Z","title":"CG-MER: A Card Game-based Multimodal dataset for Emotion Recognition","summary":"  The field of affective computing has seen significant advancements in\nexploring the relationship between emotions and emerging technologies. This\npaper presents a novel and valuable contribution to this field with the\nintroduction of a comprehensive French multimodal dataset designed specifically\nfor emotion recognition. The dataset encompasses three primary modalities:\nfacial expressions, speech, and gestures, providing a holistic perspective on\nemotions. Moreover, the dataset has the potential to incorporate additional\nmodalities, such as Natural Language Processing (NLP) to expand the scope of\nemotion recognition research. The dataset was curated through engaging\nparticipants in card game sessions, where they were prompted to express a range\nof emotions while responding to diverse questions. The study included 10\nsessions with 20 participants (9 females and 11 males). The dataset serves as a\nvaluable resource for furthering research in emotion recognition and provides\nan avenue for exploring the intricate connections between human emotions and\ndigital technologies.\n","authors":["Nessrine Farhat","Amine Bohi","Leila Ben Letaifa","Rim Slama"],"pdf_url":"https://arxiv.org/pdf/2501.08182v1.pdf","comment":"8 pages, 2 figures and 4 tables. Sixteenth International Conference\n  on Machine Vision (ICMV 2023), Yerevan, Armenia"},{"id":"http://arxiv.org/abs/2501.08180v1","updated":"2025-01-14T15:03:53Z","published":"2025-01-14T15:03:53Z","title":"D$^2$-DPM: Dual Denoising for Quantized Diffusion Probabilistic Models","summary":"  Diffusion models have achieved cutting-edge performance in image generation.\nHowever, their lengthy denoising process and computationally intensive score\nestimation network impede their scalability in low-latency and\nresource-constrained scenarios. Post-training quantization (PTQ) compresses and\naccelerates diffusion models without retraining, but it inevitably introduces\nadditional quantization noise, resulting in mean and variance deviations. In\nthis work, we propose D2-DPM, a dual denoising mechanism aimed at precisely\nmitigating the adverse effects of quantization noise on the noise estimation\nnetwork. Specifically, we first unravel the impact of quantization noise on the\nsampling equation into two components: the mean deviation and the variance\ndeviation. The mean deviation alters the drift coefficient of the sampling\nequation, influencing the trajectory trend, while the variance deviation\nmagnifies the diffusion coefficient, impacting the convergence of the sampling\ntrajectory. The proposed D2-DPM is thus devised to denoise the quantization\nnoise at each time step, and then denoise the noisy sample through the inverse\ndiffusion iterations. Experimental results demonstrate that D2-DPM achieves\nsuperior generation quality, yielding a 1.42 lower FID than the full-precision\nmodel while achieving 3.99x compression and 11.67x bit-operation acceleration.\n","authors":["Qian Zeng","Jie Song","Han Zheng","Hao Jiang","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2501.08180v1.pdf","comment":"9 pages, 4 figures, acceptted by AAAI2025"},{"id":"http://arxiv.org/abs/2501.08174v1","updated":"2025-01-14T14:56:31Z","published":"2025-01-14T14:56:31Z","title":"Object-Centric 2D Gaussian Splatting: Background Removal and\n  Occlusion-Aware Pruning for Compact Object Models","summary":"  Current Gaussian Splatting approaches are effective for reconstructing entire\nscenes but lack the option to target specific objects, making them\ncomputationally expensive and unsuitable for object-specific applications. We\npropose a novel approach that leverages object masks to enable targeted\nreconstruction, resulting in object-centric models. Additionally, we introduce\nan occlusion-aware pruning strategy to minimize the number of Gaussians without\ncompromising quality. Our method reconstructs compact object models, yielding\nobject-centric Gaussian and mesh representations that are up to 96\\% smaller\nand up to 71\\% faster to train compared to the baseline while retaining\ncompetitive quality. These representations are immediately usable for\ndownstream applications such as appearance editing and physics simulation\nwithout additional processing.\n","authors":["Marcel Rogge","Didier Stricker"],"pdf_url":"https://arxiv.org/pdf/2501.08174v1.pdf","comment":"Accepted at ICPRAM 2025 (https://icpram.scitevents.org/Home.aspx)"},{"id":"http://arxiv.org/abs/2411.19835v2","updated":"2025-01-14T14:53:10Z","published":"2024-11-29T16:45:25Z","title":"Feedback-driven object detection and iterative model improvement","summary":"  Automated object detection has become increasingly valuable across diverse\napplications, yet efficient, high-quality annotation remains a persistent\nchallenge. In this paper, we present the development and evaluation of a\nplatform designed to interactively improve object detection models. The\nplatform allows uploading and annotating images as well as fine-tuning object\ndetection models. Users can then manually review and refine annotations,\nfurther creating improved snapshots that are used for automatic object\ndetection on subsequent image uploads - a process we refer to as semi-automatic\nannotation resulting in a significant gain in annotation efficiency.\n  Whereas iterative refinement of model results to speed up annotation has\nbecome common practice, we are the first to quantitatively evaluate its\nbenefits with respect to time, effort, and interaction savings. Our\nexperimental results show clear evidence for a significant time reduction of up\nto 53% for semi-automatic compared to manual annotation. Importantly, these\nefficiency gains did not compromise annotation quality, while matching or\noccasionally even exceeding the accuracy of manual annotations. These findings\ndemonstrate the potential of our lightweight annotation platform for creating\nhigh-quality object detection datasets and provide best practices to guide\nfuture development of annotation platforms.\n  The platform is open-source, with the frontend and backend repositories\navailable on GitHub (https://github.com/ml-lab-htw/iterative-annotate). To\nsupport the understanding of our labeling process, we have created an\nexplanatory video demonstrating the methodology using microscopy images of E.\ncoli bacteria as an example. The video is available on YouTube\n(https://www.youtube.com/watch?v=CM9uhE8NN5E).\n","authors":["Sönke Tenckhoff","Mario Koddenbrock","Erik Rodner"],"pdf_url":"https://arxiv.org/pdf/2411.19835v2.pdf","comment":"AI4EA24"},{"id":"http://arxiv.org/abs/2501.08170v1","updated":"2025-01-14T14:50:57Z","published":"2025-01-14T14:50:57Z","title":"Benchmarking Multimodal Models for Fine-Grained Image Analysis: A\n  Comparative Study Across Diverse Visual Features","summary":"  This article introduces a benchmark designed to evaluate the capabilities of\nmultimodal models in analyzing and interpreting images. The benchmark focuses\non seven key visual aspects: main object, additional objects, background,\ndetail, dominant colors, style, and viewpoint. A dataset of 14,580 images,\ngenerated from diverse text prompts, was used to assess the performance of\nseven leading multimodal models. These models were evaluated on their ability\nto accurately identify and describe each visual aspect, providing insights into\ntheir strengths and weaknesses for comprehensive image understanding. The\nfindings of this benchmark have significant implications for the development\nand selection of multimodal models for various image analysis tasks.\n","authors":["Evgenii Evstafev"],"pdf_url":"https://arxiv.org/pdf/2501.08170v1.pdf","comment":"6 pages, 2 tables, 2 charts"},{"id":"http://arxiv.org/abs/2501.08169v1","updated":"2025-01-14T14:49:49Z","published":"2025-01-14T14:49:49Z","title":"Revolutionizing Communication with Deep Learning and XAI for Enhanced\n  Arabic Sign Language Recognition","summary":"  This study introduces an integrated approach to recognizing Arabic Sign\nLanguage (ArSL) using state-of-the-art deep learning models such as\nMobileNetV3, ResNet50, and EfficientNet-B2. These models are further enhanced\nby explainable AI (XAI) techniques to boost interpretability. The ArSL2018 and\nRGB Arabic Alphabets Sign Language (AASL) datasets are employed, with\nEfficientNet-B2 achieving peak accuracies of 99.48\\% and 98.99\\%, respectively.\nKey innovations include sophisticated data augmentation methods to mitigate\nclass imbalance, implementation of stratified 5-fold cross-validation for\nbetter generalization, and the use of Grad-CAM for clear model decision\ntransparency. The proposed system not only sets new benchmarks in recognition\naccuracy but also emphasizes interpretability, making it suitable for\napplications in healthcare, education, and inclusive communication\ntechnologies.\n","authors":["Mazen Balat","Rewaa Awaad","Ahmed B. Zaky","Salah A. Aly"],"pdf_url":"https://arxiv.org/pdf/2501.08169v1.pdf","comment":"13 pages, 25 figures, 16 tables"},{"id":"http://arxiv.org/abs/2412.13174v2","updated":"2025-01-14T14:48:32Z","published":"2024-12-17T18:53:43Z","title":"ORFormer: Occlusion-Robust Transformer for Accurate Facial Landmark\n  Detection","summary":"  Although facial landmark detection (FLD) has gained significant progress,\nexisting FLD methods still suffer from performance drops on partially\nnon-visible faces, such as faces with occlusions or under extreme lighting\nconditions or poses. To address this issue, we introduce ORFormer, a novel\ntransformer-based method that can detect non-visible regions and recover their\nmissing features from visible parts. Specifically, ORFormer associates each\nimage patch token with one additional learnable token called the messenger\ntoken. The messenger token aggregates features from all but its patch. This\nway, the consensus between a patch and other patches can be assessed by\nreferring to the similarity between its regular and messenger embeddings,\nenabling non-visible region identification. Our method then recovers occluded\npatches with features aggregated by the messenger tokens. Leveraging the\nrecovered features, ORFormer compiles high-quality heatmaps for the downstream\nFLD task. Extensive experiments show that our method generates heatmaps\nresilient to partial occlusions. By integrating the resultant heatmaps into\nexisting FLD methods, our method performs favorably against the state of the\narts on challenging datasets such as WFLW and COFW.\n","authors":["Jui-Che Chiang","Hou-Ning Hu","Bo-Syuan Hou","Chia-Yu Tseng","Yu-Lun Liu","Min-Hung Chen","Yen-Yu Lin"],"pdf_url":"https://arxiv.org/pdf/2412.13174v2.pdf","comment":"WACV 2025 Project Link: https://ben0919.github.io/ORFormer/"},{"id":"http://arxiv.org/abs/2501.08163v1","updated":"2025-01-14T14:41:51Z","published":"2025-01-14T14:41:51Z","title":"DM-Mamba: Dual-domain Multi-scale Mamba for MRI reconstruction","summary":"  The accelerated MRI reconstruction poses a challenging ill-posed inverse\nproblem due to the significant undersampling in k-space. Deep neural networks,\nsuch as CNNs and ViT, have shown substantial performance improvements for this\ntask while encountering the dilemma between global receptive fields and\nefficient computation. To this end, this paper pioneers exploring Mamba, a new\nparadigm for long-range dependency modeling with linear complexity, for\nefficient and effective MRI reconstruction. However, directly applying Mamba to\nMRI reconstruction faces three significant issues: (1) Mamba's row-wise and\ncolumn-wise scanning disrupts k-space's unique spectrum, leaving its potential\nin k-space learning unexplored. (2) Existing Mamba methods unfold feature maps\nwith multiple lengthy scanning paths, leading to long-range forgetting and high\ncomputational burden. (3) Mamba struggles with spatially-varying contents,\nresulting in limited diversity of local representations. To address these, we\npropose a dual-domain multi-scale Mamba for MRI reconstruction from the\nfollowing perspectives: (1) We pioneer vision Mamba in k-space learning. A\ncircular scanning is customized for spectrum unfolding, benefiting the global\nmodeling of k-space. (2) We propose a multi-scale Mamba with an efficient\nscanning strategy in both image and k-space domains. It mitigates long-range\nforgetting and achieves a better trade-off between efficiency and performance.\n(3) We develop a local diversity enhancement module to improve the\nspatially-varying representation of Mamba. Extensive experiments are conducted\non three public datasets for MRI reconstruction under various undersampling\npatterns. Comprehensive results demonstrate that our method significantly\noutperforms state-of-the-art methods with lower computational cost.\nImplementation code will be available at\nhttps://github.com/XiaoMengLiLiLi/DM-Mamba.\n","authors":["Yucong Meng","Zhiwei Yang","Zhijian Song","Yonghong Shi"],"pdf_url":"https://arxiv.org/pdf/2501.08163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06746v2","updated":"2025-01-14T14:40:35Z","published":"2025-01-12T08:04:52Z","title":"Diversified Augmentation with Domain Adaptation for Debiased Video\n  Temporal Grounding","summary":"  Temporal sentence grounding in videos (TSGV) faces challenges due to public\nTSGV datasets containing significant temporal biases, which are attributed to\nthe uneven temporal distributions of target moments. Existing methods generate\naugmented videos, where target moments are forced to have varying temporal\nlocations. However, since the video lengths of the given datasets have small\nvariations, only changing the temporal locations results in poor generalization\nability in videos with varying lengths. In this paper, we propose a novel\ntraining framework complemented by diversified data augmentation and a domain\ndiscriminator. The data augmentation generates videos with various lengths and\ntarget moment locations to diversify temporal distributions. However, augmented\nvideos inevitably exhibit distinct feature distributions which may introduce\nnoise. To address this, we design a domain adaptation auxiliary task to\ndiminish feature discrepancies between original and augmented videos. We also\nencourage the model to produce distinct predictions for videos with the same\ntext queries but different moment locations to promote debiased training.\nExperiments on Charades-CD and ActivityNet-CD datasets demonstrate the\neffectiveness and generalization abilities of our method in multiple grounding\nstructures, achieving state-of-the-art results.\n","authors":["Junlong Ren","Gangjian Zhang","Haifeng Sun","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2501.06746v2.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.06040v2","updated":"2025-01-14T14:33:55Z","published":"2025-01-10T15:18:05Z","title":"MSCViT: A Small-size ViT architecture with Multi-Scale Self-Attention\n  Mechanism for Tiny Datasets","summary":"  Vision Transformer (ViT) has demonstrated significant potential in various\nvision tasks due to its strong ability in modelling long-range dependencies.\nHowever, such success is largely fueled by training on massive samples. In real\napplications, the large-scale datasets are not always available, and ViT\nperforms worse than Convolutional Neural Networks (CNNs) if it is only trained\non small scale dataset (called tiny dataset), since it requires large amount of\ntraining data to ensure its representational capacity. In this paper, a\nsmall-size ViT architecture with multi-scale self-attention mechanism and\nconvolution blocks is presented (dubbed MSCViT) to model different scales of\nattention at each layer. Firstly, we introduced wavelet convolution, which\nselectively combines the high-frequency components obtained by frequency\ndivision with our convolution channel to extract local features. Then, a\nlightweight multi-head attention module is developed to reduce the number of\ntokens and computational costs. Finally, the positional encoding (PE) in the\nbackbone is replaced by a local feature extraction module. Compared with the\noriginal ViT, it is parameter-efficient and is particularly suitable for tiny\ndatasets. Extensive experiments have been conducted on tiny datasets, in which\nour model achieves an accuracy of 84.68% on CIFAR-100 with 14.0M parameters and\n2.5 GFLOPs, without pre-training on large datasets.\n","authors":["Bowei Zhang","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.06040v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08152v1","updated":"2025-01-14T14:26:18Z","published":"2025-01-14T14:26:18Z","title":"Energy Backdoor Attack to Deep Neural Networks","summary":"  The rise of deep learning (DL) has increased computing complexity and energy\nuse, prompting the adoption of application specific integrated circuits (ASICs)\nfor energy-efficient edge and mobile deployment. However, recent studies have\ndemonstrated the vulnerability of these accelerators to energy attacks. Despite\nthe development of various inference time energy attacks in prior research,\nbackdoor energy attacks remain unexplored. In this paper, we design an\ninnovative energy backdoor attack against deep neural networks (DNNs) operating\non sparsity-based accelerators. Our attack is carried out in two distinct\nphases: backdoor injection and backdoor stealthiness. Experimental results\nusing ResNet-18 and MobileNet-V2 models trained on CIFAR-10 and Tiny ImageNet\ndatasets show the effectiveness of our proposed attack in increasing energy\nconsumption on trigger samples while preserving the model's performance for\nclean/regular inputs. This demonstrates the vulnerability of DNNs to energy\nbackdoor attacks. The source code of our attack is available at:\nhttps://github.com/hbrachemi/energy_backdoor.\n","authors":["Hanene F. Z. Brachemi Meftah","Wassim Hamidouche","Sid Ahmed Fezza","Olivier Déforges","Kassem Kallas"],"pdf_url":"https://arxiv.org/pdf/2501.08152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.09655v2","updated":"2025-01-14T14:22:05Z","published":"2022-10-18T07:48:59Z","title":"WINE: Wavelet-Guided GAN Inversion and Editing for High-Fidelity\n  Refinement","summary":"  Recent advanced GAN inversion models aim to convey high-fidelity information\nfrom original images to generators through methods using generator tuning or\nhigh-dimensional feature learning. Despite these efforts, accurately\nreconstructing image-specific details remains as a challenge due to the\ninherent limitations both in terms of training and structural aspects, leading\nto a bias towards low-frequency information. In this paper, we look into the\nwidely used pixel loss in GAN inversion, revealing its predominant focus on the\nreconstruction of low-frequency features. We then propose WINE, a\nWavelet-guided GAN Inversion aNd Editing model, which transfers the\nhigh-frequency information through wavelet coefficients via newly proposed\nwavelet loss and wavelet fusion scheme. Notably, WINE is the first attempt to\ninterpret GAN inversion in the frequency domain. Our experimental results\nshowcase the precision of WINE in preserving high-frequency details and\nenhancing image quality. Even in editing scenarios, WINE outperforms existing\nstate-of-the-art GAN inversion models with a fine balance between editability\nand reconstruction quality.\n","authors":["Chaewon Kim","Seung-Jun Moon","Gyeong-Moon Park"],"pdf_url":"https://arxiv.org/pdf/2210.09655v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08142v1","updated":"2025-01-14T14:21:48Z","published":"2025-01-14T14:21:48Z","title":"Bootstrapping Corner Cases: High-Resolution Inpainting for Safety\n  Critical Detect and Avoid for Automated Flying","summary":"  Modern machine learning techniques have shown tremendous potential,\nespecially for object detection on camera images. For this reason, they are\nalso used to enable safety-critical automated processes such as autonomous\ndrone flights. We present a study on object detection for Detect and Avoid, a\nsafety critical function for drones that detects air traffic during automated\nflights for safety reasons. An ill-posed problem is the generation of good and\nespecially large data sets, since detection itself is the corner case. Most\nmodels suffer from limited ground truth in raw data, \\eg recorded air traffic\nor frontal flight with a small aircraft. It often leads to poor and critical\ndetection rates. We overcome this problem by using inpainting methods to\nbootstrap the dataset such that it explicitly contains the corner cases of the\nraw data. We provide an overview of inpainting methods and generative models\nand present an example pipeline given a small annotated dataset. We validate\nour method by generating a high-resolution dataset, which we make publicly\navailable and present it to an independent object detector that was fully\ntrained on real data.\n","authors":["Jonathan Lyhs","Lars Hinneburg","Michael Fischer","Florian Ölsner","Stefan Milz","Jeremy Tschirner","Patrick Mäder"],"pdf_url":"https://arxiv.org/pdf/2501.08142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08137v1","updated":"2025-01-14T14:15:10Z","published":"2025-01-14T14:15:10Z","title":"Audio-visual Deepfake Detection With Local Temporal Inconsistencies","summary":"  This paper proposes an audio-visual deepfake detection approach that aims to\ncapture fine-grained temporal inconsistencies between audio and visual\nmodalities. To achieve this, both architectural and data synthesis strategies\nare introduced. From an architectural perspective, a temporal distance map,\ncoupled with an attention mechanism, is designed to capture these\ninconsistencies while minimizing the impact of irrelevant temporal\nsubsequences. Moreover, we explore novel pseudo-fake generation techniques to\nsynthesize local inconsistencies. Our approach is evaluated against\nstate-of-the-art methods using the DFDC and FakeAVCeleb datasets, demonstrating\nits effectiveness in detecting audio-visual deepfakes.\n","authors":["Marcella Astrid","Enjie Ghorbel","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2501.08137v1.pdf","comment":"Accepted in ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.06838v2","updated":"2025-01-14T14:09:23Z","published":"2025-01-12T15:14:58Z","title":"Generalized and Efficient 2D Gaussian Splatting for Arbitrary-scale\n  Super-Resolution","summary":"  Equipped with the continuous representation capability of Multi-Layer\nPerceptron (MLP), Implicit Neural Representation (INR) has been successfully\nemployed for Arbitrary-scale Super-Resolution (ASR). However, the limited\nreceptive field of the linear layers in MLP restricts the representation\ncapability of INR, while it is computationally expensive to query the MLP\nnumerous times to render each pixel. Recently, Gaussian Splatting (GS) has\nshown its advantages over INR in both visual quality and rendering speed in 3D\ntasks, which motivates us to explore whether GS can be employed for the ASR\ntask. However, directly applying GS to ASR is exceptionally challenging because\nthe original GS is an optimization-based method through overfitting each single\nscene, while in ASR we aim to learn a single model that can generalize to\ndifferent images and scaling factors. We overcome these challenges by\ndeveloping two novel techniques. Firstly, to generalize GS for ASR, we\nelaborately design an architecture to predict the corresponding\nimage-conditioned Gaussians of the input low-resolution image in a feed-forward\nmanner. Secondly, we implement an efficient differentiable 2D GPU/CUDA-based\nscale-aware rasterization to render super-resolved images by sampling discrete\nRGB values from the predicted contiguous Gaussians. Via end-to-end training,\nour optimized network, namely GSASR, can perform ASR for any image and unseen\nscaling factors. Extensive experiments validate the effectiveness of our\nproposed method. The project page can be found at\n\\url{https://mt-cly.github.io/GSASR.github.io/}.\n","authors":["Du Chen","Liyi Chen","Zhengqiang Zhang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.06838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08131v1","updated":"2025-01-14T14:07:48Z","published":"2025-01-14T14:07:48Z","title":"SAR Strikes Back: A New Hope for RSVQA","summary":"  Remote sensing visual question answering (RSVQA) is a task that automatically\nextracts information from satellite images and processes a question to predict\nthe answer from the images in textual form, helping with the interpretation of\nthe image. While different methods have been proposed to extract information\nfrom optical images with different spectral bands and resolutions, no method\nhas been proposed to answer questions from Synthetic Aperture Radar (SAR)\nimages. SAR images capture electromagnetic information from the scene, and are\nless affected by atmospheric conditions, such as clouds. In this work, our\nobjective is to introduce SAR in the RSVQA task, finding the best way to use\nthis modality. In our research, we carry out a study on different pipelines for\nthe task of RSVQA taking into account information from both SAR and optical\ndata. To this purpose, we also present a dataset that allows for the\nintroduction of SAR images in the RSVQA framework. We propose two different\nmodels to include the SAR modality. The first one is an end-to-end method in\nwhich we add an additional encoder for the SAR modality. In the second\napproach, we build on a two-stage framework. First, relevant information is\nextracted from SAR and, optionally, optical data. This information is then\ntranslated into natural language to be used in the second step which only\nrelies on a language model to provide the answer. We find that the second\npipeline allows us to obtain good results with SAR images alone. We then try\nvarious types of fusion methods to use SAR and optical images together, finding\nthat a fusion at the decision level achieves the best results on the proposed\ndataset. We show that SAR data offers additional information when fused with\nthe optical modality, particularly for questions related to specific land cover\nclasses, such as water areas.\n","authors":["Lucrezia Tosato","Flora Weissgerber","Laurent Wendling","Sylvain Lobry"],"pdf_url":"https://arxiv.org/pdf/2501.08131v1.pdf","comment":"26 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.08118v1","updated":"2025-01-14T13:51:14Z","published":"2025-01-14T13:51:14Z","title":"Revisiting Birds Eye View Perception Models with Frozen Foundation\n  Models: DINOv2 and Metric3Dv2","summary":"  Birds Eye View perception models require extensive data to perform and\ngeneralize effectively. While traditional datasets often provide abundant\ndriving scenes from diverse locations, this is not always the case. It is\ncrucial to maximize the utility of the available training data. With the advent\nof large foundation models such as DINOv2 and Metric3Dv2, a pertinent question\narises: can these models be integrated into existing model architectures to not\nonly reduce the required training data but surpass the performance of current\nmodels? We choose two model architectures in the vehicle segmentation domain to\nalter: Lift-Splat-Shoot, and Simple-BEV. For Lift-Splat-Shoot, we explore the\nimplementation of frozen DINOv2 for feature extraction and Metric3Dv2 for depth\nestimation, where we greatly exceed the baseline results by 7.4 IoU while\nutilizing only half the training data and iterations. Furthermore, we introduce\nan innovative application of Metric3Dv2's depth information as a PseudoLiDAR\npoint cloud incorporated into the Simple-BEV architecture, replacing\ntraditional LiDAR. This integration results in a +3 IoU improvement compared to\nthe Camera-only model.\n","authors":["Seamie Hayes","Ganesh Sistu","Ciarán Eising"],"pdf_url":"https://arxiv.org/pdf/2501.08118v1.pdf","comment":"Accepted for publication at the Electronic Imaging - Autonomous\n  Vehicles and Machines Connference 2025"},{"id":"http://arxiv.org/abs/2501.08115v1","updated":"2025-01-14T13:46:07Z","published":"2025-01-14T13:46:07Z","title":"RoHan: Robust Hand Detection in Operation Room","summary":"  Hand-specific localization has garnered significant interest within the\ncomputer vision community. Although there are numerous datasets with hand\nannotations from various angles and settings, domain transfer techniques\nfrequently struggle in surgical environments. This is mainly due to the limited\navailability of gloved hand instances and the unique challenges of operating\nrooms (ORs). Thus, hand-detection models tailored to OR settings require\nextensive training and expensive annotation processes. To overcome these\nchallenges, we present \"RoHan\" - a novel approach for robust hand detection in\nthe OR, leveraging advanced semi-supervised domain adaptation techniques to\ntackle the challenges of varying recording conditions, diverse glove colors,\nand occlusions common in surgical settings. Our methodology encompasses two\nmain stages: (1) data augmentation strategy that utilizes \"Artificial Gloves,\"\na method for augmenting publicly available hand datasets with synthetic images\nof hands-wearing gloves; (2) semi-supervised domain adaptation pipeline that\nimproves detection performance in real-world OR settings through iterative\nprediction refinement and efficient frame filtering. We evaluate our method\nusing two datasets: simulated enterotomy repair and saphenous vein graft\nharvesting. \"RoHan\" substantially reduces the need for extensive labeling and\nmodel training, paving the way for the practical implementation of hand\ndetection technologies in medical settings.\n","authors":["Roi Papo","Sapir Gershov","Tom Friedman","Itay Or","Gil Bolotin","Shlomi Laufer"],"pdf_url":"https://arxiv.org/pdf/2501.08115v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2501.08114v1","updated":"2025-01-14T13:46:03Z","published":"2025-01-14T13:46:03Z","title":"Change Captioning in Remote Sensing: Evolution to SAT-Cap -- A\n  Single-Stage Transformer Approach","summary":"  Change captioning has become essential for accurately describing changes in\nmulti-temporal remote sensing data, providing an intuitive way to monitor\nEarth's dynamics through natural language. However, existing change captioning\nmethods face two key challenges: high computational demands due to multistage\nfusion strategy, and insufficient detail in object descriptions due to limited\nsemantic extraction from individual images. To solve these challenges, we\npropose SAT-Cap based on the transformers model with a single-stage feature\nfusion for remote sensing change captioning. In particular, SAT-Cap integrates\na Spatial-Channel Attention Encoder, a Difference-Guided Fusion module, and a\nCaption Decoder. Compared to typical models that require multi-stage fusion in\ntransformer encoder and fusion module, SAT-Cap uses only a simple cosine\nsimilarity-based fusion module for information integration, reducing the\ncomplexity of the model architecture. By jointly modeling spatial and channel\ninformation in Spatial-Channel Attention Encoder, our approach significantly\nenhances the model's ability to extract semantic information from objects in\nmulti-temporal remote sensing images. Extensive experiments validate the\neffectiveness of SAT-Cap, achieving CIDEr scores of 140.23% on the LEVIR-CC\ndataset and 97.74% on the DUBAI-CC dataset, surpassing current state-of-the-art\nmethods. The code and pre-trained models will be available online.\n","authors":["Yuduo Wang","Weikang Yu","Pedram Ghamisi"],"pdf_url":"https://arxiv.org/pdf/2501.08114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08111v1","updated":"2025-01-14T13:42:22Z","published":"2025-01-14T13:42:22Z","title":"EarthView: A Large Scale Remote Sensing Dataset for Self-Supervision","summary":"  This paper presents EarthView, a comprehensive dataset specifically designed\nfor self-supervision on remote sensing data, intended to enhance deep learning\napplications on Earth monitoring tasks. The dataset spans 15 tera pixels of\nglobal remote-sensing data, combining imagery from a diverse range of sources,\nincluding NEON, Sentinel, and a novel release of 1m spatial resolution data\nfrom Satellogic. Our dataset provides a wide spectrum of image data with\nvarying resolutions, harnessed from different sensors and organized coherently\ninto an accessible HuggingFace dataset in parquet format. This data spans five\nyears, from 2017 to 2022. Accompanying the dataset, we introduce EarthMAE, a\ntailored Masked Autoencoder, developed to tackle the distinct challenges of\nremote sensing data. Trained in a self-supervised fashion, EarthMAE effectively\nprocesses different data modalities such as hyperspectral, multispectral,\ntopographical data, segmentation maps, and temporal structure. This model helps\nus show that pre-training on Satellogic data improves performance on downstream\ntasks. While there is still a gap to fill in MAE for heterogeneous data, we\nregard this innovative combination of an expansive, diverse dataset and a\nversatile model adapted for self-supervised learning as a stride forward in\ndeep learning for Earth monitoring.\n","authors":["Diego Velazquez","Pau Rodriguez López","Sergio Alonso","Josep M. Gonfaus","Jordi Gonzalez","Gerardo Richarte","Javier Marin","Yoshua Bengio","Alexandre Lacoste"],"pdf_url":"https://arxiv.org/pdf/2501.08111v1.pdf","comment":"2nd Workshop on Computer Vision for Earth Observation (CV4EO)\n  Applications"},{"id":"http://arxiv.org/abs/2312.16409v2","updated":"2025-01-14T13:14:00Z","published":"2023-12-27T04:40:12Z","title":"Dynamic Sub-graph Distillation for Robust Semi-supervised Continual\n  Learning","summary":"  Continual learning (CL) has shown promising results and comparable\nperformance to learning at once in a fully supervised manner. However, CL\nstrategies typically require a large number of labeled samples, making their\nreal-life deployment challenging. In this work, we focus on semi-supervised\ncontinual learning (SSCL), where the model progressively learns from partially\nlabeled data with unknown categories. We provide a comprehensive analysis of\nSSCL and demonstrate that unreliable distributions of unlabeled data lead to\nunstable training and refinement of the progressing stages. This problem\nseverely impacts the performance of SSCL. To address the limitations, we\npropose a novel approach called Dynamic Sub-Graph Distillation (DSGD) for\nsemi-supervised continual learning, which leverages both semantic and\nstructural information to achieve more stable knowledge distillation on\nunlabeled data and exhibit robustness against distribution bias. Firstly, we\nformalize a general model of structural distillation and design a dynamic graph\nconstruction for the continual learning progress. Next, we define a structure\ndistillation vector and design a dynamic sub-graph distillation algorithm,\nwhich enables end-to-end training and adaptability to scale up tasks. The\nentire proposed method is adaptable to various CL methods and supervision\nsettings. Finally, experiments conducted on three datasets CIFAR10, CIFAR100,\nand ImageNet-100, with varying supervision ratios, demonstrate the\neffectiveness of our proposed approach in mitigating the catastrophic\nforgetting problem in semi-supervised continual learning scenarios.\n","authors":["Yan Fan","Yu Wang","Pengfei Zhu","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2312.16409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08097v1","updated":"2025-01-14T13:10:29Z","published":"2025-01-14T13:10:29Z","title":"Guiding the classification of hepatocellular carcinoma on 3D CT-scans\n  using deep and handcrafted radiological features","summary":"  Hepatocellular carcinoma is the most spread primary liver cancer across the\nworld ($\\sim$80\\% of the liver tumors). The gold standard for HCC diagnosis is\nliver biopsy. However, in the clinical routine, expert radiologists provide a\nvisual diagnosis by interpreting hepatic CT-scans according to a standardized\nprotocol, the LI-RADS, which uses five radiological criteria with an associated\ndecision tree. In this paper, we propose an automatic approach to predict\nhistology-proven HCC from CT images in order to reduce radiologists'\ninter-variability. We first show that standard deep learning methods fail to\naccurately predict HCC from CT-scans on a challenging database, and propose a\ntwo-step approach inspired by the LI-RADS system to improve the performance. We\nachieve improvements from 6 to 18 points of AUC with respect to deep learning\nbaselines trained with different architectures. We also provide clinical\nvalidation of our method, achieving results that outperform non-expert\nradiologists and are on par with expert ones.\n","authors":["E. Sarfati","A. Bône","M-M. Rohé","C. Aubé","M. Ronot","P. Gori","I. Bloch"],"pdf_url":"https://arxiv.org/pdf/2501.08097v1.pdf","comment":"IEEE ISBI 2025"},{"id":"http://arxiv.org/abs/2501.08094v1","updated":"2025-01-14T13:09:36Z","published":"2025-01-14T13:09:36Z","title":"CellOMaps: A Compact Representation for Robust Classification of Lung\n  Adenocarcinoma Growth Patterns","summary":"  Lung adenocarcinoma (LUAD) is a morphologically heterogeneous disease,\ncharacterized by five primary histological growth patterns. The classification\nof such patterns is crucial due to their direct relation to prognosis but the\nhigh subjectivity and observer variability pose a major challenge. Although\nseveral studies have developed machine learning methods for growth pattern\nclassification, they either only report the predominant pattern per slide or\nlack proper evaluation. We propose a generalizable machine learning pipeline\ncapable of classifying lung tissue into one of the five patterns or as\nnon-tumor. The proposed pipeline's strength lies in a novel compact Cell\nOrganization Maps (cellOMaps) representation that captures the cellular spatial\npatterns from Hematoxylin and Eosin whole slide images (WSIs). The proposed\npipeline provides state-of-the-art performance on LUAD growth pattern\nclassification when evaluated on both internal unseen slides and external\ndatasets, significantly outperforming the current approaches. In addition, our\npreliminary results show that the model's outputs can be used to predict\npatients Tumor Mutational Burden (TMB) levels.\n","authors":["Arwa Al-Rubaian","Gozde N. Gunesli","Wajd A. Althakfi","Ayesha Azam","David Snead","Nasir M. Rajpoot","Shan E Ahmed Raza"],"pdf_url":"https://arxiv.org/pdf/2501.08094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08088v1","updated":"2025-01-14T12:57:17Z","published":"2025-01-14T12:57:17Z","title":"AgentPose: Progressive Distribution Alignment via Feature Agent for\n  Human Pose Distillation","summary":"  Pose distillation is widely adopted to reduce model size in human pose\nestimation. However, existing methods primarily emphasize the transfer of\nteacher knowledge while often neglecting the performance degradation resulted\nfrom the curse of capacity gap between teacher and student. To address this\nissue, we propose AgentPose, a novel pose distillation method that integrates a\nfeature agent to model the distribution of teacher features and progressively\naligns the distribution of student features with that of the teacher feature,\neffectively overcoming the capacity gap and enhancing the ability of knowledge\ntransfer. Our comprehensive experiments conducted on the COCO dataset\nsubstantiate the effectiveness of our method in knowledge transfer,\nparticularly in scenarios with a high capacity gap.\n","authors":["Feng Zhang","Jinwei Liu","Xiatian Zhu","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2501.08088v1.pdf","comment":"5 pages, 1 figures"},{"id":"http://arxiv.org/abs/2302.08878v3","updated":"2025-01-14T12:53:24Z","published":"2023-02-17T13:50:53Z","title":"Less is More: The Influence of Pruning on the Explainability of CNNs","summary":"  Over the last century, deep learning models have become the state-of-the-art\nfor solving complex computer vision problems. These modern computer vision\nmodels have millions of parameters, which presents two major challenges: (1)\nthe increased computational requirements hamper the deployment in\nresource-constrained environments, such as mobile or IoT devices, and (2)\nexplaining the complex decisions of such networks to humans is challenging.\nNetwork pruning is a technical approach to reduce the complexity of models,\nwhere less important parameters are removed. The work presented in this paper\ninvestigates whether this reduction in technical complexity also helps with\nperceived explainability. To do so, we conducted a pre-study and two\nhuman-grounded experiments, assessing the effects of different pruning ratios\non explainability. Overall, we evaluate four different compression rates (i.e.,\n2, 4, 8, and 32) with 37 500 tasks on Mechanical Turk. Results indicate that\nlower compression rates have a positive influence on explainability, while\nhigher compression rates show negative effects. Furthermore, we were able to\nidentify sweet spots that increase both the perceived explainability and the\nmodel's performance.\n","authors":["Florian Merkle","David Weber","Pascal Schöttle","Stephan Schlögl","Martin Nocker"],"pdf_url":"https://arxiv.org/pdf/2302.08878v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08083v1","updated":"2025-01-14T12:51:34Z","published":"2025-01-14T12:51:34Z","title":"Benchmarking Vision Foundation Models for Input Monitoring in Autonomous\n  Driving","summary":"  Deep neural networks (DNNs) remain challenged by distribution shifts in\ncomplex open-world domains like automated driving (AD): Absolute robustness\nagainst yet unknown novel objects (semantic shift) or styles like lighting\nconditions (covariate shift) cannot be guaranteed. Hence, reliable\noperation-time monitors for identification of out-of-training-data-distribution\n(OOD) scenarios are imperative. Current approaches for OOD classification are\nuntested for complex domains like AD, are limited in the kinds of shifts they\ndetect, or even require supervision with OOD samples. To prepare for\nunanticipated shifts, we instead establish a framework around a principled,\nunsupervised, and model-agnostic method that unifies detection of all kinds of\nshifts: Find a full model of the training data's feature distribution, to then\nuse its density at new points as in-distribution (ID) score. To implement this,\nwe propose to combine the newly available Vision Foundation Models (VFM) as\nfeature extractors with one of four alternative density modeling techniques. In\nan extensive benchmark of 4 VFMs against 20 baselines, we show the superior\nperformance of VFM feature encodings compared to shift-specific OOD monitors.\nAdditionally, we find that sophisticated architectures outperform larger latent\nspace dimensionality; and our method identifies samples with higher risk of\nerrors on downstream tasks, despite being model-agnostic. This suggests that\nVFMs are promising to realize model-agnostic, unsupervised, reliable safety\nmonitors in complex vision tasks.\n","authors":["Nert Keser","Halil Ibrahim Orhan","Niki Amini-Naieni","Gesina Schwalbe","Alois Knoll","Matthias Rottmann"],"pdf_url":"https://arxiv.org/pdf/2501.08083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00376v3","updated":"2025-01-14T12:37:26Z","published":"2024-03-01T09:01:53Z","title":"Spurious Feature Eraser: Stabilizing Test-Time Adaptation for\n  Vision-Language Foundation Model","summary":"  Vision-language foundation models have exhibited remarkable success across a\nmultitude of downstream tasks due to their scalability on extensive image-text\npaired data. However, these models also display significant limitations when\napplied to downstream tasks, such as fine-grained image classification, as a\nresult of ``decision shortcuts'' that hinder their generalization capabilities.\nIn this work, we find that the CLIP model possesses a rich set of features,\nencompassing both \\textit{desired invariant causal features} and\n\\textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP\non downstream tasks originates from its inability to effectively utilize\npre-trained features in accordance with specific task requirements. To address\nthis challenge, we propose a simple yet effective method, Spurious Feature\nEraser (SEraser), to alleviate the decision shortcuts by erasing the spurious\nfeatures. Specifically, we introduce a test-time prompt tuning paradigm that\noptimizes a learnable prompt, thereby compelling the model to exploit invariant\nfeatures while disregarding decision shortcuts during the inference phase. The\nproposed method effectively alleviates excessive dependence on potentially\nmisleading spurious information. We conduct comparative analysis of the\nproposed method against various approaches which validates the significant\nsuperiority.\n","authors":["Huan Ma","Yan Zhu","Changqing Zhang","Peilin Zhao","Baoyuan Wu","Long-Kai Huang","Qinghua Hu","Bingzhe Wu"],"pdf_url":"https://arxiv.org/pdf/2403.00376v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20020v3","updated":"2025-01-14T12:31:48Z","published":"2024-07-29T13:57:24Z","title":"ImagiNet: A Multi-Content Benchmark for Synthetic Image Detection","summary":"  Recent generative models produce images with a level of authenticity that\nmakes them nearly indistinguishable from real photos and artwork. Potential\nharmful use cases of these models, necessitate the creation of robust synthetic\nimage detectors. However, current datasets in the field contain generated\nimages with questionable quality or have examples from one predominant content\ntype which leads to poor generalizability of the underlying detectors. We find\nthat the curation of a balanced amount of high-resolution generated images\nacross various content types is crucial for the generalizability of detectors,\nand introduce ImagiNet, a dataset of 200K examples, spanning four categories:\nphotos, paintings, faces, and miscellaneous. Synthetic images in ImagiNet are\nproduced with both open-source and proprietary generators, whereas real\ncounterparts for each content type are collected from public datasets. The\nstructure of ImagiNet allows for a two-track evaluation system: i)\nclassification as real or synthetic and ii) identification of the generative\nmodel. To establish a strong baseline, we train a ResNet-50 model using a\nself-supervised contrastive objective (SelfCon) for each track which achieves\nevaluation AUC of up to 0.99 and balanced accuracy ranging from 86% to 95%,\neven under conditions that involve compression and resizing. The provided model\nis generalizable enough to achieve zero-shot state-of-the-art performance on\nprevious synthetic detection benchmarks. We provide ablations to demonstrate\nthe importance of content types and publish code and data.\n","authors":["Delyan Boychev","Radostin Cholakov"],"pdf_url":"https://arxiv.org/pdf/2407.20020v3.pdf","comment":"Workshop on Datasets and Evaluators of AI Safety, AAAI 2025"},{"id":"http://arxiv.org/abs/2411.02188v4","updated":"2025-01-14T12:27:32Z","published":"2024-11-04T15:42:22Z","title":"Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition\n  via Foundation Models","summary":"  The accuracy of face recognition systems has improved significantly in the\npast few years, thanks to the large amount of data collected and advancements\nin neural network architectures. However, these large-scale datasets are often\ncollected without explicit consent, raising ethical and privacy concerns. To\naddress this, there have been proposals to use synthetic datasets for training\nface recognition models. Yet, such models still rely on real data to train the\ngenerative models and generally exhibit inferior performance compared to those\ntrained on real datasets. One of these datasets, DigiFace, uses a graphics\npipeline to generate different identities and intra-class variations without\nusing real data in model training. However, the performance of this approach is\npoor on face recognition benchmarks, possibly due to the lack of realism in the\nimages generated by the graphics pipeline. In this work, we introduce a novel\nframework for realism transfer aimed at enhancing the realism of synthetically\ngenerated face images. Our method leverages the large-scale face foundation\nmodel, and we adapt the pipeline for realism enhancement. By integrating the\ncontrollable aspects of the graphics pipeline with our realism enhancement\ntechnique, we generate a large amount of realistic variations, combining the\nadvantages of both approaches. Our empirical evaluations demonstrate that\nmodels trained using our enhanced dataset significantly improve the performance\nof face recognition systems over the baseline. The source code and dataset will\nbe publicly accessible at the following link:\nhttps://www.idiap.ch/paper/digi2real\n","authors":["Anjith George","Sebastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2411.02188v4.pdf","comment":"The dataset would be available here:\n  https://www.idiap.ch/paper/digi2real Accepted for Publication in WACV 2025"},{"id":"http://arxiv.org/abs/2501.08062v1","updated":"2025-01-14T12:15:49Z","published":"2025-01-14T12:15:49Z","title":"Skeleton and Font Generation Network for Zero-shot Chinese Character\n  Generation","summary":"  Automatic font generation remains a challenging research issue, primarily due\nto the vast number of Chinese characters, each with unique and intricate\nstructures. Our investigation of previous studies reveals inherent bias capable\nof causing structural changes in characters. Specifically, when generating a\nChinese character similar to, but different from, those in the training\nsamples, the bias is prone to either correcting or ignoring these subtle\nvariations. To address this concern, we propose a novel Skeleton and Font\nGeneration Network (SFGN) to achieve a more robust Chinese character font\ngeneration. Our approach includes a skeleton builder and font generator. The\nskeleton builder synthesizes content features using low-resource text input,\nenabling our technique to realize font generation independently of content\nimage inputs. Unlike previous font generation methods that treat font style as\na global embedding, we introduce a font generator to align content and style\nfeatures on the radical level, which is a brand-new perspective for font\ngeneration. Except for common characters, we also conduct experiments on\nmisspelled characters, a substantial portion of which slightly differs from the\ncommon ones. Our approach visually demonstrates the efficacy of generated\nimages and outperforms current state-of-the-art font generation methods.\nMoreover, we believe that misspelled character generation have significant\npedagogical implications and verify such supposition through experiments. We\nused generated misspelled characters as data augmentation in Chinese character\nerror correction tasks, simulating the scenario where students learn\nhandwritten Chinese characters with the help of misspelled characters. The\nsignificantly improved performance of error correction tasks demonstrates the\neffectiveness of our proposed approach and the value of misspelled character\ngeneration.\n","authors":["Mobai Xue","Jun Du","Zhenrong Zhang","Jiefeng Ma","Qikai Chang","Pengfei Hu","Jianshu Zhang","Yu Hu"],"pdf_url":"https://arxiv.org/pdf/2501.08062v1.pdf","comment":"36 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.09471v5","updated":"2025-01-14T11:59:06Z","published":"2024-03-14T15:10:54Z","title":"MambaTalk: Efficient Holistic Gesture Synthesis with Selective State\n  Space Models","summary":"  Gesture synthesis is a vital realm of human-computer interaction, with\nwide-ranging applications across various fields like film, robotics, and\nvirtual reality. Recent advancements have utilized the diffusion model and\nattention mechanisms to improve gesture synthesis. However, due to the high\ncomputational complexity of these techniques, generating long and diverse\nsequences with low latency remains a challenge. We explore the potential of\nstate space models (SSMs) to address the challenge, implementing a two-stage\nmodeling strategy with discrete motion priors to enhance the quality of\ngestures. Leveraging the foundational Mamba block, we introduce MambaTalk,\nenhancing gesture diversity and rhythm through multimodal integration.\nExtensive experiments demonstrate that our method matches or exceeds the\nperformance of state-of-the-art models.\n","authors":["Zunnan Xu","Yukang Lin","Haonan Han","Sicheng Yang","Ronghui Li","Yachao Zhang","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2403.09471v5.pdf","comment":"NeurlPS 2024, Camera Ready"},{"id":"http://arxiv.org/abs/2410.03335v2","updated":"2025-01-14T11:59:03Z","published":"2024-10-04T11:40:53Z","title":"Audio-Agent: Leveraging LLMs For Audio Generation, Editing and\n  Composition","summary":"  We introduce Audio-Agent, a multimodal framework for audio generation,\nediting and composition based on text or video inputs. Conventional approaches\nfor text-to-audio (TTA) tasks often make single-pass inferences from text\ndescriptions. While straightforward, this design struggles to produce\nhigh-quality audio when given complex text conditions. In our method, we\nutilize a pre-trained TTA diffusion network as the audio generation agent to\nwork in tandem with GPT-4, which decomposes the text condition into atomic,\nspecific instructions and calls the agent for audio generation. In doing so,\nAudio-Agent can generate high-quality audio that is closely aligned with the\nprovided text or video exhibiting complex and multiple events, while supporting\nvariable-length and variable-volume generation. For video-to-audio (VTA) tasks,\nmost existing methods require training a timestamp detector to synchronize\nvideo events with the generated audio, a process that can be tedious and\ntime-consuming. Instead, we propose a simpler approach by fine-tuning a\npre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both\nsemantic and temporal conditions that bridge the video and audio modality.\nConsequently, our framework contributes a comprehensive solution for both TTA\nand VTA tasks without substantial computational overhead in training.\n","authors":["Zixuan Wang","Chi-Keung Tang","Yu-Wing Tai"],"pdf_url":"https://arxiv.org/pdf/2410.03335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08049v1","updated":"2025-01-14T11:56:00Z","published":"2025-01-14T11:56:00Z","title":"Self-Attentive Spatio-Temporal Calibration for Precise Intermediate\n  Layer Matching in ANN-to-SNN Distillation","summary":"  Spiking Neural Networks (SNNs) are promising for low-power computation due to\ntheir event-driven mechanism but often suffer from lower accuracy compared to\nArtificial Neural Networks (ANNs). ANN-to-SNN knowledge distillation can\nimprove SNN performance, but previous methods either focus solely on label\ninformation, missing valuable intermediate layer features, or use a layer-wise\napproach that neglects spatial and temporal semantic inconsistencies, leading\nto performance degradation.To address these limitations, we propose a novel\nmethod called self-attentive spatio-temporal calibration (SASTC). SASTC uses\nself-attention to identify semantically aligned layer pairs between ANN and\nSNN, both spatially and temporally. This enables the autonomous transfer of\nrelevant semantic information. Extensive experiments show that SASTC\noutperforms existing methods, effectively solving the mismatching problem.\nSuperior accuracy results include 95.12% on CIFAR-10, 79.40% on CIFAR-100 with\n2 time steps, and 68.69% on ImageNet with 4 time steps for static datasets, and\n97.92% on DVS-Gesture and 83.60% on DVS-CIFAR10 for neuromorphic datasets. This\nmarks the first time SNNs have outperformed ANNs on both CIFAR-10 and\nCIFAR-100, shedding the new light on the potential applications of SNNs.\n","authors":["Di Hong","Yueming Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08042v1","updated":"2025-01-14T11:47:35Z","published":"2025-01-14T11:47:35Z","title":"Exploring visual language models as a powerful tool in the diagnosis of\n  Ewing Sarcoma","summary":"  Ewing's sarcoma (ES), characterized by a high density of small round blue\ncells without structural organization, presents a significant health concern,\nparticularly among adolescents aged 10 to 19. Artificial intelligence-based\nsystems for automated analysis of histopathological images are promising to\ncontribute to an accurate diagnosis of ES. In this context, this study explores\nthe feature extraction ability of different pre-training strategies for\ndistinguishing ES from other soft tissue or bone sarcomas with similar\nmorphology in digitized tissue microarrays for the first time, as far as we\nknow. Vision-language supervision (VLS) is compared to fully-supervised\nImageNet pre-training within a multiple instance learning paradigm. Our\nfindings indicate a substantial improvement in diagnostic accuracy with the\nadaption of VLS using an in-domain dataset. Notably, these models not only\nenhance the accuracy of predicted classes but also drastically reduce the\nnumber of trainable parameters and computational costs.\n","authors":["Alvaro Pastor-Naranjo","Pablo Meseguer","Rocío del Amor","Jose Antonio Lopez-Guerrero","Samuel Navarro","Katia Scotlandi","Antonio Llombart-Bosch","Isidro Machado","Valery Naranjo"],"pdf_url":"https://arxiv.org/pdf/2501.08042v1.pdf","comment":"11 pages, 5 figures, 2 tables. Oral presentation at KES-InMed 2024\n  held in Madeira, Portugal"},{"id":"http://arxiv.org/abs/2501.08038v1","updated":"2025-01-14T11:42:54Z","published":"2025-01-14T11:42:54Z","title":"Robust Low-Light Human Pose Estimation through Illumination-Texture\n  Modulation","summary":"  As critical visual details become obscured, the low visibility and high ISO\nnoise in extremely low-light images pose a significant challenge to human pose\nestimation. Current methods fail to provide high-quality representations due to\nreliance on pixel-level enhancements that compromise semantics and the\ninability to effectively handle extreme low-light conditions for robust feature\nlearning. In this work, we propose a frequency-based framework for low-light\nhuman pose estimation, rooted in the \"divide-and-conquer\" principle. Instead of\nuniformly enhancing the entire image, our method focuses on task-relevant\ninformation. By applying dynamic illumination correction to the low-frequency\ncomponents and low-rank denoising to the high-frequency components, we\neffectively enhance both the semantic and texture information essential for\naccurate pose estimation. As a result, this targeted enhancement method results\nin robust, high-quality representations, significantly improving pose\nestimation performance. Extensive experiments demonstrating its superiority\nover state-of-the-art methods in various challenging low-light scenarios.\n","authors":["Feng Zhang","Ze Li","Xiatian Zhu","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2501.08038v1.pdf","comment":"5 pages, 2 figures, conference"},{"id":"http://arxiv.org/abs/2409.16597v3","updated":"2025-01-14T11:27:28Z","published":"2024-09-25T03:49:46Z","title":"EventHallusion: Diagnosing Event Hallucinations in Video LLMs","summary":"  Recently, Multimodal Large Language Models (MLLMs) have made significant\nprogress in the video comprehension field. Despite remarkable content reasoning\nand instruction following capabilities they demonstrated, the hallucination\nproblem of these VideoLLMs is less explored compared with its counterpart in\nthe image domain. To mitigate this gap, we propose EventHallusion, a novel\nbenchmark that focuses on assessing the VideoLLMs' hallucination toward event,\nthe crux of video analysis. From a hallucination attribution perspective, our\nEventHallusion benchmark is curated to assess a VideoLLM's susceptibility\ntoward language priors and vision-language biases. On the other hand, we also\npropose a simple yet effective method, called Temporal Contrastive Decoding\n(TCD), to tackle the hallucination problems of VideoLLMs. The proposed TCD\nmethod rectifies the model's bias toward its priors during the decoding stage\nby comparing the original video with a modified version, in which temporal cues\nare disrupted. Through comprehensive evaluation of eight open-source and two\nclosed-source VideoLLMs on the proposed EventHallusion benchmark, we observe\nthat the open-source models suffer significantly from hallucination problems,\nwhereas the closed-source ones perform markedly better. By further equipping\nopen-source VideoLLMs with the proposed TCD approach, evident performance\nimprovements are achieved across most metrics in the EventHallusion benchmark.\nOur codes and benchmark data are available at\nhttps://github.com/Stevetich/EventHallusion.\n","authors":["Jiacheng Zhang","Yang Jiao","Shaoxiang Chen","Na Zhao","Jingjing Chen"],"pdf_url":"https://arxiv.org/pdf/2409.16597v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02774v3","updated":"2025-01-14T11:14:57Z","published":"2024-03-05T08:41:41Z","title":"Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System\n  Model Fields with Generative Machine Learning","summary":"  Accurate and high-resolution Earth system model (ESM) simulations are\nessential to assess the ecological and socio-economic impacts of anthropogenic\nclimate change, but are computationally too expensive to be run at sufficiently\nhigh spatial resolution. Recent machine learning approaches have shown\npromising results in downscaling ESM simulations, outperforming\nstate-of-the-art statistical approaches. However, existing methods require\ncomputationally costly retraining for each ESM and extrapolate poorly to\nclimates unseen during training. We address these shortcomings by learning a\nconsistency model (CM) that efficiently and accurately downscales arbitrary ESM\nsimulations without retraining in a zero-shot manner. Our approach yields\nprobabilistic downscaled fields at a resolution only limited by the\nobservational reference data. We show that the CM outperforms state-of-the-art\ndiffusion models at a fraction of computational cost while maintaining high\ncontrollability on the downscaling task. Further, our method generalizes to\nclimate states unseen during training without explicitly formulated physical\nconstraints.\n","authors":["Philipp Hess","Michael Aich","Baoxiang Pan","Niklas Boers"],"pdf_url":"https://arxiv.org/pdf/2403.02774v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04594v2","updated":"2025-01-14T11:03:05Z","published":"2024-12-05T20:15:34Z","title":"Learning Symmetries via Weight-Sharing with Doubly Stochastic Tensors","summary":"  Group equivariance has emerged as a valuable inductive bias in deep learning,\nenhancing generalization, data efficiency, and robustness. Classically, group\nequivariant methods require the groups of interest to be known beforehand,\nwhich may not be realistic for real-world data. Additionally, baking in fixed\ngroup equivariance may impose overly restrictive constraints on model\narchitecture. This highlights the need for methods that can dynamically\ndiscover and apply symmetries as soft constraints. For neural network\narchitectures, equivariance is commonly achieved through group transformations\nof a canonical weight tensor, resulting in weight sharing over a given group\n$G$. In this work, we propose to learn such a weight-sharing scheme by defining\na collection of learnable doubly stochastic matrices that act as soft\npermutation matrices on canonical weight tensors, which can take regular group\nrepresentations as a special case. This yields learnable kernel transformations\nthat are jointly optimized with downstream tasks. We show that when the dataset\nexhibits strong symmetries, the permutation matrices will converge to regular\ngroup representations and our weight-sharing networks effectively become\nregular group convolutions. Additionally, the flexibility of the method enables\nit to effectively pick up on partial symmetries.\n","authors":["Putri A. van der Linden","Alejandro García-Castellanos","Sharvaree Vadgama","Thijs P. Kuipers","Erik J. Bekkers"],"pdf_url":"https://arxiv.org/pdf/2412.04594v2.pdf","comment":"19 pages, 14 figures, 4 tables"},{"id":"http://arxiv.org/abs/2407.15500v3","updated":"2025-01-14T11:02:13Z","published":"2024-07-22T09:31:30Z","title":"TextureCrop: Enhancing Synthetic Image Detection through Texture-based\n  Cropping","summary":"  Generative AI technologies produce increasingly realistic imagery, which,\ndespite its potential for creative applications, can also be misused to produce\nmisleading and harmful content. This renders Synthetic Image Detection (SID)\nmethods essential for identifying AI-generated content online. State-of-the-art\nSID methods typically resize or center-crop input images due to architectural\nor computational constraints, which hampers the detection of artifacts that\nappear in high-resolution images. To address this limitation, we propose\nTextureCrop, an image pre-processing component that can be plugged in any\npre-trained SID model to improve its performance. By focusing on high-frequency\nimage parts where generative artifacts are prevalent, TextureCrop enhances SID\nperformance with manageable memory requirements. Experimental results\ndemonstrate a consistent improvement in AUC across various detectors by 6.1%\ncompared to center cropping and by 15% compared to resizing, across\nhigh-resolution images from the Forensynths, Synthbuster and TWIGMA datasets.\nCode available at https : //github.com/mever-team/texture-crop.\n","authors":["Despina Konstantinidou","Christos Koutlis","Symeon Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.15500v3.pdf","comment":"10 pages, 7 images"},{"id":"http://arxiv.org/abs/2408.07583v2","updated":"2025-01-14T10:52:15Z","published":"2024-08-14T14:28:11Z","title":"Transformers and Large Language Models for Efficient Intrusion Detection\n  Systems: A Comprehensive Survey","summary":"  With significant advancements in Transformers LLMs, NLP has extended its\nreach into many research fields due to its enhanced capabilities in text\ngeneration and user interaction. One field benefiting greatly from these\nadvancements is cybersecurity. In cybersecurity, many parameters that need to\nbe protected and exchanged between senders and receivers are in the form of\ntext and tabular data, making NLP a valuable tool in enhancing the security\nmeasures of communication protocols. This survey paper provides a comprehensive\nanalysis of the utilization of Transformers and LLMs in cyber-threat detection\nsystems. The methodology of paper selection and bibliometric analysis is\noutlined to establish a rigorous framework for evaluating existing research.\nThe fundamentals of Transformers are discussed, including background\ninformation on various cyber-attacks and datasets commonly used in this field.\nThe survey explores the application of Transformers in IDSs, focusing on\ndifferent architectures such as Attention-based models, LLMs like BERT and GPT,\nCNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.\nFurthermore, it explores the diverse environments and applications where\nTransformers and LLMs-based IDS have been implemented, including computer\nnetworks, IoT devices, critical infrastructure protection, cloud computing,\nSDN, as well as in autonomous vehicles. The paper also addresses research\nchallenges and future directions in this area, identifying key issues such as\ninterpretability, scalability, and adaptability to evolving threats, and more.\nFinally, the conclusion summarizes the findings and highlights the significance\nof Transformers and LLMs in enhancing cyber-threat detection capabilities,\nwhile also outlining potential avenues for further research and development.\n","authors":["Hamza Kheddar"],"pdf_url":"https://arxiv.org/pdf/2408.07583v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2405.04760 by other authors"},{"id":"http://arxiv.org/abs/2501.08005v1","updated":"2025-01-14T10:49:26Z","published":"2025-01-14T10:49:26Z","title":"DisCoPatch: Batch Statistics Are All You Need For OOD Detection, But\n  Only If You Can Trust Them","summary":"  Out-of-distribution (OOD) detection holds significant importance across many\napplications. While semantic and domain-shift OOD problems are well-studied,\nthis work focuses on covariate shifts - subtle variations in the data\ndistribution that can degrade machine learning performance. We hypothesize that\ndetecting these subtle shifts can improve our understanding of in-distribution\nboundaries, ultimately improving OOD detection. In adversarial discriminators\ntrained with Batch Normalization (BN), real and adversarial samples form\ndistinct domains with unique batch statistics - a property we exploit for OOD\ndetection. We introduce DisCoPatch, an unsupervised Adversarial Variational\nAutoencoder (VAE) framework that harnesses this mechanism. During inference,\nbatches consist of patches from the same image, ensuring a consistent data\ndistribution that allows the model to rely on batch statistics. DisCoPatch uses\nthe VAE's suboptimal outputs (generated and reconstructed) as negative samples\nto train the discriminator, thereby improving its ability to delineate the\nboundary between in-distribution samples and covariate shifts. By tightening\nthis boundary, DisCoPatch achieves state-of-the-art results in public OOD\ndetection benchmarks. The proposed model not only excels in detecting covariate\nshifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior\nmethods on public Near-OOD (95.0%) benchmarks. With a compact model size of\n25MB, it achieves high OOD detection performance at notably lower latency than\nexisting methods, making it an efficient and practical solution for real-world\nOOD detection applications. The code will be made publicly available\n","authors":["Francisco Caetano","Christiaan Viviers","Luis A. Zavala-Mondragón","Peter H. N. de With","Fons van der Sommen"],"pdf_url":"https://arxiv.org/pdf/2501.08005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08002v1","updated":"2025-01-14T10:46:41Z","published":"2025-01-14T10:46:41Z","title":"Maximizing Uncertainty for Federated learning via Bayesian\n  Optimisation-based Model Poisoning","summary":"  As we transition from Narrow Artificial Intelligence towards Artificial Super\nIntelligence, users are increasingly concerned about their privacy and the\ntrustworthiness of machine learning (ML) technology. A common denominator for\nthe metrics of trustworthiness is the quantification of uncertainty inherent in\nDL algorithms, and specifically in the model parameters, input data, and model\npredictions. One of the common approaches to address privacy-related issues in\nDL is to adopt distributed learning such as federated learning (FL), where\nprivate raw data is not shared among users. Despite the privacy-preserving\nmechanisms in FL, it still faces challenges in trustworthiness. Specifically,\nthe malicious users, during training, can systematically create malicious model\nparameters to compromise the models predictive and generative capabilities,\nresulting in high uncertainty about their reliability. To demonstrate malicious\nbehaviour, we propose a novel model poisoning attack method named Delphi which\naims to maximise the uncertainty of the global model output. We achieve this by\ntaking advantage of the relationship between the uncertainty and the model\nparameters of the first hidden layer of the local model. Delphi employs two\ntypes of optimisation , Bayesian Optimisation and Least Squares Trust Region,\nto search for the optimal poisoned model parameters, named as Delphi-BO and\nDelphi-LSTR. We quantify the uncertainty using the KL Divergence to minimise\nthe distance of the predictive probability distribution towards an uncertain\ndistribution of model output. Furthermore, we establish a mathematical proof\nfor the attack effectiveness demonstrated in FL. Numerical results demonstrate\nthat Delphi-BO induces a higher amount of uncertainty than Delphi-LSTR\nhighlighting vulnerability of FL systems to model poisoning attacks.\n","authors":["Marios Aristodemou","Xiaolan Liu","Yuan Wang","Konstantinos G. Kyriakopoulos","Sangarapillai Lambotharan","Qingsong Wei"],"pdf_url":"https://arxiv.org/pdf/2501.08002v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2501.07994v1","updated":"2025-01-14T10:38:18Z","published":"2025-01-14T10:38:18Z","title":"Combining imaging and shape features for prediction tasks of Alzheimer's\n  disease classification and brain age regression","summary":"  We investigate combining imaging and shape features extracted from MRI for\nthe clinically relevant tasks of brain age prediction and Alzheimer's disease\nclassification. Our proposed model fuses ResNet-extracted image embeddings with\nshape embeddings from a bespoke graph neural network. The shape embeddings are\nderived from surface meshes of 15 brain structures, capturing detailed\ngeometric information. Combined with the appearance features from T1-weighted\nimages, we observe improvements in the prediction performance on both tasks,\nwith substantial gains for classification. We evaluate the model using public\ndatasets, including CamCAN, IXI, and OASIS3, demonstrating the effectiveness of\nfusing imaging and shape features for brain analysis.\n","authors":["Nairouz Shehata","Carolina Piçarra","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2501.07994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03033v3","updated":"2025-01-14T10:34:00Z","published":"2024-11-05T12:10:02Z","title":"Rethinking Decoders for Transformer-based Semantic Segmentation: A\n  Compression Perspective","summary":"  State-of-the-art methods for Transformer-based semantic segmentation\ntypically adopt Transformer decoders that are used to extract additional\nembeddings from image embeddings via cross-attention, refine either or both\ntypes of embeddings via self-attention, and project image embeddings onto the\nadditional embeddings via dot-product. Despite their remarkable success, these\nempirical designs still lack theoretical justifications or interpretations,\nthus hindering potentially principled improvements. In this paper, we argue\nthat there are fundamental connections between semantic segmentation and\ncompression, especially between the Transformer decoders and Principal\nComponent Analysis (PCA). From such a perspective, we derive a white-box, fully\nattentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the\ninterpretations as follows: 1) the self-attention operator refines image\nembeddings to construct an ideal principal subspace that aligns with the\nsupervision and retains most information; 2) the cross-attention operator seeks\nto find a low-rank approximation of the refined image embeddings, which is\nexpected to be a set of orthonormal bases of the principal subspace and\ncorresponds to the predefined classes; 3) the dot-product operation yields\ncompact representation for image embeddings as segmentation masks. Experiments\nconducted on dataset ADE20K find that DEPICT consistently outperforms its\nblack-box counterpart, Segmenter, and it is light weight and more robust.\n","authors":["Qishuai Wen","Chun-Guang Li"],"pdf_url":"https://arxiv.org/pdf/2411.03033v3.pdf","comment":"NeurIPS2024. Code:https://github.com/QishuaiWen/DEPICT/"},{"id":"http://arxiv.org/abs/2407.10377v3","updated":"2025-01-14T10:30:19Z","published":"2024-07-15T01:11:30Z","title":"Enhanced Masked Image Modeling to Avoid Model Collapse on Multi-modal\n  MRI Datasets","summary":"  Multi-modal magnetic resonance imaging (MRI) provides information of lesions\nfor computer-aided diagnosis from different views. Deep learning algorithms are\nsuitable for identifying specific anatomical structures, segmenting lesions,\nand classifying diseases. Manual labels are limited due to the high expense,\nwhich hinders further improvement of accuracy. Self-supervised learning,\nparticularly masked image modeling (MIM), has shown promise in utilizing\nunlabeled data. However, we spot model collapse when applying MIM to\nmulti-modal MRI datasets. The performance of downstream tasks does not see any\nimprovement following the collapsed model. To solve model collapse, we analyze\nand address it in two types: complete collapse and dimensional collapse. We\nfind complete collapse occurs because the collapsed loss value in multi-modal\nMRI datasets falls below the normally converged loss value. Based on this, the\nhybrid mask pattern (HMP) masking strategy is introduced to elevate the\ncollapsed loss above the normally converged loss value and avoid complete\ncollapse. Additionally, we reveal that dimensional collapse stems from\ninsufficient feature uniformity in MIM. We mitigate dimensional collapse by\nintroducing the pyramid barlow twins (PBT) module as an explicit regularization\nmethod. Overall, we construct the enhanced MIM (E-MIM) with HMP and PBT module\nto avoid model collapse multi-modal MRI. Experiments are conducted on three\nmulti-modal MRI datasets to validate the effectiveness of our approach in\npreventing both types of model collapse. By preventing model collapse, the\ntraining of the model becomes more stable, resulting in a decent improvement in\nperformance for segmentation and classification tasks. The code is available at\nhttps://github.com/LinxuanHan/E-MIM.\n","authors":["Linxuan Han","Sa Xiao","Zimeng Li","Haidong Li","Xiuchao Zhao","Yeqing Han","Fumin Guo","Xin Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.10377v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02297v2","updated":"2025-01-14T10:27:40Z","published":"2024-08-05T08:14:28Z","title":"Perception Matters: Enhancing Embodied AI with Uncertainty-Aware\n  Semantic Segmentation","summary":"  Embodied AI has made significant progress acting in unexplored environments.\nHowever, tasks such as object search have largely focused on efficient policy\nlearning. In this work, we identify several gaps in current search methods:\nThey largely focus on dated perception models, neglect temporal aggregation,\nand transfer from ground truth directly to noisy perception at test time,\nwithout accounting for the resulting overconfidence in the perceived state. We\naddress the identified problems through calibrated perception probabilities and\nuncertainty across aggregation and found decisions, thereby adapting the models\nfor sequential tasks. The resulting methods can be directly integrated with\npretrained models across a wide family of existing search approaches at no\nadditional training cost. We perform extensive evaluations of aggregation\nmethods across both different semantic perception models and policies,\nconfirming the importance of calibrated uncertainties in both the aggregation\nand found decisions. We make the code and trained models available at\nhttps://semantic-search.cs.uni-freiburg.de.\n","authors":["Sai Prasanna","Daniel Honerkamp","Kshitij Sirohi","Tim Welschehold","Wolfram Burgard","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2408.02297v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07988v1","updated":"2025-01-14T10:24:20Z","published":"2025-01-14T10:24:20Z","title":"GAC-Net_Geometric and attention-based Network for Depth Completion","summary":"  Depth completion is a key task in autonomous driving, aiming to complete\nsparse LiDAR depth measurements into high-quality dense depth maps through\nimage guidance. However, existing methods usually treat depth maps as an\nadditional channel of color images, or directly perform convolution on sparse\ndata, failing to fully exploit the 3D geometric information in depth maps,\nespecially with limited performance in complex boundaries and sparse areas. To\naddress these issues, this paper proposes a depth completion network combining\nchannel attention mechanism and 3D global feature perception (CGA-Net). The\nmain innovations include: 1) Utilizing PointNet++ to extract global 3D\ngeometric features from sparse depth maps, enhancing the scene perception\nability of low-line LiDAR data; 2) Designing a channel-attention-based\nmultimodal feature fusion module to efficiently integrate sparse depth, RGB\nimages, and 3D geometric features; 3) Combining residual learning with CSPN++\nto optimize the depth refinement stage, further improving the completion\nquality in edge areas and complex scenes. Experiments on the KITTI depth\ncompletion dataset show that CGA-Net can significantly improve the prediction\naccuracy of dense depth maps, achieving a new state-of-the-art (SOTA), and\ndemonstrating strong robustness to sparse and complex scenes.\n","authors":["Kuang Zhu","Xingli Gan","Min Sun"],"pdf_url":"https://arxiv.org/pdf/2501.07988v1.pdf","comment":"13pages,4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2501.07984v1","updated":"2025-01-14T10:09:55Z","published":"2025-01-14T10:09:55Z","title":"Threshold Attention Network for Semantic Segmentation of Remote Sensing\n  Images","summary":"  Semantic segmentation of remote sensing images is essential for various\napplications, including vegetation monitoring, disaster management, and urban\nplanning. Previous studies have demonstrated that the self-attention mechanism\n(SA) is an effective approach for designing segmentation networks that can\ncapture long-range pixel dependencies. SA enables the network to model the\nglobal dependencies between the input features, resulting in improved\nsegmentation outcomes. However, the high density of attentional feature maps\nused in this mechanism causes exponential increases in computational\ncomplexity. Additionally, it introduces redundant information that negatively\nimpacts the feature representation. Inspired by traditional threshold\nsegmentation algorithms, we propose a novel threshold attention mechanism\n(TAM). This mechanism significantly reduces computational effort while also\nbetter modeling the correlation between different regions of the feature map.\nBased on TAM, we present a threshold attention network (TANet) for semantic\nsegmentation. TANet consists of an attentional feature enhancement module\n(AFEM) for global feature enhancement of shallow features and a threshold\nattention pyramid pooling module (TAPP) for acquiring feature information at\ndifferent scales for deep features. We have conducted extensive experiments on\nthe ISPRS Vaihingen and Potsdam datasets. The results demonstrate the validity\nand superiority of our proposed TANet compared to the most state-of-the-art\nmodels.\n","authors":["Wei Long","Yongjun Zhang","Zhongwei Cui","Yujie Xu","Xuexue Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.07984v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07983v1","updated":"2025-01-14T10:06:02Z","published":"2025-01-14T10:06:02Z","title":"V-Trans4Style: Visual Transition Recommendation for Video Production\n  Style Adaptation","summary":"  We introduce V-Trans4Style, an innovative algorithm tailored for dynamic\nvideo content editing needs. It is designed to adapt videos to different\nproduction styles like documentaries, dramas, feature films, or a specific\nYouTube channel's video-making technique. Our algorithm recommends optimal\nvisual transitions to help achieve this flexibility using a more bottom-up\napproach. We first employ a transformer-based encoder-decoder network to learn\nrecommending temporally consistent and visually seamless sequences of visual\ntransitions using only the input videos. We then introduce a style conditioning\nmodule that leverages this model to iteratively adjust the visual transitions\nobtained from the decoder through activation maximization. We demonstrate the\nefficacy of our method through experiments conducted on our newly introduced\nAutoTransition++ dataset. It is a 6k video version of AutoTransition Dataset\nthat additionally categorizes its videos into different production style\ncategories. Our encoder-decoder model outperforms the state-of-the-art\ntransition recommendation method, achieving improvements of 10% to 80% in\nRecall@K and mean rank values over baseline. Our style conditioning module\nresults in visual transitions that improve the capture of the desired video\nproduction style characteristics by an average of around 12% in comparison to\nother methods when measured with similarity metrics. We hope that our work\nserves as a foundation for exploring and understanding video production styles\nfurther.\n","authors":["Pooja Guhan","Tsung-Wei Huang","Guan-Ming Su","Subhadra Gopalakrishnan","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2501.07983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07978v1","updated":"2025-01-14T09:52:56Z","published":"2025-01-14T09:52:56Z","title":"Facial Dynamics in Video: Instruction Tuning for Improved Facial\n  Expression Perception and Contextual Awareness","summary":"  Facial expression captioning has found widespread application across various\ndomains. Recently, the emergence of video Multimodal Large Language Models\n(MLLMs) has shown promise in general video understanding tasks. However,\ndescribing facial expressions within videos poses two major challenges for\nthese models: (1) the lack of adequate datasets and benchmarks, and (2) the\nlimited visual token capacity of video MLLMs. To address these issues, this\npaper introduces a new instruction-following dataset tailored for dynamic\nfacial expression caption. The dataset comprises 5,033 high-quality video clips\nannotated manually, containing over 700,000 tokens. Its purpose is to improve\nthe capability of video MLLMs to discern subtle facial nuances. Furthermore, we\npropose FaceTrack-MM, which leverages a limited number of tokens to encode the\nmain character's face. This model demonstrates superior performance in tracking\nfaces and focusing on the facial expressions of the main characters, even in\nintricate multi-person scenarios. Additionally, we introduce a novel evaluation\nmetric combining event extraction, relation classification, and the longest\ncommon subsequence (LCS) algorithm to assess the content consistency and\ntemporal sequence consistency of generated text. Moreover, we present\nFEC-Bench, a benchmark designed to assess the performance of existing video\nMLLMs in this specific task. All data and source code will be made publicly\navailable.\n","authors":["Jiaxing Zhao","Boyuan Sun","Xiang Chen","Xihan Wei"],"pdf_url":"https://arxiv.org/pdf/2501.07978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07972v1","updated":"2025-01-14T09:45:10Z","published":"2025-01-14T09:45:10Z","title":"Zero-shot Video Moment Retrieval via Off-the-shelf Multimodal Large\n  Language Models","summary":"  The target of video moment retrieval (VMR) is predicting temporal spans\nwithin a video that semantically match a given linguistic query. Existing VMR\nmethods based on multimodal large language models (MLLMs) overly rely on\nexpensive high-quality datasets and time-consuming fine-tuning. Although some\nrecent studies introduce a zero-shot setting to avoid fine-tuning, they\noverlook inherent language bias in the query, leading to erroneous\nlocalization. To tackle the aforementioned challenges, this paper proposes\nMoment-GPT, a tuning-free pipeline for zero-shot VMR utilizing frozen MLLMs.\nSpecifically, we first employ LLaMA-3 to correct and rephrase the query to\nmitigate language bias. Subsequently, we design a span generator combined with\nMiniGPT-v2 to produce candidate spans adaptively. Finally, to leverage the\nvideo comprehension capabilities of MLLMs, we apply VideoChatGPT and span\nscorer to select the most appropriate spans. Our proposed method substantially\noutperforms the state-ofthe-art MLLM-based and zero-shot models on several\npublic datasets, including QVHighlights, ActivityNet-Captions, and\nCharades-STA.\n","authors":["Yifang Xu","Yunzhuo Sun","Benxiang Zhai","Ming Li","Wenxin Liang","Yang Li","Sidan Du"],"pdf_url":"https://arxiv.org/pdf/2501.07972v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2409.09610v2","updated":"2025-01-14T09:44:01Z","published":"2024-09-15T04:34:38Z","title":"TextureDiffusion: Target Prompt Disentangled Editing for Various Texture\n  Transfer","summary":"  Recently, text-guided image editing has achieved significant success.\nHowever, existing methods can only apply simple textures like wood or gold when\nchanging the texture of an object. Complex textures such as cloud or fire pose\na challenge. This limitation stems from that the target prompt needs to contain\nboth the input image content and <texture>, restricting the texture\nrepresentation. In this paper, we propose TextureDiffusion, a tuning-free image\nediting method applied to various texture transfer. Initially, the target\nprompt is directly set to \"<texture>\", making the texture disentangled from the\ninput image content to enhance texture representation. Subsequently, query\nfeatures in self-attention and features in residual blocks are utilized to\npreserve the structure of the input image. Finally, to maintain the background,\nwe introduce an edit localization technique which blends the self-attention\nresults and the intermediate latents. Comprehensive experiments demonstrate\nthat TextureDiffusion can harmoniously transfer various textures with excellent\nstructure and background preservation. Code is publicly available at\nhttps://github.com/THU-CVML/TextureDiffusion\n","authors":["Zihan Su","Junhao Zhuang","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2409.09610v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03907v2","updated":"2025-01-14T09:40:53Z","published":"2024-12-05T06:26:32Z","title":"ONER: Online Experience Replay for Incremental Anomaly Detection","summary":"  Incremental anomaly detection sequentially recognizes abnormal regions in\nnovel categories for dynamic industrial scenarios. This remains highly\nchallenging due to knowledge overwriting and feature conflicts, leading to\ncatastrophic forgetting. In this work, we propose ONER, an end-to-end ONline\nExperience Replay method, which efficiently mitigates catastrophic forgetting\nwhile adapting to new tasks with minimal cost. Specifically, our framework\nutilizes two types of experiences from past tasks: decomposed prompts and\nsemantic prototypes, addressing both model parameter updates and feature\noptimization. The decomposed prompts consist of learnable components that\nassemble to produce attention-conditioned prompts. These prompts reuse\npreviously learned knowledge, enabling model to learn novel tasks effectively.\nThe semantic prototypes operate at both pixel and image levels, performing\nregularization in the latent feature space to prevent forgetting across various\ntasks. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance in incremental anomaly detection with\nsignificantly reduced forgetting, as well as efficiently adapting to new\ncategories with minimal costs. These results confirm the efficiency and\nstability of ONER, making it a powerful solution for real-world applications.\n","authors":["Yizhou Jin","Jiahui Zhu","Guodong Wang","Shiwei Li","Jinjin Zhang","Qingjie Liu","Xinyue Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2412.03907v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07960v1","updated":"2025-01-14T09:24:27Z","published":"2025-01-14T09:24:27Z","title":"SkipClick: Combining Quick Responses and Low-Level Features for\n  Interactive Segmentation in Winter Sports Contexts","summary":"  In this paper, we present a novel architecture for interactive segmentation\nin winter sports contexts. The field of interactive segmentation deals with the\nprediction of high-quality segmentation masks by informing the network about\nthe objects position with the help of user guidance. In our case the guidance\nconsists of click prompts. For this task, we first present a baseline\narchitecture which is specifically geared towards quickly responding after each\nclick. Afterwards, we motivate and describe a number of architectural\nmodifications which improve the performance when tasked with segmenting winter\nsports equipment on the WSESeg dataset. With regards to the average NoC@85\nmetric on the WSESeg classes, we outperform SAM and HQ-SAM by 2.336 and 7.946\nclicks, respectively. When applied to the HQSeg-44k dataset, our system\ndelivers state-of-the-art results with a NoC@90 of 6.00 and NoC@95 of 9.89. In\naddition to that, we test our model on a novel dataset containing masks for\nhumans during skiing.\n","authors":["Robin Schön","Julian Lorenz","Daniel Kienzle","Rainer Lienhart"],"pdf_url":"https://arxiv.org/pdf/2501.07960v1.pdf","comment":"4 figures, 6 tables, 12 pages"},{"id":"http://arxiv.org/abs/2501.07957v1","updated":"2025-01-14T09:21:17Z","published":"2025-01-14T09:21:17Z","title":"AI Guide Dog: Egocentric Path Prediction on Smartphone","summary":"  This paper introduces AI Guide Dog (AIGD), a lightweight egocentric\nnavigation assistance system for visually impaired individuals, designed for\nreal-time deployment on smartphones. AIGD addresses key challenges in blind\nnavigation by employing a vision-only, multi-label classification approach to\npredict directional commands, ensuring safe traversal across diverse\nenvironments. We propose a novel technique to enable goal-based outdoor\nnavigation by integrating GPS signals and high-level directions, while also\naddressing uncertain multi-path predictions for destination-free indoor\nnavigation. Our generalized model is the first navigation assistance system to\nhandle both goal-oriented and exploratory navigation scenarios across indoor\nand outdoor settings, establishing a new state-of-the-art in blind navigation.\nWe present methods, datasets, evaluations, and deployment insights to encourage\nfurther innovations in assistive navigation systems.\n","authors":["Aishwarya Jadhav","Jeffery Cao","Abhishree Shetty","Urvashi Priyam Kumar","Aditi Sharma","Ben Sukboontip","Jayant Sravan Tamarapalli","Jingyi Zhang","Anirudh Koul"],"pdf_url":"https://arxiv.org/pdf/2501.07957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04665v3","updated":"2025-01-14T09:11:42Z","published":"2025-01-08T18:22:44Z","title":"HyFusion: Enhanced Reception Field Transformer for Hyperspectral Image\n  Fusion","summary":"  Hyperspectral image (HSI) fusion addresses the challenge of reconstructing\nHigh-Resolution HSIs (HR-HSIs) from High-Resolution Multispectral images\n(HR-MSIs) and Low-Resolution HSIs (LR-HSIs), a critical task given the high\ncosts and hardware limitations associated with acquiring high-quality HSIs.\nWhile existing methods leverage spatial and spectral relationships, they often\nsuffer from limited receptive fields and insufficient feature utilization,\nleading to suboptimal performance. Furthermore, the scarcity of high-quality\nHSI data highlights the importance of efficient data utilization to maximize\nreconstruction quality. To address these issues, we propose HyFusion, a novel\nDual-Coupled Network (DCN) framework designed to enhance cross-domain feature\nextraction and enable effective feature map reusing. The framework first\nprocesses HR-MSI and LR-HSI inputs through specialized subnetworks that\nmutually enhance each other during feature extraction, preserving complementary\nspatial and spectral details. At its core, HyFusion utilizes an Enhanced\nReception Field Block (ERFB), which combines shifting-window attention and\ndense connections to expand the receptive field, effectively capturing\nlong-range dependencies while minimizing information loss. Extensive\nexperiments demonstrate that HyFusion achieves state-of-the-art performance in\nHR-MSI/LR-HSI fusion, significantly improving reconstruction quality while\nmaintaining a compact model size and computational efficiency. By integrating\nenhanced receptive fields and feature map reusing into a coupled network\narchitecture, HyFusion provides a practical and effective solution for HSI\nfusion in resource-constrained scenarios, setting a new benchmark in\nhyperspectral imaging. Our code will be publicly available.\n","authors":["Chia-Ming Lee","Yu-Fan Lin","Yu-Hao Ho","Li-Wei Kang","Chih-Chung Hsu"],"pdf_url":"https://arxiv.org/pdf/2501.04665v3.pdf","comment":"Submitted to IGARSS 2025"},{"id":"http://arxiv.org/abs/2501.07953v1","updated":"2025-01-14T09:09:14Z","published":"2025-01-14T09:09:14Z","title":"Robust Hyperspectral Image Panshapring via Sparse Spatial-Spectral\n  Representation","summary":"  High-resolution hyperspectral imaging plays a crucial role in various remote\nsensing applications, yet its acquisition often faces fundamental limitations\ndue to hardware constraints. This paper introduces S$^{3}$RNet, a novel\nframework for hyperspectral image pansharpening that effectively combines\nlow-resolution hyperspectral images (LRHSI) with high-resolution multispectral\nimages (HRMSI) through sparse spatial-spectral representation. The core of\nS$^{3}$RNet is the Multi-Branch Fusion Network (MBFN), which employs parallel\nbranches to capture complementary features at different spatial and spectral\nscales. Unlike traditional approaches that treat all features equally, our\nSpatial-Spectral Attention Weight Block (SSAWB) dynamically adjusts feature\nweights to maintain sparse representation while suppressing noise and\nredundancy. To enhance feature propagation, we incorporate the Dense Feature\nAggregation Block (DFAB), which efficiently aggregates inputted features\nthrough dense connectivity patterns. This integrated design enables S$^{3}$RNet\nto selectively emphasize the most informative features from differnt scale\nwhile maintaining computational efficiency. Comprehensive experiments\ndemonstrate that S$^{3}$RNet achieves state-of-the-art performance across\nmultiple evaluation metrics, showing particular strength in maintaining high\nreconstruction quality even under challenging noise conditions. The code will\nbe made publicly available.\n","authors":["Chia-Ming Lee","Yu-Fan Lin","Li-Wei Kang","Chih-Chung Hsu"],"pdf_url":"https://arxiv.org/pdf/2501.07953v1.pdf","comment":"Submitted to IGARSS 2025"},{"id":"http://arxiv.org/abs/2501.00700v2","updated":"2025-01-14T09:04:35Z","published":"2025-01-01T02:18:18Z","title":"Knowledge-Guided Prompt Learning for Deepfake Facial Image Detection","summary":"  Recent generative models demonstrate impressive performance on synthesizing\nphotographic images, which makes humans hardly to distinguish them from\npristine ones, especially on realistic-looking synthetic facial images.\nPrevious works mostly focus on mining discriminative artifacts from vast amount\nof visual data. However, they usually lack the exploration of prior knowledge\nand rarely pay attention to the domain shift between training categories (e.g.,\nnatural and indoor objects) and testing ones (e.g., fine-grained human facial\nimages), resulting in unsatisfactory detection performance. To address these\nissues, we propose a novel knowledge-guided prompt learning method for deepfake\nfacial image detection. Specifically, we retrieve forgery-related prompts from\nlarge language models as expert knowledge to guide the optimization of\nlearnable prompts. Besides, we elaborate test-time prompt tuning to alleviate\nthe domain shift, achieving significant performance improvement and boosting\nthe application in real-world scenarios. Extensive experiments on\nDeepFakeFaceForensics dataset show that our proposed approach notably\noutperforms state-of-the-art methods.\n","authors":["Hao Wang","Cheng Deng","Zhidong Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.00700v2.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2305.11421v3","updated":"2025-01-14T08:59:17Z","published":"2023-05-19T04:16:50Z","title":"PastNet: Introducing Physical Inductive Biases for Spatio-temporal Video\n  Prediction","summary":"  In this paper, we investigate the challenge of spatio-temporal video\nprediction task, which involves generating future video frames based on\nhistorical spatio-temporal observation streams. Existing approaches typically\nutilize external information such as semantic maps to improve video prediction\naccuracy, which often neglect the inherent physical knowledge embedded within\nvideos. Worse still, their high computational costs could impede their\napplications for high-resolution videos. To address these constraints, we\nintroduce a novel framework called \\underline{P}hysics-\\underline{a}ssisted\n\\underline{S}patio-\\underline{t}emporal \\underline{Net}work (PastNet) for\nhigh-quality video prediction. The core of PastNet lies in incorporating a\nspectral convolution operator in the Fourier domain, which efficiently\nintroduces inductive biases from the underlying physical laws. Additionally, we\nemploy a memory bank with the estimated intrinsic dimensionality to discretize\nlocal features during the processing of complex spatio-temporal signals,\nthereby reducing computational costs and facilitating efficient high-resolution\nvideo prediction. Extensive experiments on various widely-used spatio-temporal\nvideo benchmarks demonstrate the effectiveness and efficiency of the proposed\nPastNet compared with a range of state-of-the-art methods, particularly in\nhigh-resolution scenarios.\n","authors":["Hao Wu","Fan Xu","Chong Chen","Xian-Sheng Hua","Xiao Luo","Haixin Wang"],"pdf_url":"https://arxiv.org/pdf/2305.11421v3.pdf","comment":"11"},{"id":"http://arxiv.org/abs/2501.07945v1","updated":"2025-01-14T08:56:59Z","published":"2025-01-14T08:56:59Z","title":"Early prediction of the transferability of bovine embryos from\n  videomicroscopy","summary":"  Videomicroscopy is a promising tool combined with machine learning for\nstudying the early development of in vitro fertilized bovine embryos and\nassessing its transferability as soon as possible. We aim to predict the embryo\ntransferability within four days at most, taking 2D time-lapse microscopy\nvideos as input. We formulate this problem as a supervised binary\nclassification problem for the classes transferable and not transferable. The\nchallenges are three-fold: 1) poorly discriminating appearance and motion, 2)\nclass ambiguity, 3) small amount of annotated data. We propose a 3D\nconvolutional neural network involving three pathways, which makes it\nmulti-scale in time and able to handle appearance and motion in different ways.\nFor training, we retain the focal loss. Our model, named SFR, compares\nfavorably to other methods. Experiments demonstrate its effectiveness and\naccuracy for our challenging biological task.\n","authors":["Yasmine Hachani","Patrick Bouthemy","Elisa Fromont","Sylvie Ruffini","Ludivine Laffont","Alline de Paula Reis"],"pdf_url":"https://arxiv.org/pdf/2501.07945v1.pdf","comment":"Accepted at the 2024 IEEE International Conference on Image\n  Processing"},{"id":"http://arxiv.org/abs/2501.03659v2","updated":"2025-01-14T08:52:51Z","published":"2025-01-07T09:47:46Z","title":"DehazeGS: Seeing Through Fog with 3D Gaussian Splatting","summary":"  Current novel view synthesis tasks primarily rely on high-quality and clear\nimages. However, in foggy scenes, scattering and attenuation can significantly\ndegrade the reconstruction and rendering quality. Although NeRF-based dehazing\nreconstruction algorithms have been developed, their use of deep fully\nconnected neural networks and per-ray sampling strategies leads to high\ncomputational costs. Moreover, NeRF's implicit representation struggles to\nrecover fine details from hazy scenes. In contrast, recent advancements in 3D\nGaussian Splatting achieve high-quality 3D scene reconstruction by explicitly\nmodeling point clouds into 3D Gaussians. In this paper, we propose leveraging\nthe explicit Gaussian representation to explain the foggy image formation\nprocess through a physically accurate forward rendering process. We introduce\nDehazeGS, a method capable of decomposing and rendering a fog-free background\nfrom participating media using only muti-view foggy images as input. We model\nthe transmission within each Gaussian distribution to simulate the formation of\nfog. During this process, we jointly learn the atmospheric light and scattering\ncoefficient while optimizing the Gaussian representation of the hazy scene. In\nthe inference stage, we eliminate the effects of scattering and attenuation on\nthe Gaussians and directly project them onto a 2D plane to obtain a clear view.\nExperiments on both synthetic and real-world foggy datasets demonstrate that\nDehazeGS achieves state-of-the-art performance in terms of both rendering\nquality and computational efficiency.\n","authors":["Jinze Yu","Yiqun Wang","Zhengda Lu","Jianwei Guo","Yong Li","Hongxing Qin","Xiaopeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.03659v2.pdf","comment":"9 pages,4 figures"},{"id":"http://arxiv.org/abs/2409.16793v2","updated":"2025-01-14T08:47:17Z","published":"2024-09-25T10:14:01Z","title":"Spacewalker: Traversing Representation Spaces for Fast Interactive\n  Exploration and Annotation of Unstructured Data","summary":"  In industries such as healthcare, finance, and manufacturing, analysis of\nunstructured textual data presents significant challenges for analysis and\ndecision making. Uncovering patterns within large-scale corpora and\nunderstanding their semantic impact is critical, but depends on domain experts\nor resource-intensive manual reviews. In response, we introduce Spacewalker in\nthis system demonstration paper, an interactive tool designed to analyze,\nexplore, and annotate data across multiple modalities. It allows users to\nextract data representations, visualize them in low-dimensional spaces and\ntraverse large datasets either exploratory or by querying regions of interest.\nWe evaluated Spacewalker through extensive experiments and annotation studies,\nassessing its efficacy in improving data integrity verification and annotation.\nWe show that Spacewalker reduces time and effort compared to traditional\nmethods. The code of this work is open-source and can be found at:\nhttps://github.com/code-lukas/Spacewalker\n","authors":["Lukas Heine","Fabian Hörst","Jana Fragemann","Gijs Luijten","Jan Egger","Fin Bahnsen","M. Saquib Sarfraz","Jens Kleesiek","Constantin Seibold"],"pdf_url":"https://arxiv.org/pdf/2409.16793v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06664v3","updated":"2025-01-14T08:33:08Z","published":"2024-12-09T17:01:42Z","title":"Knowledge Transfer and Domain Adaptation for Fine-Grained Remote Sensing\n  Image Segmentation","summary":"  Fine-grained remote sensing image segmentation is essential for accurately\nidentifying detailed objects in remote sensing images. Recently, vision\ntransformer models (VTMs) pre-trained on large-scale datasets have demonstrated\nstrong zero-shot generalization. However, directly applying them to specific\ntasks may lead to domain shift. We introduce a novel end-to-end learning\nparadigm combining knowledge guidance with domain refinement to enhance\nperformance. We present two key components: the Feature Alignment Module (FAM)\nand the Feature Modulation Module (FMM). FAM aligns features from a CNN-based\nbackbone with those from the pretrained VTM's encoder using channel\ntransformation and spatial interpolation, and transfers knowledge via KL\ndivergence and L2 normalization constraint. FMM further adapts the knowledge to\nthe specific domain to address domain shift. We also introduce a fine-grained\ngrass segmentation dataset and demonstrate, through experiments on two\ndatasets, that our method achieves a significant improvement of 2.57 mIoU on\nthe grass dataset and 3.73 mIoU on the cloud dataset. The results highlight the\npotential of combining knowledge transfer and domain adaptation to overcome\ndomain-related challenges and data limitations. The project page is available\nat https://xavierjiezou.github.io/KTDA/.\n","authors":["Shun Zhang","Xuechao Zou","Kai Li","Congyan Lang","Shiying Wang","Pin Tao","Tengfei Cao"],"pdf_url":"https://arxiv.org/pdf/2412.06664v3.pdf","comment":"6 pages, 3 figures, 6 tables"},{"id":"http://arxiv.org/abs/2412.21079v3","updated":"2025-01-14T08:23:30Z","published":"2024-12-30T16:56:44Z","title":"Edicho: Consistent Image Editing in the Wild","summary":"  As a verified need, consistent editing across in-the-wild images remains a\ntechnical challenge arising from various unmanageable factors, like object\nposes, lighting conditions, and photography environments. Edicho steps in with\na training-free solution based on diffusion models, featuring a fundamental\ndesign principle of using explicit image correspondence to direct editing.\nSpecifically, the key components include an attention manipulation module and a\ncarefully refined classifier-free guidance (CFG) denoising strategy, both of\nwhich take into account the pre-estimated correspondence. Such an\ninference-time algorithm enjoys a plug-and-play nature and is compatible to\nmost diffusion-based editing methods, such as ControlNet and BrushNet.\nExtensive results demonstrate the efficacy of Edicho in consistent cross-image\nediting under diverse settings. We will release the code to facilitate future\nstudies.\n","authors":["Qingyan Bai","Hao Ouyang","Yinghao Xu","Qiuyu Wang","Ceyuan Yang","Ka Leong Cheng","Yujun Shen","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2412.21079v3.pdf","comment":"Project page: https://ant-research.github.io/edicho/"},{"id":"http://arxiv.org/abs/2501.07922v1","updated":"2025-01-14T08:12:20Z","published":"2025-01-14T08:12:20Z","title":"VENOM: Text-driven Unrestricted Adversarial Example Generation with\n  Diffusion Models","summary":"  Adversarial attacks have proven effective in deceiving machine learning\nmodels by subtly altering input images, motivating extensive research in recent\nyears. Traditional methods constrain perturbations within $l_p$-norm bounds,\nbut advancements in Unrestricted Adversarial Examples (UAEs) allow for more\ncomplex, generative-model-based manipulations. Diffusion models now lead UAE\ngeneration due to superior stability and image quality over GANs. However,\nexisting diffusion-based UAE methods are limited to using reference images and\nface challenges in generating Natural Adversarial Examples (NAEs) directly from\nrandom noise, often producing uncontrolled or distorted outputs. In this work,\nwe introduce VENOM, the first text-driven framework for high-quality\nunrestricted adversarial examples generation through diffusion models. VENOM\nunifies image content generation and adversarial synthesis into a single\nreverse diffusion process, enabling high-fidelity adversarial examples without\nsacrificing attack success rate (ASR). To stabilize this process, we\nincorporate an adaptive adversarial guidance strategy with momentum, ensuring\nthat the generated adversarial examples $x^*$ align with the distribution\n$p(x)$ of natural images. Extensive experiments demonstrate that VENOM achieves\nsuperior ASR and image quality compared to prior methods, marking a significant\nadvancement in adversarial example generation and providing insights into model\nvulnerabilities for improved defense development.\n","authors":["Hui Kuurila-Zhang","Haoyu Chen","Guoying Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.07922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10568v3","updated":"2025-01-14T08:01:17Z","published":"2024-03-14T17:47:10Z","title":"MoPE: Mixture of Prompt Experts for Parameter-Efficient and Scalable\n  Multimodal Fusion","summary":"  Despite the demonstrated parameter efficiency of prompt-based multimodal\nfusion methods, their limited adaptivity and expressiveness often result in\nsuboptimal performance compared to other tuning approaches. In this paper, we\nintroduce the Mixture of Prompt Experts (MoPE), the first technique designed to\novercome these limitations by decomposing standard prompts to capture\ninstance-level features adaptively. Building on this decomposition, MoPE\nenhances prompt fusion's expressiveness by leveraging multimodal pairing priors\nto route the most effective prompt for each instance dynamically. Compared to\nvanilla prompting, our MoPE-based fusion method exhibits greater\nexpressiveness, scaling more effectively with the training data and the overall\nnumber of trainable parameters. We also investigate regularization terms for\nexpert routing, which lead to emergent expert specialization with enhanced\nadaptiveness and interpretablity. Extensive experiments across six multimodal\ndatasets spanning four modalities demonstrate state-of-the-art performance for\nprompt fusion, matching or even surpassing the performance of fine-tuning while\nrequiring only 0.8% of the trainable parameters. Project homepage:\nhttps://github.com/songrise/MoPE\n","authors":["Ruixiang Jiang","Lingbo Liu","Changwen Chen"],"pdf_url":"https://arxiv.org/pdf/2403.10568v3.pdf","comment":"Under Review, Extended version of arxiv:2312.03734"},{"id":"http://arxiv.org/abs/2501.07901v1","updated":"2025-01-14T07:35:14Z","published":"2025-01-14T07:35:14Z","title":"Cloud Removal With PolSAR-Optical Data Fusion Using A Two-Flow Residual\n  Network","summary":"  Optical remote sensing images play a crucial role in the observation of the\nEarth's surface. However, obtaining complete optical remote sensing images is\nchallenging due to cloud cover. Reconstructing cloud-free optical images has\nbecome a major task in recent years. This paper presents a two-flow\nPolarimetric Synthetic Aperture Radar (PolSAR)-Optical data fusion cloud\nremoval algorithm (PODF-CR), which achieves the reconstruction of missing\noptical images. PODF-CR consists of an encoding module and a decoding module.\nThe encoding module includes two parallel branches that extract PolSAR image\nfeatures and optical image features. To address speckle noise in PolSAR images,\nwe introduce dynamic filters in the PolSAR branch for image denoising. To\nbetter facilitate the fusion between multimodal optical images and PolSAR\nimages, we propose fusion blocks based on cross-skip connections to enable\ninteraction of multimodal data information. The obtained fusion features are\nrefined through an attention mechanism to provide better conditions for the\nsubsequent decoding of the fused images. In the decoding module, multi-scale\nconvolution is introduced to obtain multi-scale information. Additionally, to\nbetter utilize comprehensive scattering information and polarization\ncharacteristics to assist in the restoration of optical images, we use a\ndataset for cloud restoration called OPT-BCFSAR-PFSAR, which includes\nbackscatter coefficient feature images and polarization feature images obtained\nfrom PoLSAR data and optical images. Experimental results demonstrate that this\nmethod outperforms existing methods in both qualitative and quantitative\nevaluations.\n","authors":["Yuxi Wang","Wenjuan Zhang","Bing Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.07901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07898v1","updated":"2025-01-14T07:26:55Z","published":"2025-01-14T07:26:55Z","title":"Demographic Variability in Face Image Quality Measures","summary":"  Face image quality assessment (FIQA) algorithms are being integrated into\nonline identity management applications. These applications allow users to\nupload a face image as part of their document issuance process, where the image\nis then run through a quality assessment process to make sure it meets the\nquality and compliance requirements. Concerns about demographic bias have been\nraised about biometric systems, given the societal implications this may cause.\nIt is therefore important that demographic variability in FIQA algorithms is\nassessed such that mitigation measures can be created. In this work, we study\nthe demographic variability of all face image quality measures included in the\nISO/IEC 29794-5 international standard across three demographic variables: age,\ngender, and skin tone. The results are rather promising and show no clear bias\ntoward any specific demographic group for most measures. Only two quality\nmeasures are found to have considerable variations in their outcomes for\ndifferent groups on the skin tone variable.\n","authors":["Wassim Kabbani","Kiran Raja","Raghavendra Ramachandra","Christoph Busch"],"pdf_url":"https://arxiv.org/pdf/2501.07898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07888v1","updated":"2025-01-14T06:54:39Z","published":"2025-01-14T06:54:39Z","title":"Tarsier2: Advancing Large Vision-Language Models from Detailed Video\n  Description to Comprehensive Video Understanding","summary":"  We introduce Tarsier2, a state-of-the-art large vision-language model (LVLM)\ndesigned for generating detailed and accurate video descriptions, while also\nexhibiting superior general video understanding capabilities. Tarsier2 achieves\nsignificant advancements through three key upgrades: (1) Scaling pre-training\ndata from 11M to 40M video-text pairs, enriching both volume and diversity; (2)\nPerforming fine-grained temporal alignment during supervised fine-tuning; (3)\nUsing model-based sampling to automatically construct preference data and\napplying DPO training for optimization. Extensive experiments show that\nTarsier2-7B consistently outperforms leading proprietary models, including\nGPT-4o and Gemini 1.5 Pro, in detailed video description tasks. On the DREAM-1K\nbenchmark, Tarsier2-7B improves F1 by 2.8\\% over GPT-4o and 5.8\\% over\nGemini-1.5-Pro. In human side-by-side evaluations, Tarsier2-7B shows a +8.6\\%\nperformance advantage over GPT-4o and +24.9\\% over Gemini-1.5-Pro. Tarsier2-7B\nalso sets new state-of-the-art results across 15 public benchmarks, spanning\ntasks such as video question-answering, video grounding, hallucination test,\nand embodied question-answering, demonstrating its versatility as a robust\ngeneralist vision-language model.\n","authors":["Liping Yuan","Jiawei Wang","Haomiao Sun","Yuchen Zhang","Yuan Lin"],"pdf_url":"https://arxiv.org/pdf/2501.07888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07885v1","updated":"2025-01-14T06:51:27Z","published":"2025-01-14T06:51:27Z","title":"Mitigating Algorithmic Bias in Multiclass CNN Classifications Using\n  Causal Modeling","summary":"  This study describes a procedure for applying causal modeling to detect and\nmitigate algorithmic bias in a multiclass classification problem. The dataset\nwas derived from the FairFace dataset, supplemented with emotional labels\ngenerated by the DeepFace pre-trained model. A custom Convolutional Neural\nNetwork (CNN) was developed, consisting of four convolutional blocks, followed\nby fully connected layers and dropout layers to mitigate overfitting. Gender\nbias was identified in the CNN model's classifications: Females were more\nlikely to be classified as \"happy\" or \"sad,\" while males were more likely to be\nclassified as \"neutral.\" To address this, the one-vs-all (OvA) technique was\napplied. A causal model was constructed for each emotion class to adjust the\nCNN model's predicted class probabilities. The adjusted probabilities for the\nvarious classes were then aggregated by selecting the class with the highest\nprobability. The resulting debiased classifications demonstrated enhanced\ngender fairness across all classes, with negligible impact--or even a slight\nimprovement--on overall accuracy. This study highlights that algorithmic\nfairness and accuracy are not necessarily trade-offs. All data and code for\nthis study are publicly available for download.\n","authors":["Min Sik Byun","Wendy Wan Yee Hui","Wai Kwong Lau"],"pdf_url":"https://arxiv.org/pdf/2501.07885v1.pdf","comment":"7 pages; 6 figures"},{"id":"http://arxiv.org/abs/2501.07171v2","updated":"2025-01-14T06:46:14Z","published":"2025-01-13T09:58:03Z","title":"BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and\n  Vision-Language Models Derived from Scientific Literature","summary":"  The development of vision-language models (VLMs) is driven by large-scale and\ndiverse multimodal datasets. However, progress toward generalist biomedical\nVLMs is limited by the lack of annotated, publicly accessible datasets across\nbiology and medicine. Existing efforts are restricted to narrow domains,\nmissing the full diversity of biomedical knowledge encoded in scientific\nliterature. To address this gap, we introduce BIOMEDICA, a scalable,\nopen-source framework to extract, annotate, and serialize the entirety of the\nPubMed Central Open Access subset into an easy-to-use, publicly accessible\ndataset. Our framework produces a comprehensive archive with over 24 million\nunique image-text pairs from over 6 million articles. Metadata and\nexpert-guided annotations are also provided. We demonstrate the utility and\naccessibility of our resource by releasing BMCA-CLIP, a suite of CLIP-style\nmodels continuously pre-trained on the BIOMEDICA dataset via streaming,\neliminating the need to download 27 TB of data locally. On average, our models\nachieve state-of-the-art performance across 40 tasks - spanning pathology,\nradiology, ophthalmology, dermatology, surgery, molecular biology,\nparasitology, and cell biology - excelling in zero-shot classification with a\n6.56% average improvement (as high as 29.8% and 17.5% in dermatology and\nophthalmology, respectively), and stronger image-text retrieval, all while\nusing 10x less compute. To foster reproducibility and collaboration, we release\nour codebase and dataset for the broader research community.\n","authors":["Alejandro Lozano","Min Woo Sun","James Burgess","Liangyu Chen","Jeffrey J Nirschl","Jeffrey Gu","Ivan Lopez","Josiah Aklilu","Austin Wolfgang Katzer","Collin Chiu","Anita Rau","Xiaohan Wang","Yuhui Zhang","Alfred Seunghoon Song","Robert Tibshirani","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2501.07171v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07870v1","updated":"2025-01-14T06:21:31Z","published":"2025-01-14T06:21:31Z","title":"Make-A-Character 2: Animatable 3D Character Generation From a Single\n  Image","summary":"  This report introduces Make-A-Character 2, an advanced system for generating\nhigh-quality 3D characters from single portrait photographs, ideal for game\ndevelopment and digital human applications. Make-A-Character 2 builds upon its\npredecessor by incorporating several significant improvements for image-based\nhead generation. We utilize the IC-Light method to correct non-ideal\nillumination in input photos and apply neural network-based color correction to\nharmonize skin tones between the photos and game engine renders. We also employ\nthe Hierarchical Representation Network to capture high-frequency facial\nstructures and conduct adaptive skeleton calibration for accurate and\nexpressive facial animations. The entire image-to-3D-character generation\nprocess takes less than 2 minutes. Furthermore, we leverage transformer\narchitecture to generate co-speech facial and gesture actions, enabling\nreal-time conversation with the generated character. These technologies have\nbeen integrated into our conversational AI avatar products.\n","authors":["Lin Liu","Yutong Wang","Jiahao Chen","Jianfang Li","Tangli Xue","Longlong Li","Jianqiang Ren","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2501.07870v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2501.07859v1","updated":"2025-01-14T05:55:20Z","published":"2025-01-14T05:55:20Z","title":"deepTerra -- AI Land Classification Made Easy","summary":"  deepTerra is a comprehensive platform designed to facilitate the\nclassification of land surface features using machine learning and satellite\nimagery. The platform includes modules for data collection, image augmentation,\ntraining, testing, and prediction, streamlining the entire workflow for image\nclassification tasks. This paper presents a detailed overview of the\ncapabilities of deepTerra, shows how it has been applied to various research\nareas, and discusses the future directions it might take.\n","authors":["Andrew Keith Wilkinson"],"pdf_url":"https://arxiv.org/pdf/2501.07859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00836v2","updated":"2025-01-14T05:49:16Z","published":"2025-01-01T13:38:15Z","title":"Recognizing Artistic Style of Archaeological Image Fragments Using Deep\n  Style Extrapolation","summary":"  Ancient artworks obtained in archaeological excavations usually suffer from a\ncertain degree of fragmentation and physical degradation. Often, fragments of\nmultiple artifacts from different periods or artistic styles could be found on\nthe same site. With each fragment containing only partial information about its\nsource, and pieces from different objects being mixed, categorizing broken\nartifacts based on their visual cues could be a challenging task, even for\nprofessionals. As classification is a common function of many machine learning\nmodels, the power of modern architectures can be harnessed for efficient and\naccurate fragment classification. In this work, we present a generalized\ndeep-learning framework for predicting the artistic style of image fragments,\nachieving state-of-the-art results for pieces with varying styles and\ngeometries.\n","authors":["Gur Elkin","Ofir Itzhak Shahar","Yaniv Ohayon","Nadav Alali","Ohad Ben-Shahar"],"pdf_url":"https://arxiv.org/pdf/2501.00836v2.pdf","comment":"To be published in the 27th International Conference on\n  Human-Computer Interaction (HCII 2025)"},{"id":"http://arxiv.org/abs/2501.07855v1","updated":"2025-01-14T05:43:59Z","published":"2025-01-14T05:43:59Z","title":"State-of-the-Art Transformer Models for Image Super-Resolution:\n  Techniques, Challenges, and Applications","summary":"  Image Super-Resolution (SR) aims to recover a high-resolution image from its\nlow-resolution counterpart, which has been affected by a specific degradation\nprocess. This is achieved by enhancing detail and visual quality. Recent\nadvancements in transformer-based methods have remolded image super-resolution\nby enabling high-quality reconstructions surpassing previous deep-learning\napproaches like CNN and GAN-based. This effectively addresses the limitations\nof previous methods, such as limited receptive fields, poor global context\ncapture, and challenges in high-frequency detail recovery. Additionally, the\npaper reviews recent trends and advancements in transformer-based SR models,\nexploring various innovative techniques and architectures that combine\ntransformers with traditional networks to balance global and local contexts.\nThese neoteric methods are critically analyzed, revealing promising yet\nunexplored gaps and potential directions for future research. Several\nvisualizations of models and techniques are included to foster a holistic\nunderstanding of recent trends. This work seeks to offer a structured roadmap\nfor researchers at the forefront of deep learning, specifically exploring the\nimpact of transformers on super-resolution techniques.\n","authors":["Debasish Dutta","Deepjyoti Chetia","Neeharika Sonowal","Sanjib Kr Kalita"],"pdf_url":"https://arxiv.org/pdf/2501.07855v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2501.07850v1","updated":"2025-01-14T05:23:42Z","published":"2025-01-14T05:23:42Z","title":"An Intra- and Cross-frame Topological Consistency Scheme for\n  Semi-supervised Atherosclerotic Coronary Plaque Segmentation","summary":"  Enhancing the precision of segmenting coronary atherosclerotic plaques from\nCT Angiography (CTA) images is pivotal for advanced Coronary Atherosclerosis\nAnalysis (CAA), which distinctively relies on the analysis of vessel\ncross-section images reconstructed via Curved Planar Reformation. This task\npresents significant challenges due to the indistinct boundaries and structures\nof plaques and blood vessels, leading to the inadequate performance of current\ndeep learning models, compounded by the inherent difficulty in annotating such\ncomplex data. To address these issues, we propose a novel dual-consistency\nsemi-supervised framework that integrates Intra-frame Topological Consistency\n(ITC) and Cross-frame Topological Consistency (CTC) to leverage labeled and\nunlabeled data. ITC employs a dual-task network for simultaneous segmentation\nmask and Skeleton-aware Distance Transform (SDT) prediction, achieving similar\nprediction of topology structure through consistency constraint without\nadditional annotations. Meanwhile, CTC utilizes an unsupervised estimator for\nanalyzing pixel flow between skeletons and boundaries of adjacent frames,\nensuring spatial continuity. Experiments on two CTA datasets show that our\nmethod surpasses existing semi-supervised methods and approaches the\nperformance of supervised methods on CAA. In addition, our method also performs\nbetter than other methods on the ACDC dataset, demonstrating its\ngeneralization.\n","authors":["Ziheng Zhang","Zihan Li","Dandan Shan","Yuehui Qiu","Qingqi Hong","Qingqiang Wu"],"pdf_url":"https://arxiv.org/pdf/2501.07850v1.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.06480v2","updated":"2025-01-14T04:16:54Z","published":"2025-01-11T08:13:13Z","title":"Flash Window Attention: speedup the attention computation for Swin\n  Transformer","summary":"  To address the high resolution of image pixels, the Swin Transformer\nintroduces window attention. This mechanism divides an image into\nnon-overlapping windows and restricts attention computation to within each\nwindow, significantly enhancing computational efficiency. To further optimize\nthis process, one might consider replacing standard attention with flash\nattention, which has proven to be more efficient in language models. However, a\ndirect substitution is ineffective. Flash attention is designed for long\nsequences, whereas window attention deals with shorter sequences but must\nhandle numerous of them in parallel. In this report, we present an optimized\nsolution called Flash Window Attention, tailored specifically for window\nattention. Flash Window Attention improves attention computation efficiency by\nup to 300% and enhances end-to-end runtime efficiency by up to 30%. Our code is\navailable online.\n","authors":["Zhendong Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.06480v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15975v4","updated":"2025-01-14T03:55:17Z","published":"2022-11-29T07:18:32Z","title":"Analyzing Infrastructure LiDAR Placement with Realistic LiDAR Simulation\n  Library","summary":"  Recently, Vehicle-to-Everything(V2X) cooperative perception has attracted\nincreasing attention. Infrastructure sensors play a critical role in this\nresearch field; however, how to find the optimal placement of infrastructure\nsensors is rarely studied. In this paper, we investigate the problem of\ninfrastructure sensor placement and propose a pipeline that can efficiently and\neffectively find optimal installation positions for infrastructure sensors in a\nrealistic simulated environment. To better simulate and evaluate LiDAR\nplacement, we establish a Realistic LiDAR Simulation library that can simulate\nthe unique characteristics of different popular LiDARs and produce\nhigh-fidelity LiDAR point clouds in the CARLA simulator. Through simulating\npoint cloud data in different LiDAR placements, we can evaluate the perception\naccuracy of these placements using multiple detection models. Then, we analyze\nthe correlation between the point cloud distribution and perception accuracy by\ncalculating the density and uniformity of regions of interest. Experiments show\nthat when using the same number and type of LiDAR, the placement scheme\noptimized by our proposed method improves the average precision by 15%,\ncompared with the conventional placement scheme in the standard lane scene. We\nalso analyze the correlation between perception performance in the region of\ninterest and LiDAR point cloud distribution and validate that density and\nuniformity can be indicators of performance. Both the RLS Library and related\ncode will be released at https://github.com/PJLab-ADG/PCSim.\n","authors":["Xinyu Cai","Wentao Jiang","Runsheng Xu","Wenquan Zhao","Jiaqi Ma","Si Liu","Yikang Li"],"pdf_url":"https://arxiv.org/pdf/2211.15975v4.pdf","comment":"7 pages, 6 figures, accepted to the IEEE International Conference on\n  Robotics and Automation (ICRA'23)"},{"id":"http://arxiv.org/abs/2501.07819v1","updated":"2025-01-14T03:50:23Z","published":"2025-01-14T03:50:23Z","title":"3UR-LLM: An End-to-End Multimodal Large Language Model for 3D Scene\n  Understanding","summary":"  Multi-modal Large Language Models (MLLMs) exhibit impressive capabilities in\n2D tasks, yet encounter challenges in discerning the spatial positions,\ninterrelations, and causal logic in scenes when transitioning from 2D to 3D\nrepresentations. We find that the limitations mainly lie in: i) the high\nannotation cost restricting the scale-up of volumes of 3D scene data, and ii)\nthe lack of a straightforward and effective way to perceive 3D information\nwhich results in prolonged training durations and complicates the streamlined\nframework. To this end, we develop pipeline based on open-source 2D MLLMs and\nLLMs to generate high-quality 3D-text pairs and construct 3DS-160K , to enhance\nthe pre-training process. Leveraging this high-quality pre-training data, we\nintroduce the 3UR-LLM model, an end-to-end 3D MLLM designed for precise\ninterpretation of 3D scenes, showcasing exceptional capability in navigating\nthe complexities of the physical world. 3UR-LLM directly receives 3D point\ncloud as input and project 3D features fused with text instructions into a\nmanageable set of tokens. Considering the computation burden derived from these\nhybrid tokens, we design a 3D compressor module to cohesively compress the 3D\nspatial cues and textual narrative. 3UR-LLM achieves promising performance with\nrespect to the previous SOTAs, for instance, 3UR-LLM exceeds its counterparts\nby 7.1\\% CIDEr on ScanQA, while utilizing fewer training resources. The code\nand model weights for 3UR-LLM and the 3DS-160K benchmark are available at\n3UR-LLM.\n","authors":["Haomiao Xiong","Yunzhi Zhuge","Jiawen Zhu","Lu Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2501.07819v1.pdf","comment":"Accepted to IEEE Transactions on Multimedia (TMM)"},{"id":"http://arxiv.org/abs/2412.10106v4","updated":"2025-01-14T03:43:02Z","published":"2024-12-13T12:47:30Z","title":"A Cascaded Dilated Convolution Approach for Mpox Lesion Classification","summary":"  The global outbreak of the Mpox virus, classified as a Public Health\nEmergency of International Concern (PHEIC) by the World Health Organization,\npresents significant diagnostic challenges due to its visual similarity to\nother skin lesion diseases. Traditional diagnostic methods for Mpox, which rely\non clinical symptoms and laboratory tests, are slow and labor intensive. Deep\nlearning-based approaches for skin lesion classification offer a promising\nalternative. However, developing a model that balances efficiency with accuracy\nis crucial to ensure reliable and timely diagnosis without compromising\nperformance. This study introduces the Cascaded Atrous Group Attention (CAGA)\nframework to address these challenges, combining the Cascaded Atrous Attention\nmodule and the Cascaded Group Attention mechanism. The Cascaded Atrous\nAttention module utilizes dilated convolutions and cascades the outputs to\nenhance multi-scale representation. This is integrated into the Cascaded Group\nAttention mechanism, which reduces redundancy in Multi-Head Self-Attention. By\nintegrating the Cascaded Atrous Group Attention module with EfficientViT-L1 as\nthe backbone architecture, this approach achieves state-of-the-art performance,\nreaching an accuracy of 98% on the Mpox Close Skin Image (MCSI) dataset while\nreducing model parameters by 37.5% compared to the original EfficientViT-L1.\nThe model's robustness is demonstrated through extensive validation on two\nadditional benchmark datasets, where it consistently outperforms existing\napproaches.\n","authors":["Ayush Deshmukh"],"pdf_url":"https://arxiv.org/pdf/2412.10106v4.pdf","comment":"8 pages, 4 figures, Submitted to Medical Imaging with Deep Learning"},{"id":"http://arxiv.org/abs/2501.07810v1","updated":"2025-01-14T03:20:20Z","published":"2025-01-14T03:20:20Z","title":"AVS-Mamba: Exploring Temporal and Multi-modal Mamba for Audio-Visual\n  Segmentation","summary":"  The essence of audio-visual segmentation (AVS) lies in locating and\ndelineating sound-emitting objects within a video stream. While\nTransformer-based methods have shown promise, their handling of long-range\ndependencies struggles due to quadratic computational costs, presenting a\nbottleneck in complex scenarios. To overcome this limitation and facilitate\ncomplex multi-modal comprehension with linear complexity, we introduce\nAVS-Mamba, a selective state space model to address the AVS task. Our framework\nincorporates two key components for video understanding and cross-modal\nlearning: Temporal Mamba Block for sequential video processing and\nVision-to-Audio Fusion Block for advanced audio-vision integration. Building on\nthis, we develop the Multi-scale Temporal Encoder, aimed at enhancing the\nlearning of visual features across scales, facilitating the perception of\nintra- and inter-frame information. To perform multi-modal fusion, we propose\nthe Modality Aggregation Decoder, leveraging the Vision-to-Audio Fusion Block\nto integrate visual features into audio features across both frame and temporal\nlevels. Further, we adopt the Contextual Integration Pyramid to perform\naudio-to-vision spatial-temporal context collaboration. Through these\ninnovative contributions, our approach achieves new state-of-the-art results on\nthe AVSBench-object and AVSBench-semantic datasets. Our source code and model\nweights are available at AVS-Mamba.\n","authors":["Sitong Gong","Yunzhi Zhuge","Lu Zhang","Yifan Wang","Pingping Zhang","Lijun Wang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2501.07810v1.pdf","comment":"Accepted to IEEE Transactions on Multimedia (TMM)"},{"id":"http://arxiv.org/abs/2501.07808v1","updated":"2025-01-14T03:19:10Z","published":"2025-01-14T03:19:10Z","title":"A Low-cost and Ultra-lightweight Binary Neural Network for Traffic\n  Signal Recognition","summary":"  The deployment of neural networks in vehicle platforms and wearable\nArtificial Intelligence-of-Things (AIOT) scenarios has become a research area\nthat has attracted much attention. With the continuous evolution of deep\nlearning technology, many image classification models are committed to\nimproving recognition accuracy, but this is often accompanied by problems such\nas large model resource usage, complex structure, and high power consumption,\nwhich makes it challenging to deploy on resource-constrained platforms. Herein,\nwe propose an ultra-lightweight binary neural network (BNN) model designed for\nhardware deployment, and conduct image classification research based on the\nGerman Traffic Sign Recognition Benchmark (GTSRB) dataset. In addition, we also\nverify it on the Chinese Traffic Sign (CTS) and Belgian Traffic Sign (BTS)\ndatasets. The proposed model shows excellent recognition performance with an\naccuracy of up to 97.64%, making it one of the best performing BNN models in\nthe GTSRB dataset. Compared with the full-precision model, the accuracy loss is\ncontrolled within 1%, and the parameter storage overhead of the model is only\n10% of that of the full-precision model. More importantly, our network model\nonly relies on logical operations and low-bit width fixed-point addition and\nsubtraction operations during the inference phase, which greatly simplifies the\ndesign complexity of the processing element (PE). Our research shows the great\npotential of BNN in the hardware deployment of computer vision models,\nespecially in the field of computer vision tasks related to autonomous driving.\n","authors":["Mingke Xiao","Yue Su","Liang Yu","Guanglong Qu","Yutong Jia","Yukuan Chang","Xu Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.07808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07806v1","updated":"2025-01-14T03:15:46Z","published":"2025-01-14T03:15:46Z","title":"Learning Motion and Temporal Cues for Unsupervised Video Object\n  Segmentation","summary":"  In this paper, we address the challenges in unsupervised video object\nsegmentation (UVOS) by proposing an efficient algorithm, termed MTNet, which\nconcurrently exploits motion and temporal cues. Unlike previous methods that\nfocus solely on integrating appearance with motion or on modeling temporal\nrelations, our method combines both aspects by integrating them within a\nunified framework. MTNet is devised by effectively merging appearance and\nmotion features during the feature extraction process within encoders,\npromoting a more complementary representation. To capture the intricate\nlong-range contextual dynamics and information embedded within videos, a\ntemporal transformer module is introduced, facilitating efficacious inter-frame\ninteractions throughout a video clip. Furthermore, we employ a cascade of\ndecoders all feature levels across all feature levels to optimally exploit the\nderived features, aiming to generate increasingly precise segmentation masks.\nAs a result, MTNet provides a strong and compact framework that explores both\ntemporal and cross-modality knowledge to robustly localize and track the\nprimary object accurately in various challenging scenarios efficiently.\nExtensive experiments across diverse benchmarks conclusively show that our\nmethod not only attains state-of-the-art performance in unsupervised video\nobject segmentation but also delivers competitive results in video salient\nobject detection. These findings highlight the method's robust versatility and\nits adeptness in adapting to a range of segmentation tasks. Source code is\navailable on https://github.com/hy0523/MTNet.\n","authors":["Yunzhi Zhuge","Hongyu Gu","Lu Zhang","Jinqing Qi","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2501.07806v1.pdf","comment":"Accepted to IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS)"},{"id":"http://arxiv.org/abs/2501.07804v1","updated":"2025-01-14T03:12:25Z","published":"2025-01-14T03:12:25Z","title":"Balance Divergence for Knowledge Distillation","summary":"  Knowledge distillation has been widely adopted in computer vision task\nprocessing, since it can effectively enhance the performance of lightweight\nstudent networks by leveraging the knowledge transferred from cumbersome\nteacher networks. Most existing knowledge distillation methods utilize\nKullback-Leibler divergence to mimic the logit output probabilities between the\nteacher network and the student network. Nonetheless, these methods may neglect\nthe negative parts of the teacher's ''dark knowledge'' because the divergence\ncalculations may ignore the effect of the minute probabilities from the\nteacher's logit output. This deficiency may lead to suboptimal performance in\nlogit mimicry during the distillation process and result in an imbalance of\ninformation acquired by the student network. In this paper, we investigate the\nimpact of this imbalance and propose a novel method, named Balance Divergence\nDistillation. By introducing a compensatory operation using reverse\nKullback-Leibler divergence, our method can improve the modeling of the\nextremely small values in the negative from the teacher and preserve the\nlearning capacity for the positive. Furthermore, we test the impact of\ndifferent temperature coefficients adjustments, which may conducted to further\nbalance for knowledge transferring. We evaluate the proposed method on several\ncomputer vision tasks, including image classification and semantic\nsegmentation. The evaluation results show that our method achieves an accuracy\nimprovement of 1%~3% for lightweight students on both CIFAR-100 and ImageNet\ndataset, and a 4.55% improvement in mIoU for PSP-ResNet18 on the Cityscapes\ndataset. The experiments show that our method is a simple yet highly effective\nsolution that can be smoothly applied to different knowledge distillation\nmethods.\n","authors":["Yafei Qi","Chen Wang","Zhaoning Zhang","Yaping Liu","Yongmin Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.07804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07800v1","updated":"2025-01-14T02:56:19Z","published":"2025-01-14T02:56:19Z","title":"BioPose: Biomechanically-accurate 3D Pose Estimation from Monocular\n  Videos","summary":"  Recent advancements in 3D human pose estimation from single-camera images and\nvideos have relied on parametric models, like SMPL. However, these models\noversimplify anatomical structures, limiting their accuracy in capturing true\njoint locations and movements, which reduces their applicability in\nbiomechanics, healthcare, and robotics. Biomechanically accurate pose\nestimation, on the other hand, typically requires costly marker-based motion\ncapture systems and optimization techniques in specialized labs. To bridge this\ngap, we propose BioPose, a novel learning-based framework for predicting\nbiomechanically accurate 3D human pose directly from monocular videos. BioPose\nincludes three key components: a Multi-Query Human Mesh Recovery model\n(MQ-HMR), a Neural Inverse Kinematics (NeurIK) model, and a 2D-informed pose\nrefinement technique. MQ-HMR leverages a multi-query deformable transformer to\nextract multi-scale fine-grained image features, enabling precise human mesh\nrecovery. NeurIK treats the mesh vertices as virtual markers, applying a\nspatial-temporal network to regress biomechanically accurate 3D poses under\nanatomical constraints. To further improve 3D pose estimations, a 2D-informed\nrefinement step optimizes the query tokens during inference by aligning the 3D\nstructure with 2D pose observations. Experiments on benchmark datasets\ndemonstrate that BioPose significantly outperforms state-of-the-art methods.\nProject website:\n\\url{https://m-usamasaleem.github.io/publication/BioPose/BioPose.html}.\n","authors":["Farnoosh Koleini","Muhammad Usama Saleem","Pu Wang","Hongfei Xue","Ahmed Helmy","Abbey Fenwick"],"pdf_url":"https://arxiv.org/pdf/2501.07800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09323v3","updated":"2025-01-14T02:56:00Z","published":"2024-09-14T05:53:33Z","title":"Implicit Neural Representations with Fourier Kolmogorov-Arnold Networks","summary":"  Implicit neural representations (INRs) use neural networks to provide\ncontinuous and resolution-independent representations of complex signals with a\nsmall number of parameters. However, existing INR models often fail to capture\nimportant frequency components specific to each task. To address this issue, in\nthis paper, we propose a Fourier Kolmogorov Arnold network (FKAN) for INRs. The\nproposed FKAN utilizes learnable activation functions modeled as Fourier series\nin the first layer to effectively control and learn the task-specific frequency\ncomponents. In addition, the activation functions with learnable Fourier\ncoefficients improve the ability of the network to capture complex patterns and\ndetails, which is beneficial for high-resolution and high-dimensional data.\nExperimental results show that our proposed FKAN model outperforms three\nstate-of-the-art baseline schemes, and improves the peak signal-to-noise ratio\n(PSNR) and structural similarity index measure (SSIM) for the image\nrepresentation task and intersection over union (IoU) for the 3D occupancy\nvolume representation task, respectively. The code is available at\ngithub.com/Ali-Meh619/FKAN.\n","authors":["Ali Mehrabian","Parsa Mojarad Adi","Moein Heidari","Ilker Hacihaliloglu"],"pdf_url":"https://arxiv.org/pdf/2409.09323v3.pdf","comment":"Accepted for publication in Proc. IEEE ICASSP 2025"},{"id":"http://arxiv.org/abs/2407.02772v2","updated":"2025-01-14T02:30:09Z","published":"2024-07-03T03:01:43Z","title":"Gradient descent with generalized Newton's method","summary":"  We propose the generalized Newton's method (GeN) -- a Hessian-informed\napproach that applies to any optimizer such as SGD and Adam, and covers the\nNewton-Raphson method as a sub-case. Our method automatically and dynamically\nselects the learning rate that accelerates the convergence, without the\nintensive tuning of the learning rate scheduler. In practice, our method is\neasily implementable, since it only requires additional forward passes with\nalmost zero computational overhead (in terms of training time and memory cost),\nif the overhead is amortized over many iterations. We present extensive\nexperiments on language and vision tasks (e.g. GPT and ResNet) to showcase that\nGeN optimizers match the state-of-the-art performance, which was achieved with\ncarefully tuned learning rate schedulers.\n","authors":["Zhiqi Bu","Shiyun Xu"],"pdf_url":"https://arxiv.org/pdf/2407.02772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15761v2","updated":"2025-01-14T02:10:46Z","published":"2024-11-24T09:12:37Z","title":"MambaTrack: Exploiting Dual-Enhancement for Night UAV Tracking","summary":"  Night unmanned aerial vehicle (UAV) tracking is impeded by the challenges of\npoor illumination, with previous daylight-optimized methods demonstrating\nsuboptimal performance in low-light conditions, limiting the utility of UAV\napplications. To this end, we propose an efficient mamba-based tracker,\nleveraging dual enhancement techniques to boost night UAV tracking. The\nmamba-based low-light enhancer, equipped with an illumination estimator and a\ndamage restorer, achieves global image enhancement while preserving the details\nand structure of low-light images. Additionally, we advance a cross-modal mamba\nnetwork to achieve efficient interactive learning between vision and language\nmodalities. Extensive experiments showcase that our method achieves advanced\nperformance and exhibits significantly improved computation and memory\nefficiency. For instance, our method is 2.8$\\times$ faster than CiteTracker and\nreduces 50.2$\\%$ GPU memory. Our codes are available at\n\\url{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.\n","authors":["Chunhui Zhang","Li Liu","Hao Wen","Xi Zhou","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15761v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2405.14880v4","updated":"2025-01-14T01:57:44Z","published":"2024-04-04T20:06:07Z","title":"Dissecting Query-Key Interaction in Vision Transformers","summary":"  Self-attention in vision transformers is often thought to perform perceptual\ngrouping where tokens attend to other tokens with similar embeddings, which\ncould correspond to semantically similar features of an object. However,\nattending to dissimilar tokens can be beneficial by providing contextual\ninformation. We propose to analyze the query-key interaction by the singular\nvalue decomposition of the interaction matrix (i.e.\n${\\textbf{W}_q}^\\top\\textbf{W}_k$). We find that in many ViTs, especially those\nwith classification training objectives, early layers attend more to similar\ntokens, while late layers show increased attention to dissimilar tokens,\nproviding evidence corresponding to perceptual grouping and contextualization,\nrespectively. Many of these interactions between features represented by\nsingular vectors are interpretable and semantic, such as attention between\nrelevant objects, between parts of an object, or between the foreground and\nbackground. This offers a novel perspective on interpreting the attention\nmechanism, which contributes to understanding how transformer models utilize\ncontext and salient features when processing images.\n","authors":["Xu Pan","Aaron Philip","Ziqian Xie","Odelia Schwartz"],"pdf_url":"https://arxiv.org/pdf/2405.14880v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07783v1","updated":"2025-01-14T01:57:41Z","published":"2025-01-14T01:57:41Z","title":"Parameter-Inverted Image Pyramid Networks for Visual Perception and\n  Multimodal Understanding","summary":"  Image pyramids are widely adopted in top-performing methods to obtain\nmulti-scale features for precise visual perception and understanding. However,\ncurrent image pyramids use the same large-scale model to process multiple\nresolutions of images, leading to significant computational cost. To address\nthis challenge, we propose a novel network architecture, called\nParameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses\npretrained models (ViTs or CNNs) as branches to process multi-scale images,\nwhere images of higher resolutions are processed by smaller network branches to\nbalance computational cost and performance. To integrate information from\ndifferent spatial scales, we further propose a novel cross-branch feature\ninteraction mechanism. To validate PIIP, we apply it to various perception\nmodels and a representative multimodal large language model called LLaVA, and\nconduct extensive experiments on various tasks such as object detection,\nsegmentation, image classification and multimodal understanding. PIIP achieves\nsuperior performance compared to single-branch and existing multi-resolution\napproaches with lower computational cost. When applied to InternViT-6B, a\nlarge-scale vision foundation model, PIIP can improve its performance by 1%-2%\non detection and segmentation with only 40%-60% of the original computation,\nfinally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For\nmultimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and\n74.5% on MMBench with only 2.8M training data. Our code is released at\nhttps://github.com/OpenGVLab/PIIP.\n","authors":["Zhaokai Wang","Xizhou Zhu","Xue Yang","Gen Luo","Hao Li","Changyao Tian","Wenhan Dou","Junqi Ge","Lewei Lu","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2501.07783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12463v2","updated":"2025-01-14T01:57:04Z","published":"2024-08-22T15:04:59Z","title":"Smartphone-based Eye Tracking System using Edge Intelligence and Model\n  Optimisation","summary":"  A significant limitation of current smartphone-based eye-tracking algorithms\nis their low accuracy when applied to video-type visual stimuli, as they are\ntypically trained on static images. Also, the increasing demand for real-time\ninteractive applications like games, VR, and AR on smartphones requires\novercoming the limitations posed by resource constraints such as limited\ncomputational power, battery life, and network bandwidth. Therefore, we\ndeveloped two new smartphone eye-tracking techniques for video-type visuals by\ncombining Convolutional Neural Networks (CNN) with two different Recurrent\nNeural Networks (RNN), namely Long Short Term Memory (LSTM) and Gated Recurrent\nUnit (GRU). Our CNN+LSTM and CNN+GRU models achieved an average Root Mean\nSquare Error of 0.955 cm and 1.091 cm, respectively. To address the\ncomputational constraints of smartphones, we developed an edge intelligence\narchitecture to enhance the performance of smartphone-based eye tracking. We\napplied various optimisation methods like quantisation and pruning to deep\nlearning models for better energy, CPU, and memory usage on edge devices,\nfocusing on real-time processing. Using model quantisation, the model inference\ntime in the CNN+LSTM and CNN+GRU models was reduced by 21.72% and 19.50%,\nrespectively, on edge devices.\n","authors":["Nishan Gunawardena","Gough Yumu Lui","Jeewani Anupama Ginige","Bahman Javadi"],"pdf_url":"https://arxiv.org/pdf/2408.12463v2.pdf","comment":"I have included the three papers as reference, which are closely\n  related. We have expanded the future work section to provide a more thorough\n  discussion of the concepts of \"varying lighting conditions\" and \"dynamic user\n  environments.\" We have added a note below Table 4 to clarify the\n  abbreviations' meaning. Elaborated the role of the Domain Expert within the\n  presentation layer in Section 4.1"},{"id":"http://arxiv.org/abs/2407.14649v2","updated":"2025-01-14T01:39:22Z","published":"2024-07-19T19:56:53Z","title":"The Collection of a Human Robot Collaboration Dataset for Cooperative\n  Assembly in Glovebox Environments","summary":"  Industry 4.0 introduced AI as a transformative solution for modernizing\nmanufacturing processes. Its successor, Industry 5.0, envisions humans as\ncollaborators and experts guiding these AI-driven manufacturing solutions.\nDeveloping these techniques necessitates algorithms capable of safe, real-time\nidentification of human positions in a scene, particularly their hands, during\ncollaborative assembly. Although substantial efforts have curated datasets for\nhand segmentation, most focus on residential or commercial domains. Existing\ndatasets targeting industrial settings predominantly rely on synthetic data,\nwhich we demonstrate does not effectively transfer to real-world operations.\nMoreover, these datasets lack uncertainty estimations critical for safe\ncollaboration. Addressing these gaps, we present HAGS: Hand and Glove\nSegmentation Dataset. This dataset provides challenging examples to build\napplications toward hand and glove segmentation in industrial human-robot\ncollaboration scenarios as well as assess out-of-distribution images,\nconstructed via green screen augmentations, to determine ML-classifier\nrobustness. We study state-of-the-art, real-time segmentation models to\nevaluate existing methods. Our dataset and baselines are publicly available.\n","authors":["Shivansh Sharma","Mathew Huang","Sanat Nair","Alan Wen","Christina Petlowany","Juston Moore","Selma Wanna","Mitch Pryor"],"pdf_url":"https://arxiv.org/pdf/2407.14649v2.pdf","comment":"draft paper to be submitted to IJRR"},{"id":"http://arxiv.org/abs/2210.01272v3","updated":"2025-01-14T01:34:10Z","published":"2022-10-03T23:44:38Z","title":"A systematic review of the use of Deep Learning in Satellite Imagery for\n  Agriculture","summary":"  Agricultural research is essential for increasing food production to meet the\nrequirements of an increasing population in the coming decades. Recently,\nsatellite technology has been improving rapidly and deep learning has seen much\nsuccess in generic computer vision tasks and many application areas which\npresents an important opportunity to improve analysis of agricultural land.\nHere we present a systematic review of 150 studies to find the current uses of\ndeep learning on satellite imagery for agricultural research. Although we\nidentify 5 categories of agricultural monitoring tasks, the majority of the\nresearch interest is in crop segmentation and yield prediction. We found that,\nwhen used, modern deep learning methods consistently outperformed traditional\nmachine learning across most tasks; the only exception was that Long Short-Term\nMemory (LSTM) Recurrent Neural Networks did not consistently outperform Random\nForests (RF) for yield prediction. The reviewed studies have largely adopted\nmethodologies from generic computer vision, except for one major omission:\nbenchmark datasets are not utilised to evaluate models across studies, making\nit difficult to compare results. Additionally, some studies have specifically\nutilised the extra spectral resolution available in satellite imagery, but\nother divergent properties of satellite images - such as the hugely different\nscales of spatial patterns - are not being taken advantage of in the reviewed\nstudies.\n","authors":["Brandon Victor","Zhen He","Aiden Nibali"],"pdf_url":"https://arxiv.org/pdf/2210.01272v3.pdf","comment":"23 pages, 5 figures and 10 tables in main paper. Final version, as\n  submitted and accepted at JSTARS"},{"id":"http://arxiv.org/abs/2408.06170v4","updated":"2025-01-14T01:27:36Z","published":"2024-08-12T14:16:10Z","title":"Zero-shot 3D Segmentation of Abdominal Organs in CT Scans Using Segment\n  Anything Model 2: Adapting Video Tracking Capabilities for 3D Medical Imaging","summary":"  Objectives: To evaluate the zero-shot performance of Segment Anything Model 2\n(SAM 2) in 3D segmentation of abdominal organs in CT scans, and to investigate\nthe effects of prompt settings on segmentation results.\n  Materials and Methods: In this retrospective study, we used a subset of the\nTotalSegmentator CT dataset from eight institutions to assess SAM 2's ability\nto segment eight abdominal organs. Segmentation was initiated from three\ndifferent z-coordinate levels (caudal, mid, and cranial levels) of each organ.\nPerformance was measured using the Dice similarity coefficient (DSC). We also\nanalyzed the impact of \"negative prompts,\" which explicitly exclude certain\nregions from the segmentation process, on accuracy.\n  Results: 123 patients (mean age, 60.7 \\pm 15.5 years; 63 men, 60 women) were\nevaluated. As a zero-shot approach, larger organs with clear boundaries\ndemonstrated high segmentation performance, with mean DSCs as follows: liver\n0.821 \\pm 0.192, right kidney 0.862 \\pm 0.212, left kidney 0.870 \\pm 0.154, and\nspleen 0.891 \\pm 0.131. Smaller organs showed lower performance: gallbladder\n0.531 \\pm 0.291, pancreas 0.361 \\pm 0.197, and adrenal glands, right 0.203 \\pm\n0.222, left 0.308 \\pm 0.234. The initial slice for segmentation and the use of\nnegative prompts significantly influenced the results. By removing negative\nprompts from the input, the DSCs significantly decreased for six organs.\n  Conclusion: SAM 2 demonstrated promising zero-shot performance in segmenting\ncertain abdominal organs in CT scans, particularly larger organs. Performance\nwas significantly influenced by input negative prompts and initial slice\nselection, highlighting the importance of optimizing these factors.\n","authors":["Yosuke Yamagishi","Shouhei Hanaoka","Tomohiro Kikuchi","Takahiro Nakao","Yuta Nakamura","Yukihiro Nomura","Soichiro Miki","Takeharu Yoshikawa","Osamu Abe"],"pdf_url":"https://arxiv.org/pdf/2408.06170v4.pdf","comment":"20 pages, 7 figures (including 2 supplemental figure), 4 tables"},{"id":"http://arxiv.org/abs/2306.03983v2","updated":"2025-01-14T01:10:52Z","published":"2023-06-06T19:36:11Z","title":"XVertNet: Unsupervised Contrast Enhancement of Vertebral Structures with\n  Dynamic Self-Tuning Guidance and Multi-Stage Analysis","summary":"  Chest X-rays remain the primary diagnostic tool in emergency medicine, yet\ntheir limited ability to capture fine anatomical details can result in missed\nor delayed diagnoses. To address this, we introduce XVertNet, a novel\ndeep-learning framework designed to enhance vertebral structure visualization\nin X-ray images significantly. Our framework introduces two key innovations:\n(1) An unsupervised learning architecture that eliminates reliance on manually\nlabeled training data a persistent bottleneck in medical imaging, and (2) a\ndynamic self-tuned internal guidance mechanism featuring an adaptive feedback\nloop for real-time image optimization. Extensive validation across four major\npublic datasets revealed that XVertNet outperforms state-of-the-art enhancement\nmethods, as demonstrated by improvements in entropy scores, Tenengrad criterion\nvalues, the local phase coherence sharpness index (LPC-SI), and thetone mapped\nimage quality index (TMQI). Furthermore, clinical validation conducted with two\nboard-certified radiologists confirmed that the enhanced images enabled more\nsensitive detection of subtle vertebral fractures and degenerative changes. The\nunsupervised nature of XVertNet facilitates immediate clinical deployment\nwithout requiring additional training overhead. This innovation represents a\ntransformative advancement in emergency radiology, providing a scalable and\ntime-efficient solution to enhance diagnostic accuracy in high-pressure\nclinical environments.\n","authors":["Ella Eidlin","Assaf Hoogi","Hila Rozen","Mohammad Badarne","Nathan S. Netanyahu"],"pdf_url":"https://arxiv.org/pdf/2306.03983v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2501.07769v1","updated":"2025-01-14T00:59:55Z","published":"2025-01-14T00:59:55Z","title":"BMIP: Bi-directional Modality Interaction Prompt Learning for VLM","summary":"  Vision-language models (VLMs) have exhibited remarkable generalization\ncapabilities, and prompt learning for VLMs has attracted great attention for\nthe ability to adapt pre-trained VLMs to specific downstream tasks. However,\nexisting studies mainly focus on single-modal prompts or uni-directional\nmodality interaction, overlooking the powerful alignment effects resulting from\nthe interaction between the vision and language modalities. To this end, we\npropose a novel prompt learning method called\n$\\underline{\\textbf{B}}i-directional \\underline{\\textbf{M}}odality\n\\underline{\\textbf{I}}nteraction \\underline{\\textbf{P}}rompt (BMIP)$, which\ndynamically weights bi-modal information through learning the information of\nthe attention layer, enhancing trainability and inter-modal consistency\ncompared to simple information aggregation methods. To evaluate the\neffectiveness of prompt learning methods, we propose a more realistic\nevaluation paradigm called open-world generalization complementing the widely\nadopted cross-dataset transfer and domain generalization tasks. Comprehensive\nexperiments on various datasets reveal that BMIP not only outperforms current\nstate-of-the-art methods across all three evaluation paradigms but is also\nflexible enough to be combined with other prompt-based methods for consistent\nperformance enhancement.\n","authors":["Song-Lin Lv","Yu-Yang Chen","Zhi Zhou","Ming Yang","Lan-Zhe Guo"],"pdf_url":"https://arxiv.org/pdf/2501.07769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07762v1","updated":"2025-01-14T00:30:22Z","published":"2025-01-14T00:30:22Z","title":"PSReg: Prior-guided Sparse Mixture of Experts for Point Cloud\n  Registration","summary":"  The discriminative feature is crucial for point cloud registration. Recent\nmethods improve the feature discriminative by distinguishing between\nnon-overlapping and overlapping region points. However, they still face\nchallenges in distinguishing the ambiguous structures in the overlapping\nregions. Therefore, the ambiguous features they extracted resulted in a\nsignificant number of outlier matches from overlapping regions. To solve this\nproblem, we propose a prior-guided SMoE-based registration method to improve\nthe feature distinctiveness by dispatching the potential correspondences to the\nsame experts. Specifically, we propose a prior-guided SMoE module by fusing\nprior overlap and potential correspondence embeddings for routing, assigning\ntokens to the most suitable experts for processing. In addition, we propose a\nregistration framework by a specific combination of Transformer layer and\nprior-guided SMoE module. The proposed method not only pays attention to the\nimportance of locating the overlapping areas of point clouds, but also commits\nto finding more accurate correspondences in overlapping areas. Our extensive\nexperiments demonstrate the effectiveness of our method, achieving\nstate-of-the-art registration recall (95.7\\%/79.3\\%) on the 3DMatch/3DLoMatch\nbenchmark. Moreover, we also test the performance on ModelNet40 and demonstrate\nexcellent performance.\n","authors":["Xiaoshui Huang","Zhou Huang","Yifan Zuo","Yongshun Gong","Chengdong Zhang","Deyang Liu","Yuming Fang"],"pdf_url":"https://arxiv.org/pdf/2501.07762v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2501.08495v1","updated":"2025-01-14T23:57:35Z","published":"2025-01-14T23:57:35Z","title":"Automotive Elevation Mapping with Interferometric Synthetic Aperture\n  Radar","summary":"  Radar is a low-cost and ubiquitous automotive sensor, but is limited by array\nresolution and sensitivity when performing direction of arrival analysis.\nSynthetic Aperture Radar (SAR) is a class of techniques to improve azimuth\nresolution and sensitivity for radar. Interferometric SAR (InSAR) can be used\nto extract elevation from the variations in phase measurements in SAR images.\nUtilizing InSAR we show that a typical, low-resolution radar array mounted on a\nvehicle can be used to accurately localize detections in 3D space for both\nurban and agricultural environments. We generate point clouds in each\nenvironment by combining InSAR with a signal processing scheme tailored to\nautomotive driving. This low-compute approach allows radar to be used as a\nprimary sensor to map fine details in complex driving environments, and be used\nto make autonomous perception decisions.\n","authors":["Leyla A. Kabuli","Griffin Foster"],"pdf_url":"https://arxiv.org/pdf/2501.08495v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.08490v1","updated":"2025-01-14T23:31:20Z","published":"2025-01-14T23:31:20Z","title":"FLAVARS: A Multimodal Foundational Language and Vision Alignment Model\n  for Remote Sensing","summary":"  Remote sensing imagery is dense with objects and contextual visual\ninformation. There is a recent trend to combine paired satellite images and\ntext captions for pretraining performant encoders for downstream tasks.\nHowever, while contrastive image-text methods like CLIP enable vision-language\nalignment and zero-shot classification ability, vision-only downstream\nperformance tends to degrade compared to image-only pretraining, such as MAE.\nIn this paper, we propose FLAVARS, a pretraining method that combines the best\nof both contrastive learning and masked modeling, along with geospatial\nalignment via contrastive location encoding. We find that FLAVARS significantly\noutperforms a baseline of SkyCLIP for vision-only tasks such as KNN\nclassification and semantic segmentation, +6\\% mIOU on SpaceNet1, while\nretaining the ability to perform zero-shot classification, unlike MAE\npretrained methods.\n","authors":["Isaac Corley","Simone Fobi Nsutezo","Anthony Ortiz","Caleb Robinson","Rahul Dodhia","Juan M. Lavista Ferres","Peyman Najafirad"],"pdf_url":"https://arxiv.org/pdf/2501.08490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08471v1","updated":"2025-01-14T22:36:11Z","published":"2025-01-14T22:36:11Z","title":"Benchmarking Classical, Deep, and Generative Models for Human Activity\n  Recognition","summary":"  Human Activity Recognition (HAR) has gained significant importance with the\ngrowing use of sensor-equipped devices and large datasets. This paper evaluates\nthe performance of three categories of models : classical machine learning,\ndeep learning architectures, and Restricted Boltzmann Machines (RBMs) using\nfive key benchmark datasets of HAR (UCI-HAR, OPPORTUNITY, PAMAP2, WISDM, and\nBerkeley MHAD). We assess various models, including Decision Trees, Random\nForests, Convolutional Neural Networks (CNN), and Deep Belief Networks (DBNs),\nusing metrics such as accuracy, precision, recall, and F1-score for a\ncomprehensive comparison. The results show that CNN models offer superior\nperformance across all datasets, especially on the Berkeley MHAD. Classical\nmodels like Random Forest do well on smaller datasets but face challenges with\nlarger, more complex data. RBM-based models also show notable potential,\nparticularly for feature learning. This paper offers a detailed comparison to\nhelp researchers choose the most suitable model for HAR tasks.\n","authors":["Md Meem Hossain","The Anh Han","Safina Showkat Ara","Zia Ush Shamszaman"],"pdf_url":"https://arxiv.org/pdf/2501.08471v1.pdf","comment":"48 pages, 21 Figures"},{"id":"http://arxiv.org/abs/2501.08470v1","updated":"2025-01-14T22:33:07Z","published":"2025-01-14T22:33:07Z","title":"Detecting Contextual Anomalies by Discovering Consistent Spatial Regions","summary":"  We describe a method for modeling spatial context to enable video anomaly\ndetection. The main idea is to discover regions that share similar object-level\nactivities by clustering joint object attributes using Gaussian mixture models.\nWe demonstrate that this straightforward approach, using orders of magnitude\nfewer parameters than competing models, achieves state-of-the-art performance\nin the challenging spatial-context-dependent Street Scene dataset. As a side\nbenefit, the high-resolution discovered regions learned by the model also\nprovide explainable normalcy maps for human operators without the need for any\npre-trained segmentation model.\n","authors":["Zhengye Yang","Richard J. Radke"],"pdf_url":"https://arxiv.org/pdf/2501.08470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.06720v4","updated":"2025-01-14T22:30:10Z","published":"2023-04-13T17:59:55Z","title":"Expressive Text-to-Image Generation with Rich Text","summary":"  Plain text has become a prevalent interface for text-to-image synthesis.\nHowever, its limited customization options hinder users from accurately\ndescribing desired outputs. For example, plain text makes it hard to specify\ncontinuous quantities, such as the precise RGB color value or importance of\neach word. Furthermore, creating detailed text prompts for complex scenes is\ntedious for humans to write and challenging for text encoders to interpret. To\naddress these challenges, we propose using a rich-text editor supporting\nformats such as font style, size, color, and footnote. We extract each word's\nattributes from rich text to enable local style control, explicit token\nreweighting, precise color rendering, and detailed region synthesis. We achieve\nthese capabilities through a region-based diffusion process. We first obtain\neach word's region based on attention maps of a diffusion process using plain\ntext. For each region, we enforce its text attributes by creating\nregion-specific detailed prompts and applying region-specific guidance, and\nmaintain its fidelity against plain-text generation through region-based\ninjections. We present various examples of image generation from rich text and\ndemonstrate that our method outperforms strong baselines with quantitative\nevaluations.\n","authors":["Songwei Ge","Taesung Park","Jun-Yan Zhu","Jia-Bin Huang"],"pdf_url":"https://arxiv.org/pdf/2304.06720v4.pdf","comment":"Project webpage: https://rich-text-to-image.github.io/"},{"id":"http://arxiv.org/abs/2501.08465v1","updated":"2025-01-14T22:23:11Z","published":"2025-01-14T22:23:11Z","title":"Predicting Performance of Object Detection Models in Electron Microscopy\n  Using Random Forests","summary":"  Quantifying prediction uncertainty when applying object detection models to\nnew, unlabeled datasets is critical in applied machine learning. This study\nintroduces an approach to estimate the performance of deep learning-based\nobject detection models for quantifying defects in transmission electron\nmicroscopy (TEM) images, focusing on detecting irradiation-induced cavities in\nTEM images of metal alloys. We developed a random forest regression model that\npredicts the object detection F1 score, a statistical metric used to evaluate\nthe ability to accurately locate and classify objects of interest. The random\nforest model uses features extracted from the predictions of the object\ndetection model whose uncertainty is being quantified, enabling fast prediction\non new, unlabeled images. The mean absolute error (MAE) for predicting F1 of\nthe trained model on test data is 0.09, and the $R^2$ score is 0.77, indicating\nthere is a significant correlation between the random forest regression model\npredicted and true defect detection F1 scores. The approach is shown to be\nrobust across three distinct TEM image datasets with varying imaging and\nmaterial domains. Our approach enables users to estimate the reliability of a\ndefect detection and segmentation model predictions and assess the\napplicability of the model to their specific datasets, providing valuable\ninformation about possible domain shifts and whether the model needs to be\nfine-tuned or trained on additional data to be maximally effective for the\ndesired use case.\n","authors":["Ni Li","Ryan Jacobs","Matthew Lynch","Vidit Agrawal","Kevin Field","Dane Morgan"],"pdf_url":"https://arxiv.org/pdf/2501.08465v1.pdf","comment":"14 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2501.08460v1","updated":"2025-01-14T22:09:06Z","published":"2025-01-14T22:09:06Z","title":"Towards Zero-Shot & Explainable Video Description by Reasoning over\n  Graphs of Events in Space and Time","summary":"  In the current era of Machine Learning, Transformers have become the de facto\napproach across a variety of domains, such as computer vision and natural\nlanguage processing. Transformer-based solutions are the backbone of current\nstate-of-the-art methods for language generation, image and video\nclassification, segmentation, action and object recognition, among many others.\nInterestingly enough, while these state-of-the-art methods produce impressive\nresults in their respective domains, the problem of understanding the\nrelationship between vision and language is still beyond our reach. In this\nwork, we propose a common ground between vision and language based on events in\nspace and time in an explainable and programmatic way, to connect\nlearning-based vision and language state of the art models and provide a\nsolution to the long standing problem of describing videos in natural language.\nWe validate that our algorithmic approach is able to generate coherent, rich\nand relevant textual descriptions on videos collected from a variety of\ndatasets, using both standard metrics (e.g. Bleu, ROUGE) and the modern\nLLM-as-a-Jury approach.\n","authors":["Mihai Masala","Marius Leordeanu"],"pdf_url":"https://arxiv.org/pdf/2501.08460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.09349v4","updated":"2025-01-14T22:05:06Z","published":"2023-06-15T17:59:59Z","title":"UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video","summary":"  We present UrbanIR (Urban Scene Inverse Rendering), a new inverse graphics\nmodel that enables realistic, free-viewpoint renderings of scenes under various\nlighting conditions with a single video. It accurately infers shape, albedo,\nvisibility, and sun and sky illumination from wide-baseline videos, such as\nthose from car-mounted cameras, differing from NeRF's dense view settings. In\nthis context, standard methods often yield subpar geometry and material\nestimates, such as inaccurate roof representations and numerous 'floaters'.\nUrbanIR addresses these issues with novel losses that reduce errors in inverse\ngraphics inference and rendering artifacts. Its techniques allow for precise\nshadow volume estimation in the original scene. The model's outputs support\ncontrollable editing, enabling photorealistic free-viewpoint renderings of\nnight simulations, relit scenes, and inserted objects, marking a significant\nimprovement over existing state-of-the-art methods.\n","authors":["Chih-Hao Lin","Bohan Liu","Yi-Ting Chen","Kuan-Sheng Chen","David Forsyth","Jia-Bin Huang","Anand Bhattad","Shenlong Wang"],"pdf_url":"https://arxiv.org/pdf/2306.09349v4.pdf","comment":"https://urbaninverserendering.github.io/"},{"id":"http://arxiv.org/abs/2501.08458v1","updated":"2025-01-14T22:03:00Z","published":"2025-01-14T22:03:00Z","title":"RWKV-UNet: Improving UNet with Long-Range Cooperation for Effective\n  Medical Image Segmentation","summary":"  In recent years, there have been significant advancements in deep learning\nfor medical image analysis, especially with convolutional neural networks\n(CNNs) and transformer models. However, CNNs face limitations in capturing\nlong-range dependencies while transformers suffer high computational\ncomplexities. To address this, we propose RWKV-UNet, a novel model that\nintegrates the RWKV (Receptance Weighted Key Value) structure into the U-Net\narchitecture. This integration enhances the model's ability to capture\nlong-range dependencies and improve contextual understanding, which is crucial\nfor accurate medical image segmentation. We build a strong encoder with\ndeveloped inverted residual RWKV (IR-RWKV) blocks combining CNNs and RWKVs. We\nalso propose a Cross-Channel Mix (CCM) module to improve skip connections with\nmulti-scale feature fusion, achieving global channel information integration.\nExperiments on benchmark datasets, including Synapse, ACDC, BUSI, CVC-ClinicDB,\nCVC-ColonDB, Kvasir-SEG, ISIC 2017 and GLAS show that RWKV-UNet achieves\nstate-of-the-art performance on various types of medical image segmentation.\nAdditionally, smaller variants, RWKV-UNet-S and RWKV-UNet-T, balance accuracy\nand computational efficiency, making them suitable for broader clinical\napplications.\n","authors":["Juntao Jiang","Jiangning Zhang","Weixuan Liu","Muxuan Gao","Xiaobin Hu","Xiaoxiao Yan","Feiyue Huang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08453v1","updated":"2025-01-14T21:53:11Z","published":"2025-01-14T21:53:11Z","title":"Vchitect-2.0: Parallel Transformer for Scaling Up Video Diffusion Models","summary":"  We present Vchitect-2.0, a parallel transformer architecture designed to\nscale up video diffusion models for large-scale text-to-video generation. The\noverall Vchitect-2.0 system has several key designs. (1) By introducing a novel\nMultimodal Diffusion Block, our approach achieves consistent alignment between\ntext descriptions and generated video frames, while maintaining temporal\ncoherence across sequences. (2) To overcome memory and computational\nbottlenecks, we propose a Memory-efficient Training framework that incorporates\nhybrid parallelism and other memory reduction techniques, enabling efficient\ntraining of long video sequences on distributed systems. (3) Additionally, our\nenhanced data processing pipeline ensures the creation of Vchitect T2V\nDataVerse, a high-quality million-scale training dataset through rigorous\nannotation and aesthetic evaluation. Extensive benchmarking demonstrates that\nVchitect-2.0 outperforms existing methods in video quality, training\nefficiency, and scalability, serving as a suitable base for high-fidelity video\ngeneration.\n","authors":["Weichen Fan","Chenyang Si","Junhao Song","Zhenyu Yang","Yinan He","Long Zhuo","Ziqi Huang","Ziyue Dong","Jingwen He","Dongwei Pan","Yi Wang","Yuming Jiang","Yaohui Wang","Peng Gao","Xinyuan Chen","Hengjie Li","Dahua Lin","Yu Qiao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16689v3","updated":"2025-01-14T21:37:31Z","published":"2024-03-25T12:23:39Z","title":"SYNAPSE: SYmbolic Neural-Aided Preference Synthesis Engine","summary":"  This paper addresses the problem of preference learning, which aims to align\nrobot behaviors through learning user specific preferences (e.g. \"good\npull-over location\") from visual demonstrations. Despite its similarity to\nlearning factual concepts (e.g. \"red door\"), preference learning is a\nfundamentally harder problem due to its subjective nature and the paucity of\nperson-specific training data. We address this problem using a novel framework\ncalled SYNAPSE, which is a neuro-symbolic approach designed to efficiently\nlearn preferential concepts from limited data. SYNAPSE represents preferences\nas neuro-symbolic programs, facilitating inspection of individual parts for\nalignment, in a domain-specific language (DSL) that operates over images and\nleverages a novel combination of visual parsing, large language models, and\nprogram synthesis to learn programs representing individual preferences. We\nperform extensive evaluations on various preferential concepts as well as user\ncase studies demonstrating its ability to align well with dissimilar user\npreferences. Our method significantly outperforms baselines, especially when it\ncomes to out of distribution generalization. We show the importance of the\ndesign choices in the framework through multiple ablation studies. Code,\nadditional results, and supplementary material can be found on the website:\nhttps://amrl.cs.utexas.edu/synapse\n","authors":["Sadanand Modak","Noah Patton","Isil Dillig","Joydeep Biswas"],"pdf_url":"https://arxiv.org/pdf/2403.16689v3.pdf","comment":"Accepted (oral) at AAAI 25"},{"id":"http://arxiv.org/abs/2501.08446v1","updated":"2025-01-14T21:34:34Z","published":"2025-01-14T21:34:34Z","title":"Poseidon: A ViT-based Architecture for Multi-Frame Pose Estimation with\n  Adaptive Frame Weighting and Multi-Scale Feature Fusion","summary":"  Human pose estimation, a vital task in computer vision, involves detecting\nand localising human joints in images and videos. While single-frame pose\nestimation has seen significant progress, it often fails to capture the\ntemporal dynamics for understanding complex, continuous movements. We propose\nPoseidon, a novel multi-frame pose estimation architecture that extends the\nViTPose model by integrating temporal information for enhanced accuracy and\nrobustness to address these limitations. Poseidon introduces key innovations:\n(1) an Adaptive Frame Weighting (AFW) mechanism that dynamically prioritises\nframes based on their relevance, ensuring that the model focuses on the most\ninformative data; (2) a Multi-Scale Feature Fusion (MSFF) module that\naggregates features from different backbone layers to capture both fine-grained\ndetails and high-level semantics; and (3) a Cross-Attention module for\neffective information exchange between central and contextual frames, enhancing\nthe model's temporal coherence. The proposed architecture improves performance\nin complex video scenarios and offers scalability and computational efficiency\nsuitable for real-world applications. Our approach achieves state-of-the-art\nperformance on the PoseTrack21 and PoseTrack18 datasets, achieving mAP scores\nof 88.3 and 87.8, respectively, outperforming existing methods.\n","authors":["Cesare Davide Pace","Alessandro Marco De Nunzio","Claudio De Stefano","Francesco Fontanella","Mario Molinara"],"pdf_url":"https://arxiv.org/pdf/2501.08446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07298v3","updated":"2025-01-14T21:26:13Z","published":"2024-10-09T17:07:34Z","title":"Enhancing Performance of Point Cloud Completion Networks with\n  Consistency Loss","summary":"  Point cloud completion networks are conventionally trained to minimize the\ndisparities between the completed point cloud and the ground-truth counterpart.\nHowever, an incomplete object-level point cloud can have multiple valid\ncompletion solutions when it is examined in isolation. This one-to-many mapping\nissue can cause contradictory supervision signals to the network because the\nloss function may produce different values for identical input-output pairs of\nthe network. In many cases, this issue could adversely affect the network\noptimization process. In this work, we propose to enhance the conventional\nlearning objective using a novel completion consistency loss to mitigate the\none-to-many mapping problem. Specifically, the proposed consistency loss ensure\nthat a point cloud completion network generates a coherent completion solution\nfor incomplete objects originating from the same source point cloud.\nExperimental results across multiple well-established datasets and benchmarks\ndemonstrated the proposed completion consistency loss have excellent capability\nto enhance the completion performance of various existing networks without any\nmodification to the design of the networks. The proposed consistency loss\nenhances the performance of the point completion network without affecting the\ninference speed, thereby increasing the accuracy of point cloud completion.\nNotably, a state-of-the-art point completion network trained with the proposed\nconsistency loss can achieve state-of-the-art accuracy on the challenging new\nMVP dataset. The code and result of experiment various point completion models\nusing proposed consistency loss will be available at:\nhttps://github.com/kaist-avelab/ConsistencyLoss .\n","authors":["Kevin Tirta Wijaya","Christofel Rio Goenawan","Seung-Hyun Kong"],"pdf_url":"https://arxiv.org/pdf/2410.07298v3.pdf","comment":"First version of Paper \"Enhancing Performance of Point Cloud\n  Completion Networks with Consistency Loss\" by Kevin Tirta Wijaya and\n  Christofel Rio Goenawan. In process submission to Neurocomputing Journal 2024"},{"id":"http://arxiv.org/abs/2501.08440v1","updated":"2025-01-14T21:08:08Z","published":"2025-01-14T21:08:08Z","title":"FARE: A Deep Learning-Based Framework for Radar-based Face Recognition\n  and Out-of-distribution Detection","summary":"  In this work, we propose a novel pipeline for face recognition and\nout-of-distribution (OOD) detection using short-range FMCW radar. The proposed\nsystem utilizes Range-Doppler and micro Range-Doppler Images. The architecture\nfeatures a primary path (PP) responsible for the classification of\nin-distribution (ID) faces, complemented by intermediate paths (IPs) dedicated\nto OOD detection. The network is trained in two stages: first, the PP is\ntrained using triplet loss to optimize ID face classification. In the second\nstage, the PP is frozen, and the IPs-comprising simple linear autoencoder\nnetworks-are trained specifically for OOD detection. Using our dataset\ngenerated with a 60 GHz FMCW radar, our method achieves an ID classification\naccuracy of 99.30% and an OOD detection AUROC of 96.91%.\n","authors":["Sabri Mustafa Kahya","Boran Hamdi Sivrikaya","Muhammet Sami Yavuz","Eckehard Steinbach"],"pdf_url":"https://arxiv.org/pdf/2501.08440v1.pdf","comment":"Accepted at ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.07015v2","updated":"2025-01-14T21:02:31Z","published":"2025-01-13T02:28:13Z","title":"SplatMAP: Online Dense Monocular SLAM with 3D Gaussian Splatting","summary":"  Achieving high-fidelity 3D reconstruction from monocular video remains\nchallenging due to the inherent limitations of traditional methods like\nStructure-from-Motion (SfM) and monocular SLAM in accurately capturing scene\ndetails. While differentiable rendering techniques such as Neural Radiance\nFields (NeRF) address some of these challenges, their high computational costs\nmake them unsuitable for real-time applications. Additionally, existing 3D\nGaussian Splatting (3DGS) methods often focus on photometric consistency,\nneglecting geometric accuracy and failing to exploit SLAM's dynamic depth and\npose updates for scene refinement. We propose a framework integrating dense\nSLAM with 3DGS for real-time, high-fidelity dense reconstruction. Our approach\nintroduces SLAM-Informed Adaptive Densification, which dynamically updates and\ndensifies the Gaussian model by leveraging dense point clouds from SLAM.\nAdditionally, we incorporate Geometry-Guided Optimization, which combines\nedge-aware geometric constraints and photometric consistency to jointly\noptimize the appearance and geometry of the 3DGS scene representation, enabling\ndetailed and accurate SLAM mapping reconstruction. Experiments on the Replica\nand TUM-RGBD datasets demonstrate the effectiveness of our approach, achieving\nstate-of-the-art results among monocular systems. Specifically, our method\nachieves a PSNR of 36.864, SSIM of 0.985, and LPIPS of 0.040 on Replica,\nrepresenting improvements of 10.7%, 6.4%, and 49.4%, respectively, over the\nprevious SOTA. On TUM-RGBD, our method outperforms the closest baseline by\n10.2%, 6.6%, and 34.7% in the same metrics. These results highlight the\npotential of our framework in bridging the gap between photometric and\ngeometric dense 3D scene representations, paving the way for practical and\nefficient monocular dense reconstruction.\n","authors":["Yue Hu","Rong Liu","Meida Chen","Peter Beerel","Andrew Feng"],"pdf_url":"https://arxiv.org/pdf/2501.07015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08415v1","updated":"2025-01-14T20:12:09Z","published":"2025-01-14T20:12:09Z","title":"Cross-Modal Transferable Image-to-Video Attack on Video Quality Metrics","summary":"  Recent studies have revealed that modern image and video quality assessment\n(IQA/VQA) metrics are vulnerable to adversarial attacks. An attacker can\nmanipulate a video through preprocessing to artificially increase its quality\nscore according to a certain metric, despite no actual improvement in visual\nquality. Most of the attacks studied in the literature are white-box attacks,\nwhile black-box attacks in the context of VQA have received less attention.\nMoreover, some research indicates a lack of transferability of adversarial\nexamples generated for one model to another when applied to VQA. In this paper,\nwe propose a cross-modal attack method, IC2VQA, aimed at exploring the\nvulnerabilities of modern VQA models. This approach is motivated by the\nobservation that the low-level feature spaces of images and videos are similar.\nWe investigate the transferability of adversarial perturbations across\ndifferent modalities; specifically, we analyze how adversarial perturbations\ngenerated on a white-box IQA model with an additional CLIP module can\neffectively target a VQA model. The addition of the CLIP module serves as a\nvaluable aid in increasing transferability, as the CLIP model is known for its\neffective capture of low-level semantics. Extensive experiments demonstrate\nthat IC2VQA achieves a high success rate in attacking three black-box VQA\nmodels. We compare our method with existing black-box attack strategies,\nhighlighting its superiority in terms of attack success within the same number\nof iterations and levels of attack strength. We believe that the proposed\nmethod will contribute to the deeper analysis of robust VQA metrics.\n","authors":["Georgii Gotin","Ekaterina Shumitskaya","Anastasia Antsiferova","Dmitriy Vatolin"],"pdf_url":"https://arxiv.org/pdf/2501.08415v1.pdf","comment":"Accepted for VISAPP 2025"},{"id":"http://arxiv.org/abs/2501.08411v1","updated":"2025-01-14T19:59:59Z","published":"2025-01-14T19:59:59Z","title":"BiDepth Multimodal Neural Network: Bidirectional Depth Deep Learning\n  Arcitecture for Spatial-Temporal Prediction","summary":"  Accurate prediction of spatial-temporal (ST) information in dynamic systems,\nsuch as urban mobility and weather patterns, is a crucial yet challenging\nproblem. The complexity stems from the intricate interplay between spatial\nproximity and temporal relevance, where both long-term trends and short-term\nfluctuations are present in convoluted patterns. Existing approaches, including\ntraditional statistical methods and conventional neural networks, may provide\ninaccurate results due to the lack of an effective mechanism that\nsimultaneously incorporates information at variable temporal depths while\nmaintaining spatial context, resulting in a trade-off between comprehensive\nlong-term historical analysis and responsiveness to short-term new information.\nTo bridge this gap, this paper proposes the BiDepth Multimodal Neural Network\n(BDMNN) with bidirectional depth modulation that enables a comprehensive\nunderstanding of both long-term seasonality and short-term fluctuations,\nadapting to the complex ST context. Case studies with real-world public data\ndemonstrate significant improvements in prediction accuracy, with a 12%\nreduction in Mean Squared Error for urban traffic prediction and a 15%\nimprovement in rain precipitation forecasting compared to state-of-the-art\nbenchmarks, without demanding extra computational resources.\n","authors":["Sina Ehsani","Fenglian Pan","Qingpei Hu","Jian Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08411v1.pdf","comment":"This paper has been submitted to Applied Intelligence for review"},{"id":"http://arxiv.org/abs/2501.08408v1","updated":"2025-01-14T19:56:43Z","published":"2025-01-14T19:56:43Z","title":"Leveraging 2D Masked Reconstruction for Domain Adaptation of 3D Pose\n  Estimation","summary":"  RGB-based 3D pose estimation methods have been successful with the\ndevelopment of deep learning and the emergence of high-quality 3D pose\ndatasets. However, most existing methods do not operate well for testing images\nwhose distribution is far from that of training data. However, most existing\nmethods do not operate well for testing images whose distribution is far from\nthat of training data. This problem might be alleviated by involving diverse\ndata during training, however it is non-trivial to collect such diverse data\nwith corresponding labels (i.e. 3D pose). In this paper, we introduced an\nunsupervised domain adaptation framework for 3D pose estimation that utilizes\nthe unlabeled data in addition to labeled data via masked image modeling (MIM)\nframework. Foreground-centric reconstruction and attention regularization are\nfurther proposed to increase the effectiveness of unlabeled data usage.\nExperiments are conducted on the various datasets in human and hand pose\nestimation tasks, especially using the cross-domain scenario. We demonstrated\nthe effectiveness of ours by achieving the state-of-the-art accuracy on all\ndatasets.\n","authors":["Hansoo Park","Chanwoo Kim","Jihyeon Kim","Hoseong Cho","Nhat Nguyen Bao Truong","Taehwan Kim","Seungryul Baek"],"pdf_url":"https://arxiv.org/pdf/2501.08408v1.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.04809v2","updated":"2025-01-14T19:42:28Z","published":"2024-08-09T01:40:12Z","title":"On the Geometry of Deep Learning","summary":"  In this paper, we overview one promising avenue of progress at the\nmathematical foundation of deep learning: the connection between deep networks\nand function approximation by affine splines (continuous piecewise linear\nfunctions in multiple dimensions). In particular, we will overview work over\nthe past decade on understanding certain geometrical properties of a deep\nnetwork's affine spline mapping, in particular how it tessellates its input\nspace. As we will see, the affine spline connection and geometrical viewpoint\nprovide a powerful portal through which to view, analyze, and improve the inner\nworkings of a deep network.\n","authors":["Randall Balestriero","Ahmed Imtiaz Humayun","Richard Baraniuk"],"pdf_url":"https://arxiv.org/pdf/2408.04809v2.pdf","comment":"Accepted for publication at 'Notices of the American Mathematical\n  Society'"},{"id":"http://arxiv.org/abs/2501.08370v1","updated":"2025-01-14T18:40:33Z","published":"2025-01-14T18:40:33Z","title":"3D Gaussian Splatting with Normal Information for Mesh Extraction and\n  Improved Rendering","summary":"  Differentiable 3D Gaussian splatting has emerged as an efficient and flexible\nrendering technique for representing complex scenes from a collection of 2D\nviews and enabling high-quality real-time novel-view synthesis. However, its\nreliance on photometric losses can lead to imprecisely reconstructed geometry\nand extracted meshes, especially in regions with high curvature or fine detail.\nWe propose a novel regularization method using the gradients of a signed\ndistance function estimated from the Gaussians, to improve the quality of\nrendering while also extracting a surface mesh. The regularizing normal\nsupervision facilitates better rendering and mesh reconstruction, which is\ncrucial for downstream applications in video generation, animation, AR-VR and\ngaming. We demonstrate the effectiveness of our approach on datasets such as\nMip-NeRF360, Tanks and Temples, and Deep-Blending. Our method scores higher on\nphotorealism metrics compared to other mesh extracting rendering methods\nwithout compromising mesh quality.\n","authors":["Meenakshi Krishnan","Liam Fowl","Ramani Duraiswami"],"pdf_url":"https://arxiv.org/pdf/2501.08370v1.pdf","comment":"ICASSP 2025: Workshop on Generative Data Augmentation for Real-World\n  Signal Processing Applications"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.11646v3","updated":"2025-01-14T18:34:12Z","published":"2024-08-21T14:17:24Z","title":"Mathematical Information Retrieval: Search and Question Answering","summary":"  Mathematical information is essential for technical work, but its creation,\ninterpretation, and search are challenging. To help address these challenges,\nresearchers have developed multimodal search engines and mathematical question\nanswering systems. This book begins with a simple framework characterizing the\ninformation tasks that people and systems perform as we work to answer\nmath-related questions. The framework is used to organize and relate the other\ncore topics of the book, including interactions between people and systems,\nrepresenting math formulas in sources, and evaluation. We close by addressing\nsome key questions and presenting directions for future work. This book is\nintended for students, instructors, and researchers interested in systems that\nhelp us find and use mathematical information.\n","authors":["Richard Zanibbi","Behrooz Mansouri","Anurag Agarwal"],"pdf_url":"https://arxiv.org/pdf/2408.11646v3.pdf","comment":"[DRAFT] Revised (3rd) draft"},{"id":"http://arxiv.org/abs/2501.08267v1","updated":"2025-01-14T17:29:41Z","published":"2025-01-14T17:29:41Z","title":"TriMod Fusion for Multimodal Named Entity Recognition in Social Media","summary":"  Social media platforms serve as invaluable sources of user-generated content,\noffering insights into various aspects of human behavior. Named Entity\nRecognition (NER) plays a crucial role in analyzing such content by identifying\nand categorizing named entities into predefined classes. However, traditional\nNER models often struggle with the informal, contextually sparse, and ambiguous\nnature of social media language. To address these challenges, recent research\nhas focused on multimodal approaches that leverage both textual and visual cues\nfor enhanced entity recognition. Despite advances, existing methods face\nlimitations in capturing nuanced mappings between visual objects and textual\nentities and addressing distributional disparities between modalities. In this\npaper, we propose a novel approach that integrates textual, visual, and hashtag\nfeatures (TriMod), utilizing Transformer-attention for effective modality\nfusion. The improvements exhibited by our model suggest that named entities can\ngreatly benefit from the auxiliary context provided by multiple modalities,\nenabling more accurate recognition. Through the experiments on a multimodal\nsocial media dataset, we demonstrate the superiority of our approach over\nexisting state-of-the-art methods, achieving significant improvements in\nprecision, recall, and F1 score.\n","authors":["Mosab Alfaqeeh"],"pdf_url":"https://arxiv.org/pdf/2501.08267v1.pdf","comment":"Accepted at CASCON"},{"id":"http://arxiv.org/abs/2501.08248v1","updated":"2025-01-14T16:38:33Z","published":"2025-01-14T16:38:33Z","title":"Eliciting In-context Retrieval and Reasoning for Long-context Large\n  Language Models","summary":"  Recent advancements in long-context language models (LCLMs) promise to\ntransform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With\ntheir expanded context windows, LCLMs can process entire knowledge bases and\nperform retrieval and reasoning directly -- a capability we define as\nIn-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like\nLOFT often overestimate LCLM performance by providing overly simplified\ncontexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs\nin more realistic scenarios by including confounding passages retrieved with\nstrong retrievers. We then propose three methods to enhance LCLM performance:\n(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which\nuses attention heads to filter and de-noise long contexts during decoding, and\n(3) joint retrieval head training alongside the generation head. Our evaluation\nof five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with\nour best approach applied to Mistral-7B: +17 and +15 points by Exact Match on\nLOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised\nfine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks\ndespite being a much smaller model.\n","authors":["Yifu Qiu","Varun Embar","Yizhe Zhang","Navdeep Jaitly","Shay B. Cohen","Benjamin Han"],"pdf_url":"https://arxiv.org/pdf/2501.08248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16793v2","updated":"2025-01-14T08:47:17Z","published":"2024-09-25T10:14:01Z","title":"Spacewalker: Traversing Representation Spaces for Fast Interactive\n  Exploration and Annotation of Unstructured Data","summary":"  In industries such as healthcare, finance, and manufacturing, analysis of\nunstructured textual data presents significant challenges for analysis and\ndecision making. Uncovering patterns within large-scale corpora and\nunderstanding their semantic impact is critical, but depends on domain experts\nor resource-intensive manual reviews. In response, we introduce Spacewalker in\nthis system demonstration paper, an interactive tool designed to analyze,\nexplore, and annotate data across multiple modalities. It allows users to\nextract data representations, visualize them in low-dimensional spaces and\ntraverse large datasets either exploratory or by querying regions of interest.\nWe evaluated Spacewalker through extensive experiments and annotation studies,\nassessing its efficacy in improving data integrity verification and annotation.\nWe show that Spacewalker reduces time and effort compared to traditional\nmethods. The code of this work is open-source and can be found at:\nhttps://github.com/code-lukas/Spacewalker\n","authors":["Lukas Heine","Fabian Hörst","Jana Fragemann","Gijs Luijten","Jan Egger","Fin Bahnsen","M. Saquib Sarfraz","Jens Kleesiek","Constantin Seibold"],"pdf_url":"https://arxiv.org/pdf/2409.16793v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07793v1","updated":"2025-01-14T02:27:06Z","published":"2025-01-14T02:27:06Z","title":"Unsupervised Query Routing for Retrieval Augmented Generation","summary":"  Query routing for retrieval-augmented generation aims to assign an input\nquery to the most suitable search engine. Existing works rely heavily on\nsupervised datasets that require extensive manual annotation, resulting in high\ncosts and limited scalability, as well as poor generalization to\nout-of-distribution scenarios. To address these challenges, we introduce a\nnovel unsupervised method that constructs the \"upper-bound\" response to\nevaluate the quality of retrieval-augmented responses. This evaluation enables\nthe decision of the most suitable search engine for a given query. By\neliminating manual annotations, our approach can automatically process\nlarge-scale real user queries and create training data. We conduct extensive\nexperiments across five datasets, demonstrating that our method significantly\nenhances scalability and generalization capabilities.\n","authors":["Feiteng Mu","Liwen Zhang","Yong Jiang","Wenjie Li","Zhen Zhang","Pengjun Xie","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2501.07793v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2501.08330v1","updated":"2025-01-14T18:59:09Z","published":"2025-01-14T18:59:09Z","title":"Gradient Equilibrium in Online Learning: Theory and Applications","summary":"  We present a new perspective on online learning that we refer to as gradient\nequilibrium: a sequence of iterates achieves gradient equilibrium if the\naverage of gradients of losses along the sequence converges to zero. In\ngeneral, this condition is not implied by nor implies sublinear regret. It\nturns out that gradient equilibrium is achievable by standard online learning\nmethods such as gradient descent and mirror descent with constant step sizes\n(rather than decaying step sizes, as is usually required for no regret).\nFurther, as we show through examples, gradient equilibrium translates into an\ninterpretable and meaningful property in online prediction problems spanning\nregression, classification, quantile estimation, and others. Notably, we show\nthat the gradient equilibrium framework can be used to develop a debiasing\nscheme for black-box predictions under arbitrary distribution shift, based on\nsimple post hoc online descent updates. We also show that post hoc gradient\nupdates can be used to calibrate predicted quantiles under distribution shift,\nand that the framework leads to unbiased Elo scores for pairwise preference\nprediction.\n","authors":["Anastasios N. Angelopoulos","Michael I. Jordan","Ryan J. Tibshirani"],"pdf_url":"https://arxiv.org/pdf/2501.08330v1.pdf","comment":"Code available at\n  https://github.com/aangelopoulos/gradient-equilibrium/"},{"id":"http://arxiv.org/abs/2501.08317v1","updated":"2025-01-14T18:52:27Z","published":"2025-01-14T18:52:27Z","title":"A Similarity Measure Between Functions with Applications to Statistical\n  Learning and Optimization","summary":"  In this note, we present a novel measure of similarity between two functions.\nIt quantifies how the sub-optimality gaps of two functions convert to each\nother, and unifies several existing notions of functional similarity. We show\nthat it has convenient operation rules, and illustrate its use in empirical\nrisk minimization and non-stationary online optimization.\n","authors":["Chengpiao Huang","Kaizheng Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08317v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2501.08316v1","updated":"2025-01-14T18:51:48Z","published":"2025-01-14T18:51:48Z","title":"Diffusion Adversarial Post-Training for One-Step Video Generation","summary":"  The diffusion models are widely used for image and video generation, but\ntheir iterative generation process is slow and expansive. While existing\ndistillation approaches have demonstrated the potential for one-step generation\nin the image domain, they still suffer from significant quality degradation. In\nthis work, we propose Adversarial Post-Training (APT) against real data\nfollowing diffusion pre-training for one-step video generation. To improve the\ntraining stability and quality, we introduce several improvements to the model\narchitecture and training procedures, along with an approximated R1\nregularization objective. Empirically, our experiments show that our\nadversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720,\n24fps videos in real time using a single forward evaluation step. Additionally,\nour model is capable of generating 1024px images in a single step, achieving\nquality comparable to state-of-the-art methods.\n","authors":["Shanchuan Lin","Xin Xia","Yuxi Ren","Ceyuan Yang","Xuefeng Xiao","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2501.08316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07169v3","updated":"2025-01-14T18:51:43Z","published":"2024-12-10T04:03:46Z","title":"Rate-In: Information-Driven Adaptive Dropout Rates for Improved\n  Inference-Time Uncertainty Estimation","summary":"  Accurate uncertainty estimation is crucial for deploying neural networks in\nrisk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a\nwidely used technique for approximating predictive uncertainty by performing\nstochastic forward passes with dropout during inference. However, using static\ndropout rates across all layers and inputs can lead to suboptimal uncertainty\nestimates, as it fails to adapt to the varying characteristics of individual\ninputs and network layers. Existing approaches optimize dropout rates during\ntraining using labeled data, resulting in fixed inference-time parameters that\ncannot adjust to new data distributions, compromising uncertainty estimates in\nMonte Carlo simulations.\n  In this paper, we propose Rate-In, an algorithm that dynamically adjusts\ndropout rates during inference by quantifying the information loss induced by\ndropout in each layer's feature maps. By treating dropout as controlled noise\ninjection and leveraging information-theoretic principles, Rate-In adapts\ndropout rates per layer and per input instance without requiring ground truth\nlabels. By quantifying the functional information loss in feature maps, we\nadaptively tune dropout rates to maintain perceptual quality across diverse\nmedical imaging tasks and architectural configurations. Our extensive empirical\nstudy on synthetic data and real-world medical imaging tasks demonstrates that\nRate-In improves calibration and sharpens uncertainty estimates compared to\nfixed or heuristic dropout rates without compromising predictive performance.\nRate-In offers a practical, unsupervised, inference-time approach to optimizing\ndropout for more reliable predictive uncertainty estimation in critical\napplications.\n","authors":["Tal Zeevi","Ravid Shwartz-Ziv","Yann LeCun","Lawrence H. Staib","John A. Onofrey"],"pdf_url":"https://arxiv.org/pdf/2412.07169v3.pdf","comment":"Updated author affiliation"},{"id":"http://arxiv.org/abs/2501.08306v1","updated":"2025-01-14T18:44:35Z","published":"2025-01-14T18:44:35Z","title":"Path Loss Prediction Using Machine Learning with Extended Features","summary":"  Wireless communications rely on path loss modeling, which is most effective\nwhen it includes the physical details of the propagation environment. Acquiring\nthis data has historically been challenging, but geographic information system\ndata is becoming increasingly available with higher resolution and accuracy.\nAccess to such details enables propagation models to more accurately predict\ncoverage and minimize interference in wireless deployments. Machine\nlearning-based modeling can significantly support this effort, with\nfeature-based approaches allowing for accurate, efficient, and scalable\npropagation modeling. Building on previous work, we introduce an extended set\nof features that improves prediction accuracy while, most importantly,\nmaintaining model generalization across a broad range of environments.\n","authors":["Jonathan Ethier","Mathieu Chateauvert","Ryan G. Dempsey","Alexis Bose"],"pdf_url":"https://arxiv.org/pdf/2501.08306v1.pdf","comment":"4 pages, 4 figures, conference paper"},{"id":"http://arxiv.org/abs/2501.08305v1","updated":"2025-01-14T18:41:15Z","published":"2025-01-14T18:41:15Z","title":"Benchmarking Graph Representations and Graph Neural Networks for\n  Multivariate Time Series Classification","summary":"  Multivariate Time Series Classification (MTSC) enables the analysis if\ncomplex temporal data, and thus serves as a cornerstone in various real-world\napplications, ranging from healthcare to finance. Since the relationship among\nvariables in MTS usually contain crucial cues, a large number of graph-based\nMTSC approaches have been proposed, as the graph topology and edges can\nexplicitly represent relationships among variables (channels), where not only\nvarious MTS graph representation learning strategies but also different Graph\nNeural Networks (GNNs) have been explored. Despite such progresses, there is no\ncomprehensive study that fairly benchmarks and investigates the performances of\nexisting widely-used graph representation learning strategies/GNN classifiers\nin the application of different MTSC tasks. In this paper, we present the first\nbenchmark which systematically investigates the effectiveness of the\nwidely-used three node feature definition strategies, four edge feature\nlearning strategies and five GNN architecture, resulting in 60 different\nvariants for graph-based MTSC. These variants are developed and evaluated with\na standardized data pipeline and training/validation/testing strategy on 26\nwidely-used suspensor MTSC datasets. Our experiments highlight that node\nfeatures significantly influence MTSC performance, while the visualization of\nedge features illustrates why adaptive edge learning outperforms other edge\nfeature learning methods. The code of the proposed benchmark is publicly\navailable at\n\\url{https://github.com/CVI-yangwn/Benchmark-GNN-for-Multivariate-Time-Series-Classification}.\n","authors":["Wennuo Yang","Shiling Wu","Yuzhi Zhou","Weicheng Xie","Linlin Shen","Siyang Song"],"pdf_url":"https://arxiv.org/pdf/2501.08305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08297v1","updated":"2025-01-14T18:28:08Z","published":"2025-01-14T18:28:08Z","title":"Polynomial Threshold Functions of Bounded Tree-Width: Some\n  Explainability and Complexity Aspects","summary":"  The tree-width of a multivariate polynomial is the tree-width of the\nhypergraph with hyperedges corresponding to its terms. Multivariate polynomials\nof bounded tree-width have been studied by Makowsky and Meer as a new sparsity\ncondition that allows for polynomial solvability of problems which are\nintractable in general. We consider a variation on this theme for Boolean\nvariables. A representation of a Boolean function as the sign of a polynomial\nis called a polynomial threshold representation. We discuss Boolean functions\nrepresentable as polynomial threshold functions of bounded tree-width and\npresent two applications to Bayesian network classifiers, a probabilistic\ngraphical model. Both applications are in Explainable Artificial Intelligence\n(XAI), the research area dealing with the black-box nature of many recent\nmachine learning models. We also give a separation result between the\nrepresentational power of positive and general polynomial threshold functions.\n","authors":["Karine Chubarian","Johnny Joyce","Gyorgy Turan"],"pdf_url":"https://arxiv.org/pdf/2501.08297v1.pdf","comment":"22 pages, 3 figures. To be published in Festschrift in honor of\n  Johann A. Makowsky"},{"id":"http://arxiv.org/abs/2501.08288v1","updated":"2025-01-14T18:08:52Z","published":"2025-01-14T18:08:52Z","title":"Avoiding subtraction and division of stochastic signals using\n  normalizing flows: NFdeconvolve","summary":"  Across the scientific realm, we find ourselves subtracting or dividing\nstochastic signals. For instance, consider a stochastic realization, $x$,\ngenerated from the addition or multiplication of two stochastic signals $a$ and\n$b$, namely $x=a+b$ or $x = ab$. For the $x=a+b$ example, $a$ can be\nfluorescence background and $b$ the signal of interest whose statistics are to\nbe learned from the measured $x$. Similarly, when writing $x=ab$, $a$ can be\nthought of as the illumination intensity and $b$ the density of fluorescent\nmolecules of interest. Yet dividing or subtracting stochastic signals amplifies\nnoise, and we ask instead whether, using the statistics of $a$ and the\nmeasurement of $x$ as input, we can recover the statistics of $b$. Here, we\nshow how normalizing flows can generate an approximation of the probability\ndistribution over $b$, thereby avoiding subtraction or division altogether.\nThis method is implemented in our software package, NFdeconvolve, available on\nGitHub with a tutorial linked in the main text.\n","authors":["Pedro Pessoa","Max Schweiger","Lance W. Q. Xu","Tristan Manha","Ayush Saurabh","Julian Antolin Camarena","Steve Pressé"],"pdf_url":"https://arxiv.org/pdf/2501.08288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08285v1","updated":"2025-01-14T18:00:41Z","published":"2025-01-14T18:00:41Z","title":"Can Bayesian Neural Networks Explicitly Model Input Uncertainty?","summary":"  Inputs to machine learning models can have associated noise or uncertainties,\nbut they are often ignored and not modelled. It is unknown if Bayesian Neural\nNetworks and their approximations are able to consider uncertainty in their\ninputs. In this paper we build a two input Bayesian Neural Network (mean and\nstandard deviation) and evaluate its capabilities for input uncertainty\nestimation across different methods like Ensembles, MC-Dropout, and Flipout.\nOur results indicate that only some uncertainty estimation methods for\napproximate Bayesian NNs can model input uncertainty, in particular Ensembles\nand Flipout.\n","authors":["Matias Valdenegro-Toro","Marco Zullich"],"pdf_url":"https://arxiv.org/pdf/2501.08285v1.pdf","comment":"12 pages, 11 figures, VISAPP 2025 camera ready"},{"id":"http://arxiv.org/abs/2501.08281v1","updated":"2025-01-14T17:57:26Z","published":"2025-01-14T17:57:26Z","title":"Decoding Interpretable Logic Rules from Neural Networks","summary":"  As deep neural networks continue to excel across various domains, their\nblack-box nature has raised concerns about transparency and trust. In\nparticular, interpretability has become increasingly essential for applications\nthat demand high safety and knowledge rigor, such as drug discovery, autonomous\ndriving, and genomics. However, progress in understanding even the simplest\ndeep neural networks - such as fully connected networks - has been limited,\ndespite their role as foundational elements in state-of-the-art models like\nResNet and Transformer. In this paper, we address this challenge by introducing\nNeuroLogic, a novel approach for decoding interpretable logic rules from neural\nnetworks. NeuroLogic leverages neural activation patterns to capture the\nmodel's critical decision-making processes, translating them into logical rules\nrepresented by hidden predicates. Thanks to its flexible design in the\ngrounding phase, NeuroLogic can be adapted to a wide range of neural networks.\nFor simple fully connected neural networks, hidden predicates can be grounded\nin certain split patterns of original input features to derive\ndecision-tree-like rules. For large, complex vision neural networks, NeuroLogic\ngrounds hidden predicates into high-level visual concepts that are\nunderstandable to humans. Our empirical study demonstrates that NeuroLogic can\nextract global and interpretable rules from state-of-the-art models such as\nResNet, a task at which existing work struggles. We believe NeuroLogic can help\npave the way for understanding the black-box nature of neural networks.\n","authors":["Chuqin Geng","Xiaojie Xu","Zhaoyue Wang","Ziyu Zhao","Xujie Si"],"pdf_url":"https://arxiv.org/pdf/2501.08281v1.pdf","comment":"23 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.06993v2","updated":"2025-01-14T17:52:40Z","published":"2024-10-09T15:40:04Z","title":"Efficient Distribution Matching of Representations via Noise-Injected\n  Deep InfoMax","summary":"  Deep InfoMax (DIM) is a well-established method for self-supervised\nrepresentation learning (SSRL) based on maximization of the mutual information\nbetween the input and the output of a deep neural network encoder. Despite the\nDIM and contrastive SSRL in general being well-explored, the task of learning\nrepresentations conforming to a specific distribution (i.e., distribution\nmatching, DM) is still under-addressed. Motivated by the importance of DM to\nseveral downstream tasks (including generative modeling, disentanglement,\noutliers detection and other), we enhance DIM to enable automatic matching of\nlearned representations to a selected prior distribution. To achieve this, we\npropose injecting an independent noise into the normalized outputs of the\nencoder, while keeping the same InfoMax training objective. We show that such\nmodification allows for learning uniformly and normally distributed\nrepresentations, as well as representations of other absolutely continuous\ndistributions. Our approach is tested on various downstream tasks. The results\nindicate a moderate trade-off between the performance on the downstream tasks\nand quality of DM.\n","authors":["Ivan Butakov","Alexander Semenenko","Alexander Tolmachev","Andrey Gladkov","Marina Munkhoeva","Alexey Frolov"],"pdf_url":"https://arxiv.org/pdf/2410.06993v2.pdf","comment":"25 pages, 7 fugures"},{"id":"http://arxiv.org/abs/2501.08266v1","updated":"2025-01-14T17:26:02Z","published":"2025-01-14T17:26:02Z","title":"AI Driven Water Segmentation with deep learning models for Enhanced\n  Flood Monitoring","summary":"  Flooding is a major natural hazard causing significant fatalities and\neconomic losses annually, with increasing frequency due to climate change.\nRapid and accurate flood detection and monitoring are crucial for mitigating\nthese impacts. This study compares the performance of three deep learning\nmodels UNet, ResNet, and DeepLabv3 for pixelwise water segmentation to aid in\nflood detection, utilizing images from drones, in field observations, and\nsocial media. This study involves creating a new dataset that augments\nwellknown benchmark datasets with flood-specific images, enhancing the\nrobustness of the models. The UNet, ResNet, and DeepLab v3 architectures are\ntested to determine their effectiveness in various environmental conditions and\ngeographical locations, and the strengths and limitations of each model are\nalso discussed here, providing insights into their applicability in different\nscenarios by predicting image segmentation masks. This fully automated approach\nallows these models to isolate flooded areas in images, significantly reducing\nprocessing time compared to traditional semi-automated methods. The outcome of\nthis study is to predict segmented masks for each image effected by a flood\ndisaster and the validation accuracy of these models. This methodology\nfacilitates timely and continuous flood monitoring, providing vital data for\nemergency response teams to reduce loss of life and economic damages. It offers\na significant reduction in the time required to generate flood maps, cutting\ndown the manual processing time. Additionally, we present avenues for future\nresearch, including the integration of multimodal data sources and the\ndevelopment of robust deep learning architectures tailored specifically for\nflood detection tasks. Overall, our work contributes to the advancement of\nflood management strategies through innovative use of deep learning\ntechnologies.\n","authors":["Sanjida Afrin Mou","Tasfia Noor Chowdhury","Adib Ibn Mannan","Sadia Nourin Mim","Lubana Tarannum","Tasrin Noman","Jamal Uddin Ahamed"],"pdf_url":"https://arxiv.org/pdf/2501.08266v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.08263v1","updated":"2025-01-14T17:23:14Z","published":"2025-01-14T17:23:14Z","title":"Multiplayer Federated Learning: Reaching Equilibrium with Less\n  Communication","summary":"  Traditional Federated Learning (FL) approaches assume collaborative clients\nwith aligned objectives working towards a shared global model. However, in many\nreal-world scenarios, clients act as rational players with individual\nobjectives and strategic behaviors, a concept that existing FL frameworks are\nnot equipped to adequately address. To bridge this gap, we introduce\nMultiplayer Federated Learning (MpFL), a novel framework that models the\nclients in the FL environment as players in a game-theoretic context, aiming to\nreach an equilibrium. In this scenario, each player tries to optimize their own\nutility function, which may not align with the collective goal. Within MpFL, we\npropose Per-Player Local Stochastic Gradient Descent (PEARL-SGD), an algorithm\nin which each player/client performs local updates independently and\nperiodically communicates with other players. We theoretically analyze\nPEARL-SGD and prove that it reaches a neighborhood of equilibrium with less\ncommunication in the stochastic setup compared to its non-local counterpart.\nFinally, we verify our theoretical findings through numerical experiments.\n","authors":["TaeHo Yoon","Sayantan Choudhury","Nicolas Loizou"],"pdf_url":"https://arxiv.org/pdf/2501.08263v1.pdf","comment":"43 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.02748v3","updated":"2025-01-14T17:20:04Z","published":"2024-10-03T17:57:01Z","title":"CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation","summary":"  Existing automatic prompt engineering methods are typically designed for\ndiscriminative tasks, where new task prompts are iteratively refined with\nlimited feedback from a single metric reflecting a single aspect. However,\nthese approaches are suboptimal for generative tasks, which require more\nnuanced guidance beyond a single numeric metric to improve the prompt and\noptimize multiple aspects of the generated text. To address these challenges,\nwe propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt\nOptimization (CriSPO) approach. CriSPO introduces a critique-suggestion module\nas its core component. This module spontaneously discovers aspects, and\ncompares generated and reference texts across these aspects, providing specific\nsuggestions for prompt modification. These clear critiques and actionable\nsuggestions guide a receptive optimizer module to make more substantial\nchanges, exploring a broader and more effective search space. To further\nimprove CriSPO with multi-metric optimization, we introduce an Automatic Suffix\nTuning (AST) extension to enhance the performance of task prompts across\nmultiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4\nsummarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE score\nimprovement on summarization and substantial improvement of various metrics on\nQA. Code available at https://github.com/amazon-science/crispo\n","authors":["Han He","Qianchu Liu","Lei Xu","Chaitanya Shivade","Yi Zhang","Sundararajan Srinivasan","Katrin Kirchhoff"],"pdf_url":"https://arxiv.org/pdf/2410.02748v3.pdf","comment":"Accepted to AAAI-2025"},{"id":"http://arxiv.org/abs/2501.08259v1","updated":"2025-01-14T17:15:27Z","published":"2025-01-14T17:15:27Z","title":"FDPP: Fine-tune Diffusion Policy with Human Preference","summary":"  Imitation learning from human demonstrations enables robots to perform\ncomplex manipulation tasks and has recently witnessed huge success. However,\nthese techniques often struggle to adapt behavior to new preferences or changes\nin the environment. To address these limitations, we propose Fine-tuning\nDiffusion Policy with Human Preference (FDPP). FDPP learns a reward function\nthrough preference-based learning. This reward is then used to fine-tune the\npre-trained policy with reinforcement learning (RL), resulting in alignment of\npre-trained policy with new human preferences while still solving the original\ntask. Our experiments across various robotic tasks and preferences demonstrate\nthat FDPP effectively customizes policy behavior without compromising\nperformance. Additionally, we show that incorporating Kullback-Leibler (KL)\nregularization during fine-tuning prevents over-fitting and helps maintain the\ncompetencies of the initial policy.\n","authors":["Yuxin Chen","Devesh K. Jha","Masayoshi Tomizuka","Diego Romeres"],"pdf_url":"https://arxiv.org/pdf/2501.08259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06457v2","updated":"2025-01-14T16:58:26Z","published":"2025-01-11T07:09:57Z","title":"Automated Detection and Analysis of Minor Deformations in Flat Walls Due\n  to Railway Vibrations Using LiDAR and Machine Learning","summary":"  This study introduces an advanced methodology for automatically identifying\nminor deformations in flat walls caused by vibrations from nearby railway\ntracks. It leverages high-density Terrestrial Laser Scanner (TLS) LiDAR surveys\nand AI/ML techniques to collect and analyze data. The scan data is processed\ninto a detailed point cloud, which is segmented to distinguish ground points,\ntrees, buildings, and other objects. The analysis focuses on identifying\nsections along flat walls and estimating their deformations relative to the\nground orientation.\n  Findings from the study, conducted at the RGIPT campus, reveal significant\ndeformations in walls close to the railway corridor, with the highest\ndeformations ranging from 7 to 8 cm and an average of 3 to 4 cm. In contrast,\nwalls further from the corridor show negligible deformations. The developed\nautomated process for feature extraction and deformation monitoring\ndemonstrates potential for structural health monitoring. By integrating LiDAR\ndata with machine learning, the methodology provides an efficient system for\nidentifying and analyzing structural deformations, highlighting the importance\nof continuous monitoring for ensuring structural integrity and public safety in\nurban infrastructure. This approach represents a substantial advancement in\nautomated feature extraction and deformation analysis, contributing to more\neffective management of urban infrastructure.\n","authors":["Surjo Dey","Ankit Sharma","Hritu Raj","Susham Biswas"],"pdf_url":"https://arxiv.org/pdf/2501.06457v2.pdf","comment":"I am requesting the withdrawal of my paper due to the need for\n  significant revisions to ensure the accuracy and integrity of the presented\n  findings"},{"id":"http://arxiv.org/abs/2410.18803v2","updated":"2025-01-14T16:54:54Z","published":"2024-10-24T14:52:21Z","title":"Language-Agnostic Modeling of Source Reliability on Wikipedia","summary":"  Over the last few years, content verification through reliable sources has\nbecome a fundamental need to combat disinformation. Here, we present a\nlanguage-agnostic model designed to assess the reliability of sources across\nmultiple language editions of Wikipedia. Utilizing editorial activity data, the\nmodel evaluates source reliability within different articles of varying\ncontroversiality such as Climate Change, COVID-19, History, Media, and Biology\ntopics. Crafting features that express domain usage across articles, the model\neffectively predicts source reliability, achieving an F1 Macro score of\napproximately 0.80 for English and other high-resource languages. For\nmid-resource languages, we achieve 0.65 while the performance of low-resource\nlanguages varies; in all cases, the time the domain remains present in the\narticles (which we dub as permanence) is one of the most predictive features.\nWe highlight the challenge of maintaining consistent model performance across\nlanguages of varying resource levels and demonstrate that adapting models from\nhigher-resource languages can improve performance. This work contributes not\nonly to Wikipedia's efforts in ensuring content verifiability but in ensuring\nreliability across diverse user-generated content in various language\ncommunities.\n","authors":["Jacopo D'Ignazi","Andreas Kaltenbrunner","Yelena Mejova","Michele Tizzani","Kyriaki Kalimeri","Mariano Beiró","Pablo Aragón"],"pdf_url":"https://arxiv.org/pdf/2410.18803v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08248v1","updated":"2025-01-14T16:38:33Z","published":"2025-01-14T16:38:33Z","title":"Eliciting In-context Retrieval and Reasoning for Long-context Large\n  Language Models","summary":"  Recent advancements in long-context language models (LCLMs) promise to\ntransform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With\ntheir expanded context windows, LCLMs can process entire knowledge bases and\nperform retrieval and reasoning directly -- a capability we define as\nIn-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like\nLOFT often overestimate LCLM performance by providing overly simplified\ncontexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs\nin more realistic scenarios by including confounding passages retrieved with\nstrong retrievers. We then propose three methods to enhance LCLM performance:\n(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which\nuses attention heads to filter and de-noise long contexts during decoding, and\n(3) joint retrieval head training alongside the generation head. Our evaluation\nof five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with\nour best approach applied to Mistral-7B: +17 and +15 points by Exact Match on\nLOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised\nfine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks\ndespite being a much smaller model.\n","authors":["Yifu Qiu","Varun Embar","Yizhe Zhang","Navdeep Jaitly","Shay B. Cohen","Benjamin Han"],"pdf_url":"https://arxiv.org/pdf/2501.08248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08246v1","updated":"2025-01-14T16:32:01Z","published":"2025-01-14T16:32:01Z","title":"Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful\n  Behaviors with Proximity Constraints","summary":"  Recent work has proposed automated red-teaming methods for testing the\nvulnerabilities of a given target large language model (LLM). These methods use\nred-teaming LLMs to uncover inputs that induce harmful behavior in a target\nLLM. In this paper, we study red-teaming strategies that enable a targeted\nsecurity assessment. We propose an optimization framework for red-teaming with\nproximity constraints, where the discovered prompts must be similar to\nreference prompts from a given dataset. This dataset serves as a template for\nthe discovered prompts, anchoring the search for test-cases to specific topics,\nwriting styles, or types of harmful behavior. We show that established\nauto-regressive model architectures do not perform well in this setting. We\ntherefore introduce a black-box red-teaming method inspired by text-diffusion\nmodels: Diffusion for Auditing and Red-Teaming (DART). DART modifies the\nreference prompt by perturbing it in the embedding space, directly controlling\nthe amount of change introduced. We systematically evaluate our method by\ncomparing its effectiveness with established methods based on model fine-tuning\nand zero- and few-shot prompting. Our results show that DART is significantly\nmore effective at discovering harmful inputs in close proximity to the\nreference prompt.\n","authors":["Jonathan Nöther","Adish Singla","Goran Radanović"],"pdf_url":"https://arxiv.org/pdf/2501.08246v1.pdf","comment":"This is an extended version of a paper published at AAAI 25"},{"id":"http://arxiv.org/abs/2501.08245v1","updated":"2025-01-14T16:31:01Z","published":"2025-01-14T16:31:01Z","title":"Continual Deep Active Learning for Medical Imaging: Replay-Base\n  Architecture for Context Adaptation","summary":"  Deep Learning for medical imaging faces challenges in adapting and\ngeneralizing to new contexts. Additionally, it often lacks sufficient labeled\ndata for specific tasks requiring significant annotation effort. Continual\nLearning (CL) tackles adaptability and generalizability by enabling lifelong\nlearning from a data stream while mitigating forgetting of previously learned\nknowledge. Active Learning (AL) reduces the number of required annotations for\neffective training. This work explores both approaches (CAL) to develop a novel\nframework for robust medical image analysis. Based on the automatic recognition\nof shifts in image characteristics, Replay-Base Architecture for Context\nAdaptation (RBACA) employs a CL rehearsal method to continually learn from\ndiverse contexts, and an AL component to select the most informative instances\nfor annotation. A novel approach to evaluate CAL methods is established using a\ndefined metric denominated IL-Score, which allows for the simultaneous\nassessment of transfer learning, forgetting, and final model performance. We\nshow that RBACA works in domain and class-incremental learning scenarios, by\nassessing its IL-Score on the segmentation and diagnosis of cardiac images. The\nresults show that RBACA outperforms a baseline framework without CAL, and a\nstate-of-the-art CAL method across various memory sizes and annotation budgets.\nOur code is available in https://github.com/RuiDaniel/RBACA .\n","authors":["Rui Daniel","M. Rita Verdelho","Catarina Barata","Carlos Santiago"],"pdf_url":"https://arxiv.org/pdf/2501.08245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08243v1","updated":"2025-01-14T16:30:10Z","published":"2025-01-14T16:30:10Z","title":"Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps","summary":"  Cloud Operations (CloudOps) is a rapidly growing field focused on the\nautomated management and optimization of cloud infrastructure which is\nessential for organizations navigating increasingly complex cloud environments.\nMontyCloud Inc. is one of the major companies in the CloudOps domain that\nleverages autonomous bots to manage cloud compliance, security, and continuous\noperations. To make the platform more accessible and effective to the\ncustomers, we leveraged the use of GenAI.\n  Developing a GenAI-based solution for autonomous CloudOps for the existing\nMontyCloud system presented us with various challenges such as i) diverse data\nsources; ii) orchestration of multiple processes; and iii) handling complex\nworkflows to automate routine tasks. To this end, we developed MOYA, a\nmulti-agent framework that leverages GenAI and balances autonomy with the\nnecessary human control. This framework integrates various internal and\nexternal systems and is optimized for factors like task orchestration,\nsecurity, and error mitigation while producing accurate, reliable, and relevant\ninsights by utilizing Retrieval Augmented Generation (RAG). Evaluations of our\nmulti-agent system with the help of practitioners as well as using automated\nchecks demonstrate enhanced accuracy, responsiveness, and effectiveness over\nnon-agentic approaches across complex workflows.\n","authors":["Kannan Parthasarathy","Karthik Vaidhyanathan","Rudra Dhar","Venkat Krishnamachari","Basil Muhammed","Adyansh Kakran","Sreemaee Akshathala","Shrikara Arun","Sumant Dubey","Mohan Veerubhotla","Amey Karan"],"pdf_url":"https://arxiv.org/pdf/2501.08243v1.pdf","comment":"The paper has been accepted as full paper to CAIN 2025\n  (https://conf.researchr.org/home/cain-2025), co-located with ICSE 2025\n  (https://conf.researchr.org/home/icse-2025). The paper was submitted to CAIN\n  for review on 9 November 2024"},{"id":"http://arxiv.org/abs/2501.08241v1","updated":"2025-01-14T16:28:02Z","published":"2025-01-14T16:28:02Z","title":"A Feature-Level Ensemble Model for COVID-19 Identification in CXR Images\n  using Choquet Integral and Differential Evolution Optimization","summary":"  The COVID-19 pandemic has profoundly impacted billions globally. It\nchallenges public health and healthcare systems due to its rapid spread and\nsevere respiratory effects. An effective strategy to mitigate the COVID-19\npandemic involves integrating testing to identify infected individuals. While\nRT-PCR is considered the gold standard for diagnosing COVID-19, it has some\nlimitations such as the risk of false negatives. To address this problem, this\npaper introduces a novel Deep Learning Diagnosis System that integrates\npre-trained Deep Convolutional Neural Networks (DCNNs) within an ensemble\nlearning framework to achieve precise identification of COVID-19 cases from\nChest X-ray (CXR) images. We combine feature vectors from the final hidden\nlayers of pre-trained DCNNs using the Choquet integral to capture interactions\nbetween different DCNNs that a linear approach cannot. We employed\nSugeno-$\\lambda$ measure theory to derive fuzzy measures for subsets of\nnetworks to enable aggregation. We utilized Differential Evolution to estimate\nfuzzy densities. We developed a TensorFlow-based layer for Choquet operation to\nfacilitate efficient aggregation, due to the intricacies involved in\naggregating feature vectors. Experimental results on the COVIDx dataset show\nthat our ensemble model achieved 98\\% accuracy in three-class classification\nand 99.50\\% in binary classification, outperforming its components-DenseNet-201\n(97\\% for three-class, 98.75\\% for binary), Inception-v3 (96.25\\% for\nthree-class, 98.50\\% for binary), and Xception (94.50\\% for three-class, 98\\%\nfor binary)-and surpassing many previous methods.\n","authors":["Amir Reza Takhsha","Maryam Rastgarpour","Mozhgan Naderi"],"pdf_url":"https://arxiv.org/pdf/2501.08241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08236v1","updated":"2025-01-14T16:21:54Z","published":"2025-01-14T16:21:54Z","title":"Privacy-Preserving Model and Preprocessing Verification for Machine\n  Learning","summary":"  This paper presents a framework for privacy-preserving verification of\nmachine learning models, focusing on models trained on sensitive data.\nIntegrating Local Differential Privacy (LDP) with model explanations from LIME\nand SHAP, our framework enables robust verification without compromising\nindividual privacy. It addresses two key tasks: binary classification, to\nverify if a target model was trained correctly by applying the appropriate\npreprocessing steps, and multi-class classification, to identify specific\npreprocessing errors. Evaluations on three real-world datasets-Diabetes, Adult,\nand Student Record-demonstrate that while the ML-based approach is particularly\neffective in binary tasks, the threshold-based method performs comparably in\nmulti-class tasks. Results indicate that although verification accuracy varies\nacross datasets and noise levels, the framework provides effective detection of\npreprocessing errors, strong privacy guarantees, and practical applicability\nfor safeguarding sensitive data.\n","authors":["Wenbiao Li","Anisa Halimi","Xiaoqian Jiang","Jaideep Vaidya","Erman Ayday"],"pdf_url":"https://arxiv.org/pdf/2501.08236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08234v1","updated":"2025-01-14T16:19:25Z","published":"2025-01-14T16:19:25Z","title":"Dynamic Pricing in High-Speed Railways Using Multi-Agent Reinforcement\n  Learning","summary":"  This paper addresses a critical challenge in the high-speed passenger railway\nindustry: designing effective dynamic pricing strategies in the context of\ncompeting and cooperating operators. To address this, a multi-agent\nreinforcement learning (MARL) framework based on a non-zero-sum Markov game is\nproposed, incorporating random utility models to capture passenger decision\nmaking. Unlike prior studies in areas such as energy, airlines, and mobile\nnetworks, dynamic pricing for railway systems using deep reinforcement learning\nhas received limited attention. A key contribution of this paper is a\nparametrisable and versatile reinforcement learning simulator designed to model\na variety of railway network configurations and demand patterns while enabling\nrealistic, microscopic modelling of user behaviour, called RailPricing-RL. This\nenvironment supports the proposed MARL framework, which models heterogeneous\nagents competing to maximise individual profits while fostering cooperative\nbehaviour to synchronise connecting services. Experimental results validate the\nframework, demonstrating how user preferences affect MARL performance and how\npricing policies influence passenger choices, utility, and overall system\ndynamics. This study provides a foundation for advancing dynamic pricing\nstrategies in railway systems, aligning profitability with system-wide\nefficiency, and supporting future research on optimising pricing policies.\n","authors":["Enrique Adrian Villarrubia-Martin","Luis Rodriguez-Benitez","David Muñoz-Valero","Giovanni Montana","Luis Jimenez-Linares"],"pdf_url":"https://arxiv.org/pdf/2501.08234v1.pdf","comment":"37 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.10729v2","updated":"2025-01-14T16:17:00Z","published":"2024-06-15T20:04:06Z","title":"A Comprehensive Survey of Foundation Models in Medicine","summary":"  Foundation models (FMs) are large-scale deep learning models that are\ndeveloped using large datasets and self-supervised learning methods. These\nmodels serve as a base for different downstream tasks, including healthcare.\nFMs have been adopted with great success across various domains within\nhealthcare. Existing healthcare-based surveys have not yet included all of\nthese domains. Therefore, we provide a detailed survey of FMs in healthcare. We\nfocus on the history, learning strategies, flagship models, applications, and\nchallenges of FMs. We explore how FMs such as the BERT and GPT families are\nreshaping various healthcare domains, including clinical large language models,\nmedical image analysis, and omics. Furthermore, we provide a detailed taxonomy\nof healthcare applications facilitated by FMs, such as clinical NLP, medical\ncomputer vision, graph learning, and other biology-related tasks. Despite the\npromising opportunities FMs provide, they also have several associated\nchallenges, which are explained in detail. We also outline open research issues\nand potential lessons learned to provide researchers and practitioners with\ninsights into the capabilities of FMs in healthcare to advance their deployment\nand mitigate associated risks.\n","authors":["Wasif Khan","Seowung Leem","Kyle B. See","Joshua K. Wong","Shaoting Zhang","Ruogu Fang"],"pdf_url":"https://arxiv.org/pdf/2406.10729v2.pdf","comment":"Currently under review in IEEE REVIEWS IN BIOMEDICAL ENGINEERING"},{"id":"http://arxiv.org/abs/2501.08226v1","updated":"2025-01-14T16:10:25Z","published":"2025-01-14T16:10:25Z","title":"Efficient Deep Learning-based Forward Solvers for Brain Tumor Growth\n  Models","summary":"  Glioblastoma, a highly aggressive brain tumor, poses major challenges due to\nits poor prognosis and high morbidity rates. Partial differential\nequation-based models offer promising potential to enhance therapeutic outcomes\nby simulating patient-specific tumor behavior for improved radiotherapy\nplanning. However, model calibration remains a bottleneck due to the high\ncomputational demands of optimization methods like Monte Carlo sampling and\nevolutionary algorithms. To address this, we recently introduced an approach\nleveraging a neural forward solver with gradient-based optimization to\nsignificantly reduce calibration time. This approach requires a highly accurate\nand fully differentiable forward model. We investigate multiple architectures,\nincluding (i) an enhanced TumorSurrogate, (ii) a modified nnU-Net, and (iii) a\n3D Vision Transformer (ViT). The optimized TumorSurrogate achieved the best\noverall results, excelling in both tumor outline matching and voxel-level\nprediction of tumor cell concentration. It halved the MSE relative to the\nbaseline model and achieved the highest Dice score across all tumor cell\nconcentration thresholds. Our study demonstrates significant enhancement in\nforward solver performance and outlines important future research directions.\n","authors":["Zeineb Haouari","Jonas Weidner","Ivan Ezhov","Aswathi Varma","Daniel Rueckert","Bjoern Menze","Benedikt Wiestler"],"pdf_url":"https://arxiv.org/pdf/2501.08226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06773v2","updated":"2025-01-14T16:08:28Z","published":"2025-01-12T10:43:05Z","title":"Pareto Set Learning for Multi-Objective Reinforcement Learning","summary":"  Multi-objective decision-making problems have emerged in numerous real-world\nscenarios, such as video games, navigation and robotics. Considering the clear\nadvantages of Reinforcement Learning (RL) in optimizing decision-making\nprocesses, researchers have delved into the development of Multi-Objective RL\n(MORL) methods for solving multi-objective decision problems. However, previous\nmethods either cannot obtain the entire Pareto front, or employ only a single\npolicy network for all the preferences over multiple objectives, which may not\nproduce personalized solutions for each preference. To address these\nlimitations, we propose a novel decomposition-based framework for MORL, Pareto\nSet Learning for MORL (PSL-MORL), that harnesses the generation capability of\nhypernetwork to produce the parameters of the policy network for each\ndecomposition weight, generating relatively distinct policies for various\nscalarized subproblems with high efficiency. PSL-MORL is a general framework,\nwhich is compatible for any RL algorithm. The theoretical result guarantees the\nsuperiority of the model capacity of PSL-MORL and the optimality of the\nobtained policy network. Through extensive experiments on diverse benchmarks,\nwe demonstrate the effectiveness of PSL-MORL in achieving dense coverage of the\nPareto front, significantly outperforming state-of-the-art MORL methods in the\nhypervolume and sparsity indicators.\n","authors":["Erlong Liu","Yu-Chang Wu","Xiaobin Huang","Chengrui Gao","Ren-Jian Wang","Ke Xue","Chao Qian"],"pdf_url":"https://arxiv.org/pdf/2501.06773v2.pdf","comment":"AAAI 2025 Accept"},{"id":"http://arxiv.org/abs/2501.08223v1","updated":"2025-01-14T16:06:54Z","published":"2025-01-14T16:06:54Z","title":"Big Batch Bayesian Active Learning by Considering Predictive\n  Probabilities","summary":"  We observe that BatchBALD, a popular acquisition function for batch Bayesian\nactive learning for classification, can conflate epistemic and aleatoric\nuncertainty, leading to suboptimal performance. Motivated by this observation,\nwe propose to focus on the predictive probabilities, which only exhibit\nepistemic uncertainty. The result is an acquisition function that not only\nperforms better, but is also faster to evaluate, allowing for larger batches\nthan before.\n","authors":["Sebastian W. Ober","Samuel Power","Tom Diethe","Henry B. Moss"],"pdf_url":"https://arxiv.org/pdf/2501.08223v1.pdf","comment":"7 pages, 2 figures; presented as a lightning talk at the NeurIPS\n  Workshop on Bayesian Decision-making and Uncertainty (BDU; 2024)"},{"id":"http://arxiv.org/abs/2501.08219v1","updated":"2025-01-14T16:02:33Z","published":"2025-01-14T16:02:33Z","title":"Investigating Energy Efficiency and Performance Trade-offs in LLM\n  Inference Across Tasks and DVFS Settings","summary":"  Large language models (LLMs) have shown significant improvements in many\nnatural language processing (NLP) tasks, accelerating their rapid adoption\nacross many industries. These models are resource-intensive, requiring\nextensive computational resources both during training and inference, leading\nto increased energy consumption and negative environmental impact. As their\nadoption accelerates, the sustainability of LLMs has become a critical issue,\nnecessitating strategies to optimize their runtime efficiency without\ncompromising performance. Hence, it is imperative to identify the parameters\nthat significantly influence the performance and energy efficiency of LLMs. To\nthat end, in this work, we investigate the effect of important parameters on\nthe performance and energy efficiency of LLMs during inference and examine\ntheir trade-offs.\n  First, we analyze how different types of models with varying numbers of\nparameters and architectures perform on tasks like text generation, question\nanswering, and summarization by benchmarking LLMs such as Falcon-7B,\nMistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study\ninput and output sequence characteristics such as sequence length concerning\nenergy consumption, performance, and throughput. Finally, we explore the impact\nof hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency\nScaling (DVFS), on the models' latency and energy efficiency. Our extensive\nbenchmarking and statistical analysis reveal many interesting findings,\nuncovering how specific optimizations can reduce energy consumption while\nmaintaining throughput and accuracy. This study provides actionable insights\nfor researchers and practitioners to design energy-efficient LLM inference\nsystems.\n","authors":["Paul Joe Maliakel","Shashikant Ilager","Ivona Brandic"],"pdf_url":"https://arxiv.org/pdf/2501.08219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08205v1","updated":"2025-01-14T15:45:27Z","published":"2025-01-14T15:45:27Z","title":"Modeling Feature Maps for Quantum Machine Learning","summary":"  Quantum Machine Learning (QML) offers significant potential for complex tasks\nlike genome sequence classification, but quantum noise on Noisy\nIntermediate-Scale Quantum (NISQ) devices poses practical challenges. This\nstudy systematically evaluates how various quantum noise models including\ndephasing, amplitude damping, depolarizing, thermal noise, bit-flip, and\nphase-flip affect key QML algorithms (QSVC, Peg-QSVC, QNN, VQC) and feature\nmapping techniques (ZFeatureMap, ZZFeatureMap, and PauliFeatureMap). Results\nindicate that QSVC is notably robust under noise, whereas Peg-QSVC and QNN are\nmore sensitive, particularly to depolarizing and amplitude-damping noise. The\nPauliFeatureMap is especially vulnerable, highlighting difficulties in\nmaintaining accurate classification under noisy conditions. These findings\nunderscore the critical importance of feature map selection and noise\nmitigation strategies in optimizing QML for genomic classification, with\npromising implications for personalized medicine.\n","authors":["Navneet Singh","Shiva Raj Pokhrel"],"pdf_url":"https://arxiv.org/pdf/2501.08205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08202v1","updated":"2025-01-14T15:37:03Z","published":"2025-01-14T15:37:03Z","title":"Data-driven system identification using quadratic embeddings of\n  nonlinear dynamics","summary":"  We propose a novel data-driven method called QENDy (Quadratic Embedding of\nNonlinear Dynamics) that not only allows us to learn quadratic representations\nof highly nonlinear dynamical systems, but also to identify the governing\nequations. The approach is based on an embedding of the system into a\nhigher-dimensional feature space in which the dynamics become quadratic. Just\nlike SINDy (Sparse Identification of Nonlinear Dynamics), our method requires\ntrajectory data, time derivatives for the training data points, which can also\nbe estimated using finite difference approximations, and a set of preselected\nbasis functions, called dictionary. We illustrate the efficacy and accuracy of\nQENDy with the aid of various benchmark problems and compare its performance\nwith SINDy and a deep learning method for identifying quadratic embeddings.\nFurthermore, we analyze the convergence of QENDy and SINDy in the infinite data\nlimit, highlight their similarities and main differences, and compare the\nquadratic embedding with linearization techniques based on the Koopman\noperator.\n","authors":["Stefan Klus","Joel-Pascal N'Konzi"],"pdf_url":"https://arxiv.org/pdf/2501.08202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08201v1","updated":"2025-01-14T15:36:32Z","published":"2025-01-14T15:36:32Z","title":"Globally Convergent Variational Inference","summary":"  In variational inference (VI), an approximation of the posterior distribution\nis selected from a family of distributions through numerical optimization. With\nthe most common variational objective function, known as the evidence lower\nbound (ELBO), only convergence to a local optimum can be guaranteed. In this\nwork, we instead establish the global convergence of a particular VI method.\nThis VI method, which may be considered an instance of neural posterior\nestimation (NPE), minimizes an expectation of the inclusive (forward) KL\ndivergence to fit a variational distribution that is parameterized by a neural\nnetwork. Our convergence result relies on the neural tangent kernel (NTK) to\ncharacterize the gradient dynamics that arise from considering the variational\nobjective in function space. In the asymptotic regime of a fixed,\npositive-definite neural tangent kernel, we establish conditions under which\nthe variational objective admits a unique solution in a reproducing kernel\nHilbert space (RKHS). Then, we show that the gradient descent dynamics in\nfunction space converge to this unique function. In ablation studies and\npractical problems, we demonstrate that our results explain the behavior of NPE\nin non-asymptotic finite-neuron settings, and show that NPE outperforms\nELBO-based optimization, which often converges to shallow local optima.\n","authors":["Declan McNamara","Jackson Loper","Jeffrey Regier"],"pdf_url":"https://arxiv.org/pdf/2501.08201v1.pdf","comment":"Accepted to the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2501.08200v1","updated":"2025-01-14T15:27:01Z","published":"2025-01-14T15:27:01Z","title":"CWEval: Outcome-driven Evaluation on Functionality and Security of LLM\n  Code Generation","summary":"  Large Language Models (LLMs) have significantly aided developers by\ngenerating or assisting in code writing, enhancing productivity across various\ntasks. While identifying incorrect code is often straightforward, detecting\nvulnerabilities in functionally correct code is more challenging, especially\nfor developers with limited security knowledge, which poses considerable\nsecurity risks of using LLM-generated code and underscores the need for robust\nevaluation benchmarks that assess both functional correctness and security.\nCurrent benchmarks like CyberSecEval and SecurityEval attempt to solve it but\nare hindered by unclear and impractical specifications, failing to assess both\nfunctionality and security accurately. To tackle these deficiencies, we\nintroduce CWEval, a novel outcome-driven evaluation framework designed to\nenhance the evaluation of secure code generation by LLMs. This framework not\nonly assesses code functionality but also its security simultaneously with\nhigh-quality task specifications and outcome-driven test oracles which provides\nhigh accuracy. Coupled with CWEval-bench, a multilingual, security-critical\ncoding benchmark, CWEval provides a rigorous empirical security evaluation on\nLLM-generated code, overcoming previous benchmarks' shortcomings. Through our\nevaluations, CWEval reveals a notable portion of functional but insecure code\nproduced by LLMs, and shows a serious inaccuracy of previous evaluations,\nultimately contributing significantly to the field of secure code generation.\nWe open-source our artifact at: https://github.com/Co1lin/CWEval .\n","authors":["Jinjun Peng","Leyi Cui","Kele Huang","Junfeng Yang","Baishakhi Ray"],"pdf_url":"https://arxiv.org/pdf/2501.08200v1.pdf","comment":"to be published in LLM4Code 2025"},{"id":"http://arxiv.org/abs/2501.08195v1","updated":"2025-01-14T15:18:28Z","published":"2025-01-14T15:18:28Z","title":"Self-supervised Deep Hyperspectral Inpainting with the Plug and Play and\n  Deep Image Prior Models","summary":"  Hyperspectral images are typically composed of hundreds of narrow and\ncontiguous spectral bands, each containing information regarding the material\ncomposition of the imaged scene. However, these images can be affected by\nvarious sources of noise, distortions, or data loss, which can significantly\ndegrade their quality and usefulness. This paper introduces a convergent\nguaranteed algorithm, LRS-PnP-DIP(1-Lip), which successfully addresses the\ninstability issue of DHP that has been reported before. The proposed algorithm\nextends the successful joint low-rank and sparse model to further exploit the\nunderlying data structures beyond the conventional and sometimes restrictive\nunions of subspace models. A stability analysis guarantees the convergence of\nthe proposed algorithm under mild assumptions , which is crucial for its\napplication in real-world scenarios. Extensive experiments demonstrate that the\nproposed solution consistently delivers visually and quantitatively superior\ninpainting results, establishing state-of-the-art performance.\n","authors":["Shuo Li","Mehrdad Yaghoobi"],"pdf_url":"https://arxiv.org/pdf/2501.08195v1.pdf","comment":"31 pages, 9 Figures, 7 Tables. arXiv admin note: text overlap with\n  arXiv:2306.08128"},{"id":"http://arxiv.org/abs/2501.08193v1","updated":"2025-01-14T15:14:26Z","published":"2025-01-14T15:14:26Z","title":"Modeling Quantum Machine Learning for Genomic Data Analysis","summary":"  Quantum Machine Learning (QML) continues to evolve, unlocking new\nopportunities for diverse applications. In this study, we investigate and\nevaluate the applicability of QML models for binary classification of genome\nsequence data by employing various feature mapping techniques. We present an\nopen-source, independent Qiskit-based implementation to conduct experiments on\na benchmark genomic dataset. Our simulations reveal that the interplay between\nfeature mapping techniques and QML algorithms significantly influences\nperformance. Notably, the Pegasos Quantum Support Vector Classifier\n(Pegasos-QSVC) exhibits high sensitivity, particularly excelling in recall\nmetrics, while Quantum Neural Networks (QNN) achieve the highest training\naccuracy across all feature maps. However, the pronounced variability in\nclassifier performance, dependent on feature mapping, highlights the risk of\noverfitting to localized output distributions in certain scenarios. This work\nunderscores the transformative potential of QML for genomic data classification\nwhile emphasizing the need for continued advancements to enhance the robustness\nand accuracy of these methodologies.\n","authors":["Navneet Singh","Shiva Raj Pokhrel"],"pdf_url":"https://arxiv.org/pdf/2501.08193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08188v1","updated":"2025-01-14T15:13:00Z","published":"2025-01-14T15:13:00Z","title":"A Critical Synthesis of Uncertainty Quantification and Foundation Models\n  in Monocular Depth Estimation","summary":"  While recent foundation models have enabled significant breakthroughs in\nmonocular depth estimation, a clear path towards safe and reliable deployment\nin the real-world remains elusive. Metric depth estimation, which involves\npredicting absolute distances, poses particular challenges, as even the most\nadvanced foundation models remain prone to critical errors. Since quantifying\nthe uncertainty has emerged as a promising endeavor to address these\nlimitations and enable trustworthy deployment, we fuse five different\nuncertainty quantification methods with the current state-of-the-art\nDepthAnythingV2 foundation model. To cover a wide range of metric depth\ndomains, we evaluate their performance on four diverse datasets. Our findings\nidentify fine-tuning with the Gaussian Negative Log-Likelihood Loss (GNLL) as a\nparticularly promising approach, offering reliable uncertainty estimates while\nmaintaining predictive performance and computational efficiency on par with the\nbaseline, encompassing both training and inference time. By fusing uncertainty\nquantification and foundation models within the context of monocular depth\nestimation, this paper lays a critical foundation for future research aimed at\nimproving not only model performance but also its explainability. Extending\nthis critical synthesis of uncertainty quantification and foundation models\ninto other crucial tasks, such as semantic segmentation and pose estimation,\npresents exciting opportunities for safer and more reliable machine vision\nsystems.\n","authors":["Steven Landgraf","Rongjun Qin","Markus Ulrich"],"pdf_url":"https://arxiv.org/pdf/2501.08188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08187v1","updated":"2025-01-14T15:12:19Z","published":"2025-01-14T15:12:19Z","title":"A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction\n  Following","summary":"  Large language models excel at interpreting complex natural language\ninstructions, enabling them to perform a wide range of tasks. In the life\nsciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language\nof cellular biology\", capturing intricate gene expression patterns at the\nsingle-cell level. However, interacting with this \"language\" through\nconventional tools is often inefficient and unintuitive, posing challenges for\nresearchers. To address these limitations, we present InstructCell, a\nmulti-modal AI copilot that leverages natural language as a medium for more\ndirect and flexible single-cell analysis. We construct a comprehensive\nmulti-modal instruction dataset that pairs text-based instructions with\nscRNA-seq profiles from diverse tissues and species. Building on this, we\ndevelop a multi-modal cell language architecture capable of simultaneously\ninterpreting and processing both modalities. InstructCell empowers researchers\nto accomplish critical tasks-such as cell type annotation, conditional\npseudo-cell generation, and drug sensitivity prediction-using straightforward\nnatural language commands. Extensive evaluations demonstrate that InstructCell\nconsistently meets or exceeds the performance of existing single-cell\nfoundation models, while adapting to diverse experimental conditions. More\nimportantly, InstructCell provides an accessible and intuitive tool for\nexploring complex single-cell data, lowering technical barriers and enabling\ndeeper biological insights.\n","authors":["Yin Fang","Xinle Deng","Kangwei Liu","Ningyu Zhang","Jingyang Qian","Penghui Yang","Xiaohui Fan","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2501.08187v1.pdf","comment":"37 pages; 13 figures; Code: https://github.com/zjunlp/Instructcell;\n  Models: https://huggingface.co/zjunlp/Instructcell-chat,\n  https://huggingface.co/zjunlp/InstructCell-instruct"},{"id":"http://arxiv.org/abs/2501.08180v1","updated":"2025-01-14T15:03:53Z","published":"2025-01-14T15:03:53Z","title":"D$^2$-DPM: Dual Denoising for Quantized Diffusion Probabilistic Models","summary":"  Diffusion models have achieved cutting-edge performance in image generation.\nHowever, their lengthy denoising process and computationally intensive score\nestimation network impede their scalability in low-latency and\nresource-constrained scenarios. Post-training quantization (PTQ) compresses and\naccelerates diffusion models without retraining, but it inevitably introduces\nadditional quantization noise, resulting in mean and variance deviations. In\nthis work, we propose D2-DPM, a dual denoising mechanism aimed at precisely\nmitigating the adverse effects of quantization noise on the noise estimation\nnetwork. Specifically, we first unravel the impact of quantization noise on the\nsampling equation into two components: the mean deviation and the variance\ndeviation. The mean deviation alters the drift coefficient of the sampling\nequation, influencing the trajectory trend, while the variance deviation\nmagnifies the diffusion coefficient, impacting the convergence of the sampling\ntrajectory. The proposed D2-DPM is thus devised to denoise the quantization\nnoise at each time step, and then denoise the noisy sample through the inverse\ndiffusion iterations. Experimental results demonstrate that D2-DPM achieves\nsuperior generation quality, yielding a 1.42 lower FID than the full-precision\nmodel while achieving 3.99x compression and 11.67x bit-operation acceleration.\n","authors":["Qian Zeng","Jie Song","Han Zheng","Hao Jiang","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2501.08180v1.pdf","comment":"9 pages, 4 figures, acceptted by AAAI2025"},{"id":"http://arxiv.org/abs/2411.19835v2","updated":"2025-01-14T14:53:10Z","published":"2024-11-29T16:45:25Z","title":"Feedback-driven object detection and iterative model improvement","summary":"  Automated object detection has become increasingly valuable across diverse\napplications, yet efficient, high-quality annotation remains a persistent\nchallenge. In this paper, we present the development and evaluation of a\nplatform designed to interactively improve object detection models. The\nplatform allows uploading and annotating images as well as fine-tuning object\ndetection models. Users can then manually review and refine annotations,\nfurther creating improved snapshots that are used for automatic object\ndetection on subsequent image uploads - a process we refer to as semi-automatic\nannotation resulting in a significant gain in annotation efficiency.\n  Whereas iterative refinement of model results to speed up annotation has\nbecome common practice, we are the first to quantitatively evaluate its\nbenefits with respect to time, effort, and interaction savings. Our\nexperimental results show clear evidence for a significant time reduction of up\nto 53% for semi-automatic compared to manual annotation. Importantly, these\nefficiency gains did not compromise annotation quality, while matching or\noccasionally even exceeding the accuracy of manual annotations. These findings\ndemonstrate the potential of our lightweight annotation platform for creating\nhigh-quality object detection datasets and provide best practices to guide\nfuture development of annotation platforms.\n  The platform is open-source, with the frontend and backend repositories\navailable on GitHub (https://github.com/ml-lab-htw/iterative-annotate). To\nsupport the understanding of our labeling process, we have created an\nexplanatory video demonstrating the methodology using microscopy images of E.\ncoli bacteria as an example. The video is available on YouTube\n(https://www.youtube.com/watch?v=CM9uhE8NN5E).\n","authors":["Sönke Tenckhoff","Mario Koddenbrock","Erik Rodner"],"pdf_url":"https://arxiv.org/pdf/2411.19835v2.pdf","comment":"AI4EA24"},{"id":"http://arxiv.org/abs/2501.08169v1","updated":"2025-01-14T14:49:49Z","published":"2025-01-14T14:49:49Z","title":"Revolutionizing Communication with Deep Learning and XAI for Enhanced\n  Arabic Sign Language Recognition","summary":"  This study introduces an integrated approach to recognizing Arabic Sign\nLanguage (ArSL) using state-of-the-art deep learning models such as\nMobileNetV3, ResNet50, and EfficientNet-B2. These models are further enhanced\nby explainable AI (XAI) techniques to boost interpretability. The ArSL2018 and\nRGB Arabic Alphabets Sign Language (AASL) datasets are employed, with\nEfficientNet-B2 achieving peak accuracies of 99.48\\% and 98.99\\%, respectively.\nKey innovations include sophisticated data augmentation methods to mitigate\nclass imbalance, implementation of stratified 5-fold cross-validation for\nbetter generalization, and the use of Grad-CAM for clear model decision\ntransparency. The proposed system not only sets new benchmarks in recognition\naccuracy but also emphasizes interpretability, making it suitable for\napplications in healthcare, education, and inclusive communication\ntechnologies.\n","authors":["Mazen Balat","Rewaa Awaad","Ahmed B. Zaky","Salah A. Aly"],"pdf_url":"https://arxiv.org/pdf/2501.08169v1.pdf","comment":"13 pages, 25 figures, 16 tables"},{"id":"http://arxiv.org/abs/2412.13174v2","updated":"2025-01-14T14:48:32Z","published":"2024-12-17T18:53:43Z","title":"ORFormer: Occlusion-Robust Transformer for Accurate Facial Landmark\n  Detection","summary":"  Although facial landmark detection (FLD) has gained significant progress,\nexisting FLD methods still suffer from performance drops on partially\nnon-visible faces, such as faces with occlusions or under extreme lighting\nconditions or poses. To address this issue, we introduce ORFormer, a novel\ntransformer-based method that can detect non-visible regions and recover their\nmissing features from visible parts. Specifically, ORFormer associates each\nimage patch token with one additional learnable token called the messenger\ntoken. The messenger token aggregates features from all but its patch. This\nway, the consensus between a patch and other patches can be assessed by\nreferring to the similarity between its regular and messenger embeddings,\nenabling non-visible region identification. Our method then recovers occluded\npatches with features aggregated by the messenger tokens. Leveraging the\nrecovered features, ORFormer compiles high-quality heatmaps for the downstream\nFLD task. Extensive experiments show that our method generates heatmaps\nresilient to partial occlusions. By integrating the resultant heatmaps into\nexisting FLD methods, our method performs favorably against the state of the\narts on challenging datasets such as WFLW and COFW.\n","authors":["Jui-Che Chiang","Hou-Ning Hu","Bo-Syuan Hou","Chia-Yu Tseng","Yu-Lun Liu","Min-Hung Chen","Yen-Yu Lin"],"pdf_url":"https://arxiv.org/pdf/2412.13174v2.pdf","comment":"WACV 2025 Project Link: https://ben0919.github.io/ORFormer/"},{"id":"http://arxiv.org/abs/2501.08156v1","updated":"2025-01-14T14:31:45Z","published":"2025-01-14T14:31:45Z","title":"Inference-Time-Compute: More Faithful? A Research Note","summary":"  Models trained specifically to generate long Chains of Thought (CoTs) have\nrecently achieved impressive results. We refer to these models as\nInference-Time-Compute (ITC) models. Are the CoTs of ITC models more faithful\ncompared to traditional non-ITC models? We evaluate two ITC models (based on\nQwen-2.5 and Gemini-2) on an existing test of faithful CoT To measure\nfaithfulness, we test if models articulate cues in their prompt that influence\ntheir answers to MMLU questions. For example, when the cue \"A Stanford\nProfessor thinks the answer is D'\" is added to the prompt, models sometimes\nswitch their answer to D. In such cases, the Gemini ITC model articulates the\ncue 54% of the time, compared to 14% for the non-ITC Gemini.\n  We evaluate 7 types of cue, such as misleading few-shot examples and\nanchoring on past responses. ITC models articulate cues that influence them\nmuch more reliably than all the 6 non-ITC models tested, such as\nClaude-3.5-Sonnet and GPT-4o, which often articulate close to 0% of the time.\n  However, our study has important limitations. We evaluate only two ITC models\n-- we cannot evaluate OpenAI's SOTA o1 model. We also lack details about the\ntraining of these ITC models, making it hard to attribute our findings to\nspecific processes.\n  We think faithfulness of CoT is an important property for AI Safety. The ITC\nmodels we tested show a large improvement in faithfulness, which is worth\ninvestigating further. To speed up this investigation, we release these early\nresults as a research note.\n","authors":["James Chua","Owain Evans"],"pdf_url":"https://arxiv.org/pdf/2501.08156v1.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.08155v1","updated":"2025-01-14T14:29:36Z","published":"2025-01-14T14:29:36Z","title":"FairTTTS: A Tree Test Time Simulation Method for Fairness-Aware\n  Classification","summary":"  Algorithmic decision-making has become deeply ingrained in many domains, yet\nbiases in machine learning models can still produce discriminatory outcomes,\noften harming unprivileged groups. Achieving fair classification is inherently\nchallenging, requiring a careful balance between predictive performance and\nethical considerations. We present FairTTTS, a novel post-processing bias\nmitigation method inspired by the Tree Test Time Simulation (TTTS) method.\nOriginally developed to enhance accuracy and robustness against adversarial\ninputs through probabilistic decision-path adjustments, TTTS serves as the\nfoundation for FairTTTS. By building on this accuracy-enhancing technique,\nFairTTTS mitigates bias and improves predictive performance. FairTTTS uses a\ndistance-based heuristic to adjust decisions at protected attribute nodes,\nensuring fairness for unprivileged samples. This fairness-oriented adjustment\noccurs as a post-processing step, allowing FairTTTS to be applied to\npre-trained models, diverse datasets, and various fairness metrics without\nretraining. Extensive evaluation on seven benchmark datasets shows that\nFairTTTS outperforms traditional methods in fairness improvement, achieving a\n20.96% average increase over the baseline compared to 18.78% for related work,\nand further enhances accuracy by 0.55%. In contrast, competing methods\ntypically reduce accuracy by 0.42%. These results confirm that FairTTTS\neffectively promotes more equitable decision-making while simultaneously\nimproving predictive performance.\n","authors":["Nurit Cohen-Inger","Lior Rokach","Bracha Shapira","Seffi Cohen"],"pdf_url":"https://arxiv.org/pdf/2501.08155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08149v1","updated":"2025-01-14T14:25:10Z","published":"2025-01-14T14:25:10Z","title":"Multiple-Input Variational Auto-Encoder for Anomaly Detection in\n  Heterogeneous Data","summary":"  Anomaly detection (AD) plays a pivotal role in AI applications, e.g., in\nclassification, and intrusion/threat detection in cybersecurity. However, most\nexisting methods face challenges of heterogeneity amongst feature subsets posed\nby non-independent and identically distributed (non-IID) data. We propose a\nnovel neural network model called Multiple-Input Auto-Encoder for AD (MIAEAD)\nto address this. MIAEAD assigns an anomaly score to each feature subset of a\ndata sample to indicate its likelihood of being an anomaly. This is done by\nusing the reconstruction error of its sub-encoder as the anomaly score. All\nsub-encoders are then simultaneously trained using unsupervised learning to\ndetermine the anomaly scores of feature subsets. The final AUC of MIAEAD is\ncalculated for each sub-dataset, and the maximum AUC obtained among the\nsub-datasets is selected. To leverage the modelling of the distribution of\nnormal data to identify anomalies of the generative models, we develop a novel\nneural network architecture/model called Multiple-Input Variational\nAuto-Encoder (MIVAE). MIVAE can process feature subsets through its\nsub-encoders before learning distribution of normal data in the latent space.\nThis allows MIVAE to identify anomalies that deviate from the learned\ndistribution. We theoretically prove that the difference in the average anomaly\nscore between normal samples and anomalies obtained by the proposed MIVAE is\ngreater than that of the Variational Auto-Encoder (VAEAD), resulting in a\nhigher AUC for MIVAE. Extensive experiments on eight real-world anomaly\ndatasets demonstrate the superior performance of MIAEAD and MIVAE over\nconventional methods and the state-of-the-art unsupervised models, by up to 6%\nin terms of AUC score. Alternatively, MIAEAD and MIVAE have a high AUC when\napplied to feature subsets with low heterogeneity based on the coefficient of\nvariation (CV) score.\n","authors":["Phai Vu Dinh","Diep N. Nguyen","Dinh Thai Hoang","Quang Uy Nguyen","Eryk Dutkiewicz"],"pdf_url":"https://arxiv.org/pdf/2501.08149v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2210.09655v2","updated":"2025-01-14T14:22:05Z","published":"2022-10-18T07:48:59Z","title":"WINE: Wavelet-Guided GAN Inversion and Editing for High-Fidelity\n  Refinement","summary":"  Recent advanced GAN inversion models aim to convey high-fidelity information\nfrom original images to generators through methods using generator tuning or\nhigh-dimensional feature learning. Despite these efforts, accurately\nreconstructing image-specific details remains as a challenge due to the\ninherent limitations both in terms of training and structural aspects, leading\nto a bias towards low-frequency information. In this paper, we look into the\nwidely used pixel loss in GAN inversion, revealing its predominant focus on the\nreconstruction of low-frequency features. We then propose WINE, a\nWavelet-guided GAN Inversion aNd Editing model, which transfers the\nhigh-frequency information through wavelet coefficients via newly proposed\nwavelet loss and wavelet fusion scheme. Notably, WINE is the first attempt to\ninterpret GAN inversion in the frequency domain. Our experimental results\nshowcase the precision of WINE in preserving high-frequency details and\nenhancing image quality. Even in editing scenarios, WINE outperforms existing\nstate-of-the-art GAN inversion models with a fine balance between editability\nand reconstruction quality.\n","authors":["Chaewon Kim","Seung-Jun Moon","Gyeong-Moon Park"],"pdf_url":"https://arxiv.org/pdf/2210.09655v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08142v1","updated":"2025-01-14T14:21:48Z","published":"2025-01-14T14:21:48Z","title":"Bootstrapping Corner Cases: High-Resolution Inpainting for Safety\n  Critical Detect and Avoid for Automated Flying","summary":"  Modern machine learning techniques have shown tremendous potential,\nespecially for object detection on camera images. For this reason, they are\nalso used to enable safety-critical automated processes such as autonomous\ndrone flights. We present a study on object detection for Detect and Avoid, a\nsafety critical function for drones that detects air traffic during automated\nflights for safety reasons. An ill-posed problem is the generation of good and\nespecially large data sets, since detection itself is the corner case. Most\nmodels suffer from limited ground truth in raw data, \\eg recorded air traffic\nor frontal flight with a small aircraft. It often leads to poor and critical\ndetection rates. We overcome this problem by using inpainting methods to\nbootstrap the dataset such that it explicitly contains the corner cases of the\nraw data. We provide an overview of inpainting methods and generative models\nand present an example pipeline given a small annotated dataset. We validate\nour method by generating a high-resolution dataset, which we make publicly\navailable and present it to an independent object detector that was fully\ntrained on real data.\n","authors":["Jonathan Lyhs","Lars Hinneburg","Michael Fischer","Florian Ölsner","Stefan Milz","Jeremy Tschirner","Patrick Mäder"],"pdf_url":"https://arxiv.org/pdf/2501.08142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08139v1","updated":"2025-01-14T14:19:40Z","published":"2025-01-14T14:19:40Z","title":"EEG-ReMinD: Enhancing Neurodegenerative EEG Decoding through\n  Self-Supervised State Reconstruction-Primed Riemannian Dynamics","summary":"  The development of EEG decoding algorithms confronts challenges such as data\nsparsity, subject variability, and the need for precise annotations, all of\nwhich are vital for advancing brain-computer interfaces and enhancing the\ndiagnosis of diseases. To address these issues, we propose a novel two-stage\napproach named Self-Supervised State Reconstruction-Primed Riemannian Dynamics\n(EEG-ReMinD) , which mitigates reliance on supervised learning and integrates\ninherent geometric features. This approach efficiently handles EEG data\ncorruptions and reduces the dependency on labels. EEG-ReMinD utilizes\nself-supervised and geometric learning techniques, along with an attention\nmechanism, to analyze the temporal dynamics of EEG features within the\nframework of Riemannian geometry, referred to as Riemannian dynamics.\nComparative analyses on both intact and corrupted datasets from two different\nneurodegenerative disorders underscore the enhanced performance of EEG-ReMinD.\n","authors":["Zirui Wang","Zhenxi Song","Yi Guo","Yuxin Liu","Guoyang Xu","Min Zhang","Zhiguo Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.08139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08134v1","updated":"2025-01-14T14:14:22Z","published":"2025-01-14T14:14:22Z","title":"An Empirical Wall-Pressure Spectrum Model for Aeroacoustic Predictions\n  Based on Symbolic Regression","summary":"  Fast-turn around methods to predict airfoil trailing-edge noise are crucial\nfor incorporating noise limitations into design optimization loops of several\napplications. Among these aeroacoustic predictive models, Amiet's theory offers\nthe best balance between accuracy and simplicity. The accuracy of the model\nrelies heavily on precise wall-pressure spectrum predictions, which are often\nbased on single-equation formulations with adjustable parameters. These\nparameters are calibrated for particular airfoils and flow conditions and\nconsequently tend to fail when applied outside their calibration range. This\npaper introduces a new wall-pressure spectrum empirical model designed to\nenhance the robustness and accuracy of current state-of-the-art predictions\nwhile widening the range of applicability of the model to different airfoils\nand flow conditions. The model is developed using AI-based symbolic regression\nvia a genetic-algorithm-based approach, and applied to a dataset of\nwall-pressure fluctuations measured on NACA 0008 and NACA 63018 airfoils at\nmultiple angles of attack and inflow velocities, covering turbulent boundary\nlayers with both adverse and favorable pressure gradients. Validation against\nexperimental data (outside the training dataset) demonstrates the robustness of\nthe model compared to well-accepted semi-empirical models. Finally, the model\nis integrated with Amiet's theory to predict the aeroacoustic noise of a\nfull-scale wind turbine, showing good agreement with experimental measurements.\n","authors":["Laura Botero Bolívar","David Huergo","Fernanda L. dos Santos","Cornelis H. Venner","Leandro D. de Santana","Esteban Ferrer"],"pdf_url":"https://arxiv.org/pdf/2501.08134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15102v2","updated":"2025-01-14T14:07:55Z","published":"2024-11-22T18:06:14Z","title":"AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution","summary":"  The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.\n","authors":["Fengyuan Liu","Nikhil Kandpal","Colin Raffel"],"pdf_url":"https://arxiv.org/pdf/2411.15102v2.pdf","comment":"29 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.00123v3","updated":"2025-01-14T14:01:36Z","published":"2024-11-28T10:32:50Z","title":"Electricity Price Prediction Using Multi-Kernel Gaussian Process\n  Regression Combined with Kernel-Based Support Vector Regression","summary":"  This paper presents a new hybrid model for predicting German electricity\nprices. The algorithm is based on combining Gaussian Process Regression (GPR)\nand Support Vector Regression (SVR). While GPR is a competent model for\nlearning the stochastic pattern within the data and interpolation, its\nperformance for out-of-sample data is not very promising. By choosing a\nsuitable data-dependent covariance function, we can enhance the performance of\nGPR for the tested German hourly power prices. However, since the out-of-sample\nprediction depends on the training data, the prediction is vulnerable to noise\nand outliers. To overcome this issue, a separate prediction is made using SVR,\nwhich applies margin-based optimization, having an advantage in dealing with\nnon-linear processes and outliers, since only certain necessary points (support\nvectors) in the training data are responsible for regression. Both individual\npredictions are later combined using the performance-based weight assignment\nmethod. A test on historic German power prices shows that this approach\noutperforms its chosen benchmarks such as the autoregressive exogenous model,\nthe naive approach, as well as the long short-term memory approach of\nprediction.\n","authors":["Abhinav Das","Stephan Schlüter","Lorenz Schneider"],"pdf_url":"https://arxiv.org/pdf/2412.00123v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16625v3","updated":"2025-01-14T13:48:49Z","published":"2023-05-26T04:34:28Z","title":"Set-based Neural Network Encoding Without Weight Tying","summary":"  We propose a neural network weight encoding method for network property\nprediction that utilizes set-to-set and set-to-vector functions to efficiently\nencode neural network parameters. Our approach is capable of encoding neural\nnetworks in a model zoo of mixed architecture and different parameter sizes as\nopposed to previous approaches that require custom encoding models for\ndifferent architectures. Furthermore, our \\textbf{S}et-based \\textbf{N}eural\nnetwork \\textbf{E}ncoder (SNE) takes into consideration the hierarchical\ncomputational structure of neural networks. To respect symmetries inherent in\nnetwork weight space, we utilize Logit Invariance to learn the required minimal\ninvariance properties. Additionally, we introduce a \\textit{pad-chunk-encode}\npipeline to efficiently encode neural network layers that is adjustable to\ncomputational and memory constraints. We also introduce two new tasks for\nneural network property prediction: cross-dataset and cross-architecture. In\ncross-dataset property prediction, we evaluate how well property predictors\ngeneralize across model zoos trained on different datasets but of the same\narchitecture. In cross-architecture property prediction, we evaluate how well\nproperty predictors transfer to model zoos of different architecture not seen\nduring training. We show that SNE outperforms the relevant baselines on\nstandard benchmarks.\n","authors":["Bruno Andreis","Soro Bedionita","Philip H. S. Torr","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2305.16625v3.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2501.08115v1","updated":"2025-01-14T13:46:07Z","published":"2025-01-14T13:46:07Z","title":"RoHan: Robust Hand Detection in Operation Room","summary":"  Hand-specific localization has garnered significant interest within the\ncomputer vision community. Although there are numerous datasets with hand\nannotations from various angles and settings, domain transfer techniques\nfrequently struggle in surgical environments. This is mainly due to the limited\navailability of gloved hand instances and the unique challenges of operating\nrooms (ORs). Thus, hand-detection models tailored to OR settings require\nextensive training and expensive annotation processes. To overcome these\nchallenges, we present \"RoHan\" - a novel approach for robust hand detection in\nthe OR, leveraging advanced semi-supervised domain adaptation techniques to\ntackle the challenges of varying recording conditions, diverse glove colors,\nand occlusions common in surgical settings. Our methodology encompasses two\nmain stages: (1) data augmentation strategy that utilizes \"Artificial Gloves,\"\na method for augmenting publicly available hand datasets with synthetic images\nof hands-wearing gloves; (2) semi-supervised domain adaptation pipeline that\nimproves detection performance in real-world OR settings through iterative\nprediction refinement and efficient frame filtering. We evaluate our method\nusing two datasets: simulated enterotomy repair and saphenous vein graft\nharvesting. \"RoHan\" substantially reduces the need for extensive labeling and\nmodel training, paving the way for the practical implementation of hand\ndetection technologies in medical settings.\n","authors":["Roi Papo","Sapir Gershov","Tom Friedman","Itay Or","Gil Bolotin","Shlomi Laufer"],"pdf_url":"https://arxiv.org/pdf/2501.08115v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2501.04023v2","updated":"2025-01-14T13:40:35Z","published":"2024-12-27T20:16:04Z","title":"Approximation Rates in Fréchet Metrics: Barron Spaces, Paley-Wiener\n  Spaces, and Fourier Multipliers","summary":"  Operator learning is a recent development in the simulation of Partial\nDifferential Equations (PDEs) by means of neural networks. The idea behind this\napproach is to learn the behavior of an operator, such that the resulting\nneural network is an (approximate) mapping in infinite-dimensional spaces that\nis capable of (approximately) simulating the solution operator governed by the\nPDE. In our work, we study some general approximation capabilities for linear\ndifferential operators by approximating the corresponding symbol in the Fourier\ndomain. Analogous to the structure of the class of H\\\"ormander-Symbols, we\nconsider the approximation with respect to a topology that is induced by a\nsequence of semi-norms. In that sense, we measure the approximation error in\nterms of a Fr\\'echet metric, and our main result identifies sufficient\nconditions for achieving a predefined approximation error. Secondly, we then\nfocus on a natural extension of our main theorem, in which we manage to reduce\nthe assumptions on the sequence of semi-norms. Based on existing approximation\nresults for the exponential spectral Barron space, we then present a concrete\nexample of symbols that can be approximated well.\n","authors":["Ahmed Abdeljawad","Thomas Dittrich"],"pdf_url":"https://arxiv.org/pdf/2501.04023v2.pdf","comment":"Minor revision"},{"id":"http://arxiv.org/abs/2501.08109v1","updated":"2025-01-14T13:40:08Z","published":"2025-01-14T13:40:08Z","title":"Data-driven inventory management for new products: A warm-start and\n  adjusted Dyna-$Q$ approach","summary":"  In this paper, we propose a novel reinforcement learning algorithm for\ninventory management of newly launched products with no or limited historical\ndemand information. The algorithm follows the classic Dyna-$Q$ structure,\nbalancing the model-based and model-free approaches, while accelerating the\ntraining process of Dyna-$Q$ and mitigating the model discrepancy generated by\nthe model-based feedback. Warm-start information from the demand data of\nexisting similar products can be incorporated into the algorithm to further\nstabilize the early-stage training and reduce the variance of the estimated\noptimal policy. Our approach is validated through a case study of bakery\ninventory management with real data. The adjusted Dyna-$Q$ shows up to a 23.7\\%\nreduction in average daily cost compared with $Q$-learning, and up to a 77.5\\%\nreduction in training time within the same horizon compared with classic\nDyna-$Q$. By incorporating the warm-start information, it can be found that the\nadjusted Dyna-$Q$ has the lowest total cost, lowest variance in total cost, and\nrelatively low shortage percentages among all the algorithms under a 30-day\ntesting.\n","authors":["Xinyu Qu","Longxiao Liu","Wenjie Huang"],"pdf_url":"https://arxiv.org/pdf/2501.08109v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2411.11304v4","updated":"2025-01-14T13:36:51Z","published":"2024-11-18T05:59:29Z","title":"Towards Federated Graph Learning in One-shot Communication","summary":"  Federated Graph Learning (FGL) has emerged as a promising paradigm for\nbreaking data silos among distributed private graphs. In practical scenarios\ninvolving heterogeneous distributed graph data, personalized Federated Graph\nLearning (pFGL) aims to enhance model utility by training personalized models\ntailored to client needs. However, existing pFGL methods often require numerous\ncommunication rounds under heterogeneous graphs, leading to significant\ncommunication overhead and security concerns. While One-shot Federated Learning\n(OFL) enables collaboration in a single round, existing OFL methods are\ndesigned for image-centric tasks and ineffective for graph data, leaving a\ncritical gap in the field. Additionally, personalized models derived from\nexisting methods suffer from bias, failing to effectively generalize to the\nminority. To address these challenges, we propose the first $\\textbf{O}$ne-shot\n$\\textbf{p}$ersonalized $\\textbf{F}$ederated $\\textbf{G}$raph\n$\\textbf{L}$earning method ($\\textbf{O-pFGL}$) for node classification,\ncompatible with Secure Aggregation protocols for privacy preservation.\nSpecifically, for effective graph learning in one communication round, our\nmethod estimates and aggregates class-wise feature distribution statistics to\nconstruct a global pseudo-graph on the server, facilitating the training of a\nglobal graph model. To mitigate bias, we introduce a two-stage personalized\ntraining approach that adaptively balances local personal information and\nglobal insights from the pseudo-graph, improving both personalization and\ngeneralization. Extensive experiments on 12 multi-scale graph datasets\ndemonstrate that our method significantly outperforms state-of-the-art\nbaselines across various settings.\n","authors":["Guochen Yan","Xunkai Li","Luyuan Xie","Wentao Zhang","Qingni Shen","Yuejian Fang","Zhonghai Wu"],"pdf_url":"https://arxiv.org/pdf/2411.11304v4.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2501.08099v1","updated":"2025-01-14T13:16:33Z","published":"2025-01-14T13:16:33Z","title":"Smooth Handovers via Smoothed Online Learning","summary":"  With users demanding seamless connectivity, handovers (HOs) have become a\nfundamental element of cellular networks. However, optimizing HOs is a\nchallenging problem, further exacerbated by the growing complexity of mobile\nnetworks. This paper presents the first countrywide study of HO optimization,\nthrough the prism of Smoothed Online Learning (SOL). We first analyze an\nextensive dataset from a commercial mobile network operator (MNO) in Europe\nwith more than 40M users, to understand and reveal important features and\nperformance impacts on HOs. Our findings highlight a correlation between HO\nfailures/delays, and the characteristics of radio cells and end-user devices,\nshowcasing the impact of heterogeneity in mobile networks nowadays. We\nsubsequently model UE-cell associations as dynamic decisions and propose a\nrealistic system model for smooth and accurate HOs that extends existing\napproaches by (i) incorporating device and cell features on HO optimization,\nand (ii) eliminating (prior) strong assumptions about requiring future signal\nmeasurements and knowledge of end-user mobility. Our algorithm, aligned with\nthe O-RAN paradigm, provides robust dynamic regret guarantees, even in\nchallenging environments, and shows superior performance in multiple scenarios\nwith real-world and synthetic data.\n","authors":["Michail Kalntis","Andra Lutu","Jesús Omaña Iglesias","Fernando A. Kuipers","George Iosifidis"],"pdf_url":"https://arxiv.org/pdf/2501.08099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16409v2","updated":"2025-01-14T13:14:00Z","published":"2023-12-27T04:40:12Z","title":"Dynamic Sub-graph Distillation for Robust Semi-supervised Continual\n  Learning","summary":"  Continual learning (CL) has shown promising results and comparable\nperformance to learning at once in a fully supervised manner. However, CL\nstrategies typically require a large number of labeled samples, making their\nreal-life deployment challenging. In this work, we focus on semi-supervised\ncontinual learning (SSCL), where the model progressively learns from partially\nlabeled data with unknown categories. We provide a comprehensive analysis of\nSSCL and demonstrate that unreliable distributions of unlabeled data lead to\nunstable training and refinement of the progressing stages. This problem\nseverely impacts the performance of SSCL. To address the limitations, we\npropose a novel approach called Dynamic Sub-Graph Distillation (DSGD) for\nsemi-supervised continual learning, which leverages both semantic and\nstructural information to achieve more stable knowledge distillation on\nunlabeled data and exhibit robustness against distribution bias. Firstly, we\nformalize a general model of structural distillation and design a dynamic graph\nconstruction for the continual learning progress. Next, we define a structure\ndistillation vector and design a dynamic sub-graph distillation algorithm,\nwhich enables end-to-end training and adaptability to scale up tasks. The\nentire proposed method is adaptable to various CL methods and supervision\nsettings. Finally, experiments conducted on three datasets CIFAR10, CIFAR100,\nand ImageNet-100, with varying supervision ratios, demonstrate the\neffectiveness of our proposed approach in mitigating the catastrophic\nforgetting problem in semi-supervised continual learning scenarios.\n","authors":["Yan Fan","Yu Wang","Pengfei Zhu","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2312.16409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10174v3","updated":"2025-01-14T13:11:05Z","published":"2024-10-14T05:45:52Z","title":"Balanced Neural ODEs: nonlinear model order reduction and Koopman\n  operator approximations","summary":"  Variational Autoencoders (VAEs) are a powerful framework for learning latent\nrepresentations of reduced dimensionality, while Neural ODEs excel in learning\ntransient system dynamics. This work combines the strengths of both to generate\nfast surrogate models with adjustable complexity reacting on time-varying\ninputs signals. By leveraging the VAE's dimensionality reduction using a\nnonhierarchical prior, our method adaptively assigns stochastic noise,\nnaturally complementing known NeuralODE training enhancements and enabling\nprobabilistic time series modeling. We show that standard Latent ODEs struggle\nwith dimensionality reduction in systems with time-varying inputs. Our approach\nmitigates this by continuously propagating variational parameters through time,\nestablishing fixed information channels in latent space. This results in a\nflexible and robust method that can learn different system complexities, e.g.\ndeep neural networks or linear matrices. Hereby, it enables efficient\napproximation of the Koopman operator without the need for predefining its\ndimensionality. As our method balances dimensionality reduction and\nreconstruction accuracy, we call it Balanced Neural ODE (B-NODE). We\ndemonstrate the effectiveness of this methods on several academic and\nreal-world test cases, e.g. a power plant or MuJoCo data.\n","authors":["Julius Aka","Johannes Brunnemann","Jörg Eiden","Arne Speerforck","Lars Mikelsons"],"pdf_url":"https://arxiv.org/pdf/2410.10174v3.pdf","comment":"Conference paper under review, after revision"},{"id":"http://arxiv.org/abs/2501.08096v1","updated":"2025-01-14T13:10:13Z","published":"2025-01-14T13:10:13Z","title":"Hybrid Action Based Reinforcement Learning for Multi-Objective\n  Compatible Autonomous Driving","summary":"  Reinforcement Learning (RL) has shown excellent performance in solving\ndecision-making and control problems of autonomous driving, which is\nincreasingly applied in diverse driving scenarios. However, driving is a\nmulti-attribute problem, leading to challenges in achieving multi-objective\ncompatibility for current RL methods, especially in both policy execution and\npolicy iteration. On the one hand, the common action space structure with\nsingle action type limits driving flexibility or results in large behavior\nfluctuations during policy execution. On the other hand, the multi-attribute\nweighted single reward function result in the agent's disproportionate\nattention to certain objectives during policy iterations. To this end, we\npropose a Multi-objective Ensemble-Critic reinforcement learning method with\nHybrid Parametrized Action for multi-objective compatible autonomous driving.\nSpecifically, a parameterized action space is constructed to generate hybrid\ndriving actions, combining both abstract guidance and concrete control\ncommands. A multi-objective critics architecture is constructed considering\nmultiple attribute rewards, to ensure simultaneously focusing on different\ndriving objectives. Additionally, uncertainty-based exploration strategy is\nintroduced to help the agent faster approach viable driving policy. The\nexperimental results in both the simulated traffic environment and the HighD\ndataset demonstrate that our method can achieve multi-objective compatible\nautonomous driving in terms of driving efficiency, action consistency, and\nsafety. It enhances the general performance of the driving while significantly\nincreasing training efficiency.\n","authors":["Guizhe Jin","Zhuoren Li","Bo Leng","Wei Han","Lu Xiong","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2501.08096v1.pdf","comment":"12 pages, 9 figures, 5 tables"},{"id":"http://arxiv.org/abs/2501.08085v1","updated":"2025-01-14T12:54:19Z","published":"2025-01-14T12:54:19Z","title":"Dynamic Multimodal Sentiment Analysis: Leveraging Cross-Modal Attention\n  for Enabled Classification","summary":"  This paper explores the development of a multimodal sentiment analysis model\nthat integrates text, audio, and visual data to enhance sentiment\nclassification. The goal is to improve emotion detection by capturing the\ncomplex interactions between these modalities, thereby enabling more accurate\nand nuanced sentiment interpretation. The study evaluates three feature fusion\nstrategies -- late stage fusion, early stage fusion, and multi-headed attention\n-- within a transformer-based architecture. Experiments were conducted using\nthe CMU-MOSEI dataset, which includes synchronized text, audio, and visual\ninputs labeled with sentiment scores. Results show that early stage fusion\nsignificantly outperforms late stage fusion, achieving an accuracy of 71.87\\%,\nwhile the multi-headed attention approach offers marginal improvement, reaching\n72.39\\%. The findings suggest that integrating modalities early in the process\nenhances sentiment classification, while attention mechanisms may have limited\nimpact within the current framework. Future work will focus on refining feature\nfusion techniques, incorporating temporal data, and exploring dynamic feature\nweighting to further improve model performance.\n","authors":["Hui Lee","Singh Suniljit","Yong Siang Ong"],"pdf_url":"https://arxiv.org/pdf/2501.08085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00376v3","updated":"2025-01-14T12:37:26Z","published":"2024-03-01T09:01:53Z","title":"Spurious Feature Eraser: Stabilizing Test-Time Adaptation for\n  Vision-Language Foundation Model","summary":"  Vision-language foundation models have exhibited remarkable success across a\nmultitude of downstream tasks due to their scalability on extensive image-text\npaired data. However, these models also display significant limitations when\napplied to downstream tasks, such as fine-grained image classification, as a\nresult of ``decision shortcuts'' that hinder their generalization capabilities.\nIn this work, we find that the CLIP model possesses a rich set of features,\nencompassing both \\textit{desired invariant causal features} and\n\\textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP\non downstream tasks originates from its inability to effectively utilize\npre-trained features in accordance with specific task requirements. To address\nthis challenge, we propose a simple yet effective method, Spurious Feature\nEraser (SEraser), to alleviate the decision shortcuts by erasing the spurious\nfeatures. Specifically, we introduce a test-time prompt tuning paradigm that\noptimizes a learnable prompt, thereby compelling the model to exploit invariant\nfeatures while disregarding decision shortcuts during the inference phase. The\nproposed method effectively alleviates excessive dependence on potentially\nmisleading spurious information. We conduct comparative analysis of the\nproposed method against various approaches which validates the significant\nsuperiority.\n","authors":["Huan Ma","Yan Zhu","Changqing Zhang","Peilin Zhao","Baoyuan Wu","Long-Kai Huang","Qinghua Hu","Bingzhe Wu"],"pdf_url":"https://arxiv.org/pdf/2403.00376v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08071v1","updated":"2025-01-14T12:36:18Z","published":"2025-01-14T12:36:18Z","title":"CuAsmRL: Optimizing GPU SASS Schedules via Deep Reinforcement Learning","summary":"  Large language models (LLMs) are remarked by their substantial computational\nrequirements. To mitigate the cost, researchers develop specialized CUDA\nkernels, which often fuse several tensor operations to maximize the utilization\nof GPUs as much as possible. However, those specialized kernels may still leave\nperformance on the table as CUDA assembly experts show that manual optimization\nof GPU SASS schedules can lead to better performance, and trial-and-error is\nlargely employed to manually find the best GPU SASS schedules.\n  In this work, we employ an automatic approach to optimize GPU SASS schedules,\nwhich thus can be integrated into existing compiler frameworks. The key to\nautomatic optimization is training an RL agent to mimic how human experts\nperform manual scheduling. To this end, we formulate an assembly game, where RL\nagents can play to find the best GPU SASS schedules. The assembly game starts\nfrom a \\textit{-O3} optimized SASS schedule, and the RL agents can iteratively\napply actions to mutate the current schedules. Positive rewards are generated\nif the mutated schedules get higher throughput by executing on GPUs.\nExperiments show that CuAsmRL can further improve the performance of existing\nspecialized CUDA kernels transparently by up to $26\\%$, and on average $9\\%$.\nMoreover, it is used as a tool to reveal potential optimization moves learned\nautomatically.\n","authors":["Guoliang He","Eiko Yoneki"],"pdf_url":"https://arxiv.org/pdf/2501.08071v1.pdf","comment":"cgo 2025"},{"id":"http://arxiv.org/abs/2501.08067v1","updated":"2025-01-14T12:33:02Z","published":"2025-01-14T12:33:02Z","title":"Optimal Policy Adaptation under Covariate Shift","summary":"  Transfer learning of prediction models has been extensively studied, while\nthe corresponding policy learning approaches are rarely discussed. In this\npaper, we propose principled approaches for learning the optimal policy in the\ntarget domain by leveraging two datasets: one with full information from the\nsource domain and the other from the target domain with only covariates. First,\nunder the setting of covariate shift, we formulate the problem from a\nperspective of causality and present the identifiability assumptions for the\nreward induced by a given policy. Then, we derive the efficient influence\nfunction and the semiparametric efficiency bound for the reward. Based on this,\nwe construct a doubly robust and semiparametric efficient estimator for the\nreward and then learn the optimal policy by optimizing the estimated reward.\nMoreover, we theoretically analyze the bias and the generalization error bound\nfor the learned policy. Furthermore, in the presence of both covariate and\nconcept shifts, we propose a novel sensitivity analysis method to evaluate the\nrobustness of the proposed policy learning approach. Extensive experiments\ndemonstrate that the approach not only estimates the reward more accurately but\nalso yields a policy that closely approximates the theoretically optimal\npolicy.\n","authors":["Xueqing Liu","Qinwei Yang","Zhaoqing Tian","Ruocheng Guo","Peng Wu"],"pdf_url":"https://arxiv.org/pdf/2501.08067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20020v3","updated":"2025-01-14T12:31:48Z","published":"2024-07-29T13:57:24Z","title":"ImagiNet: A Multi-Content Benchmark for Synthetic Image Detection","summary":"  Recent generative models produce images with a level of authenticity that\nmakes them nearly indistinguishable from real photos and artwork. Potential\nharmful use cases of these models, necessitate the creation of robust synthetic\nimage detectors. However, current datasets in the field contain generated\nimages with questionable quality or have examples from one predominant content\ntype which leads to poor generalizability of the underlying detectors. We find\nthat the curation of a balanced amount of high-resolution generated images\nacross various content types is crucial for the generalizability of detectors,\nand introduce ImagiNet, a dataset of 200K examples, spanning four categories:\nphotos, paintings, faces, and miscellaneous. Synthetic images in ImagiNet are\nproduced with both open-source and proprietary generators, whereas real\ncounterparts for each content type are collected from public datasets. The\nstructure of ImagiNet allows for a two-track evaluation system: i)\nclassification as real or synthetic and ii) identification of the generative\nmodel. To establish a strong baseline, we train a ResNet-50 model using a\nself-supervised contrastive objective (SelfCon) for each track which achieves\nevaluation AUC of up to 0.99 and balanced accuracy ranging from 86% to 95%,\neven under conditions that involve compression and resizing. The provided model\nis generalizable enough to achieve zero-shot state-of-the-art performance on\nprevious synthetic detection benchmarks. We provide ablations to demonstrate\nthe importance of content types and publish code and data.\n","authors":["Delyan Boychev","Radostin Cholakov"],"pdf_url":"https://arxiv.org/pdf/2407.20020v3.pdf","comment":"Workshop on Datasets and Evaluators of AI Safety, AAAI 2025"},{"id":"http://arxiv.org/abs/2410.03335v2","updated":"2025-01-14T11:59:03Z","published":"2024-10-04T11:40:53Z","title":"Audio-Agent: Leveraging LLMs For Audio Generation, Editing and\n  Composition","summary":"  We introduce Audio-Agent, a multimodal framework for audio generation,\nediting and composition based on text or video inputs. Conventional approaches\nfor text-to-audio (TTA) tasks often make single-pass inferences from text\ndescriptions. While straightforward, this design struggles to produce\nhigh-quality audio when given complex text conditions. In our method, we\nutilize a pre-trained TTA diffusion network as the audio generation agent to\nwork in tandem with GPT-4, which decomposes the text condition into atomic,\nspecific instructions and calls the agent for audio generation. In doing so,\nAudio-Agent can generate high-quality audio that is closely aligned with the\nprovided text or video exhibiting complex and multiple events, while supporting\nvariable-length and variable-volume generation. For video-to-audio (VTA) tasks,\nmost existing methods require training a timestamp detector to synchronize\nvideo events with the generated audio, a process that can be tedious and\ntime-consuming. Instead, we propose a simpler approach by fine-tuning a\npre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both\nsemantic and temporal conditions that bridge the video and audio modality.\nConsequently, our framework contributes a comprehensive solution for both TTA\nand VTA tasks without substantial computational overhead in training.\n","authors":["Zixuan Wang","Chi-Keung Tang","Yu-Wing Tai"],"pdf_url":"https://arxiv.org/pdf/2410.03335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08050v1","updated":"2025-01-14T11:56:05Z","published":"2025-01-14T11:56:05Z","title":"On the use of Statistical Learning Theory for model selection in\n  Structural Health Monitoring","summary":"  Whenever data-based systems are employed in engineering applications,\ndefining an optimal statistical representation is subject to the problem of\nmodel selection. This paper focusses on how well models can generalise in\nStructural Health Monitoring (SHM). Although statistical model validation in\nthis field is often performed heuristically, it is possible to estimate\ngeneralisation more rigorously using the bounds provided by Statistical\nLearning Theory (SLT). Therefore, this paper explores the selection process of\na kernel smoother for modelling the impulse response of a linear oscillator\nfrom the perspective of SLT. It is demonstrated that incorporating domain\nknowledge into the regression problem yields a lower guaranteed risk, thereby\nenhancing generalisation.\n","authors":["C. A. Lindley","N. Dervilis","K. Worden"],"pdf_url":"https://arxiv.org/pdf/2501.08050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08049v1","updated":"2025-01-14T11:56:00Z","published":"2025-01-14T11:56:00Z","title":"Self-Attentive Spatio-Temporal Calibration for Precise Intermediate\n  Layer Matching in ANN-to-SNN Distillation","summary":"  Spiking Neural Networks (SNNs) are promising for low-power computation due to\ntheir event-driven mechanism but often suffer from lower accuracy compared to\nArtificial Neural Networks (ANNs). ANN-to-SNN knowledge distillation can\nimprove SNN performance, but previous methods either focus solely on label\ninformation, missing valuable intermediate layer features, or use a layer-wise\napproach that neglects spatial and temporal semantic inconsistencies, leading\nto performance degradation.To address these limitations, we propose a novel\nmethod called self-attentive spatio-temporal calibration (SASTC). SASTC uses\nself-attention to identify semantically aligned layer pairs between ANN and\nSNN, both spatially and temporally. This enables the autonomous transfer of\nrelevant semantic information. Extensive experiments show that SASTC\noutperforms existing methods, effectively solving the mismatching problem.\nSuperior accuracy results include 95.12% on CIFAR-10, 79.40% on CIFAR-100 with\n2 time steps, and 68.69% on ImageNet with 4 time steps for static datasets, and\n97.92% on DVS-Gesture and 83.60% on DVS-CIFAR10 for neuromorphic datasets. This\nmarks the first time SNNs have outperformed ANNs on both CIFAR-10 and\nCIFAR-100, shedding the new light on the potential applications of SNNs.\n","authors":["Di Hong","Yueming Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08047v1","updated":"2025-01-14T11:54:45Z","published":"2025-01-14T11:54:45Z","title":"Gen-A: Generalizing Ambisonics Neural Encoding to Unseen Microphone\n  Arrays","summary":"  Using deep neural networks (DNNs) for encoding of microphone array (MA)\nsignals to the Ambisonics spatial audio format can surpass certain limitations\nof established conventional methods, but existing DNN-based methods need to be\ntrained separately for each MA. This paper proposes a DNN-based method for\nAmbisonics encoding that can generalize to arbitrary MA geometries unseen\nduring training. The method takes as inputs the MA geometry and MA signals and\nuses a multi-level encoder consisting of separate paths for geometry and signal\ndata, where geometry features inform the signal encoder at each level. The\nmethod is validated in simulated anechoic and reverberant conditions with one\nand two sources. The results indicate improvement over conventional encoding\nacross the whole frequency range for dry scenes, while for reverberant scenes\nthe improvement is frequency-dependent.\n","authors":["Mikko Heikkinen","Archontis Politis","Konstantinos Drossos","Tuomas Virtanen"],"pdf_url":"https://arxiv.org/pdf/2501.08047v1.pdf","comment":"Accepted for publication in Proceedings of the 2025 IEEE\n  International Conference on Acoustics, Speech and Signal Processing"},{"id":"http://arxiv.org/abs/2501.08044v1","updated":"2025-01-14T11:52:16Z","published":"2025-01-14T11:52:16Z","title":"UFGraphFR: An attempt at a federated recommendation system based on user\n  text characteristics","summary":"  Federated learning has become an important research area in 'private\ncomputing' due to the 'useable invisibility' of data during training. Inspired\nby Federated learning, the federated recommendation system has gradually become\na new recommendation service architecture that can protect users' privacy. The\nuse of user diagrams to enhance federated recommendations is a promising topic.\nHow to use user diagrams to enhance federated recommendations is a promising\nresearch topic. However, it's a great challenge to construct a user diagram\nwithout compromising privacy in a federated learning scenario. Inspired by the\nsimple idea that similar users often have the same attribute characteristics,\nwe propose a personalized federated recommendation algorithm based on the user\nrelationship graph constructed by the user text characteristics(Graph\nFederation Recommendation System based on User Text description Features,\nUFGraphFR). The method uses the embedding layer weight of the user's text\nfeature description to construct the user relationship graph. It introduces the\nTransformer mechanism to capture the sequence modeling of the user's historical\ninteraction sequence. Without access to user history interactions and specific\nuser attributes, the federal learning privacy protection of data 'useable\ninvisibility' is embodied. Preliminary experiments on some benchmark datasets\ndemonstrate the superior performance of UFGraphFR. Our experiments show that\nthis model can protect user privacy to some extent without affecting the\nperformance of the recommendation system. The code will be easily available on\nhttps://github.com/trueWangSyutung/UFGraphFR.\n","authors":["Xudong Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08043v1","updated":"2025-01-14T11:51:57Z","published":"2025-01-14T11:51:57Z","title":"PolyLUT: Ultra-low Latency Polynomial Inference with Hardware-Aware\n  Structured Pruning","summary":"  Standard deep neural network inference involves the computation of\ninterleaved linear maps and nonlinear activation functions. Prior work for\nultra-low latency implementations has hardcoded these operations inside FPGA\nlookup tables (LUTs). However, FPGA LUTs can implement a much greater variety\nof functions. In this paper, we propose a novel approach to training DNNs for\nFPGA deployment using multivariate polynomials as the basic building block. Our\nmethod takes advantage of the flexibility offered by the soft logic, hiding the\npolynomial evaluation inside the LUTs with minimal overhead. By using\npolynomial building blocks, we achieve the same accuracy using considerably\nfewer layers of soft logic than by using linear functions, leading to\nsignificant latency and area improvements. LUT-based implementations also face\na significant challenge: the LUT size grows exponentially with the number of\ninputs. Prior work relies on a priori fixed sparsity, with results heavily\ndependent on seed selection. To address this, we propose a structured pruning\nstrategy using a bespoke hardware-aware group regularizer that encourages a\nparticular sparsity pattern that leads to a small number of inputs per neuron.\nWe demonstrate the effectiveness of PolyLUT on three tasks: network intrusion\ndetection, jet identification at the CERN Large Hadron Collider, and MNIST.\n","authors":["Marta Andronic","Jiawen Li","George A. Constantinides"],"pdf_url":"https://arxiv.org/pdf/2501.08043v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2309.02334"},{"id":"http://arxiv.org/abs/2501.08040v1","updated":"2025-01-14T11:46:36Z","published":"2025-01-14T11:46:36Z","title":"Convergence Analysis of Real-time Recurrent Learning (RTRL) for a class\n  of Recurrent Neural Networks","summary":"  Recurrent neural networks (RNNs) are commonly trained with the truncated\nbackpropagation-through-time (TBPTT) algorithm. For the purposes of\ncomputational tractability, the TBPTT algorithm truncates the chain rule and\ncalculates the gradient on a finite block of the overall data sequence. Such\napproximation could lead to significant inaccuracies, as the block length for\nthe truncated backpropagation is typically limited to be much smaller than the\noverall sequence length. In contrast, Real-time recurrent learning (RTRL) is an\nonline optimization algorithm which asymptotically follows the true gradient of\nthe loss on the data sequence as the number of sequence time steps $t\n\\rightarrow \\infty$. RTRL forward propagates the derivatives of the RNN\nhidden/memory units with respect to the parameters and, using the forward\nderivatives, performs online updates of the parameters at each time step in the\ndata sequence. RTRL's online forward propagation allows for exact optimization\nover extremely long data sequences, although it can be computationally costly\nfor models with large numbers of parameters. We prove convergence of the RTRL\nalgorithm for a class of RNNs. The convergence analysis establishes a fixed\npoint for the joint distribution of the data sequence, RNN hidden layer, and\nthe RNN hidden layer forward derivatives as the number of data samples from the\nsequence and the number of training steps tend to infinity. We prove\nconvergence of the RTRL algorithm to a stationary point of the loss. Numerical\nstudies illustrate our theoretical results. One potential application area for\nRTRL is the analysis of financial data, which typically involve long time\nseries and models with small to medium numbers of parameters. This makes RTRL\ncomputationally tractable and a potentially appealing optimization method for\ntraining models. Thus, we include an example of RTRL applied to limit order\nbook data.\n","authors":["Samuel Chun-Hei Lam","Justin Sirignano","Konstantinos Spiliopoulos"],"pdf_url":"https://arxiv.org/pdf/2501.08040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08037v1","updated":"2025-01-14T11:42:51Z","published":"2025-01-14T11:42:51Z","title":"Enhanced SPS Velocity-adaptive Scheme: Access Fariness in 5G NR V2I\n  Networks","summary":"  Vehicle-to-Infrastructure (V2I) technology enables information exchange\nbetween vehicles and road infrastructure. Specifically, when a vehicle\napproaches a roadside unit (RSU), it can exchange information with the RSU to\nobtain accurate data that assists in driving. With the release of the 3rd\nGeneration Partnership Project (3GPP) Release 16, which includes the 5G New\nRadio (NR) Vehicle-to-Everything (V2X) standards, vehicles typically adopt\nmode-2 communication using sensing-based semi-persistent scheduling (SPS) for\nresource allocation. In this approach, vehicles identify candidate resources\nwithin a selection window and exclude ineligible resources based on information\nfrom a sensing window. However, vehicles often drive at different speeds,\nresulting in varying amounts of data transmission with RSUs as they pass by,\nwhich leads to unfair access. Therefore, it is essential to design an access\nscheme that accounts for different vehicle speeds to achieve fair access across\nthe network. This paper formulates an optimization problem for vehicular\nnetworks and proposes a multi-objective optimization scheme to address it by\nadjusting the selection window in the SPS mechanism of 5G NR V2I mode-2.\nSimulation results demonstrate the effectiveness of the proposed scheme\n","authors":["Xiao Xu","Qiong Wu","Pingyi Fan","Kezhi Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08037v1.pdf","comment":"This paper has been submitted to IEEE Journal. The source code has\n  been released at:\n  https://github.com/qiongwu86/Enhanced-SPS-Velocity-adaptiveScheme-Access-Fariness-in-5G-NR-V2I-Networks"},{"id":"http://arxiv.org/abs/2411.17350v2","updated":"2025-01-14T11:34:26Z","published":"2024-11-26T11:52:47Z","title":"Correlation-Aware Graph Convolutional Networks for Multi-Label Node\n  Classification","summary":"  Multi-label node classification is an important yet under-explored domain in\ngraph mining as many real-world nodes belong to multiple categories rather than\njust a single one. Although a few efforts have been made by utilizing Graph\nConvolution Networks (GCNs) to learn node representations and model\ncorrelations between multiple labels in the embedding space, they still suffer\nfrom the ambiguous feature and ambiguous topology induced by multiple labels,\nwhich reduces the credibility of the messages delivered in graphs and overlooks\nthe label correlations on graph data. Therefore, it is crucial to reduce the\nambiguity and empower the GCNs for accurate classification. However, this is\nquite challenging due to the requirement of retaining the distinctiveness of\neach label while fully harnessing the correlation between labels\nsimultaneously. To address these issues, in this paper, we propose a\nCorrelation-aware Graph Convolutional Network (CorGCN) for multi-label node\nclassification. By introducing a novel Correlation-Aware Graph Decomposition\nmodule, CorGCN can learn a graph that contains rich label-correlated\ninformation for each label. It then employs a Correlation-Enhanced Graph\nConvolution to model the relationships between labels during message passing to\nfurther bolster the classification process. Extensive experiments on five\ndatasets demonstrate the effectiveness of our proposed CorGCN.\n","authors":["Yuanchen Bei","Weizhi Chen","Hao Chen","Sheng Zhou","Carl Yang","Jiapei Fan","Longtao Huang","Jiajun Bu"],"pdf_url":"https://arxiv.org/pdf/2411.17350v2.pdf","comment":"12 pages, accepted by KDD2025"},{"id":"http://arxiv.org/abs/2402.03169v3","updated":"2025-01-14T11:32:56Z","published":"2024-02-05T16:38:30Z","title":"A Random Matrix Approach to Low-Multilinear-Rank Tensor Approximation","summary":"  This work presents a comprehensive understanding of the estimation of a\nplanted low-rank signal from a general spiked tensor model near the\ncomputational threshold. Relying on standard tools from the theory of large\nrandom matrices, we characterize the large-dimensional spectral behavior of the\nunfoldings of the data tensor and exhibit relevant signal-to-noise ratios\ngoverning the detectability of the principal directions of the signal. These\nresults allow to accurately predict the reconstruction performance of truncated\nmultilinear SVD (MLSVD) in the non-trivial regime. This is particularly\nimportant since it serves as an initialization of the higher-order orthogonal\niteration (HOOI) scheme, whose convergence to the best low-multilinear-rank\napproximation depends entirely on its initialization. We give a sufficient\ncondition for the convergence of HOOI and show that the number of iterations\nbefore convergence tends to $1$ in the large-dimensional limit.\n","authors":["Hugo Lebeau","Florent Chatelain","Romain Couillet"],"pdf_url":"https://arxiv.org/pdf/2402.03169v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08019v1","updated":"2025-01-14T11:19:52Z","published":"2025-01-14T11:19:52Z","title":"An AI-driven framework for rapid and localized optimizations of urban\n  open spaces","summary":"  As urbanization accelerates, open spaces are increasingly recognized for\ntheir role in enhancing sustainability and well-being, yet they remain\nunderexplored compared to built spaces. This study introduces an AI-driven\nframework that integrates machine learning models (MLMs) and explainable AI\ntechniques to optimize Sky View Factor (SVF) and visibility, key spatial\nmetrics influencing thermal comfort and perceived safety in urban spaces.\nUnlike global optimization methods, which are computationally intensive and\nimpractical for localized adjustments, this framework supports incremental\ndesign improvements with lower computational costs and greater flexibility. The\nframework employs SHapley Adaptive Explanations (SHAP) to analyze feature\nimportance and Counterfactual Explanations (CFXs) to propose minimal design\nchanges. Simulations tested five MLMs, identifying XGBoost as the most\naccurate, with building width, park area, and heights of surrounding buildings\nas critical for SVF, and distances from southern buildings as key for\nvisibility. Compared to Genetic Algorithms, which required approximately 15/30\nminutes across 3/4 generations to converge, the tested CFX approach achieved\noptimized results in 1 minute with a 5% RMSE error, demonstrating significantly\nfaster performance and suitability for scalable retrofitting strategies. This\ninterpretable and computationally efficient framework advances urban\nperformance optimization, providing data-driven insights and practical\nretrofitting solutions for enhancing usability and environmental quality across\ndiverse urban contexts.\n","authors":["Pegah Eshraghi","Arman Nikkhah Dehnavi","Maedeh Mirdamadi","Riccardo Talami","Zahra-Sadat Zomorodian"],"pdf_url":"https://arxiv.org/pdf/2501.08019v1.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2403.02774v3","updated":"2025-01-14T11:14:57Z","published":"2024-03-05T08:41:41Z","title":"Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System\n  Model Fields with Generative Machine Learning","summary":"  Accurate and high-resolution Earth system model (ESM) simulations are\nessential to assess the ecological and socio-economic impacts of anthropogenic\nclimate change, but are computationally too expensive to be run at sufficiently\nhigh spatial resolution. Recent machine learning approaches have shown\npromising results in downscaling ESM simulations, outperforming\nstate-of-the-art statistical approaches. However, existing methods require\ncomputationally costly retraining for each ESM and extrapolate poorly to\nclimates unseen during training. We address these shortcomings by learning a\nconsistency model (CM) that efficiently and accurately downscales arbitrary ESM\nsimulations without retraining in a zero-shot manner. Our approach yields\nprobabilistic downscaled fields at a resolution only limited by the\nobservational reference data. We show that the CM outperforms state-of-the-art\ndiffusion models at a fraction of computational cost while maintaining high\ncontrollability on the downscaling task. Further, our method generalizes to\nclimate states unseen during training without explicitly formulated physical\nconstraints.\n","authors":["Philipp Hess","Michael Aich","Baoxiang Pan","Niklas Boers"],"pdf_url":"https://arxiv.org/pdf/2403.02774v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04594v2","updated":"2025-01-14T11:03:05Z","published":"2024-12-05T20:15:34Z","title":"Learning Symmetries via Weight-Sharing with Doubly Stochastic Tensors","summary":"  Group equivariance has emerged as a valuable inductive bias in deep learning,\nenhancing generalization, data efficiency, and robustness. Classically, group\nequivariant methods require the groups of interest to be known beforehand,\nwhich may not be realistic for real-world data. Additionally, baking in fixed\ngroup equivariance may impose overly restrictive constraints on model\narchitecture. This highlights the need for methods that can dynamically\ndiscover and apply symmetries as soft constraints. For neural network\narchitectures, equivariance is commonly achieved through group transformations\nof a canonical weight tensor, resulting in weight sharing over a given group\n$G$. In this work, we propose to learn such a weight-sharing scheme by defining\na collection of learnable doubly stochastic matrices that act as soft\npermutation matrices on canonical weight tensors, which can take regular group\nrepresentations as a special case. This yields learnable kernel transformations\nthat are jointly optimized with downstream tasks. We show that when the dataset\nexhibits strong symmetries, the permutation matrices will converge to regular\ngroup representations and our weight-sharing networks effectively become\nregular group convolutions. Additionally, the flexibility of the method enables\nit to effectively pick up on partial symmetries.\n","authors":["Putri A. van der Linden","Alejandro García-Castellanos","Sharvaree Vadgama","Thijs P. Kuipers","Erik J. Bekkers"],"pdf_url":"https://arxiv.org/pdf/2412.04594v2.pdf","comment":"19 pages, 14 figures, 4 tables"},{"id":"http://arxiv.org/abs/2501.08002v1","updated":"2025-01-14T10:46:41Z","published":"2025-01-14T10:46:41Z","title":"Maximizing Uncertainty for Federated learning via Bayesian\n  Optimisation-based Model Poisoning","summary":"  As we transition from Narrow Artificial Intelligence towards Artificial Super\nIntelligence, users are increasingly concerned about their privacy and the\ntrustworthiness of machine learning (ML) technology. A common denominator for\nthe metrics of trustworthiness is the quantification of uncertainty inherent in\nDL algorithms, and specifically in the model parameters, input data, and model\npredictions. One of the common approaches to address privacy-related issues in\nDL is to adopt distributed learning such as federated learning (FL), where\nprivate raw data is not shared among users. Despite the privacy-preserving\nmechanisms in FL, it still faces challenges in trustworthiness. Specifically,\nthe malicious users, during training, can systematically create malicious model\nparameters to compromise the models predictive and generative capabilities,\nresulting in high uncertainty about their reliability. To demonstrate malicious\nbehaviour, we propose a novel model poisoning attack method named Delphi which\naims to maximise the uncertainty of the global model output. We achieve this by\ntaking advantage of the relationship between the uncertainty and the model\nparameters of the first hidden layer of the local model. Delphi employs two\ntypes of optimisation , Bayesian Optimisation and Least Squares Trust Region,\nto search for the optimal poisoned model parameters, named as Delphi-BO and\nDelphi-LSTR. We quantify the uncertainty using the KL Divergence to minimise\nthe distance of the predictive probability distribution towards an uncertain\ndistribution of model output. Furthermore, we establish a mathematical proof\nfor the attack effectiveness demonstrated in FL. Numerical results demonstrate\nthat Delphi-BO induces a higher amount of uncertainty than Delphi-LSTR\nhighlighting vulnerability of FL systems to model poisoning attacks.\n","authors":["Marios Aristodemou","Xiaolan Liu","Yuan Wang","Konstantinos G. Kyriakopoulos","Sangarapillai Lambotharan","Qingsong Wei"],"pdf_url":"https://arxiv.org/pdf/2501.08002v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2501.07999v1","updated":"2025-01-14T10:41:46Z","published":"2025-01-14T10:41:46Z","title":"Unsupervised Feature Construction for Anomaly Detection in Time Series\n  -- An Evaluation","summary":"  To detect anomalies with precision and without prior knowledge in time\nseries, is it better to build a detector from the initial temporal\nrepresentation, or to compute a new (tabular) representation using an existing\nautomatic variable construction library? In this article, we address this\nquestion by conducting an in-depth experimental study for two popular detectors\n(Isolation Forest and Local Outlier Factor). The obtained results, for 5\ndifferent datasets, show that the new representation, computed using the\ntsfresh library, allows Isolation Forest to significantly improve its\nperformance.\n","authors":["Marine Hamon","Vincent Lemaire","Nour Eddine Yassine Nair-Benrekia","Samuel Berlemont","Julien Cumin"],"pdf_url":"https://arxiv.org/pdf/2501.07999v1.pdf","comment":"7"},{"id":"http://arxiv.org/abs/2410.07662v3","updated":"2025-01-14T10:41:34Z","published":"2024-10-10T07:12:32Z","title":"Scalable and Resource-Efficient Second-Order Federated Learning via\n  Over-the-Air Aggregation","summary":"  Second-order federated learning (FL) algorithms offer faster convergence than\ntheir first-order counterparts by leveraging curvature information. However,\nthey are hindered by high computational and storage costs, particularly for\nlarge-scale models. Furthermore, the communication overhead associated with\nlarge models and digital transmission exacerbates these challenges, causing\ncommunication bottlenecks. In this work, we propose a scalable second-order FL\nalgorithm using a sparse Hessian estimate and leveraging over-the-air\naggregation, making it feasible for larger models. Our simulation results\ndemonstrate more than $67\\%$ of communication resources and energy savings\ncompared to other first and second-order baselines.\n","authors":["Abdulmomen Ghalkha","Chaouki Ben Issaid","Mehdi Bennis"],"pdf_url":"https://arxiv.org/pdf/2410.07662v3.pdf","comment":"6 pages, 1 figure, 4 subfigures, letter"},{"id":"http://arxiv.org/abs/2501.07996v1","updated":"2025-01-14T10:39:04Z","published":"2025-01-14T10:39:04Z","title":"Reward Compatibility: A Framework for Inverse RL","summary":"  We provide an original theoretical study of Inverse Reinforcement Learning\n(IRL) through the lens of reward compatibility, a novel framework to quantify\nthe compatibility of a reward with the given expert's demonstrations.\nIntuitively, a reward is more compatible with the demonstrations the closer the\nperformance of the expert's policy computed with that reward is to the optimal\nperformance for that reward. This generalizes the notion of feasible reward\nset, the most common framework in the theoretical IRL literature, for which a\nreward is either compatible or not compatible. The grayscale introduced by the\nreward compatibility is the key to extend the realm of provably efficient IRL\nfar beyond what is attainable with the feasible reward set: from tabular to\nlarge-scale MDPs. We analyze the IRL problem across various settings, including\noptimal and suboptimal expert's demonstrations and both online and offline data\ncollection. For all of these dimensions, we provide a tractable algorithm and\ncorresponding sample complexity analysis, as well as various insights on reward\ncompatibility and how the framework can pave the way to yet more general\nproblem settings.\n","authors":["Filippo Lazzati","Mirco Mutti","Alberto Metelli"],"pdf_url":"https://arxiv.org/pdf/2501.07996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07994v1","updated":"2025-01-14T10:38:18Z","published":"2025-01-14T10:38:18Z","title":"Combining imaging and shape features for prediction tasks of Alzheimer's\n  disease classification and brain age regression","summary":"  We investigate combining imaging and shape features extracted from MRI for\nthe clinically relevant tasks of brain age prediction and Alzheimer's disease\nclassification. Our proposed model fuses ResNet-extracted image embeddings with\nshape embeddings from a bespoke graph neural network. The shape embeddings are\nderived from surface meshes of 15 brain structures, capturing detailed\ngeometric information. Combined with the appearance features from T1-weighted\nimages, we observe improvements in the prediction performance on both tasks,\nwith substantial gains for classification. We evaluate the model using public\ndatasets, including CamCAN, IXI, and OASIS3, demonstrating the effectiveness of\nfusing imaging and shape features for brain analysis.\n","authors":["Nairouz Shehata","Carolina Piçarra","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2501.07994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03033v3","updated":"2025-01-14T10:34:00Z","published":"2024-11-05T12:10:02Z","title":"Rethinking Decoders for Transformer-based Semantic Segmentation: A\n  Compression Perspective","summary":"  State-of-the-art methods for Transformer-based semantic segmentation\ntypically adopt Transformer decoders that are used to extract additional\nembeddings from image embeddings via cross-attention, refine either or both\ntypes of embeddings via self-attention, and project image embeddings onto the\nadditional embeddings via dot-product. Despite their remarkable success, these\nempirical designs still lack theoretical justifications or interpretations,\nthus hindering potentially principled improvements. In this paper, we argue\nthat there are fundamental connections between semantic segmentation and\ncompression, especially between the Transformer decoders and Principal\nComponent Analysis (PCA). From such a perspective, we derive a white-box, fully\nattentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the\ninterpretations as follows: 1) the self-attention operator refines image\nembeddings to construct an ideal principal subspace that aligns with the\nsupervision and retains most information; 2) the cross-attention operator seeks\nto find a low-rank approximation of the refined image embeddings, which is\nexpected to be a set of orthonormal bases of the principal subspace and\ncorresponds to the predefined classes; 3) the dot-product operation yields\ncompact representation for image embeddings as segmentation masks. Experiments\nconducted on dataset ADE20K find that DEPICT consistently outperforms its\nblack-box counterpart, Segmenter, and it is light weight and more robust.\n","authors":["Qishuai Wen","Chun-Guang Li"],"pdf_url":"https://arxiv.org/pdf/2411.03033v3.pdf","comment":"NeurIPS2024. Code:https://github.com/QishuaiWen/DEPICT/"},{"id":"http://arxiv.org/abs/2406.03912v2","updated":"2025-01-14T10:32:32Z","published":"2024-06-06T09:51:30Z","title":"GenSafe: A Generalizable Safety Enhancer for Safe Reinforcement Learning\n  Algorithms Based on Reduced Order Markov Decision Process Model","summary":"  Safe Reinforcement Learning (SRL) aims to realize a safe learning process for\nDeep Reinforcement Learning (DRL) algorithms by incorporating safety\nconstraints. However, the efficacy of SRL approaches often relies on accurate\nfunction approximations, which are notably challenging to achieve in the early\nlearning stages due to data insufficiency. To address this issue, we introduce\nin this work a novel Generalizable Safety enhancer (GenSafe) that is able to\novercome the challenge of data insufficiency and enhance the performance of SRL\napproaches. Leveraging model order reduction techniques, we first propose an\ninnovative method to construct a Reduced Order Markov Decision Process (ROMDP)\nas a low-dimensional approximator of the original safety constraints. Then, by\nsolving the reformulated ROMDP-based constraints, GenSafe refines the actions\nof the agent to increase the possibility of constraint satisfaction.\nEssentially, GenSafe acts as an additional safety layer for SRL algorithms. We\nevaluate GenSafe on multiple SRL approaches and benchmark problems. The results\ndemonstrate its capability to improve safety performance, especially in the\nearly learning phases, while maintaining satisfactory task performance. Our\nproposed GenSafe not only offers a novel measure to augment existing SRL\nmethods but also shows broad compatibility with various SRL algorithms, making\nit applicable to a wide range of systems and SRL problems.\n","authors":["Zhehua Zhou","Xuan Xie","Jiayang Song","Zhan Shu","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2406.03912v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07985v1","updated":"2025-01-14T10:13:41Z","published":"2025-01-14T10:13:41Z","title":"CHEQ-ing the Box: Safe Variable Impedance Learning for Robotic Polishing","summary":"  Robotic systems are increasingly employed for industrial automation, with\ncontact-rich tasks like polishing requiring dexterity and compliant behaviour.\nThese tasks are difficult to model, making classical control challenging. Deep\nreinforcement learning (RL) offers a promising solution by enabling the\nlearning of models and control policies directly from data. However, its\napplication to real-world problems is limited by data inefficiency and unsafe\nexploration. Adaptive hybrid RL methods blend classical control and RL\nadaptively, combining the strengths of both: structure from control and\nlearning from RL. This has led to improvements in data efficiency and\nexploration safety. However, their potential for hardware applications remains\nunderexplored, with no evaluations on physical systems to date. Such\nevaluations are critical to fully assess the practicality and effectiveness of\nthese methods in real-world settings. This work presents an experimental\ndemonstration of the hybrid RL algorithm CHEQ for robotic polishing with\nvariable impedance, a task requiring precise force and velocity tracking. In\nsimulation, we show that variable impedance enhances polishing performance. We\ncompare standalone RL with adaptive hybrid RL, demonstrating that CHEQ achieves\neffective learning while adhering to safety constraints. On hardware, CHEQ\nachieves effective polishing behaviour, requiring only eight hours of training\nand incurring just five failures. These results highlight the potential of\nadaptive hybrid RL for real-world, contact-rich tasks trained directly on\nhardware.\n","authors":["Emma Cramer","Lukas Jäschke","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2501.07985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08558v2","updated":"2025-01-14T10:02:39Z","published":"2024-09-13T06:24:18Z","title":"Fair CoVariance Neural Networks","summary":"  Covariance-based data processing is widespread across signal processing and\nmachine learning applications due to its ability to model data\ninterconnectivities and dependencies. However, harmful biases in the data may\nbecome encoded in the sample covariance matrix and cause data-driven methods to\ntreat different subpopulations unfairly. Existing works such as fair principal\ncomponent analysis (PCA) mitigate these effects, but remain unstable in low\nsample regimes, which in turn may jeopardize the fairness goal. To address both\nbiases and instability, we propose Fair coVariance Neural Networks (FVNNs),\nwhich perform graph convolutions on the covariance matrix for both fair and\naccurate predictions. Our FVNNs provide a flexible model compatible with\nseveral existing bias mitigation techniques. In particular, FVNNs allow for\nmitigating the bias in two ways: first, they operate on fair covariance\nestimates that remove biases from their principal components; second, they are\ntrained in an end-to-end fashion via a fairness regularizer in the loss\nfunction so that the model parameters are tailored to solve the task directly\nin a fair manner. We prove that FVNNs are intrinsically fairer than analogous\nPCA approaches thanks to their stability in low sample regimes. We validate the\nrobustness and fairness of our model on synthetic and real-world data,\nshowcasing the flexibility of FVNNs along with the tradeoff between fair and\naccurate performance.\n","authors":["Andrea Cavallo","Madeline Navarro","Santiago Segarra","Elvin Isufi"],"pdf_url":"https://arxiv.org/pdf/2409.08558v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02931v2","updated":"2025-01-14T10:01:41Z","published":"2025-01-06T11:14:18Z","title":"Self-Attention as a Parametric Endofunctor: A Categorical Framework for\n  Transformer Architectures","summary":"  Self-attention mechanisms have revolutionised deep learning architectures,\nyet their core mathematical structures remain incompletely understood. In this\nwork, we develop a category-theoretic framework focusing on the linear\ncomponents of self-attention. Specifically, we show that the query, key, and\nvalue maps naturally define a parametric 1-morphism in the 2-category\n$\\mathbf{Para(Vect)}$. On the underlying 1-category $\\mathbf{Vect}$, these maps\ninduce an endofunctor whose iterated composition precisely models multi-layer\nattention. We further prove that stacking multiple self-attention layers\ncorresponds to constructing the free monad on this endofunctor. For positional\nencodings, we demonstrate that strictly additive embeddings correspond to\nmonoid actions in an affine sense, while standard sinusoidal encodings, though\nnot additive, retain a universal property among injective (faithful)\nposition-preserving maps. We also establish that the linear portions of\nself-attention exhibit natural equivariance to permutations of input tokens,\nand show how the \"circuits\" identified in mechanistic interpretability can be\ninterpreted as compositions of parametric 1-morphisms. This categorical\nperspective unifies geometric, algebraic, and interpretability-based approaches\nto transformer analysis, making explicit the underlying structures of\nattention. We restrict to linear maps throughout, deferring the treatment of\nnonlinearities such as softmax and layer normalisation, which require more\nadvanced categorical constructions. Our results build on and extend recent work\non category-theoretic foundations for deep learning, offering deeper insights\ninto the algebraic structure of attention mechanisms.\n","authors":["Charles O'Neill"],"pdf_url":"https://arxiv.org/pdf/2501.02931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21151v2","updated":"2025-01-14T09:58:24Z","published":"2024-07-30T19:28:28Z","title":"Private Collaborative Edge Inference via Over-the-Air Computation","summary":"  We consider collaborative inference at the wireless edge, where each client's\nmodel is trained independently on its local dataset. Clients are queried in\nparallel to make an accurate decision collaboratively. In addition to\nmaximizing the inference accuracy, we also want to ensure the privacy of local\nmodels. To this end, we leverage the superposition property of the multiple\naccess channel to implement bandwidth-efficient multi-user inference methods.\nWe propose different methods for ensemble and multi-view classification that\nexploit over-the-air computation (OAC). We show that these schemes perform\nbetter than their orthogonal counterparts with statistically significant\ndifferences while using fewer resources and providing privacy guarantees. We\nalso provide experimental results verifying the benefits of the proposed OAC\napproach to multi-user inference, and perform an ablation study to demonstrate\nthe effectiveness of our design choices. We share the source code of the\nframework publicly on Github to facilitate further research and\nreproducibility.\n","authors":["Selim F. Yilmaz","Burak Hasircioglu","Li Qiao","Deniz Gunduz"],"pdf_url":"https://arxiv.org/pdf/2407.21151v2.pdf","comment":"17 pages, 8 figures. This work extends from our preliminary study\n  presented at the 2022 IEEE International Symposium on Information Theory [1].\n  arXiv admin note: text overlap with arXiv:2202.03129"},{"id":"http://arxiv.org/abs/2410.11005v2","updated":"2025-01-14T09:52:50Z","published":"2024-10-14T18:44:23Z","title":"One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of\n  Large Language Models in Reasoning Tasks","summary":"  Language is not monolithic. While benchmarks, including those designed for\nmultiple languages, are often used as proxies to evaluate the performance of\nLarge Language Models (LLMs), they tend to overlook the nuances of\nwithin-language variation, and thus fail to model the experience of speakers of\nnon-standard dialects. Focusing on African American Vernacular English (AAVE),\nwe present the first study aimed at objectively assessing the fairness and\nrobustness of LLMs in handling dialects in canonical reasoning tasks, including\nalgorithm, math, logic, and integrated reasoning. We introduce \\textbf{ReDial}\n(\\textbf{Re}asoning with \\textbf{Dial}ect Queries), a benchmark containing\n1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE\nspeakers, including experts with computer science backgrounds, to rewrite seven\npopular benchmarks, such as HumanEval and GSM8K. With ReDial, we evaluate\nwidely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model\nfamilies. Our findings reveal that \\textbf{almost all of these widely used\nmodels show significant brittleness and unfairness to queries in AAVE}. Our\nwork establishes a systematic and objective framework for analyzing LLM bias in\ndialectal queries. Moreover, it highlights how mainstream LLMs provide unfair\nservice to dialect speakers in reasoning tasks, laying a critical foundation\nfor relevant future research. Code and data can be accessed at\nhttps://github.com/fangru-lin/redial_dialect_robustness_fairness.\n","authors":["Fangru Lin","Shaoguang Mao","Emanuele La Malfa","Valentin Hofmann","Adrian de Wynter","Xun Wang","Si-Qing Chen","Michael Wooldridge","Janet B. Pierrehumbert","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.11005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07964v1","updated":"2025-01-14T09:35:49Z","published":"2025-01-14T09:35:49Z","title":"Derivation of Output Correlation Inferences for Multi-Output (aka\n  Multi-Task) Gaussian Process","summary":"  Gaussian process (GP) is arguably one of the most widely used machine\nlearning algorithms in practice. One of its prominent applications is Bayesian\noptimization (BO). Although the vanilla GP itself is already a powerful tool\nfor BO, it is often beneficial to be able to consider the dependencies of\nmultiple outputs. To do so, Multi-task GP (MTGP) is formulated, but it is not\ntrivial to fully understand the derivations of its formulations and their\ngradients from the previous literature. This paper serves friendly derivations\nof the MTGP formulations and their gradients.\n","authors":["Shuhei Watanabe"],"pdf_url":"https://arxiv.org/pdf/2501.07964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07957v1","updated":"2025-01-14T09:21:17Z","published":"2025-01-14T09:21:17Z","title":"AI Guide Dog: Egocentric Path Prediction on Smartphone","summary":"  This paper introduces AI Guide Dog (AIGD), a lightweight egocentric\nnavigation assistance system for visually impaired individuals, designed for\nreal-time deployment on smartphones. AIGD addresses key challenges in blind\nnavigation by employing a vision-only, multi-label classification approach to\npredict directional commands, ensuring safe traversal across diverse\nenvironments. We propose a novel technique to enable goal-based outdoor\nnavigation by integrating GPS signals and high-level directions, while also\naddressing uncertain multi-path predictions for destination-free indoor\nnavigation. Our generalized model is the first navigation assistance system to\nhandle both goal-oriented and exploratory navigation scenarios across indoor\nand outdoor settings, establishing a new state-of-the-art in blind navigation.\nWe present methods, datasets, evaluations, and deployment insights to encourage\nfurther innovations in assistive navigation systems.\n","authors":["Aishwarya Jadhav","Jeffery Cao","Abhishree Shetty","Urvashi Priyam Kumar","Aditi Sharma","Ben Sukboontip","Jayant Sravan Tamarapalli","Jingyi Zhang","Anirudh Koul"],"pdf_url":"https://arxiv.org/pdf/2501.07957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07446v2","updated":"2025-01-14T09:17:26Z","published":"2025-01-13T16:16:53Z","title":"Synthesis and Analysis of Data as Probability Measures with\n  Entropy-Regularized Optimal Transport","summary":"  We consider synthesis and analysis of probability measures using the\nentropy-regularized Wasserstein-2 cost and its unbiased version, the Sinkhorn\ndivergence. The synthesis problem consists of computing the barycenter, with\nrespect to these costs, of $m$ reference measures given a set of coefficients\nbelonging to the $m$-dimensional simplex. The analysis problem consists of\nfinding the coefficients for the closest barycenter in the Wasserstein-2\ndistance to a given measure $\\mu$. Under the weakest assumptions on the\nmeasures thus far in the literature, we compute the derivative of the\nentropy-regularized Wasserstein-2 cost. We leverage this to establish a\ncharacterization of regularized barycenters as solutions to a fixed-point\nequation for the average of the entropic maps from the barycenter to the\nreference measures. This characterization yields a finite-dimensional, convex,\nquadratic program for solving the analysis problem when $\\mu$ is a barycenter.\nIt is shown that these coordinates, as well as the value of the barycenter\nfunctional, can be estimated from samples with dimension-independent rates of\nconvergence, a hallmark of entropy-regularized optimal transport, and we verify\nthese rates experimentally. We also establish that barycentric coordinates are\nstable with respect to perturbations in the Wasserstein-2 metric, suggesting a\nrobustness of these coefficients to corruptions. We employ the barycentric\ncoefficients as features for classification of corrupted point cloud data, and\nshow that compared to neural network baselines, our approach is more efficient\nin small training data regimes.\n","authors":["Brendan Mallery","James M. Murphy","Shuchin Aeron"],"pdf_url":"https://arxiv.org/pdf/2501.07446v2.pdf","comment":"58 pages. Code to reproduce experiments:\n  https://github.com/brendanmallery9/Entropic-Barycenters"},{"id":"http://arxiv.org/abs/2501.00709v2","updated":"2025-01-14T09:05:54Z","published":"2025-01-01T03:12:18Z","title":"KAN KAN Buff Signed Graph Neural Networks?","summary":"  Graph Representation Learning focuses on creating embeddings for nodes and\nedges that capture their features and connections. Graph Neural Networks (GNNs)\nuse neural networks to model complex graph relationships. The Kolmogorov-Arnold\nNeural Network (KAN) has recently emerged as an alternative to the Multi-Layer\nPerceptron (MLP), offering better accuracy and interpretability with fewer\nparameters. KANs have been applied to GNN tasks. This paper introduces the\nintegration of KANs into Signed Graph Convolutional Networks (SGCNs). We\nevaluate KAN-enhanced SGCNs (KASGCN) on signed community detection and link\nsign prediction tasks to improve embedding quality in signed networks. While\nthe results show some variability, KASGCN performs competitively with or\nsimilarly to the standard SGCN in the functions tested. Its effectiveness\ndepends on the specific context, such as the signed graph and parameter\nsettings.\n","authors":["Muhieddine Shebaro","Jelena Tešić"],"pdf_url":"https://arxiv.org/pdf/2501.00709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07317v2","updated":"2025-01-14T09:00:27Z","published":"2025-01-13T13:28:03Z","title":"Evaluation of Artificial Intelligence Methods for Lead Time Prediction\n  in Non-Cycled Areas of Automotive Production","summary":"  The present study examines the effectiveness of applying Artificial\nIntelligence methods in an automotive production environment to predict unknown\nlead times in a non-cycle-controlled production area. Data structures are\nanalyzed to identify contextual features and then preprocessed using one-hot\nencoding. Methods selection focuses on supervised machine learning techniques.\nIn supervised learning methods, regression and classification methods are\nevaluated. Continuous regression based on target size distribution is not\nfeasible. Classification methods analysis shows that Ensemble Learning and\nSupport Vector Machines are the most suitable. Preliminary study results\nindicate that gradient boosting algorithms LightGBM, XGBoost, and CatBoost\nyield the best results. After further testing and extensive hyperparameter\noptimization, the final method choice is the LightGBM algorithm. Depending on\nfeature availability and prediction interval granularity, relative prediction\naccuracies of up to 90% can be achieved. Further tests highlight the importance\nof periodic retraining of AI models to accurately represent complex production\nprocesses using the database. The research demonstrates that AI methods can be\neffectively applied to highly variable production data, adding business value\nby providing an additional metric for various control tasks while outperforming\ncurrent non AI-based systems.\n","authors":["Cornelius Hake","Jonas Weigele","Frederik Reichert","Christian Friedrich"],"pdf_url":"https://arxiv.org/pdf/2501.07317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14961v3","updated":"2025-01-14T08:56:48Z","published":"2024-01-26T15:52:41Z","title":"Set-Based Training for Neural Network Verification","summary":"  Neural networks are vulnerable to adversarial attacks, i.e., small input\nperturbations can significantly affect the outputs of a neural network.\nTherefore, to ensure safety of safety-critical environments, the robustness of\na neural network must be formally verified against input perturbations, e.g.,\nfrom noisy sensors. To improve the robustness of neural networks and thus\nsimplify the formal verification, we present a novel set-based training\nprocedure in which we compute the set of possible outputs given the set of\npossible inputs and compute for the first time a gradient set, i.e., each\npossible output has a different gradient. Therefore, we can directly reduce the\nsize of the output enclosure by choosing gradients toward its center. Small\noutput enclosures increase the robustness of a neural network and, at the same\ntime, simplify its formal verification. The latter benefit is due to the fact\nthat a larger size of propagated sets increases the conservatism of most\nverification methods. Our extensive evaluation demonstrates that set-based\ntraining produces robust neural networks with competitive performance, which\ncan be verified using fast (polynomial-time) verification algorithms due to the\nreduced output set.\n","authors":["Lukas Koller","Tobias Ladner","Matthias Althoff"],"pdf_url":"https://arxiv.org/pdf/2401.14961v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13874v4","updated":"2025-01-14T08:42:23Z","published":"2024-10-02T13:02:17Z","title":"COOL: Efficient and Reliable Chain-Oriented Objective Logic with Neural\n  Networks Feedback Control for Program Synthesis","summary":"  Program synthesis methods, whether formal or neural-based, lack fine-grained\ncontrol and flexible modularity, which limits their adaptation to complex\nsoftware development. These limitations stem from rigid Domain-Specific\nLanguage (DSL) frameworks and neural network incorrect predictions. To this\nend, we propose the Chain of Logic (CoL), which organizes the synthesis process\ninto an activity flow and provides heuristic control to guide the process.\nFurthermore, by integrating neural networks with libraries and introducing a\nNeural Network Feedback Control (NNFC) mechanism, our approach modularizes\nsynthesis and mitigates the impact of neural network mispredictions.\nExperiments on relational and symbolic synthesis tasks show that CoL\nsignificantly enhances the efficiency and reliability of DSL program synthesis\nacross multiple metrics. Specifically, CoL improves accuracy by 70% while\nreducing tree operations by 91% and time by 95%. Additionally, NNFC further\nboosts accuracy by 6%, with a 64% reduction in tree operations under\nchallenging conditions such as insufficient training data, increased\ndifficulty, and multidomain synthesis. These improvements confirm COOL as a\nhighly efficient and reliable program synthesis framework.\n","authors":["Jipeng Han"],"pdf_url":"https://arxiv.org/pdf/2410.13874v4.pdf","comment":"31 pages, 11 figures"},{"id":"http://arxiv.org/abs/2501.07927v1","updated":"2025-01-14T08:30:49Z","published":"2025-01-14T08:30:49Z","title":"Gandalf the Red: Adaptive Security for LLMs","summary":"  Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and rigorously expresses the\nsecurity-utility in an optimizable form. We further address the shortcomings in\nexisting evaluations by introducing Gandalf, a crowd-sourced, gamified\nred-teaming platform designed to generate realistic, adaptive attack datasets.\nUsing Gandalf, we collect and release a dataset of 279k prompt attacks.\nComplemented by benign user data, our analysis reveals the interplay between\nsecurity and utility, showing that defenses integrated in the LLM (e.g., system\nprompts) can degrade usability even without blocking requests. We demonstrate\nthat restricted application domains, defense-in-depth, and adaptive defenses\nare effective strategies for building secure and useful LLM applications. Code\nis available at\n\\href{https://github.com/lakeraai/dsec-gandalf}{\\texttt{https://github.com/lakeraai/dsec-gandalf}}.\n","authors":["Niklas Pfister","Václav Volhejn","Manuel Knott","Santiago Arias","Julia Bazińska","Mykhailo Bichurin","Alan Commike","Janet Darling","Peter Dienes","Matthew Fiedler","David Haber","Matthias Kraft","Marco Lancini","Max Mathys","Damián Pascual-Ortiz","Jakub Podolak","Adrià Romero-López","Kyriacos Shiarlis","Andreas Signer","Zsolt Terek","Athanasios Theocharis","Daniel Timbrell","Samuel Trautwein","Samuel Watts","Natalie Wu","Mateo Rojas-Carulla"],"pdf_url":"https://arxiv.org/pdf/2501.07927v1.pdf","comment":"Niklas Pfister, V\\'aclav Volhejn and Manuel Knott contributed equally"},{"id":"http://arxiv.org/abs/2501.07925v1","updated":"2025-01-14T08:26:58Z","published":"2025-01-14T08:26:58Z","title":"Phase of Flight Classification in Aviation Safety using LSTM, GRU, and\n  BiLSTM: A Case Study with ASN Dataset","summary":"  Safety is the main concern in the aviation industry, where even minor\noperational issues can lead to serious consequences. This study addresses the\nneed for comprehensive aviation accident analysis by leveraging natural\nlanguage processing (NLP) and advanced AI models to classify the phase of\nflight from unstructured aviation accident analysis narratives. The research\naims to determine whether the phase of flight can be inferred from narratives\nof post-accident events using NLP techniques. The classification performance of\nvarious deep learning models was evaluated. For single RNN-based models, LSTM\nachieved an accuracy of 63%, precision 60%, and recall 61%. BiLSTM recorded an\naccuracy of 64%, precision 63%, and a recall of 64%. GRU exhibited balanced\nperformance with an accuracy and recall of 60% and a precision of 63%. Joint\nRNN-based models further enhanced predictive capabilities. GRU-LSTM,\nLSTM-BiLSTM, and GRU-BiLSTM demonstrated accuracy rates of 62%, 67%, and 60%,\nrespectively, showcasing the benefits of combining these architectures. To\nprovide a comprehensive overview of model performance, single and combined\nmodels were compared in terms of the various metrics. These results underscore\nthe models' capacity to classify the phase of flight from raw text narratives,\nequipping aviation industry stakeholders with valuable insights for proactive\ndecision-making. Therefore, this research signifies a substantial advancement\nin the application of NLP and deep learning models to enhance aviation safety.\n","authors":["Aziida Nanyonga","Hassan Wasswa","Graham Wild"],"pdf_url":"https://arxiv.org/pdf/2501.07925v1.pdf","comment":"Aviation Safety, Deep learning algorithms, Flight phase, NLP, ASN,\n  and Classification"},{"id":"http://arxiv.org/abs/2501.07923v1","updated":"2025-01-14T08:18:41Z","published":"2025-01-14T08:18:41Z","title":"Aviation Safety Enhancement via NLP & Deep Learning: Classifying Flight\n  Phases in ATSB Safety Reports","summary":"  Aviation safety is paramount, demanding precise analysis of safety\noccurrences during different flight phases. This study employs Natural Language\nProcessing (NLP) and Deep Learning models, including LSTM, CNN, Bidirectional\nLSTM (BLSTM), and simple Recurrent Neural Networks (sRNN), to classify flight\nphases in safety reports from the Australian Transport Safety Bureau (ATSB).\nThe models exhibited high accuracy, precision, recall, and F1 scores, with LSTM\nachieving the highest performance of 87%, 88%, 87%, and 88%, respectively. This\nperformance highlights their effectiveness in automating safety occurrence\nanalysis. The integration of NLP and Deep Learning technologies promises\ntransformative enhancements in aviation safety analysis, enabling targeted\nsafety measures and streamlined report handling.\n","authors":["Aziida Nanyonga","Hassan Wasswa","Graham Wild"],"pdf_url":"https://arxiv.org/pdf/2501.07923v1.pdf","comment":"NLP, Aviation Safety, ATSB, Deep learning, Flight phase. arXiv admin\n  note: substantial text overlap with arXiv:2501.01694"},{"id":"http://arxiv.org/abs/2403.10568v3","updated":"2025-01-14T08:01:17Z","published":"2024-03-14T17:47:10Z","title":"MoPE: Mixture of Prompt Experts for Parameter-Efficient and Scalable\n  Multimodal Fusion","summary":"  Despite the demonstrated parameter efficiency of prompt-based multimodal\nfusion methods, their limited adaptivity and expressiveness often result in\nsuboptimal performance compared to other tuning approaches. In this paper, we\nintroduce the Mixture of Prompt Experts (MoPE), the first technique designed to\novercome these limitations by decomposing standard prompts to capture\ninstance-level features adaptively. Building on this decomposition, MoPE\nenhances prompt fusion's expressiveness by leveraging multimodal pairing priors\nto route the most effective prompt for each instance dynamically. Compared to\nvanilla prompting, our MoPE-based fusion method exhibits greater\nexpressiveness, scaling more effectively with the training data and the overall\nnumber of trainable parameters. We also investigate regularization terms for\nexpert routing, which lead to emergent expert specialization with enhanced\nadaptiveness and interpretablity. Extensive experiments across six multimodal\ndatasets spanning four modalities demonstrate state-of-the-art performance for\nprompt fusion, matching or even surpassing the performance of fine-tuning while\nrequiring only 0.8% of the trainable parameters. Project homepage:\nhttps://github.com/songrise/MoPE\n","authors":["Ruixiang Jiang","Lingbo Liu","Changwen Chen"],"pdf_url":"https://arxiv.org/pdf/2403.10568v3.pdf","comment":"Under Review, Extended version of arxiv:2312.03734"},{"id":"http://arxiv.org/abs/2501.07905v1","updated":"2025-01-14T07:50:09Z","published":"2025-01-14T07:50:09Z","title":"Logarithmic Memory Networks (LMNs): Efficient Long-Range Sequence\n  Modeling for Resource-Constrained Environments","summary":"  Long-range sequence modeling is a crucial aspect of natural language\nprocessing and time series analysis. However, traditional models like Recurrent\nNeural Networks (RNNs) and Transformers suffer from computational and memory\ninefficiencies, especially when dealing with long sequences. This paper\nintroduces Logarithmic Memory Networks (LMNs), a novel architecture that\nleverages a hierarchical logarithmic tree structure to efficiently store and\nretrieve past information. LMNs dynamically summarize historical context,\nsignificantly reducing the memory footprint and computational complexity of\nattention mechanisms from O(n2) to O(log(n)). The model employs a\nsingle-vector, targeted attention mechanism to access stored information, and\nthe memory block construction worker (summarizer) layer operates in two modes:\na parallel execution mode during training for efficient processing of\nhierarchical tree structures and a sequential execution mode during inference,\nwhich acts as a memory management system. It also implicitly encodes positional\ninformation, eliminating the need for explicit positional encodings. These\nfeatures make LMNs a robust and scalable solution for processing long-range\nsequences in resource-constrained environments, offering practical improvements\nin efficiency and scalability. The code is publicly available under the MIT\nLicense on GitHub: https://github.com/AhmedBoin/LogarithmicMemory.\n","authors":["Mohamed A. Taha"],"pdf_url":"https://arxiv.org/pdf/2501.07905v1.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2501.07903v1","updated":"2025-01-14T07:46:33Z","published":"2025-01-14T07:46:33Z","title":"Optimal Classification Trees for Continuous Feature Data Using Dynamic\n  Programming with Branch-and-Bound","summary":"  Computing an optimal classification tree that provably maximizes training\nperformance within a given size limit, is NP-hard, and in practice, most\nstate-of-the-art methods do not scale beyond computing optimal trees of depth\nthree. Therefore, most methods rely on a coarse binarization of continuous\nfeatures to maintain scalability. We propose a novel algorithm that optimizes\ntrees directly on the continuous feature data using dynamic programming with\nbranch-and-bound. We develop new pruning techniques that eliminate many\nsub-optimal splits in the search when similar to previously computed splits and\nwe provide an efficient subroutine for computing optimal depth-two trees. Our\nexperiments demonstrate that these techniques improve runtime by one or more\norders of magnitude over state-of-the-art optimal methods and improve test\naccuracy by 5% over greedy heuristics.\n","authors":["Catalin E. Brita","Jacobus G. M. van der Linden","Emir Demirović"],"pdf_url":"https://arxiv.org/pdf/2501.07903v1.pdf","comment":"In the proceedings of AAAI-25"},{"id":"http://arxiv.org/abs/2411.02824v2","updated":"2025-01-14T07:30:20Z","published":"2024-11-05T05:50:51Z","title":"Layer-Adaptive State Pruning for Deep State Space Models","summary":"  Due to the lack of state dimension optimization methods, deep state space\nmodels (SSMs) have sacrificed model capacity, training search space, or\nstability to alleviate computational costs caused by high state dimensions. In\nthis work, we provide a structured pruning method for SSMs, Layer-Adaptive\nSTate pruning (LAST), which reduces the state dimension of each layer in\nminimizing model-level output energy loss by extending modal truncation for a\nsingle system. LAST scores are evaluated using the $\\mathcal{H}_{\\infty}$ norms\nof subsystems and layer-wise energy normalization. The scores serve as global\npruning criteria, enabling cross-layer comparison of states and layer-adaptive\npruning. Across various sequence benchmarks, LAST optimizes previous SSMs,\nrevealing the redundancy and compressibility of their state spaces. Notably, we\ndemonstrate that, on average, pruning 33% of states still maintains performance\nwith 0.52% accuracy loss in multi-input multi-output SSMs without retraining.\nCode is available at https://github.com/msgwak/LAST.\n","authors":["Minseon Gwak","Seongrok Moon","Joohwan Ko","PooGyeon Park"],"pdf_url":"https://arxiv.org/pdf/2411.02824v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2407.07368v2","updated":"2025-01-14T07:28:06Z","published":"2024-07-10T05:03:48Z","title":"Data-driven Bayesian State Estimation with Compressed Measurement of\n  Model-free Process using Semi-supervised Learning","summary":"  The research topic is: data-driven Bayesian state estimation with compressed\nmeasurement (BSCM) of model-free process, say for a (causal) tracking\napplication. The dimension of the temporal measurement vector is lower than the\ndimension of the temporal state vector to be estimated. Hence the state\nestimation problem is an underdetermined inverse problem. The underlying\ndynamical model of the states is assumed to be unknown and hence, we use the\nterminology 'model-free process'. In absence of the dynamical model, we can not\nemploy traditional model-driven methods like Kalman Filter (KF) and Particle\nFilter (PF), and instead require data-driven methods. We first experimentally\nshow that two existing unsupervised learning-based data-driven methods fail to\naddress the BSCM problem for model-free process; they are - data-driven\nnonlinear state estimation (DANSE) method and deep Markov model (DMM) method.\nThe unsupervised learning uses unlabelled data comprised of only noisy, linear\nmeasurements. While DANSE provides a good predictive / forecasting performance\nto model the temporal measurement data as time-series, its unsupervised\nlearning lacks a regularization for state estimation. We then investigate the\nuse of a semi-supervised learning approach, and develop a semi-supervised\nlearning-based DANSE method, referred to as SemiDANSE. In SemiDANSE, we use a\nlimited amount of labelled data along-with a large amount of unlabelled data,\nand that helps to bring the desired regularization for addressing the BSCM\nproblem. The labelled data means pairwise measurement-and-state data. Using\nthree chaotic dynamical systems (or processes) with nonlinear dynamical models\nas benchmark, we show that the data-driven SemiDANSE provides competitive\nperformance for BSCM against a hybrid method called KalmanNet and two\nmodel-driven methods -- an extended KF (EKF) and an unscented KF (UKF).\n","authors":["Anubhab Ghosh","Yonina C. Eldar","Saikat Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2407.07368v2.pdf","comment":"14 pages, under review at IEEE TSP"},{"id":"http://arxiv.org/abs/2410.15322v2","updated":"2025-01-14T06:59:12Z","published":"2024-10-20T07:32:16Z","title":"FoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion\n  Model","summary":"  Mobile traffic forecasting allows operators to anticipate network dynamics\nand performance in advance, offering substantial potential for enhancing\nservice quality and improving user experience. However, existing models are\noften task-oriented and are trained with tailored data, which limits their\neffectiveness in diverse mobile network tasks of Base Station (BS) deployment,\nresource allocation, energy optimization, etc. and hinders generalization\nacross different urban environments. Foundation models have made remarkable\nstrides across various domains of NLP and CV due to their multi-tasking\nadaption and zero/few-shot learning capabilities. In this paper, we propose an\ninnovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to\nhandle diverse forecasting tasks of short/long-term predictions and\ndistribution generation across multiple cities to support network planning and\noptimization. FoMo combines diffusion models and transformers, where various\nspatio-temporal masks are proposed to enable FoMo to learn intrinsic features\nof different tasks, and a contrastive learning strategy is developed to capture\nthe correlations between mobile traffic and urban contexts, thereby improving\nits transfer learning capability. Extensive experiments on 9 real-world\ndatasets demonstrate that FoMo outperforms current models concerning diverse\nforecasting tasks and zero/few-shot learning, showcasing a strong universality.\n","authors":["Haoye Chai","Xiaoqian Qi","Shiyuan Zhang","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2410.15322v2.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2501.07886v1","updated":"2025-01-14T06:54:17Z","published":"2025-01-14T06:54:17Z","title":"Iterative Label Refinement Matters More than Preference Optimization\n  under Weak Supervision","summary":"  Language model (LM) post-training relies on two stages of human supervision:\ntask demonstrations for supervised finetuning (SFT), followed by preference\ncomparisons for reinforcement learning from human feedback (RLHF). As LMs\nbecome more capable, the tasks they are given become harder to supervise. Will\npost-training remain effective under unreliable supervision? To test this, we\nsimulate unreliable demonstrations and comparison feedback using small LMs and\ntime-constrained humans. We find that in the presence of unreliable\nsupervision, SFT still retains some effectiveness, but DPO (a common RLHF\nalgorithm) fails to improve the model beyond SFT. To address this, we propose\niterative label refinement (ILR) as an alternative to RLHF. ILR improves the\nSFT data by using comparison feedback to decide whether human demonstrations\nshould be replaced by model-generated alternatives, then retrains the model via\nSFT on the updated data. SFT+ILR outperforms SFT+DPO on several tasks with\nunreliable supervision (math, coding, and safe instruction-following). Our\nfindings suggest that as LMs are used for complex tasks where human supervision\nis unreliable, RLHF may no longer be the best use of human comparison feedback;\ninstead, it is better to direct feedback towards improving the training data\nrather than continually training the model. Our code and data are available at\nhttps://github.com/helloelwin/iterative-label-refinement.\n","authors":["Yaowen Ye","Cassidy Laidlaw","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2501.07886v1.pdf","comment":"22 pages, 10 figures"},{"id":"http://arxiv.org/abs/2501.07885v1","updated":"2025-01-14T06:51:27Z","published":"2025-01-14T06:51:27Z","title":"Mitigating Algorithmic Bias in Multiclass CNN Classifications Using\n  Causal Modeling","summary":"  This study describes a procedure for applying causal modeling to detect and\nmitigate algorithmic bias in a multiclass classification problem. The dataset\nwas derived from the FairFace dataset, supplemented with emotional labels\ngenerated by the DeepFace pre-trained model. A custom Convolutional Neural\nNetwork (CNN) was developed, consisting of four convolutional blocks, followed\nby fully connected layers and dropout layers to mitigate overfitting. Gender\nbias was identified in the CNN model's classifications: Females were more\nlikely to be classified as \"happy\" or \"sad,\" while males were more likely to be\nclassified as \"neutral.\" To address this, the one-vs-all (OvA) technique was\napplied. A causal model was constructed for each emotion class to adjust the\nCNN model's predicted class probabilities. The adjusted probabilities for the\nvarious classes were then aggregated by selecting the class with the highest\nprobability. The resulting debiased classifications demonstrated enhanced\ngender fairness across all classes, with negligible impact--or even a slight\nimprovement--on overall accuracy. This study highlights that algorithmic\nfairness and accuracy are not necessarily trade-offs. All data and code for\nthis study are publicly available for download.\n","authors":["Min Sik Byun","Wendy Wan Yee Hui","Wai Kwong Lau"],"pdf_url":"https://arxiv.org/pdf/2501.07885v1.pdf","comment":"7 pages; 6 figures"},{"id":"http://arxiv.org/abs/2501.07884v1","updated":"2025-01-14T06:50:56Z","published":"2025-01-14T06:50:56Z","title":"MD-Syn: Synergistic drug combination prediction based on the\n  multidimensional feature fusion method and attention mechanisms","summary":"  Drug combination therapies have shown promising therapeutic efficacy in\ncomplex diseases and have demonstrated the potential to reduce drug resistance.\nHowever, the huge number of possible drug combinations makes it difficult to\nscreen them all in traditional experiments. In this study, we proposed MD-Syn,\na computational framework, which is based on the multidimensional feature\nfusion method and multi-head attention mechanisms. Given drug pair-cell line\ntriplets, MD-Syn considers one-dimensional and two-dimensional feature spaces\nsimultaneously. It consists of a one-dimensional feature embedding module\n(1D-FEM), a two-dimensional feature embedding module (2D-FEM), and a deep\nneural network-based classifier for synergistic drug combination prediction.\nMD-Syn achieved the AUROC of 0.919 in 5-fold cross-validation, outperforming\nthe state-of-the-art methods. Further, MD-Syn showed comparable results over\ntwo independent datasets. In addition, the multi-head attention mechanisms not\nonly learn embeddings from different feature aspects but also focus on\nessential interactive feature elements, improving the interpretability of\nMD-Syn. In summary, MD-Syn is an interpretable framework to prioritize\nsynergistic drug combination pairs with chemicals and cancer cell line gene\nexpression profiles. To facilitate broader community access to this model, we\nhave developed a web portal (https://labyeh104-2.life.nthu.edu.tw/) that\nenables customized predictions of drug combination synergy effects based on\nuser-specified compounds.\n","authors":["XinXin Ge","Yi-Ting Lee","Shan-Ju Yeh"],"pdf_url":"https://arxiv.org/pdf/2501.07884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04539v4","updated":"2025-01-14T06:42:51Z","published":"2023-10-06T19:06:13Z","title":"Generating Less Certain Adversarial Examples Improves Robust\n  Generalization","summary":"  This paper revisits the robust overfitting phenomenon of adversarial\ntraining. Observing that models with better robust generalization performance\nare less certain in predicting adversarially generated training inputs, we\nargue that overconfidence in predicting adversarial examples is a potential\ncause. Therefore, we hypothesize that generating less certain adversarial\nexamples improves robust generalization, and propose a formal definition of\nadversarial certainty that captures the variance of the model's predicted\nlogits on adversarial examples. Our theoretical analysis of synthetic\ndistributions characterizes the connection between adversarial certainty and\nrobust generalization. Accordingly, built upon the notion of adversarial\ncertainty, we develop a general method to search for models that can generate\ntraining-time adversarial inputs with reduced certainty, while maintaining the\nmodel's capability in distinguishing adversarial examples. Extensive\nexperiments on image benchmarks demonstrate that our method effectively learns\nmodels with consistently improved robustness and mitigates robust overfitting,\nconfirming the importance of generating less certain adversarial examples for\nrobust generalization. Our implementations are available as open-source code\nat: https://github.com/TrustMLRG/AdvCertainty.\n","authors":["Minxing Zhang","Michael Backes","Xiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.04539v4.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2501.07879v1","updated":"2025-01-14T06:41:55Z","published":"2025-01-14T06:41:55Z","title":"Distributed Nonparametric Estimation: from Sparse to Dense Samples per\n  Terminal","summary":"  Consider the communication-constrained problem of nonparametric function\nestimation, in which each distributed terminal holds multiple i.i.d. samples.\nUnder certain regularity assumptions, we characterize the minimax optimal rates\nfor all regimes, and identify phase transitions of the optimal rates as the\nsamples per terminal vary from sparse to dense. This fully solves the problem\nleft open by previous works, whose scopes are limited to regimes with either\ndense samples or a single sample per terminal. To achieve the optimal rates, we\ndesign a layered estimation protocol by exploiting protocols for the parametric\ndensity estimation problem. We show the optimality of the protocol using\ninformation-theoretic methods and strong data processing inequalities, and\nincorporating the classic balls and bins model. The optimal rates are immediate\nfor various special cases such as density estimation, Gaussian, binary, Poisson\nand heteroskedastic regression models.\n","authors":["Deheng Yuan","Tao Guo","Zhongyi Huang"],"pdf_url":"https://arxiv.org/pdf/2501.07879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07145v2","updated":"2025-01-14T06:38:10Z","published":"2025-01-13T09:11:13Z","title":"A User's Guide to $\\texttt{KSig}$: GPU-Accelerated Computation of the\n  Signature Kernel","summary":"  The signature kernel is a positive definite kernel for sequential and\ntemporal data that has become increasingly popular in machine learning\napplications due to powerful theoretical guarantees, strong empirical\nperformance, and recently introduced various scalable variations. In this\nchapter, we give a short introduction to $\\texttt{KSig}$, a\n$\\texttt{Scikit-Learn}$ compatible Python package that implements various\nGPU-accelerated algorithms for computing signature kernels, and performing\ndownstream learning tasks. We also introduce a new algorithm based on tensor\nsketches which gives strong performance compared to existing algorithms. The\npackage is available at https://github.com/tgcsaba/ksig.\n","authors":["Csaba Tóth","Danilo Jr Dela Cruz","Harald Oberhauser"],"pdf_url":"https://arxiv.org/pdf/2501.07145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23111v6","updated":"2025-01-14T06:25:54Z","published":"2024-10-30T15:23:44Z","title":"Exploring Gradient Subspaces: Addressing and Overcoming LoRA's\n  Limitations in Federated Fine-Tuning of Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison unmasks inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore along with\ndirect-weight aggregation is a more effective approach, outperforming federated\nLoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.\nWhile privacy remains paramount in FL discourse, our focus is on assessing\nperformance outcomes of federated fine-tuned models and evaluating various FL\nframeworks from both theoretical and empirical perspectives. Our findings\nadvocate reassessing the reliance on LoRA within FL contexts, paving the way\nfor more efficient training methodologies.\n","authors":["Navyansh Mahla","Kshitij Sharad Jadhav","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2410.23111v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19982v2","updated":"2025-01-14T06:18:03Z","published":"2024-10-25T21:46:25Z","title":"Random Policy Enables In-Context Reinforcement Learning within Trust\n  Horizons","summary":"  Pretrained foundation models have exhibited extraordinary in-context learning\nperformance, allowing zero-shot generalization to new tasks not encountered\nduring pretraining. In the case of reinforcement learning (RL), in-context RL\n(ICRL) emerges when pretraining FMs on decision-making problems in an\nautoregressive-supervised manner. Nevertheless, current state-of-the-art ICRL\nalgorithms, like Algorithm Distillation, Decision Pretrained Transformer and\nDecision Importance Transformer, impose stringent requirements on the\npretraining dataset concerning the source policies, context information, and\naction labels. Notably, these algorithms either demand optimal policies or\nrequire varying degrees of well-trained behavior policies for all pretraining\nenvironments. This significantly hinders the application of ICRL to real-world\nscenarios, where acquiring optimal or well-trained policies for a substantial\nvolume of real-world training environments can be intractable. To overcome this\nchallenge, we introduce a novel approach, termed State-Action Distillation\n(SAD), that allows to generate an effective pretraining dataset guided solely\nby random policies. In particular, SAD selects query states and corresponding\naction labels by distilling outstanding state-action pairs from the entire\nstate and action spaces by using random policies within a trust horizon, and\nthen inherits the classical autoregressive-supervised mechanism during\npretraining. To the best of our knowledge, this is the first work that enables\neffective ICRL under random policies and random contexts. We also establish\nquantitative analysis of the trustworthiness as well as the performance\nguarantees of SAD. Moreover, our empirical results across multiple popular ICRL\nbenchmark environments demonstrate that, on average, SAD outperforms the best\nbaseline by 236.3% in the offline evaluation and by 135.2% in the online\nevaluation.\n","authors":["Weiqin Chen","Santiago Paternain"],"pdf_url":"https://arxiv.org/pdf/2410.19982v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10703v2","updated":"2025-01-14T06:02:00Z","published":"2024-12-14T06:22:49Z","title":"Doubly-Bounded Queue for Constrained Online Learning: Keeping Pace with\n  Dynamics of Both Loss and Constraint","summary":"  We consider online convex optimization with time-varying constraints and\nconduct performance analysis using two stringent metrics: dynamic regret with\nrespect to the online solution benchmark, and hard constraint violation that\ndoes not allow any compensated violation over time. We propose an efficient\nalgorithm called Constrained Online Learning with Doubly-bounded Queue (COLDQ),\nwhich introduces a novel virtual queue that is both lower and upper bounded,\nallowing tight control of the constraint violation without the need for the\nSlater condition. We prove via a new Lyapunov drift analysis that COLDQ\nachieves $O(T^\\frac{1+V_x}{2})$ dynamic regret and $O(T^{V_g})$ hard constraint\nviolation, where $V_x$ and $V_g$ capture the dynamics of the loss and\nconstraint functions. For the first time, the two bounds smoothly approach to\nthe best-known $O(T^\\frac{1}{2})$ regret and $O(1)$ violation, as the dynamics\nof the losses and constraints diminish. For strongly convex loss functions,\nCOLDQ matches the best-known $O(\\log{T})$ static regret while maintaining the\n$O(T^{V_g})$ hard constraint violation. We further introduce an expert-tracking\nvariation of COLDQ, which achieves the same performance bounds without any\nprior knowledge of the system dynamics. Simulation results demonstrate that\nCOLDQ outperforms the state-of-the-art approaches.\n","authors":["Juncheng Wang","Bingjie Yan","Yituo Liu"],"pdf_url":"https://arxiv.org/pdf/2412.10703v2.pdf","comment":"To appear in AAAI 2025"},{"id":"http://arxiv.org/abs/2501.07859v1","updated":"2025-01-14T05:55:20Z","published":"2025-01-14T05:55:20Z","title":"deepTerra -- AI Land Classification Made Easy","summary":"  deepTerra is a comprehensive platform designed to facilitate the\nclassification of land surface features using machine learning and satellite\nimagery. The platform includes modules for data collection, image augmentation,\ntraining, testing, and prediction, streamlining the entire workflow for image\nclassification tasks. This paper presents a detailed overview of the\ncapabilities of deepTerra, shows how it has been applied to various research\nareas, and discusses the future directions it might take.\n","authors":["Andrew Keith Wilkinson"],"pdf_url":"https://arxiv.org/pdf/2501.07859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19255v2","updated":"2025-01-14T05:48:07Z","published":"2024-12-26T15:45:45Z","title":"Multi-matrix Factorization Attention","summary":"  We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.\n","authors":["Jingcheng Hu","Houyi Li","Yinmin Zhang","Zili Wang","Shuigeng Zhou","Xiangyu Zhang","Heung-Yeung Shum","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.19255v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07855v1","updated":"2025-01-14T05:43:59Z","published":"2025-01-14T05:43:59Z","title":"State-of-the-Art Transformer Models for Image Super-Resolution:\n  Techniques, Challenges, and Applications","summary":"  Image Super-Resolution (SR) aims to recover a high-resolution image from its\nlow-resolution counterpart, which has been affected by a specific degradation\nprocess. This is achieved by enhancing detail and visual quality. Recent\nadvancements in transformer-based methods have remolded image super-resolution\nby enabling high-quality reconstructions surpassing previous deep-learning\napproaches like CNN and GAN-based. This effectively addresses the limitations\nof previous methods, such as limited receptive fields, poor global context\ncapture, and challenges in high-frequency detail recovery. Additionally, the\npaper reviews recent trends and advancements in transformer-based SR models,\nexploring various innovative techniques and architectures that combine\ntransformers with traditional networks to balance global and local contexts.\nThese neoteric methods are critically analyzed, revealing promising yet\nunexplored gaps and potential directions for future research. Several\nvisualizations of models and techniques are included to foster a holistic\nunderstanding of recent trends. This work seeks to offer a structured roadmap\nfor researchers at the forefront of deep learning, specifically exploring the\nimpact of transformers on super-resolution techniques.\n","authors":["Debasish Dutta","Deepjyoti Chetia","Neeharika Sonowal","Sanjib Kr Kalita"],"pdf_url":"https://arxiv.org/pdf/2501.07855v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2501.07850v1","updated":"2025-01-14T05:23:42Z","published":"2025-01-14T05:23:42Z","title":"An Intra- and Cross-frame Topological Consistency Scheme for\n  Semi-supervised Atherosclerotic Coronary Plaque Segmentation","summary":"  Enhancing the precision of segmenting coronary atherosclerotic plaques from\nCT Angiography (CTA) images is pivotal for advanced Coronary Atherosclerosis\nAnalysis (CAA), which distinctively relies on the analysis of vessel\ncross-section images reconstructed via Curved Planar Reformation. This task\npresents significant challenges due to the indistinct boundaries and structures\nof plaques and blood vessels, leading to the inadequate performance of current\ndeep learning models, compounded by the inherent difficulty in annotating such\ncomplex data. To address these issues, we propose a novel dual-consistency\nsemi-supervised framework that integrates Intra-frame Topological Consistency\n(ITC) and Cross-frame Topological Consistency (CTC) to leverage labeled and\nunlabeled data. ITC employs a dual-task network for simultaneous segmentation\nmask and Skeleton-aware Distance Transform (SDT) prediction, achieving similar\nprediction of topology structure through consistency constraint without\nadditional annotations. Meanwhile, CTC utilizes an unsupervised estimator for\nanalyzing pixel flow between skeletons and boundaries of adjacent frames,\nensuring spatial continuity. Experiments on two CTA datasets show that our\nmethod surpasses existing semi-supervised methods and approaches the\nperformance of supervised methods on CAA. In addition, our method also performs\nbetter than other methods on the ACDC dataset, demonstrating its\ngeneralization.\n","authors":["Ziheng Zhang","Zihan Li","Dandan Shan","Yuehui Qiu","Qingqi Hong","Qingqiang Wu"],"pdf_url":"https://arxiv.org/pdf/2501.07850v1.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2411.03865v4","updated":"2025-01-14T05:23:03Z","published":"2024-11-06T12:19:01Z","title":"AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making","summary":"  Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety.\n","authors":["Yizhe Huang","Xingbo Wang","Hao Liu","Fanqi Kong","Aoyang Qin","Min Tang","Song-Chun Zhu","Mingjie Bi","Siyuan Qi","Xue Feng"],"pdf_url":"https://arxiv.org/pdf/2411.03865v4.pdf","comment":"Accepted at NeurIPS D&B 2024"},{"id":"http://arxiv.org/abs/2405.10480v2","updated":"2025-01-14T05:00:34Z","published":"2024-05-17T00:52:39Z","title":"Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers","summary":"  Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths.\n","authors":["Rya Sanovar","Srikant Bharadwaj","Renee St. Amant","Victor Rühle","Saravan Rajmohan"],"pdf_url":"https://arxiv.org/pdf/2405.10480v2.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2404.14389v2","updated":"2025-01-14T04:58:26Z","published":"2024-04-22T17:50:27Z","title":"Poisoning Attacks on Federated Learning-based Wireless Traffic\n  Prediction","summary":"  Federated Learning (FL) offers a distributed framework to train a global\ncontrol model across multiple base stations without compromising the privacy of\ntheir local network data. This makes it ideal for applications like wireless\ntraffic prediction (WTP), which plays a crucial role in optimizing network\nresources, enabling proactive traffic flow management, and enhancing the\nreliability of downstream communication-aided applications, such as IoT\ndevices, autonomous vehicles, and industrial automation systems. Despite its\npromise, the security aspects of FL-based distributed wireless systems,\nparticularly in regression-based WTP problems, remain inadequately\ninvestigated. In this paper, we introduce a novel fake traffic injection (FTI)\nattack, designed to undermine the FL-based WTP system by injecting fabricated\ntraffic distributions with minimal knowledge. We further propose a defense\nmechanism, termed global-local inconsistency detection (GLID), which\nstrategically removes abnormal model parameters that deviate beyond a specific\npercentile range estimated through statistical methods in each dimension.\nExtensive experimental evaluations, performed on real-world wireless traffic\ndatasets, demonstrate that both our attack and defense strategies significantly\noutperform existing baselines.\n","authors":["Zifan Zhang","Minghong Fang","Jiayuan Huang","Yuchen Liu"],"pdf_url":"https://arxiv.org/pdf/2404.14389v2.pdf","comment":"Accepted by IFIP/IEEE Networking 2024"},{"id":"http://arxiv.org/abs/2501.03461v2","updated":"2025-01-14T04:53:30Z","published":"2025-01-07T01:35:56Z","title":"Radar Signal Recognition through Self-Supervised Learning and Domain\n  Adaptation","summary":"  Automatic radar signal recognition (RSR) plays a pivotal role in electronic\nwarfare (EW), as accurately classifying radar signals is critical for informing\ndecision-making processes. Recent advances in deep learning have shown\nsignificant potential in improving RSR performance in domains with ample\nannotated data. However, these methods fall short in EW scenarios where\nannotated RF data are scarce or impractical to obtain. To address these\nchallenges, we introduce a self-supervised learning (SSL) method which utilises\nmasked signal modelling and RF domain adaption to enhance RSR performance in\nenvironments with limited RF samples and labels. Specifically, we investigate\npre-training masked autoencoders (MAE) on baseband in-phase and quadrature\n(I/Q) signals from various RF domains and subsequently transfer the learned\nrepresentation to the radar domain, where annotated data are limited. Empirical\nresults show that our lightweight self-supervised ResNet model with domain\nadaptation achieves up to a 17.5% improvement in 1-shot classification accuracy\nwhen pre-trained on in-domain signals (i.e., radar signals) and up to a 16.31%\nimprovement when pre-trained on out-of-domain signals (i.e., comm signals),\ncompared to its baseline without SSL. We also provide reference results for\nseveral MAE designs and pre-training strategies, establishing a new benchmark\nfor few-shot radar signal classification.\n","authors":["Zi Huang","Simon Denman","Akila Pemasiri","Clinton Fookes","Terrence Martin"],"pdf_url":"https://arxiv.org/pdf/2501.03461v2.pdf","comment":"5 pages, 9 figures"},{"id":"http://arxiv.org/abs/2501.06366v2","updated":"2025-01-14T04:42:08Z","published":"2025-01-10T22:27:44Z","title":"Counterfactually Fair Reinforcement Learning via Sequential Data\n  Preprocessing","summary":"  When applied in healthcare, reinforcement learning (RL) seeks to dynamically\nmatch the right interventions to subjects to maximize population benefit.\nHowever, the learned policy may disproportionately allocate efficacious actions\nto one subpopulation, creating or exacerbating disparities in other\nsocioeconomically-disadvantaged subgroups. These biases tend to occur in\nmulti-stage decision making and can be self-perpetuating, which if unaccounted\nfor could cause serious unintended consequences that limit access to care or\ntreatment benefit. Counterfactual fairness (CF) offers a promising statistical\ntool grounded in causal inference to formulate and study fairness. In this\npaper, we propose a general framework for fair sequential decision making. We\ntheoretically characterize the optimal CF policy and prove its stationarity,\nwhich greatly simplifies the search for optimal CF policies by leveraging\nexisting RL algorithms. The theory also motivates a sequential data\npreprocessing algorithm to achieve CF decision making under an additive noise\nassumption. We prove and then validate our policy learning approach in\ncontrolling unfairness and attaining optimal value through simulations.\nAnalysis of a digital health dataset designed to reduce opioid misuse shows\nthat our proposal greatly enhances fair access to counseling.\n","authors":["Jitao Wang","Chengchun Shi","John D. Piette","Joshua R. Loftus","Donglin Zeng","Zhenke Wu"],"pdf_url":"https://arxiv.org/pdf/2501.06366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07834v1","updated":"2025-01-14T04:35:37Z","published":"2025-01-14T04:35:37Z","title":"Flow: A Modular Approach to Automated Agentic Workflow Generation","summary":"  Multi-agent frameworks powered by large language models (LLMs) have\ndemonstrated great success in automated planning and task execution. However,\nthe effective adjustment of Agentic workflows during execution has not been\nwell-studied. A effective workflow adjustment is crucial, as in many real-world\nscenarios, the initial plan must adjust to unforeseen challenges and changing\nconditions in real-time to ensure the efficient execution of complex tasks. In\nthis paper, we define workflows as an activity-on-vertex (AOV) graphs. We\ncontinuously refine the workflow by dynamically adjusting task allocations\nbased on historical performance and previous AOV with LLM agents. To further\nenhance system performance, we emphasize modularity in workflow design based on\nmeasuring parallelism and dependence complexity. Our proposed multi-agent\nframework achieved efficient sub-task concurrent execution, goal achievement,\nand error tolerance. Empirical results across different practical tasks\ndemonstrate dramatic improvements in the efficiency of multi-agent frameworks\nthrough dynamic workflow updating and modularization.\n","authors":["Boye Niu","Yiliao Song","Kai Lian","Yifan Shen","Yu Yao","Kun Zhang","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2501.07834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06408v2","updated":"2025-01-14T04:30:31Z","published":"2025-01-11T02:23:08Z","title":"Computational and Statistical Asymptotic Analysis of the JKO Scheme for\n  Iterative Algorithms to update distributions","summary":"  The seminal paper of Jordan, Kinderlehrer, and Otto introduced what is now\nwidely known as the JKO scheme, an iterative algorithmic framework for\ncomputing distributions. This scheme can be interpreted as a Wasserstein\ngradient flow and has been successfully applied in machine learning contexts,\nsuch as deriving policy solutions in reinforcement learning. In this paper, we\nextend the JKO scheme to accommodate models with unknown parameters.\nSpecifically, we develop statistical methods to estimate these parameters and\nadapt the JKO scheme to incorporate the estimated values. To analyze the\nadopted statistical JKO scheme, we establish an asymptotic theory via\nstochastic partial differential equations that describes its limiting dynamic\nbehavior. Our framework allows both the sample size used in parameter\nestimation and the number of algorithmic iterations to go to infinity. This\nstudy offers a unified framework for joint computational and statistical\nasymptotic analysis of the statistical JKO scheme. On the computational side,\nwe examine the scheme's dynamic behavior as the number of iterations increases,\nwhile on the statistical side, we investigate the large-sample behavior of the\nresulting distributions computed through the scheme. We conduct numerical\nsimulations to evaluate the finite-sample performance of the proposed methods\nand validate the developed asymptotic theory.\n","authors":["Shang Wu","Yazhen Wang"],"pdf_url":"https://arxiv.org/pdf/2501.06408v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11869v3","updated":"2025-01-14T04:25:23Z","published":"2024-08-19T02:27:00Z","title":"ELDER: Enhancing Lifelong Model Editing with Mixture-of-LoRA","summary":"  Large language models (LLMs) require model editing to efficiently update\nspecific knowledge within them and avoid factual errors. Most model editing\nmethods are solely designed for single-time use and result in a significant\nforgetting effect in lifelong editing scenarios, where sequential edits are\nconducted over time. Previous approaches manage sequential edits by freezing\noriginal parameters and discretely allocating new parameters for each knowledge\nupdate. However, these methods lack robustness to minor input variations due to\nthe discrete mapping between data and parameters. To overcome this challenge,\nwe propose ELDER, a novel approach to create a continuous association between\ndata and adapters. ELDER integrates multiple LoRAs through a router network and\nis trained to establish a smooth data-adapter association, thereby enhancing\nthe edit robustness and generalization of semantically equivalent inputs. To\nensure inputs containing the same knowledge will be processed by the same\nLoRAs, we design a novel loss to guide the model link LoRA allocations with\nedit knowledge. Furthermore, we propose a deferral mechanism to retain the\noriginal LLM capabilities post-edit. Extensive experiments on GPT-2 XL and\nLLaMA2-7B demonstrate that ELDER effectively edits models in the lifelong\nsetting, outperforming eight baselines while exhibiting strong scalability and\npreserving LLMs' general abilities on downstream tasks. Our code is available\nat https://github.com/JiaangL/ELDER.\n","authors":["Jiaang Li","Quan Wang","Zhongnan Wang","Yongdong Zhang","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2408.11869v3.pdf","comment":"Accepted by AAAI-25"},{"id":"http://arxiv.org/abs/2410.12476v2","updated":"2025-01-14T04:19:49Z","published":"2024-10-16T11:46:32Z","title":"Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial\n  Generation","summary":"  Machine learning (ML) exhibits promise in the clinical domain. However, it is\nconstrained by data scarcity and ethical considerations, as the generation of\nclinical trials presents significant challenges due to stringent privacy\nregulations, high costs, and the extended duration required for conducting\nstudies with human participants. Despite the advancements of large language\nmodels (LLMs) in general generation tasks, their potential in facilitating the\ngeneration of synthetic clinical trials is under-explored. To address this gap,\nwe introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs\nto generate artificial yet realistic and diverse clinical trials with binary\nsuccess/failure labels. Experiments conducted on real clinical trials from the\n\\url{ClinicalTrials.gov} database demonstrate that our synthetic data can\neffectively augment real datasets. Furthermore, by fine-tuning a pre-trained\nmodel as a binary classifier on synthetic clinical trial datasets, we\ndemonstrate that this augmentation enhances model training for downstream tasks\nsuch as trial outcome prediction. Our findings suggest that LLMs for synthetic\nclinical trial generation hold promise for accelerating clinical research and\nupholding ethical standards for patient privacy. The code is publicly available\nat\nhttps://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.\n","authors":["Zerui Xu","Fang Wu","Yuanyuan Zhang","Yue Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.12476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15240v3","updated":"2025-01-14T04:10:46Z","published":"2024-11-22T01:58:35Z","title":"AI Foundation Models for Wearable Movement Data in Mental Health\n  Research","summary":"  Pretrained foundation models and transformer architectures have driven the\nsuccess of large language models (LLMs) and other modern AI breakthroughs.\nHowever, similar advancements in health data modeling remain limited due to the\nneed for innovative adaptations. Wearable movement data offers a valuable\navenue for exploration, as it's a core feature in nearly all commercial\nsmartwatches, well established in clinical and mental health research, and the\nsequential nature of the data shares similarities to language. We introduce the\nPretrained Actigraphy Transformer (PAT), the first open source foundation model\ndesigned for time-series wearable movement data. Leveraging transformer-based\narchitectures and novel techniques, such as patch embeddings, and pretraining\non data from 29,307 participants in a national U.S. sample, PAT achieves\nstate-of-the-art performance in several mental health prediction tasks. PAT is\nalso lightweight and easily interpretable, making it a robust tool for mental\nhealth research.\n  GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/\n","authors":["Franklin Y. Ruan","Aiwei Zhang","Jenny Y. Oh","SouYoung Jin","Nicholas C. Jacobson"],"pdf_url":"https://arxiv.org/pdf/2411.15240v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07827v1","updated":"2025-01-14T04:02:08Z","published":"2025-01-14T04:02:08Z","title":"Prediction Interval Construction Method for Electricity Prices","summary":"  Accurate prediction of electricity prices plays an essential role in the\nelectricity market. To reflect the uncertainty of electricity prices, price\nintervals are predicted. This paper proposes a novel prediction interval\nconstruction method. A conditional generative adversarial network is first\npresented to generate electricity price scenarios, with which the prediction\nintervals can be constructed. Then, different generated scenarios are stacked\nto obtain the probability densities, which can be applied to accurately reflect\nthe uncertainty of electricity prices. Furthermore, a reinforced prediction\nmechanism based on the volatility level of weather factors is introduced to\naddress the spikes or volatile prices. A case study is conducted to verify the\neffectiveness of the proposed novel prediction interval construction method.\nThe method can also provide the probability density of each price scenario\nwithin the prediction interval and has the superiority to address the volatile\nprices and price spikes with a reinforced prediction mechanism.\n","authors":["Xin Lu"],"pdf_url":"https://arxiv.org/pdf/2501.07827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07824v1","updated":"2025-01-14T03:59:48Z","published":"2025-01-14T03:59:48Z","title":"Real-time Verification and Refinement of Language Model Text Generation","summary":"  Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods.\n","authors":["Joonho Ko","Jinheon Baek","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2501.07824v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2501.07818v1","updated":"2025-01-14T03:43:23Z","published":"2025-01-14T03:43:23Z","title":"A Multi-Encoder Frozen-Decoder Approach for Fine-Tuning Large Language\n  Models","summary":"  Among parameter-efficient fine-tuning methods, freezing has emerged as a\npopular strategy for speeding up training, reducing catastrophic forgetting,\nand improving downstream performance. We investigate the impact of freezing the\ndecoder in a multi-task setup comprising diverse natural language tasks, aiming\nto reduce deployment overhead and enhance portability to novel tasks. Our\nexperiments, conducted by fine-tuning both individual and multi-task setups on\nthe AlexaTM model, reveal that freezing decoders is highly effective for tasks\nwith natural language outputs and mitigates catastrophic forgetting in\nmultilingual tasks. However, we find that pairing frozen decoders with a larger\nmodel can effectively maintain or even enhance performance in structured and QA\ntasks, making it a viable strategy for a broader range of task types.\n","authors":["Kaustubh D. Dhole"],"pdf_url":"https://arxiv.org/pdf/2501.07818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00090v2","updated":"2025-01-14T03:27:10Z","published":"2024-11-27T12:34:45Z","title":"Energy-Efficient Split Learning for Fine-Tuning Large Language Models in\n  Edge Networks","summary":"  In this letter, we propose an energy-efficient split learning (SL) framework\nfor fine-tuning large language models (LLMs) using geo-distributed personal\ndata at the network edge, where LLMs are split and alternately across massive\nmobile devices and an edge server. Considering the device heterogeneity and\nchannel dynamics in edge networks, a \\underline{C}ut l\\underline{A}yer and\ncomputing \\underline{R}esource \\underline{D}ecision (CARD) algorithm is\ndeveloped to minimize training delay and energy consumption. Simulation results\ndemonstrate that the proposed approach reduces the average training delay and\nserver's energy consumption by 70.8% and 53.1%, compared to the benchmarks,\nrespectively.\n","authors":["Zuguang Li","Shaohua Wu","Liang Li","Songge Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.00090v2.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.07814v1","updated":"2025-01-14T03:26:05Z","published":"2025-01-14T03:26:05Z","title":"STTS-EAD: Improving Spatio-Temporal Learning Based Time Series\n  Prediction via","summary":"  Handling anomalies is a critical preprocessing step in multivariate time\nseries prediction. However, existing approaches that separate anomaly\npreprocessing from model training for multivariate time series prediction\nencounter significant limitations. Specifically, these methods fail to utilize\nauxiliary information crucial for identifying latent anomalies associated with\nspatiotemporal factors during the preprocessing stage. Instead, they rely\nsolely on data distribution for anomaly detection, which can result in the\nincorrect processing of numerous samples that could otherwise contribute\npositively to model training. To address this, we propose STTS-EAD, an\nend-to-end method that seamlessly integrates anomaly detection into the\ntraining process of multivariate time series forecasting and aims to improve\nSpatio-Temporal learning based Time Series prediction via Embedded Anomaly\nDetection. Our proposed STTS-EAD leverages spatio-temporal information for\nforecasting and anomaly detection, with the two parts alternately executed and\noptimized for each other. To the best of our knowledge, STTS-EAD is the first\nto integrate anomaly detection and forecasting tasks in the training phase for\nimproving the accuracy of multivariate time series forecasting. Extensive\nexperiments on a public stock dataset and two real-world sales datasets from a\nrenowned coffee chain enterprise show that our proposed method can effectively\nprocess detected anomalies in the training stage to improve forecasting\nperformance in the inference stage and significantly outperform baselines.\n","authors":["Yuanyuan Liang","Tianhao Zhang","Tingyu Xie"],"pdf_url":"https://arxiv.org/pdf/2501.07814v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2501.07809v1","updated":"2025-01-14T03:20:17Z","published":"2025-01-14T03:20:17Z","title":"Conformal mapping Coordinates Physics-Informed Neural Networks\n  (CoCo-PINNs): learning neural networks for designing neutral inclusions","summary":"  We focus on designing and solving the neutral inclusion problem via neural\nnetworks. The neutral inclusion problem has a long history in the theory of\ncomposite materials, and it is exceedingly challenging to identify the precise\ncondition that precipitates a general-shaped inclusion into a neutral\ninclusion. Physics-informed neural networks (PINNs) have recently become a\nhighly successful approach to addressing both forward and inverse problems\nassociated with partial differential equations. We found that traditional PINNs\nperform inadequately when applied to the inverse problem of designing neutral\ninclusions with arbitrary shapes. In this study, we introduce a novel approach,\nConformal mapping Coordinates Physics-Informed Neural Networks (CoCo-PINNs),\nwhich integrates complex analysis techniques into PINNs. This method exhibits\nstrong performance in solving forward-inverse problems to construct neutral\ninclusions of arbitrary shapes in two dimensions, where the imperfect interface\ncondition on the inclusion's boundary is modeled by training neural networks.\nNotably, we mathematically prove that training with a single linear field is\nsufficient to achieve neutrality for untrained linear fields in arbitrary\ndirections, given a minor assumption. We demonstrate that CoCo-PINNs offer\nenhanced performances in terms of credibility, consistency, and stability.\n","authors":["Daehee Cho","Hyeonmin Yun","Jaeyong Lee","Mikyoung Lim"],"pdf_url":"https://arxiv.org/pdf/2501.07809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12843v3","updated":"2025-01-14T03:08:02Z","published":"2024-06-18T17:57:49Z","title":"Can Go AIs be adversarially robust?","summary":"  Prior work found that superhuman Go AIs can be defeated by simple adversarial\nstrategies, especially \"cyclic\" attacks. In this paper, we study whether adding\nnatural countermeasures can achieve robustness in Go, a favorable domain for\nrobustness since it benefits from incredible average-case capability and a\nnarrow, innately adversarial setting. We test three defenses: adversarial\ntraining on hand-constructed positions, iterated adversarial training, and\nchanging the network architecture. We find that though some of these defenses\nprotect against previously discovered attacks, none withstand freshly trained\nadversaries. Furthermore, most of the reliably effective attacks these\nadversaries discover are different realizations of the same overall class of\ncyclic attacks. Our results suggest that building robust AI systems is\nchallenging even with extremely superhuman systems in some of the most\ntractable settings, and highlight two key gaps: efficient generalization of\ndefenses, and diversity in training. For interactive examples of attacks and a\nlink to our codebase, see https://goattack.far.ai.\n","authors":["Tom Tseng","Euan McLean","Kellin Pelrine","Tony T. Wang","Adam Gleave"],"pdf_url":"https://arxiv.org/pdf/2406.12843v3.pdf","comment":"63 pages, AAAI 2025"},{"id":"http://arxiv.org/abs/2501.07800v1","updated":"2025-01-14T02:56:19Z","published":"2025-01-14T02:56:19Z","title":"BioPose: Biomechanically-accurate 3D Pose Estimation from Monocular\n  Videos","summary":"  Recent advancements in 3D human pose estimation from single-camera images and\nvideos have relied on parametric models, like SMPL. However, these models\noversimplify anatomical structures, limiting their accuracy in capturing true\njoint locations and movements, which reduces their applicability in\nbiomechanics, healthcare, and robotics. Biomechanically accurate pose\nestimation, on the other hand, typically requires costly marker-based motion\ncapture systems and optimization techniques in specialized labs. To bridge this\ngap, we propose BioPose, a novel learning-based framework for predicting\nbiomechanically accurate 3D human pose directly from monocular videos. BioPose\nincludes three key components: a Multi-Query Human Mesh Recovery model\n(MQ-HMR), a Neural Inverse Kinematics (NeurIK) model, and a 2D-informed pose\nrefinement technique. MQ-HMR leverages a multi-query deformable transformer to\nextract multi-scale fine-grained image features, enabling precise human mesh\nrecovery. NeurIK treats the mesh vertices as virtual markers, applying a\nspatial-temporal network to regress biomechanically accurate 3D poses under\nanatomical constraints. To further improve 3D pose estimations, a 2D-informed\nrefinement step optimizes the query tokens during inference by aligning the 3D\nstructure with 2D pose observations. Experiments on benchmark datasets\ndemonstrate that BioPose significantly outperforms state-of-the-art methods.\nProject website:\n\\url{https://m-usamasaleem.github.io/publication/BioPose/BioPose.html}.\n","authors":["Farnoosh Koleini","Muhammad Usama Saleem","Pu Wang","Hongfei Xue","Ahmed Helmy","Abbey Fenwick"],"pdf_url":"https://arxiv.org/pdf/2501.07800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15274v3","updated":"2025-01-14T02:52:40Z","published":"2024-10-20T04:17:59Z","title":"Physically Guided Deep Unsupervised Inversion for 1D Magnetotelluric\n  Models","summary":"  The global demand for unconventional energy sources such as geothermal energy\nand white hydrogen requires new exploration techniques for precise subsurface\nstructure characterization and potential reservoir identification. The\nMagnetotelluric (MT) method is crucial for these tasks, providing critical\ninformation on the distribution of subsurface electrical resistivity at depths\nranging from hundreds to thousands of meters. However, traditional iterative\nalgorithm-based inversion methods require the adjustment of multiple\nparameters, demanding time-consuming and exhaustive tuning processes to achieve\nproper cost function minimization. Recent advances have incorporated deep\nlearning algorithms for MT inversion, primarily based on supervised learning,\nand large labeled datasets are needed for training. This work utilizes\nTensorFlow operations to create a differentiable forward MT operator,\nleveraging its automatic differentiation capability. Moreover, instead of\nsolving for the subsurface model directly, as classical algorithms perform,\nthis paper presents a new deep unsupervised inversion algorithm guided by\nphysics to estimate 1D MT models. Instead of using datasets with the observed\ndata and their respective model as labels during training, our method employs a\ndifferentiable modeling operator that physically guides the cost function\nminimization, making the proposed method solely dependent on observed data.\nTherefore, the optimization algorithm updates the network weights to minimize\nthe data misfit. We test the proposed method with field and synthetic data at\ndifferent acquisition frequencies, demonstrating that the resistivity models\nobtained are more accurate than those calculated using other techniques.\n","authors":["Paul Goyes-Peñafiel","Umair bin Waheed","Henry Arguello"],"pdf_url":"https://arxiv.org/pdf/2410.15274v3.pdf","comment":"5 pages, 6 figures, github repository, submitted to IEEE-GRSL"},{"id":"http://arxiv.org/abs/2501.06252v2","updated":"2025-01-14T02:52:26Z","published":"2025-01-09T01:19:21Z","title":"$\\text{Transformer}^2$: Self-adaptive LLMs","summary":"  Self-adaptive large language models (LLMs) aim to solve the challenges posed\nby traditional fine-tuning methods, which are often computationally intensive\nand static in their ability to handle diverse tasks. We introduce\n$\\text{Transformer}^2$, a novel self-adaptation framework that adapts LLMs for\nunseen tasks in real-time by selectively adjusting only the singular components\nof their weight matrices. During inference, $\\text{Transformer}^2$ employs a\ntwo-pass mechanism: first, a dispatch system identifies the task properties,\nand then task-specific \"expert\" vectors, trained using reinforcement learning,\nare dynamically mixed to obtain targeted behavior for the incoming prompt. Our\nmethod outperforms ubiquitous approaches such as LoRA, with fewer parameters\nand greater efficiency. $\\text{Transformer}^2$ demonstrates versatility across\ndifferent LLM architectures and modalities, including vision-language tasks.\n$\\text{Transformer}^2$ represents a significant leap forward, offering a\nscalable, efficient solution for enhancing the adaptability and task-specific\nperformance of LLMs, paving the way for truly dynamic, self-organizing AI\nsystems.\n","authors":["Qi Sun","Edoardo Cetin","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2501.06252v2.pdf","comment":"18 panges, 11 figures, 9 tables"},{"id":"http://arxiv.org/abs/2501.07564v2","updated":"2025-01-14T02:38:26Z","published":"2025-01-13T18:53:23Z","title":"E2ESlack: An End-to-End Graph-Based Framework for Pre-Routing Slack\n  Prediction","summary":"  Pre-routing slack prediction remains a critical area of research in\nElectronic Design Automation (EDA). Despite numerous machine learning-based\napproaches targeting this task, there is still a lack of a truly end-to-end\nframework that engineers can use to obtain TNS/WNS metrics from raw circuit\ndata at the placement stage. Existing works have demonstrated effectiveness in\nArrival Time (AT) prediction but lack a mechanism for Required Arrival Time\n(RAT) prediction, which is essential for slack prediction and obtaining TNS/WNS\nmetrics. In this work, we propose E2ESlack, an end-to-end graph-based framework\nfor pre-routing slack prediction. The framework includes a TimingParser that\nsupports DEF, SDF and LIB files for feature extraction and graph construction,\nan arrival time prediction model and a fast RAT estimation module. To the best\nof our knowledge, this is the first work capable of predicting path-level\nslacks at the pre-routing stage. We perform extensive experiments and\ndemonstrate that our proposed RAT estimation method outperforms the SOTA\nML-based prediction method and also pre-routing STA tool. Additionally, the\nproposed E2ESlack framework achieves TNS/WNS values comparable to post-routing\nSTA results while saving up to 23x runtime.\n","authors":["Saurabh Bodhe","Zhanguang Zhang","Atia Hamidizadeh","Shixiong Kai","Yingxue Zhang","Mingxuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2501.07564v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07794v1","updated":"2025-01-14T02:33:40Z","published":"2025-01-14T02:33:40Z","title":"Linearly Convergent Mixup Learning","summary":"  Learning in the reproducing kernel Hilbert space (RKHS) such as the support\nvector machine has been recognized as a promising technique. It continues to be\nhighly effective and competitive in numerous prediction tasks, particularly in\nsettings where there is a shortage of training data or computational\nlimitations exist. These methods are especially valued for their ability to\nwork with small datasets and their interpretability. To address the issue of\nlimited training data, mixup data augmentation, widely used in deep learning,\nhas remained challenging to apply to learning in RKHS due to the generation of\nintermediate class labels. Although gradient descent methods handle these\nlabels effectively, dual optimization approaches are typically not directly\napplicable. In this study, we present two novel algorithms that extend to a\nbroader range of binary classification models. Unlike gradient-based\napproaches, our algorithms do not require hyperparameters like learning rates,\nsimplifying their implementation and optimization. Both the number of\niterations to converge and the computational cost per iteration scale linearly\nwith respect to the dataset size. The numerical experiments demonstrate that\nour algorithms achieve faster convergence to the optimal solution compared to\ngradient descent approaches, and that mixup data augmentation consistently\nimproves the predictive performance across various loss functions.\n","authors":["Gakuto Obi","Ayato Saito","Yuto Sasaki","Tsuyoshi Kato"],"pdf_url":"https://arxiv.org/pdf/2501.07794v1.pdf","comment":"none"},{"id":"http://arxiv.org/abs/2407.02772v2","updated":"2025-01-14T02:30:09Z","published":"2024-07-03T03:01:43Z","title":"Gradient descent with generalized Newton's method","summary":"  We propose the generalized Newton's method (GeN) -- a Hessian-informed\napproach that applies to any optimizer such as SGD and Adam, and covers the\nNewton-Raphson method as a sub-case. Our method automatically and dynamically\nselects the learning rate that accelerates the convergence, without the\nintensive tuning of the learning rate scheduler. In practice, our method is\neasily implementable, since it only requires additional forward passes with\nalmost zero computational overhead (in terms of training time and memory cost),\nif the overhead is amortized over many iterations. We present extensive\nexperiments on language and vision tasks (e.g. GPT and ResNet) to showcase that\nGeN optimizers match the state-of-the-art performance, which was achieved with\ncarefully tuned learning rate schedulers.\n","authors":["Zhiqi Bu","Shiyun Xu"],"pdf_url":"https://arxiv.org/pdf/2407.02772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19784v4","updated":"2025-01-14T02:28:28Z","published":"2024-12-27T18:25:27Z","title":"Can AI Help with Your Personal Finances?","summary":"  In recent years, Large Language Models (LLMs) have emerged as a\ntransformative development in artificial intelligence (AI), drawing significant\nattention from industry and academia. Trained on vast datasets, these\nsophisticated AI systems exhibit impressive natural language processing and\ncontent generation capabilities. This paper explores the potential of LLMs to\naddress key challenges in personal finance, focusing on the United States. We\nevaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,\nAnthropic's Claude, and Meta's Llama, to assess their effectiveness in\nproviding accurate financial advice on topics such as mortgages, taxes, loans,\nand investments. Our findings show that while these models achieve an average\naccuracy rate of approximately 70%, they also display notable limitations in\ncertain areas. Specifically, LLMs struggle to provide accurate responses for\ncomplex financial queries, with performance varying significantly across\ndifferent topics. Despite these limitations, the analysis reveals notable\nimprovements in newer versions of these models, highlighting their growing\nutility for individuals and financial advisors. As these AI systems continue to\nevolve, their potential for advancing AI-driven applications in personal\nfinance becomes increasingly promising.\n","authors":["Oudom Hean","Utsha Saha","Binita Saha"],"pdf_url":"https://arxiv.org/pdf/2412.19784v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12463v2","updated":"2025-01-14T01:57:04Z","published":"2024-08-22T15:04:59Z","title":"Smartphone-based Eye Tracking System using Edge Intelligence and Model\n  Optimisation","summary":"  A significant limitation of current smartphone-based eye-tracking algorithms\nis their low accuracy when applied to video-type visual stimuli, as they are\ntypically trained on static images. Also, the increasing demand for real-time\ninteractive applications like games, VR, and AR on smartphones requires\novercoming the limitations posed by resource constraints such as limited\ncomputational power, battery life, and network bandwidth. Therefore, we\ndeveloped two new smartphone eye-tracking techniques for video-type visuals by\ncombining Convolutional Neural Networks (CNN) with two different Recurrent\nNeural Networks (RNN), namely Long Short Term Memory (LSTM) and Gated Recurrent\nUnit (GRU). Our CNN+LSTM and CNN+GRU models achieved an average Root Mean\nSquare Error of 0.955 cm and 1.091 cm, respectively. To address the\ncomputational constraints of smartphones, we developed an edge intelligence\narchitecture to enhance the performance of smartphone-based eye tracking. We\napplied various optimisation methods like quantisation and pruning to deep\nlearning models for better energy, CPU, and memory usage on edge devices,\nfocusing on real-time processing. Using model quantisation, the model inference\ntime in the CNN+LSTM and CNN+GRU models was reduced by 21.72% and 19.50%,\nrespectively, on edge devices.\n","authors":["Nishan Gunawardena","Gough Yumu Lui","Jeewani Anupama Ginige","Bahman Javadi"],"pdf_url":"https://arxiv.org/pdf/2408.12463v2.pdf","comment":"I have included the three papers as reference, which are closely\n  related. We have expanded the future work section to provide a more thorough\n  discussion of the concepts of \"varying lighting conditions\" and \"dynamic user\n  environments.\" We have added a note below Table 4 to clarify the\n  abbreviations' meaning. Elaborated the role of the Domain Expert within the\n  presentation layer in Section 4.1"},{"id":"http://arxiv.org/abs/2312.09982v4","updated":"2025-01-14T01:42:46Z","published":"2023-12-15T17:49:24Z","title":"ACPO: AI-Enabled Compiler Framework","summary":"  The key to performance optimization of a program is to decide correctly when\na certain transformation should be applied by a compiler. This is an ideal\nopportunity to apply machine-learning models to speed up the tuning process;\nwhile this realization has been around since the late 90s, only recent\nadvancements in ML enabled a practical application of ML to compilers as an\nend-to-end framework.\n  This paper presents ACPO: An AI-Enabled Compiler Framework, a novel framework\nthat provides LLVM with simple and comprehensive tools to benefit from\nemploying ML models for different optimization passes. We first showcase the\nhigh-level view, class hierarchy, and functionalities of ACPO and subsequently,\ndemonstrate \\taco{a couple of use cases of ACPO by ML-enabling the Loop Unroll\nand Function Inlining passes used in LLVM's O3. and finally, describe how ACPO\ncan be leveraged to optimize other passes. Experimental results reveal that the\nACPO model for Loop Unroll can gain on average 4%, 3%, 5.4%, and 0.2% compared\nto LLVM's vanilla O3 optimization when deployed on Polybench, Coral-2,\nCoreMark, and Graph-500, respectively. Furthermore, by including both Function\nInlining and Loop Unroll models, ACPO can provide a combined speedup of 4.5% on\nPolybench and 2.4% on Cbench when compared with LLVM's O3, respectively.\n","authors":["Amir H. Ashouri","Muhammad Asif Manzoor","Duc Minh Vu","Raymond Zhang","Colin Toft","Ziwen Wang","Angel Zhang","Bryan Chan","Tomasz S. Czajkowski","Yaoqing Gao"],"pdf_url":"https://arxiv.org/pdf/2312.09982v4.pdf","comment":"ACPO (12 pages)"},{"id":"http://arxiv.org/abs/2404.12404v4","updated":"2025-01-14T01:41:21Z","published":"2024-04-15T17:49:16Z","title":"EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular\n  Data Classification via Large Language Models","summary":"  Large language models (LLMs) have demonstrated remarkable in-context learning\ncapabilities across diverse applications. In this work, we explore the\neffectiveness of LLMs for generating realistic synthetic tabular data,\nidentifying key prompt design elements to optimize performance. We introduce\nEPIC, a novel approach that leverages balanced, grouped data samples and\nconsistent formatting with unique variable mapping to guide LLMs in generating\naccurate synthetic data across all classes, even for imbalanced datasets.\nEvaluations on real-world datasets show that EPIC achieves state-of-the-art\nmachine learning classification performance, significantly improving generation\nefficiency. These findings highlight the effectiveness of EPIC for synthetic\ntabular data generation, particularly in addressing class imbalance. Our source\ncode for our work is available at:\nhttps://seharanul17.github.io/project-synthetic-tabular-llm/\n","authors":["Jinhee Kim","Taesung Kim","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2404.12404v4.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2210.01272v3","updated":"2025-01-14T01:34:10Z","published":"2022-10-03T23:44:38Z","title":"A systematic review of the use of Deep Learning in Satellite Imagery for\n  Agriculture","summary":"  Agricultural research is essential for increasing food production to meet the\nrequirements of an increasing population in the coming decades. Recently,\nsatellite technology has been improving rapidly and deep learning has seen much\nsuccess in generic computer vision tasks and many application areas which\npresents an important opportunity to improve analysis of agricultural land.\nHere we present a systematic review of 150 studies to find the current uses of\ndeep learning on satellite imagery for agricultural research. Although we\nidentify 5 categories of agricultural monitoring tasks, the majority of the\nresearch interest is in crop segmentation and yield prediction. We found that,\nwhen used, modern deep learning methods consistently outperformed traditional\nmachine learning across most tasks; the only exception was that Long Short-Term\nMemory (LSTM) Recurrent Neural Networks did not consistently outperform Random\nForests (RF) for yield prediction. The reviewed studies have largely adopted\nmethodologies from generic computer vision, except for one major omission:\nbenchmark datasets are not utilised to evaluate models across studies, making\nit difficult to compare results. Additionally, some studies have specifically\nutilised the extra spectral resolution available in satellite imagery, but\nother divergent properties of satellite images - such as the hugely different\nscales of spatial patterns - are not being taken advantage of in the reviewed\nstudies.\n","authors":["Brandon Victor","Zhen He","Aiden Nibali"],"pdf_url":"https://arxiv.org/pdf/2210.01272v3.pdf","comment":"23 pages, 5 figures and 10 tables in main paper. Final version, as\n  submitted and accepted at JSTARS"},{"id":"http://arxiv.org/abs/2302.01313v8","updated":"2025-01-14T01:28:03Z","published":"2023-02-02T18:39:30Z","title":"Double Equivariance for Inductive Link Prediction for Both New Nodes and\n  New Relation Types","summary":"  The task of fully inductive link prediction in knowledge graphs has gained\nsignificant attention, with various graph neural networks being proposed to\naddress it. This task presents greater challenges than traditional inductive\nlink prediction tasks with only new nodes, as models must be capable of\nzero-shot generalization to both unseen nodes and unseen relation types in the\ninference graph. Despite the development of novel models, a unifying\ntheoretical understanding of their success remains elusive, and the limitations\nof these methods are not well-studied. In this work, we introduce the concept\nof double permutation-equivariant representations and demonstrate its necessity\nfor effective performance in this task. We show that many existing models,\ndespite their diverse architectural designs, conform to this framework.\nHowever, we also identify inherent limitations in double\npermutation-equivariant representations, which restrict these models's ability\nto learn effectively on datasets with varying characteristics. Our findings\nsuggest that while double equivariance is necessary for meta-learning across\nknowledge graphs from different domains, it is not sufficient. There remains a\nfundamental gap between double permutation-equivariant models and the concept\nof foundation models designed to learn patterns across all domains.\n","authors":["Jincheng Zhou","Yucheng Zhang","Jianfei Gao","Yangze Zhou","Bruno Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2302.01313v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07774v1","updated":"2025-01-14T01:16:30Z","published":"2025-01-14T01:16:30Z","title":"Transforming Indoor Localization: Advanced Transformer Architecture for\n  NLOS Dominated Wireless Environments with Distributed Sensors","summary":"  Indoor localization in challenging non-line-of-sight (NLOS) environments\noften leads to mediocre accuracy with traditional approaches. Deep learning\n(DL) has been applied to tackle these challenges; however, many DL approaches\noverlook computational complexity, especially for floating-point operations\n(FLOPs), making them unsuitable for resource-limited devices. Transformer-based\nmodels have achieved remarkable success in natural language processing (NLP)\nand computer vision (CV) tasks, motivating their use in wireless applications.\nHowever, their use in indoor localization remains nascent, and directly\napplying Transformers for indoor localization can be both computationally\nintensive and exhibit limitations in accuracy. To address these challenges, in\nthis work, we introduce a novel tokenization approach, referred to as Sensor\nSnapshot Tokenization (SST), which preserves variable-specific representations\nof power delay profile (PDP) and enhances attention mechanisms by effectively\ncapturing multi-variate correlation. Complementing this, we propose a\nlightweight Swish-Gated Linear Unit-based Transformer (L-SwiGLU Transformer)\nmodel, designed to reduce computational complexity without compromising\nlocalization accuracy. Together, these contributions mitigate the computational\nburden and dependency on large datasets, making Transformer models more\nefficient and suitable for resource-constrained scenarios. The proposed\ntokenization method enables the Vanilla Transformer to achieve a 90th\npercentile positioning error of 0.388 m in a highly NLOS indoor factory,\nsurpassing conventional tokenization methods. The L-SwiGLU ViT further reduces\nthe error to 0.355 m, achieving an 8.51% improvement. Additionally, the\nproposed model outperforms a 14.1 times larger model with a 46.13% improvement,\nunderscoring its computational efficiency.\n","authors":["Saad Masrur"," Jung-Fu"," Cheng","Atieh R. Khamesi","Ismail Guvenc"],"pdf_url":"https://arxiv.org/pdf/2501.07774v1.pdf","comment":"The paper has been submitted to IEEE Transactions on Machine Learning\n  in Communications and Networking"},{"id":"http://arxiv.org/abs/2501.07773v1","updated":"2025-01-14T01:08:15Z","published":"2025-01-14T01:08:15Z","title":"Symmetry-Aware Generative Modeling through Learned Canonicalization","summary":"  Generative modeling of symmetric densities has a range of applications in AI\nfor science, from drug discovery to physics simulations. The existing\ngenerative modeling paradigm for invariant densities combines an invariant\nprior with an equivariant generative process. However, we observe that this\ntechnique is not necessary and has several drawbacks resulting from the\nlimitations of equivariant networks. Instead, we propose to model a learned\nslice of the density so that only one representative element per orbit is\nlearned. To accomplish this, we learn a group-equivariant canonicalization\nnetwork that maps training samples to a canonical pose and train a\nnon-equivariant generative model over these canonicalized samples. We implement\nthis idea in the context of diffusion models. Our preliminary experimental\nresults on molecular modeling are promising, demonstrating improved sample\nquality and faster inference time.\n","authors":["Kusha Sareen","Daniel Levy","Arnab Kumar Mondal","Sékou-Oumar Kaba","Tara Akhound-Sadegh","Siamak Ravanbakhsh"],"pdf_url":"https://arxiv.org/pdf/2501.07773v1.pdf","comment":"NeurReps 2024 Workshop Version"},{"id":"http://arxiv.org/abs/2501.07769v1","updated":"2025-01-14T00:59:55Z","published":"2025-01-14T00:59:55Z","title":"BMIP: Bi-directional Modality Interaction Prompt Learning for VLM","summary":"  Vision-language models (VLMs) have exhibited remarkable generalization\ncapabilities, and prompt learning for VLMs has attracted great attention for\nthe ability to adapt pre-trained VLMs to specific downstream tasks. However,\nexisting studies mainly focus on single-modal prompts or uni-directional\nmodality interaction, overlooking the powerful alignment effects resulting from\nthe interaction between the vision and language modalities. To this end, we\npropose a novel prompt learning method called\n$\\underline{\\textbf{B}}i-directional \\underline{\\textbf{M}}odality\n\\underline{\\textbf{I}}nteraction \\underline{\\textbf{P}}rompt (BMIP)$, which\ndynamically weights bi-modal information through learning the information of\nthe attention layer, enhancing trainability and inter-modal consistency\ncompared to simple information aggregation methods. To evaluate the\neffectiveness of prompt learning methods, we propose a more realistic\nevaluation paradigm called open-world generalization complementing the widely\nadopted cross-dataset transfer and domain generalization tasks. Comprehensive\nexperiments on various datasets reveal that BMIP not only outperforms current\nstate-of-the-art methods across all three evaluation paradigms but is also\nflexible enough to be combined with other prompt-based methods for consistent\nperformance enhancement.\n","authors":["Song-Lin Lv","Yu-Yang Chen","Zhi Zhou","Ming Yang","Lan-Zhe Guo"],"pdf_url":"https://arxiv.org/pdf/2501.07769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08496v1","updated":"2025-01-14T23:59:23Z","published":"2025-01-14T23:59:23Z","title":"Quantifying the Importance of Data Alignment in Downstream Model\n  Performance","summary":"  Contrary to the conventional emphasis on dataset size, we explore the role of\ndata alignment -- an often overlooked aspect of data quality -- in training\ncapable Large Language Models (LLMs). To do so, we use the Task2Vec-based\nalignment coefficient, a quantitative measure of the similarity between two\ndatasets, to quantify the impact of alignment between training data and\nevaluation data on downstream performance. In particular, we conduct controlled\n\\textit{interventional} experiments for two settings: 1. the impact of\nincreased alignment coefficients between various pre-training (pt) against\nevaluation datasets, and 2. the impact of increased alignment coefficients\nbetween domain specific fine-tuning (ft) against domain specific evaluation.\nThe domain specific task we explore is Autoformalization -- the machine\ntranslation task between natural language and code for formal verification. In\nboth settings, we find a strong, predictable negative correlation between the\nalignment coefficient of a model's training and evaluation data and the model's\nloss/perplexity on the respective downstream task. These findings suggest a\nre-evaluation of LLM training approaches, demonstrating the relevance of data\nalignment compared to data quantity, especially in specialized downstream tasks\nsuch as Autoformalization.\n","authors":["Krrish Chawla","Aryan Sahai","Mario DePavia","Sudharsan Sundar","Brando Miranda"],"pdf_url":"https://arxiv.org/pdf/2501.08496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08490v1","updated":"2025-01-14T23:31:20Z","published":"2025-01-14T23:31:20Z","title":"FLAVARS: A Multimodal Foundational Language and Vision Alignment Model\n  for Remote Sensing","summary":"  Remote sensing imagery is dense with objects and contextual visual\ninformation. There is a recent trend to combine paired satellite images and\ntext captions for pretraining performant encoders for downstream tasks.\nHowever, while contrastive image-text methods like CLIP enable vision-language\nalignment and zero-shot classification ability, vision-only downstream\nperformance tends to degrade compared to image-only pretraining, such as MAE.\nIn this paper, we propose FLAVARS, a pretraining method that combines the best\nof both contrastive learning and masked modeling, along with geospatial\nalignment via contrastive location encoding. We find that FLAVARS significantly\noutperforms a baseline of SkyCLIP for vision-only tasks such as KNN\nclassification and semantic segmentation, +6\\% mIOU on SpaceNet1, while\nretaining the ability to perform zero-shot classification, unlike MAE\npretrained methods.\n","authors":["Isaac Corley","Simone Fobi Nsutezo","Anthony Ortiz","Caleb Robinson","Rahul Dodhia","Juan M. Lavista Ferres","Peyman Najafirad"],"pdf_url":"https://arxiv.org/pdf/2501.08490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13904v2","updated":"2025-01-14T23:31:03Z","published":"2024-09-20T21:20:04Z","title":"High-dimensional learning of narrow neural networks","summary":"  Recent years have been marked with the fast-pace diversification and\nincreasing ubiquity of machine learning applications. Yet, a firm theoretical\nunderstanding of the surprising efficiency of neural networks to learn from\nhigh-dimensional data still proves largely elusive. In this endeavour, analyses\ninspired by statistical physics have proven instrumental, enabling the tight\nasymptotic characterization of the learning of neural networks in high\ndimensions, for a broad class of solvable models. This manuscript reviews the\ntools and ideas underlying recent progress in this line of work. We introduce a\ngeneric model -- the sequence multi-index model -- which encompasses numerous\npreviously studied models as special instances. This unified framework covers a\nbroad class of machine learning architectures with a finite number of hidden\nunits, including multi-layer perceptrons, autoencoders, attention mechanisms;\nand tasks, including (un)supervised learning, denoising, contrastive learning,\nin the limit of large data dimension, and comparably large number of samples.\nWe explicate in full detail the analysis of the learning of sequence\nmulti-index models, using statistical physics techniques such as the replica\nmethod and approximate message-passing algorithms. This manuscript thus\nprovides a unified presentation of analyses reported in several previous works,\nand a detailed overview of central techniques in the field of statistical\nphysics of machine learning. This review should be a useful primer for machine\nlearning theoreticians curious of statistical physics approaches; it should\nalso be of value to statistical physicists interested in the transfer of such\nideas to the study of neural networks.\n","authors":["Hugo Cui"],"pdf_url":"https://arxiv.org/pdf/2409.13904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.06720v4","updated":"2025-01-14T22:30:10Z","published":"2023-04-13T17:59:55Z","title":"Expressive Text-to-Image Generation with Rich Text","summary":"  Plain text has become a prevalent interface for text-to-image synthesis.\nHowever, its limited customization options hinder users from accurately\ndescribing desired outputs. For example, plain text makes it hard to specify\ncontinuous quantities, such as the precise RGB color value or importance of\neach word. Furthermore, creating detailed text prompts for complex scenes is\ntedious for humans to write and challenging for text encoders to interpret. To\naddress these challenges, we propose using a rich-text editor supporting\nformats such as font style, size, color, and footnote. We extract each word's\nattributes from rich text to enable local style control, explicit token\nreweighting, precise color rendering, and detailed region synthesis. We achieve\nthese capabilities through a region-based diffusion process. We first obtain\neach word's region based on attention maps of a diffusion process using plain\ntext. For each region, we enforce its text attributes by creating\nregion-specific detailed prompts and applying region-specific guidance, and\nmaintain its fidelity against plain-text generation through region-based\ninjections. We present various examples of image generation from rich text and\ndemonstrate that our method outperforms strong baselines with quantitative\nevaluations.\n","authors":["Songwei Ge","Taesung Park","Jun-Yan Zhu","Jia-Bin Huang"],"pdf_url":"https://arxiv.org/pdf/2304.06720v4.pdf","comment":"Project webpage: https://rich-text-to-image.github.io/"},{"id":"http://arxiv.org/abs/2501.08464v1","updated":"2025-01-14T22:20:55Z","published":"2025-01-14T22:20:55Z","title":"Time series forecasting for multidimensional telemetry data using GAN\n  and BiLSTM in a Digital Twin","summary":"  The research related to digital twins has been increasing in recent years.\nBesides the mirroring of the physical word into the digital, there is the need\nof providing services related to the data collected and transferred to the\nvirtual world. One of these services is the forecasting of physical part future\nbehavior, that could lead to applications, like preventing harmful events or\ndesigning improvements to get better performance. One strategy used to predict\nany system operation it is the use of time series models like ARIMA or LSTM,\nand improvements were implemented using these algorithms. Recently, deep\nlearning techniques based on generative models such as Generative Adversarial\nNetworks (GANs) have been proposed to create time series and the use of LSTM\nhas gained more relevance in time series forecasting, but both have limitations\nthat restrict the forecasting results. Another issue found in the literature is\nthe challenge of handling multivariate environments/applications in time series\ngeneration. Therefore, new methods need to be studied in order to fill these\ngaps and, consequently, provide better resources for creating useful digital\ntwins. In this proposal, it is going to be studied the integration of a BiLSTM\nlayer with a time series obtained by GAN in order to improve the forecasting of\nall the features provided by the dataset in terms of accuracy and,\nconsequently, improving behaviour prediction.\n","authors":["Joao Carmo de Almeida Neto","Claudio Miceli de Farias","Leandro Santiago de Araujo","Leopoldo Andre Dutra Lusquino Filho"],"pdf_url":"https://arxiv.org/pdf/2501.08464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01829v3","updated":"2025-01-14T22:10:21Z","published":"2024-08-03T17:43:10Z","title":"Neural Network Emulator for Atmospheric Chemical ODE","summary":"  Modeling atmospheric chemistry is complex and computationally intense. Given\nthe recent success of Deep neural networks in digital signal processing, we\npropose a Neural Network Emulator for fast chemical concentration modeling. We\nconsider atmospheric chemistry as a time-dependent Ordinary Differential\nEquation. To extract the hidden correlations between initial states and future\ntime evolution, we propose ChemNNE, an Attention based Neural Network Emulator\n(NNE) that can model the atmospheric chemistry as a neural ODE process. To\nefficiently simulate the chemical changes, we propose the sinusoidal time\nembedding to estimate the oscillating tendency over time. More importantly, we\nuse the Fourier neural operator to model the ODE process for efficient\ncomputation. We also propose three physical-informed losses to supervise the\ntraining optimization. To evaluate our model, we propose a large-scale chemical\ndataset that can be used for neural network training and evaluation. The\nextensive experiments show that our approach achieves state-of-the-art\nperformance in modeling accuracy and computational speed.\n","authors":["Zhi-Song Liu","Petri Clusius","Michael Boy"],"pdf_url":"https://arxiv.org/pdf/2408.01829v3.pdf","comment":"25 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.23326v2","updated":"2025-01-14T22:08:40Z","published":"2024-10-30T15:08:05Z","title":"MassSpecGym: A benchmark for the discovery and identification of\n  molecules","summary":"  The discovery and identification of molecules in biological and environmental\nsamples is crucial for advancing biomedical and chemical sciences. Tandem mass\nspectrometry (MS/MS) is the leading technique for high-throughput elucidation\nof molecular structures. However, decoding a molecular structure from its mass\nspectrum is exceptionally challenging, even when performed by human experts. As\na result, the vast majority of acquired MS/MS spectra remain uninterpreted,\nthereby limiting our understanding of the underlying (bio)chemical processes.\nDespite decades of progress in machine learning applications for predicting\nmolecular structures from MS/MS spectra, the development of new methods is\nseverely hindered by the lack of standard datasets and evaluation protocols. To\naddress this problem, we propose MassSpecGym -- the first comprehensive\nbenchmark for the discovery and identification of molecules from MS/MS data.\nOur benchmark comprises the largest publicly available collection of\nhigh-quality labeled MS/MS spectra and defines three MS/MS annotation\nchallenges: \\textit{de novo} molecular structure generation, molecule\nretrieval, and spectrum simulation. It includes new evaluation metrics and a\ngeneralization-demanding data split, therefore standardizing the MS/MS\nannotation tasks and rendering the problem accessible to the broad machine\nlearning community. MassSpecGym is publicly available at\n\\url{https://github.com/pluskal-lab/MassSpecGym}.\n","authors":["Roman Bushuiev","Anton Bushuiev","Niek F. de Jonge","Adamo Young","Fleming Kretschmer","Raman Samusevich","Janne Heirman","Fei Wang","Luke Zhang","Kai Dührkop","Marcus Ludwig","Nils A. Haupt","Apurva Kalia","Corinna Brungs","Robin Schmid","Russell Greiner","Bo Wang","David S. Wishart","Li-Ping Liu","Juho Rousu","Wout Bittremieux","Hannes Rost","Tytus D. Mak","Soha Hassoun","Florian Huber","Justin J. J. van der Hooft","Michael A. Stravs","Sebastian Böcker","Josef Sivic","Tomáš Pluskal"],"pdf_url":"https://arxiv.org/pdf/2410.23326v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08459v1","updated":"2025-01-14T22:07:29Z","published":"2025-01-14T22:07:29Z","title":"Head Motion Degrades Machine Learning Classification of Alzheimer's\n  Disease from Positron Emission Tomography","summary":"  Brain positron emission tomography (PET) imaging is broadly used in research\nand clinical routines to study, diagnose, and stage Alzheimer's disease (AD).\nHowever, its potential cannot be fully exploited yet due to the lack of\nportable motion correction solutions, especially in clinical settings. Head\nmotion during data acquisition has indeed been shown to degrade image quality\nand induces tracer uptake quantification error. In this study, we demonstrate\nthat it also biases machine learning-based AD classification. We start by\nproposing a binary classification algorithm solely based on PET images. We find\nthat it reaches a high accuracy in classifying motion corrected images into\ncognitive normal or AD. We demonstrate that the classification accuracy\nsubstantially decreases when images lack motion correction, thereby limiting\nthe algorithm's effectiveness and biasing image interpretation. We validate\nthese findings in cohorts of 128 $^{11}$C-UCB-J and 173 $^{18}$F-FDG scans, two\ntracers highly relevant to the study of AD. Classification accuracies decreased\nby 10% and 5% on 20 $^{18}$F-FDG and 20 $^{11}$C-UCB-J testing cases,\nrespectively. Our findings underscore the critical need for efficient motion\ncorrection methods to make the most of the diagnostic capabilities of PET-based\nmachine learning.\n","authors":["Eléonore V. Lieffrig","Takuya Toyonaga","Jiazhen Zhang","John A. Onofrey"],"pdf_url":"https://arxiv.org/pdf/2501.08459v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2501.08457v1","updated":"2025-01-14T22:02:38Z","published":"2025-01-14T22:02:38Z","title":"Large Language Models For Text Classification: Case Study And\n  Comprehensive Review","summary":"  Unlocking the potential of Large Language Models (LLMs) in data\nclassification represents a promising frontier in natural language processing.\nIn this work, we evaluate the performance of different LLMs in comparison with\nstate-of-the-art deep-learning and machine-learning models, in two different\nclassification scenarios: i) the classification of employees' working locations\nbased on job reviews posted online (multiclass classification), and 2) the\nclassification of news articles as fake or not (binary classification). Our\nanalysis encompasses a diverse range of language models differentiating in\nsize, quantization, and architecture. We explore the impact of alternative\nprompting techniques and evaluate the models based on the weighted F1-score.\nAlso, we examine the trade-off between performance (F1-score) and time\n(inference response time) for each language model to provide a more nuanced\nunderstanding of each model's practical applicability. Our work reveals\nsignificant variations in model responses based on the prompting strategies. We\nfind that LLMs, particularly Llama3 and GPT-4, can outperform traditional\nmethods in complex classification tasks, such as multiclass classification,\nthough at the cost of longer inference times. In contrast, simpler ML models\noffer better performance-to-time trade-offs in simpler binary classification\ntasks.\n","authors":["Arina Kostina","Marios D. Dikaiakos","Dimosthenis Stefanidis","George Pallis"],"pdf_url":"https://arxiv.org/pdf/2501.08457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16867v2","updated":"2025-01-14T22:01:02Z","published":"2024-12-22T05:36:51Z","title":"A Parameter-Efficient Quantum Anomaly Detection Method on a\n  Superconducting Quantum Processor","summary":"  Quantum machine learning has gained attention for its potential to address\ncomputational challenges. However, whether those algorithms can effectively\nsolve practical problems and outperform their classical counterparts,\nespecially on current quantum hardware, remains a critical question. In this\nwork, we propose a novel quantum machine learning method, called Quantum\nSupport Vector Data Description (QSVDD), for practical image anomaly detection,\nwhich aims to achieve both parameter efficiency and superior accuracy compared\nto classical models. Emulation results indicate that QSVDD demonstrates\nfavourable recognition capabilities compared to classical baselines, achieving\nan average accuracy of over 90% on benchmarks with significantly fewer\ntrainable parameters. Theoretical analysis confirms that QSVDD has a comparable\nexpressivity to classical counterparts while requiring only a fraction of the\nparameters. Furthermore, we demonstrate the first implementation of a quantum\nanomaly detection method for general image datasets on a superconducting\nquantum processor. Specifically, we achieve an accuracy of over 80% with only\n16 parameters on the device, providing initial evidence of QSVDD's practical\nviability in the noisy intermediate-scale quantum era and highlighting its\nsignificant reduction in parameter requirements.\n","authors":["Maida Wang","Jinyang Jiang","Peter V. Coveney"],"pdf_url":"https://arxiv.org/pdf/2412.16867v2.pdf","comment":"30 pages, 13 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2501.08137v1","updated":"2025-01-14T14:15:10Z","published":"2025-01-14T14:15:10Z","title":"Audio-visual Deepfake Detection With Local Temporal Inconsistencies","summary":"  This paper proposes an audio-visual deepfake detection approach that aims to\ncapture fine-grained temporal inconsistencies between audio and visual\nmodalities. To achieve this, both architectural and data synthesis strategies\nare introduced. From an architectural perspective, a temporal distance map,\ncoupled with an attention mechanism, is designed to capture these\ninconsistencies while minimizing the impact of irrelevant temporal\nsubsequences. Moreover, we explore novel pseudo-fake generation techniques to\nsynthesize local inconsistencies. Our approach is evaluated against\nstate-of-the-art methods using the DFDC and FakeAVCeleb datasets, demonstrating\nits effectiveness in detecting audio-visual deepfakes.\n","authors":["Marcella Astrid","Enjie Ghorbel","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2501.08137v1.pdf","comment":"Accepted in ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.07972v1","updated":"2025-01-14T09:45:10Z","published":"2025-01-14T09:45:10Z","title":"Zero-shot Video Moment Retrieval via Off-the-shelf Multimodal Large\n  Language Models","summary":"  The target of video moment retrieval (VMR) is predicting temporal spans\nwithin a video that semantically match a given linguistic query. Existing VMR\nmethods based on multimodal large language models (MLLMs) overly rely on\nexpensive high-quality datasets and time-consuming fine-tuning. Although some\nrecent studies introduce a zero-shot setting to avoid fine-tuning, they\noverlook inherent language bias in the query, leading to erroneous\nlocalization. To tackle the aforementioned challenges, this paper proposes\nMoment-GPT, a tuning-free pipeline for zero-shot VMR utilizing frozen MLLMs.\nSpecifically, we first employ LLaMA-3 to correct and rephrase the query to\nmitigate language bias. Subsequently, we design a span generator combined with\nMiniGPT-v2 to produce candidate spans adaptively. Finally, to leverage the\nvideo comprehension capabilities of MLLMs, we apply VideoChatGPT and span\nscorer to select the most appropriate spans. Our proposed method substantially\noutperforms the state-ofthe-art MLLM-based and zero-shot models on several\npublic datasets, including QVHighlights, ActivityNet-Captions, and\nCharades-STA.\n","authors":["Yifang Xu","Yunzhuo Sun","Benxiang Zhai","Ming Li","Wenxin Liang","Yang Li","Sidan Du"],"pdf_url":"https://arxiv.org/pdf/2501.07972v1.pdf","comment":"Accepted by AAAI 2025"}]},"2025-01-15T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2501.09012v1","updated":"2025-01-15T18:56:22Z","published":"2025-01-15T18:56:22Z","title":"Multimodal LLMs Can Reason about Aesthetics in Zero-Shot","summary":"  We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability\nshall be elicited to evaluate the aesthetics of artworks. To facilitate this\ninvestigation, we construct MM-StyleBench, a novel high-quality dataset for\nbenchmarking artistic stylization. We then develop a principled method for\nhuman preference modeling and perform a systematic correlation analysis between\nMLLMs' responses and human preference. Our experiments reveal an inherent\nhallucination issue of MLLMs in art evaluation, associated with response\nsubjectivity. ArtCoT is proposed, demonstrating that art-specific task\ndecomposition and the use of concrete language boost MLLMs' reasoning ability\nfor aesthetics. Our findings offer valuable insights into MLLMs for art and can\nbenefit a wide range of downstream applications, such as style transfer and\nartistic image generation. Code available at\nhttps://github.com/songrise/MLLM4Art.\n","authors":["Ruixiang Jiang","Changwen Chen"],"pdf_url":"https://arxiv.org/pdf/2501.09012v1.pdf","comment":"WIP, Homepage https://github.com/songrise/MLLM4Art"},{"id":"http://arxiv.org/abs/2501.09004v1","updated":"2025-01-15T18:37:08Z","published":"2025-01-15T18:37:08Z","title":"Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment\n  of LLM Guardrails","summary":"  As Large Language Models (LLMs) and generative AI become increasingly\nwidespread, concerns about content safety have grown in parallel. Currently,\nthere is a clear lack of high-quality, human-annotated datasets that address\nthe full spectrum of LLM-related safety risks and are usable for commercial\napplications. To bridge this gap, we propose a comprehensive and adaptable\ntaxonomy for categorizing safety risks, structured into 12 top-level hazard\ncategories with an extension to 9 fine-grained subcategories. This taxonomy is\ndesigned to meet the diverse requirements of downstream users, offering more\ngranular and flexible tools for managing various risk types. Using a hybrid\ndata generation pipeline that combines human annotations with a multi-LLM\n\"jury\" system to assess the safety of responses, we obtain Aegis 2.0, a\ncarefully curated collection of 34,248 samples of human-LLM interactions,\nannotated according to our proposed taxonomy. To validate its effectiveness, we\ndemonstrate that several lightweight models, trained using parameter-efficient\ntechniques on Aegis 2.0, achieve performance competitive with leading safety\nmodels fully fine-tuned on much larger, non-commercial datasets. In addition,\nwe introduce a novel training blend that combines safety with topic following\ndata.This approach enhances the adaptability of guard models, enabling them to\ngeneralize to new risk categories defined during inference. We plan to\nopen-source Aegis 2.0 data and models to the research community to aid in the\nsafety guardrailing of LLMs.\n","authors":["Shaona Ghosh","Prasoon Varshney","Makesh Narsimhan Sreedhar","Aishwarya Padmakumar","Traian Rebedea","Jibin Rajan Varghese","Christopher Parisien"],"pdf_url":"https://arxiv.org/pdf/2501.09004v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2404.05993"},{"id":"http://arxiv.org/abs/2501.06848v2","updated":"2025-01-15T18:28:37Z","published":"2025-01-12T15:34:24Z","title":"A General Framework for Inference-time Scaling and Steering of Diffusion\n  Models","summary":"  Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we propose Feynman Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models, even\nwith off-the-shelf rewards, can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering .\n","authors":["Raghav Singhal","Zachary Horvitz","Ryan Teehan","Mengye Ren","Zhou Yu","Kathleen McKeown","Rajesh Ranganath"],"pdf_url":"https://arxiv.org/pdf/2501.06848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08102v2","updated":"2025-01-15T18:10:00Z","published":"2025-01-14T13:19:47Z","title":"Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media","summary":"  Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design.\n","authors":["Wenlu Fan","Yuqi Zhu","Chenyang Wang","Bin Wang","Wentao Xu"],"pdf_url":"https://arxiv.org/pdf/2501.08102v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08985v1","updated":"2025-01-15T18:04:21Z","published":"2025-01-15T18:04:21Z","title":"Personality Modeling for Persuasion of Misinformation using AI Agent","summary":"  The proliferation of misinformation on social media platforms has highlighted\nthe need to understand how individual personality traits influence\nsusceptibility to and propagation of misinformation. This study employs an\ninnovative agent-based modeling approach to investigate the relationship\nbetween personality traits and misinformation dynamics. Using six AI agents\nembodying different dimensions of the Big Five personality traits\n(Extraversion, Agreeableness, and Neuroticism), we simulated interactions\nacross six diverse misinformation topics. The experiment, implemented through\nthe AgentScope framework using the GLM-4-Flash model, generated 90 unique\ninteractions, revealing complex patterns in how personality combinations affect\npersuasion and resistance to misinformation. Our findings demonstrate that\nanalytical and critical personality traits enhance effectiveness in\nevidence-based discussions, while non-aggressive persuasion strategies show\nunexpected success in misinformation correction. Notably, agents with critical\ntraits achieved a 59.4% success rate in HIV-related misinformation discussions,\nwhile those employing non-aggressive approaches maintained consistent\npersuasion rates above 40% across different personality combinations. The study\nalso revealed a non-transitive pattern in persuasion effectiveness, challenging\nconventional assumptions about personality-based influence. These results\nprovide crucial insights for developing personality-aware interventions in\ndigital environments and suggest that effective misinformation countermeasures\nshould prioritize emotional connection and trust-building over confrontational\napproaches. The findings contribute to both theoretical understanding of\npersonality-misinformation dynamics and practical strategies for combating\nmisinformation in social media contexts.\n","authors":["Qianmin Lou","Wentao Xu"],"pdf_url":"https://arxiv.org/pdf/2501.08985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08974v1","updated":"2025-01-15T17:36:56Z","published":"2025-01-15T17:36:56Z","title":"Learning to Extract Cross-Domain Aspects and Understanding Sentiments\n  Using Large Language Models","summary":"  Aspect-based sentiment analysis (ASBA) is a refined approach to sentiment\nanalysis that aims to extract and classify sentiments based on specific aspects\nor features of a product, service, or entity. Unlike traditional sentiment\nanalysis, which assigns a general sentiment score to entire reviews or texts,\nABSA focuses on breaking down the text into individual components or aspects\n(e.g., quality, price, service) and evaluating the sentiment towards each. This\nallows for a more granular level of understanding of customer opinions,\nenabling businesses to pinpoint specific areas of strength and improvement. The\nprocess involves several key steps, including aspect extraction, sentiment\nclassification, and aspect-level sentiment aggregation for a review paragraph\nor any other form that the users have provided. ABSA has significant\napplications in areas such as product reviews, social media monitoring,\ncustomer feedback analysis, and market research. By leveraging techniques from\nnatural language processing (NLP) and machine learning, ABSA facilitates the\nextraction of valuable insights, enabling companies to make data-driven\ndecisions that enhance customer satisfaction and optimize offerings. As ABSA\nevolves, it holds the potential to greatly improve personalized customer\nexperiences by providing a deeper understanding of sentiment across various\nproduct aspects. In this work, we have analyzed the strength of LLMs for a\ncomplete cross-domain aspect-based sentiment analysis with the aim of defining\nthe framework for certain products and using it for other similar situations.\nWe argue that it is possible to that at an effectiveness of 92\\% accuracy for\nthe Aspect Based Sentiment Analysis dataset of SemEval-2015 Task 12.\n","authors":["Karukriti Kaushik Ghosh","Chiranjib Sur"],"pdf_url":"https://arxiv.org/pdf/2501.08974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08946v1","updated":"2025-01-15T16:49:22Z","published":"2025-01-15T16:49:22Z","title":"Applying General Turn-taking Models to Conversational Human-Robot\n  Interaction","summary":"  Turn-taking is a fundamental aspect of conversation, but current Human-Robot\nInteraction (HRI) systems often rely on simplistic, silence-based models,\nleading to unnatural pauses and interruptions. This paper investigates, for the\nfirst time, the application of general turn-taking models, specifically TurnGPT\nand Voice Activity Projection (VAP), to improve conversational dynamics in HRI.\nThese models are trained on human-human dialogue data using self-supervised\nlearning objectives, without requiring domain-specific fine-tuning. We propose\nmethods for using these models in tandem to predict when a robot should begin\npreparing responses, take turns, and handle potential interruptions. We\nevaluated the proposed system in a within-subject study against a traditional\nbaseline system, using the Furhat robot with 39 adults in a conversational\nsetting, in combination with a large language model for autonomous response\ngeneration. The results show that participants significantly prefer the\nproposed system, and it significantly reduces response delays and\ninterruptions.\n","authors":["Gabriel Skantze","Bahar Irfan"],"pdf_url":"https://arxiv.org/pdf/2501.08946v1.pdf","comment":"Accepted at HRI 2025 (the IEEE/ACM International Conference on\n  Human-Robot Interaction)"},{"id":"http://arxiv.org/abs/2501.08925v1","updated":"2025-01-15T16:30:29Z","published":"2025-01-15T16:30:29Z","title":"Disentangling Exploration of Large Language Models by Optimal\n  Exploitation","summary":"  Exploration is a crucial skill for self-improvement and open-ended\nproblem-solving. However, it remains uncertain whether large language models\ncan effectively explore the state-space. Existing evaluations predominantly\nfocus on the trade-off between exploration and exploitation, often assessed in\nmulti-armed bandit problems. In contrast, this work isolates exploration as the\nsole objective, tasking the agent with delivering information that enhances\nfuture returns. For the evaluation, we propose to decompose missing rewards\ninto exploration and exploitation components by measuring the optimal\nachievable return for the states already explored. Our experiments with various\nLLMs reveal that most models struggle to sufficiently explore the state-space\nand that weak exploration is insufficient. We observe a positive correlation\nbetween model size and exploration performance, with larger models\ndemonstrating superior capabilities. Furthermore, we show that our\ndecomposition provides insights into differences in behaviors driven by agent\ninstructions during prompt engineering, offering a valuable tool for refining\nLLM performance in exploratory tasks.\n","authors":["Tim Grams","Patrick Betz","Christian Bartelt"],"pdf_url":"https://arxiv.org/pdf/2501.08925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08913v1","updated":"2025-01-15T16:21:09Z","published":"2025-01-15T16:21:09Z","title":"GenAI Content Detection Task 3: Cross-Domain Machine-Generated Text\n  Detection Challenge","summary":"  Recently there have been many shared tasks targeting the detection of\ngenerated text from Large Language Models (LLMs). However, these shared tasks\ntend to focus either on cases where text is limited to one particular domain or\ncases where text can be from many domains, some of which may not be seen during\ntest time. In this shared task, using the newly released RAID benchmark, we aim\nto answer whether or not models can detect generated text from a large, yet\nfixed, number of domains and LLMs, all of which are seen during training. Over\nthe course of three months, our task was attempted by 9 teams with 23 detector\nsubmissions. We find that multiple participants were able to obtain accuracies\nof over 99% on machine-generated text from RAID while maintaining a 5% False\nPositive Rate -- suggesting that detectors are able to robustly detect text\nfrom many domains and models simultaneously. We discuss potential\ninterpretations of this result and provide directions for future research.\n","authors":["Liam Dugan","Andrew Zhu","Firoj Alam","Preslav Nakov","Marianna Apidianaki","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2501.08913v1.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2411.18152v2","updated":"2025-01-15T15:34:13Z","published":"2024-11-27T09:01:08Z","title":"MSA-ASR: Efficient Multilingual Speaker Attribution with frozen ASR\n  Models","summary":"  Speaker-attributed automatic speech recognition (SA-ASR) aims to transcribe\nspeech while assigning transcripts to the corresponding speakers accurately.\nExisting methods often rely on complex modular systems or require extensive\nfine-tuning of joint modules, limiting their adaptability and general\nefficiency. This paper introduces a novel approach, leveraging a frozen\nmultilingual ASR model to incorporate speaker attribution into the\ntranscriptions, using only standard monolingual ASR datasets. Our method\ninvolves training a speaker module to predict speaker embeddings based on weak\nlabels without requiring additional ASR model modifications. Despite being\ntrained exclusively with non-overlapping monolingual data, our approach\neffectively extracts speaker attributes across diverse multilingual datasets,\nincluding those with overlapping speech. Experimental results demonstrate\ncompetitive performance compared to strong baselines, highlighting the model's\nrobustness and potential for practical applications.\n","authors":["Thai-Binh Nguyen","Alexander Waibel"],"pdf_url":"https://arxiv.org/pdf/2411.18152v2.pdf","comment":"Accepted at ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.08838v1","updated":"2025-01-15T14:47:02Z","published":"2025-01-15T14:47:02Z","title":"ToMATO: Verbalizing the Mental States of Role-Playing LLMs for\n  Benchmarking Theory of Mind","summary":"  Existing Theory of Mind (ToM) benchmarks diverge from real-world scenarios in\nthree aspects: 1) they assess a limited range of mental states such as beliefs,\n2) false beliefs are not comprehensively explored, and 3) the diverse\npersonality traits of characters are overlooked. To address these challenges,\nwe introduce ToMATO, a new ToM benchmark formulated as multiple-choice QA over\nconversations. ToMATO is generated via LLM-LLM conversations featuring\ninformation asymmetry. By employing a prompting method that requires\nrole-playing LLMs to verbalize their thoughts before each utterance, we capture\nboth first- and second-order mental states across five categories: belief,\nintention, desire, emotion, and knowledge. These verbalized thoughts serve as\nanswers to questions designed to assess the mental states of characters within\nconversations. Furthermore, the information asymmetry introduced by hiding\nthoughts from others induces the generation of false beliefs about various\nmental states. Assigning distinct personality traits to LLMs further\ndiversifies both utterances and thoughts. ToMATO consists of 5.4k questions,\n753 conversations, and 15 personality trait patterns. Our analysis shows that\nthis dataset construction approach frequently generates false beliefs due to\nthe information asymmetry between role-playing LLMs, and effectively reflects\ndiverse personalities. We evaluate nine LLMs on ToMATO and find that even\nGPT-4o mini lags behind human performance, especially in understanding false\nbeliefs, and lacks robustness to various personality traits.\n","authors":["Kazutoshi Shinoda","Nobukatsu Hojo","Kyosuke Nishida","Saki Mizuno","Keita Suzuki","Ryo Masumura","Hiroaki Sugiyama","Kuniko Saito"],"pdf_url":"https://arxiv.org/pdf/2501.08838v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2406.11192v2","updated":"2025-01-15T14:38:01Z","published":"2024-06-17T03:57:35Z","title":"Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets\n  and Languages for Open Named Entity Recognition","summary":"  Open Named Entity Recognition (NER), which involves identifying arbitrary\ntypes of entities from arbitrary domains, remains challenging for Large\nLanguage Models (LLMs). Recent studies suggest that fine-tuning LLMs on\nextensive NER data can boost their performance. However, training directly on\nexisting datasets neglects their inconsistent entity definitions and redundant\ndata, limiting LLMs to dataset-specific learning and hindering out-of-domain\nadaptation. To address this, we present B2NERD, a compact dataset designed to\nguide LLMs' generalization in Open NER under a universal entity taxonomy.\nB2NERD is refined from 54 existing English and Chinese datasets using a\ntwo-step process. First, we detect inconsistent entity definitions across\ndatasets and clarify them by distinguishable label names to construct a\nuniversal taxonomy of 400+ entity types. Second, we address redundancy using a\ndata pruning strategy that selects fewer samples with greater category and\nsemantic diversity. Comprehensive evaluation shows that B2NERD significantly\nenhances LLMs' Open NER capabilities. Our B2NER models, trained on B2NERD,\noutperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in 3\nout-of-domain benchmarks across 15 datasets and 6 languages. The data, models,\nand code are publicly available at https://github.com/UmeanNever/B2NER.\n","authors":["Yuming Yang","Wantong Zhao","Caishuang Huang","Junjie Ye","Xiao Wang","Huiyuan Zheng","Yang Nan","Yuran Wang","Xueying Xu","Kaixin Huang","Yunke Zhang","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2406.11192v2.pdf","comment":"Accepted at COLING 2025. Camera-ready version updated. Project page:\n  https://github.com/UmeanNever/B2NER"},{"id":"http://arxiv.org/abs/2501.08828v1","updated":"2025-01-15T14:30:13Z","published":"2025-01-15T14:30:13Z","title":"MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents","summary":"  Multi-modal document retrieval is designed to identify and retrieve various\nforms of multi-modal content, such as figures, tables, charts, and layout\ninformation from extensive documents. Despite its significance, there is a\nnotable lack of a robust benchmark to effectively evaluate the performance of\nsystems in multi-modal document retrieval. To address this gap, this work\nintroduces a new benchmark, named as MMDocIR, encompassing two distinct tasks:\npage-level and layout-level retrieval. The former focuses on localizing the\nmost relevant pages within a long document, while the latter targets the\ndetection of specific layouts, offering a more fine-grained granularity than\nwhole-page analysis. A layout can refer to a variety of elements such as\ntextual paragraphs, equations, figures, tables, or charts. The MMDocIR\nbenchmark comprises a rich dataset featuring expertly annotated labels for\n1,685 questions and bootstrapped labels for 173,843 questions, making it a\npivotal resource for advancing multi-modal document retrieval for both training\nand evaluation. Through rigorous experiments, we reveal that (i) visual\nretrievers significantly outperform their text counterparts, (ii) MMDocIR train\nset can effectively benefit the training process of multi-modal document\nretrieval and (iii) text retrievers leveraging on VLM-text perform much better\nthan those using OCR-text. These findings underscores the potential advantages\nof integrating visual elements for multi-modal document retrieval.\n","authors":["Kuicai Dong","Yujing Chang","Xin Deik Goh","Dexun Li","Ruiming Tang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08828v1.pdf","comment":"https://huggingface.co/MMDocIR"},{"id":"http://arxiv.org/abs/2501.08814v1","updated":"2025-01-15T14:12:38Z","published":"2025-01-15T14:12:38Z","title":"SAIF: A Comprehensive Framework for Evaluating the Risks of Generative\n  AI in the Public Sector","summary":"  The rapid adoption of generative AI in the public sector, encompassing\ndiverse applications ranging from automated public assistance to welfare\nservices and immigration processes, highlights its transformative potential\nwhile underscoring the pressing need for thorough risk assessments. Despite its\ngrowing presence, evaluations of risks associated with AI-driven systems in the\npublic sector remain insufficiently explored. Building upon an established\ntaxonomy of AI risks derived from diverse government policies and corporate\nguidelines, we investigate the critical risks posed by generative AI in the\npublic sector while extending the scope to account for its multimodal\ncapabilities. In addition, we propose a Systematic dAta generatIon Framework\nfor evaluating the risks of generative AI (SAIF). SAIF involves four key\nstages: breaking down risks, designing scenarios, applying jailbreak methods,\nand exploring prompt types. It ensures the systematic and consistent generation\nof prompt data, facilitating a comprehensive evaluation while providing a solid\nfoundation for mitigating the risks. Furthermore, SAIF is designed to\naccommodate emerging jailbreak methods and evolving prompt types, thereby\nenabling effective responses to unforeseen risk scenarios. We believe that this\nstudy can play a crucial role in fostering the safe and responsible integration\nof generative AI into the public sector.\n","authors":["Kyeongryul Lee","Heehyeon Kim","Joyce Jiyoung Whang"],"pdf_url":"https://arxiv.org/pdf/2501.08814v1.pdf","comment":"6 pages, 2 figures, 1 tables. AI for Public Missions (AIPM) Workshop\n  at the 39th AAAI Conference on Artificial Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2501.05816v2","updated":"2025-01-15T14:10:01Z","published":"2025-01-10T09:41:46Z","title":"IndoNLP 2025: Shared Task on Real-Time Reverse Transliteration for\n  Romanized Indo-Aryan languages","summary":"  The paper overviews the shared task on Real-Time Reverse Transliteration for\nRomanized Indo-Aryan languages. It focuses on the reverse transliteration of\nlow-resourced languages in the Indo-Aryan family to their native scripts.\nTyping Romanized Indo-Aryan languages using ad-hoc transliterals and achieving\naccurate native scripts are complex and often inaccurate processes with the\ncurrent keyboard systems. This task aims to introduce and evaluate a real-time\nreverse transliterator that converts Romanized Indo-Aryan languages to their\nnative scripts, improving the typing experience for users. Out of 11 registered\nteams, four teams participated in the final evaluation phase with\ntransliteration models for Sinhala, Hindi and Malayalam. These proposed\nsolutions not only solve the issue of ad-hoc transliteration but also empower\nlow-resource language usability in the digital arena.\n","authors":["Deshan Sumanathilaka","Isuri Anuradha","Ruvan Weerasinghe","Nicholas Micallef","Julian Hough"],"pdf_url":"https://arxiv.org/pdf/2501.05816v2.pdf","comment":"7 Pages, 1 Figure, 3 Tables"},{"id":"http://arxiv.org/abs/2403.10700v2","updated":"2025-01-15T12:45:24Z","published":"2024-03-15T21:36:15Z","title":"Mind the Error! Detection and Localization of Instruction Errors in\n  Vision-and-Language Navigation","summary":"  Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of\nthe most intuitive yet challenging embodied AI tasks. Agents are tasked to\nnavigate towards a target goal by executing a set of low-level actions,\nfollowing a series of natural language instructions. All VLN-CE methods in the\nliterature assume that language instructions are exact. However, in practice,\ninstructions given by humans can contain errors when describing a spatial\nenvironment due to inaccurate memory or confusion. Current VLN-CE benchmarks do\nnot address this scenario, making the state-of-the-art methods in VLN-CE\nfragile in the presence of erroneous instructions from human users. For the\nfirst time, we propose a novel benchmark dataset that introduces various types\nof instruction errors considering potential human causes. This benchmark\nprovides valuable insight into the robustness of VLN systems in continuous\nenvironments. We observe a noticeable performance drop (up to -25%) in Success\nRate when evaluating the state-of-the-art VLN-CE methods on our benchmark.\nMoreover, we formally define the task of Instruction Error Detection and\nLocalization, and establish an evaluation protocol on top of our benchmark\ndataset. We also propose an effective method, based on a cross-modal\ntransformer architecture, that achieves the best performance in error detection\nand localization, compared to baselines. Surprisingly, our proposed method has\nrevealed errors in the validation set of the two commonly used datasets for\nVLN-CE, i.e., R2R-CE and RxR-CE, demonstrating the utility of our technique in\nother tasks. Code and dataset available at\nhttps://intelligolabs.github.io/R2RIE-CE\n","authors":["Francesco Taioli","Stefano Rosa","Alberto Castellini","Lorenzo Natale","Alessio Del Bue","Alessandro Farinelli","Marco Cristani","Yiming Wang"],"pdf_url":"https://arxiv.org/pdf/2403.10700v2.pdf","comment":"3 figures, 8 pages. Accepted at IROS'24"},{"id":"http://arxiv.org/abs/2501.08769v1","updated":"2025-01-15T12:42:09Z","published":"2025-01-15T12:42:09Z","title":"Enhanced Large Language Models for Effective Screening of Depression and\n  Anxiety","summary":"  Depressive and anxiety disorders are widespread, necessitating timely\nidentification and management. Recent advances in Large Language Models (LLMs)\noffer potential solutions, yet high costs and ethical concerns about training\ndata remain challenges. This paper introduces a pipeline for synthesizing\nclinical interviews, resulting in 1,157 interactive dialogues (PsyInterview),\nand presents EmoScan, an LLM-based emotional disorder screening system. EmoScan\ndistinguishes between coarse (e.g., anxiety or depressive disorders) and fine\ndisorders (e.g., major depressive disorders) and conducts high-quality\ninterviews. Evaluations showed that EmoScan exceeded the performance of base\nmodels and other LLMs like GPT-4 in screening emotional disorders\n(F1-score=0.7467). It also delivers superior explanations (BERTScore=0.9408)\nand demonstrates robust generalizability (F1-score of 0.67 on an external\ndataset). Furthermore, EmoScan outperforms baselines in interviewing skills, as\nvalidated by automated ratings and human evaluations. This work highlights the\nimportance of scalable data-generative pipelines for developing effective\nmental health LLM tools.\n","authors":["June M. Liu","Mengxia Gao","Sahand Sabour","Zhuang Chen","Minlie Huang","Tatia M. C. Lee"],"pdf_url":"https://arxiv.org/pdf/2501.08769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08758v1","updated":"2025-01-15T12:22:37Z","published":"2025-01-15T12:22:37Z","title":"Expanding Vietnamese SentiWordNet to Improve Performance of Vietnamese\n  Sentiment Analysis Models","summary":"  Sentiment analysis is one of the most crucial tasks in Natural Language\nProcessing (NLP), involving the training of machine learning models to classify\ntext based on the polarity of opinions. Pre-trained Language Models (PLMs) can\nbe applied to downstream tasks through fine-tuning, eliminating the need to\ntrain the model from scratch. Specifically, PLMs have been employed for\nSentiment Analysis, a process that involves detecting, analyzing, and\nextracting the polarity of text sentiments. Numerous models have been proposed\nto address this task, with pre-trained PhoBERT-V2 models standing out as the\nstate-of-the-art language models for Vietnamese. The PhoBERT-V2 pre-training\napproach is based on RoBERTa, optimizing the BERT pre-training method for more\nrobust performance. In this paper, we introduce a novel approach that combines\nPhoBERT-V2 and SentiWordnet for Sentiment Analysis of Vietnamese reviews. Our\nproposed model utilizes PhoBERT-V2 for Vietnamese, offering a robust\noptimization for the prominent BERT model in the context of Vietnamese\nlanguage, and leverages SentiWordNet, a lexical resource explicitly designed to\nsupport sentiment classification applications. Experimental results on the VLSP\n2016 and AIVIVN 2019 datasets demonstrate that our sentiment analysis system\nhas achieved excellent performance in comparison to other models.\n","authors":["Hong-Viet Tran","Van-Tan Bui","Lam-Quan Tran"],"pdf_url":"https://arxiv.org/pdf/2501.08758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17696v5","updated":"2025-01-15T11:12:26Z","published":"2023-11-29T15:02:46Z","title":"How to Build an AI Tutor That Can Adapt to Any Course Using Knowledge\n  Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)","summary":"  This paper introduces a novel framework for adaptable AI tutors using\nKnowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG). This approach\naddresses the critical challenges of information hallucination and limited\ncourse-specific adaptation prevalent in Large Language Model (LLM)-based\ntutoring systems. By integrating Knowledge Graphs (KGs) with RAG, we provide a\nstructured representation of course concepts and their interrelationships,\ngrounding the AI tutor's responses in relevant, validated material. We leverage\nQwen2.5, a powerful and cost-effective LLM, within our KG-RAG framework. A user\nstudy (n=50) demonstrated positive student feedback regarding answer relevance,\nease of use, and overall satisfaction. This KG-RAG framework offers a promising\npathway towards personalized learning experiences and broader access to\nhigh-quality education.\n","authors":["Chenxi Dong","Yimin Yuan","Kan Chen","Shupei Cheng","Chujie Wen"],"pdf_url":"https://arxiv.org/pdf/2311.17696v5.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.08716v1","updated":"2025-01-15T10:57:55Z","published":"2025-01-15T10:57:55Z","title":"The Inherent Limits of Pretrained LLMs: The Unexpected Convergence of\n  Instruction Tuning and In-Context Learning Capabilities","summary":"  Large Language Models (LLMs), trained on extensive web-scale corpora, have\ndemonstrated remarkable abilities across diverse tasks, especially as they are\nscaled up. Nevertheless, even state-of-the-art models struggle in certain\ncases, sometimes failing at problems solvable by young children, indicating\nthat traditional notions of task complexity are insufficient for explaining LLM\ncapabilities. However, exploring LLM capabilities is complicated by the fact\nthat most widely-used models are also \"instruction-tuned\" to respond\nappropriately to prompts. With the goal of disentangling the factors\ninfluencing LLM performance, we investigate whether instruction-tuned models\npossess fundamentally different capabilities from base models that are prompted\nusing in-context examples. Through extensive experiments across various model\nfamilies, scales and task types, which included instruction tuning 90 different\nLLMs, we demonstrate that the performance of instruction-tuned models is\nsignificantly correlated with the in-context performance of their base\ncounterparts. By clarifying what instruction-tuning contributes, we extend\nprior research into in-context learning, which suggests that base models use\npriors from pretraining data to solve tasks. Specifically, we extend this\nunderstanding to instruction-tuned models, suggesting that their pretraining\ndata similarly sets a limiting boundary on the tasks they can solve, with the\nadded influence of the instruction-tuning dataset.\n","authors":["Irina Bigoulaeva","Harish Tayyar Madabushi","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2501.08716v1.pdf","comment":"The code for this paper is available at:\n  https://github.com/UKPLab/arxiv2025-inherent-limits-plms"},{"id":"http://arxiv.org/abs/2406.06484v6","updated":"2025-01-15T10:41:40Z","published":"2024-06-10T17:24:42Z","title":"Parallelizing Linear Transformers with the Delta Rule over Sequence\n  Length","summary":"  Transformers with linear attention (i.e., linear transformers) and\nstate-space models have recently been suggested as a viable linear-time\nalternative to transformers with softmax attention. However, these models still\nunderperform transformers especially on tasks that require in-context\nretrieval. While more expressive variants of linear transformers which replace\nthe additive update in linear transformers with the delta rule (DeltaNet) have\nbeen found to be more effective at associative recall, existing algorithms for\ntraining such models do not parallelize over sequence length and are thus\ninefficient to train on modern hardware. This work describes a\nhardware-efficient algorithm for training linear transformers with the delta\nrule, which exploits a memory-efficient representation for computing products\nof Householder matrices. This algorithm allows us to scale up DeltaNet to\nstandard language modeling settings. We train a 1.3B model for 100B tokens and\nfind that it outperforms recent linear-time baselines such as Mamba and GLA in\nterms of perplexity and zero-shot performance on downstream tasks. We also\nexperiment with two hybrid models which combine DeltaNet layers with (1)\nsliding-window attention layers every other layer or (2) two global attention\nlayers, and find that these hybrids outperform strong transformer baselines.\n","authors":["Songlin Yang","Bailin Wang","Yu Zhang","Yikang Shen","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2406.06484v6.pdf","comment":"Final camera ready"},{"id":"http://arxiv.org/abs/2501.08696v1","updated":"2025-01-15T10:09:38Z","published":"2025-01-15T10:09:38Z","title":"Deep Learning-Based Feature Fusion for Emotion Analysis and Suicide Risk\n  Differentiation in Chinese Psychological Support Hotlines","summary":"  Mental health is a critical global public health issue, and psychological\nsupport hotlines play a pivotal role in providing mental health assistance and\nidentifying suicide risks at an early stage. However, the emotional expressions\nconveyed during these calls remain underexplored in current research. This\nstudy introduces a method that combines pitch acoustic features with deep\nlearning-based features to analyze and understand emotions expressed during\nhotline interactions. Using data from China's largest psychological support\nhotline, our method achieved an F1-score of 79.13% for negative binary emotion\nclassification.Additionally, the proposed approach was validated on an open\ndataset for multi-class emotion classification,where it demonstrated better\nperformance compared to the state-of-the-art methods. To explore its clinical\nrelevance, we applied the model to analysis the frequency of negative emotions\nand the rate of emotional change in the conversation, comparing 46 subjects\nwith suicidal behavior to those without. While the suicidal group exhibited\nmore frequent emotional changes than the non-suicidal group, the difference was\nnot statistically significant.Importantly, our findings suggest that emotional\nfluctuation intensity and frequency could serve as novel features for\npsychological assessment scales and suicide risk prediction.The proposed method\nprovides valuable insights into emotional dynamics and has the potential to\nadvance early intervention and improve suicide prevention strategies through\nintegration with clinical tools and assessments The source code is publicly\navailable at https://github.com/Sco-field/Speechemotionrecognition/tree/main.\n","authors":["Han Wang","Jianqiang Li","Qing Zhao","Zhonglong Chen","Changwei Song","Jing Tang","Yuning Huang","Wei Zhai","Yongsheng Tong","Guanghui Fu"],"pdf_url":"https://arxiv.org/pdf/2501.08696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08686v1","updated":"2025-01-15T09:32:37Z","published":"2025-01-15T09:32:37Z","title":"Knowledge Graph-based Retrieval-Augmented Generation for Schema Matching","summary":"  Traditional similarity-based schema matching methods are incapable of\nresolving semantic ambiguities and conflicts in domain-specific complex mapping\nscenarios due to missing commonsense and domain-specific knowledge. The\nhallucination problem of large language models (LLMs) also makes it challenging\nfor LLM-based schema matching to address the above issues. Therefore, we\npropose a Knowledge Graph-based Retrieval-Augmented Generation model for Schema\nMatching, referred to as the KG-RAG4SM. In particular, KG-RAG4SM introduces\nnovel vector-based, graph traversal-based, and query-based graph retrievals, as\nwell as a hybrid approach and ranking schemes that identify the most relevant\nsubgraphs from external large knowledge graphs (KGs). We showcase that KG-based\nretrieval-augmented LLMs are capable of generating more accurate results for\ncomplex matching cases without any re-training. Our experimental results show\nthat KG-RAG4SM outperforms the LLM-based state-of-the-art (SOTA) methods (e.g.,\nJellyfish-8B) by 35.89% and 30.50% in terms of precision and F1 score on the\nMIMIC dataset, respectively; KG-RAG4SM with GPT-4o-mini outperforms the\npre-trained language model (PLM)-based SOTA methods (e.g., SMAT) by 69.20% and\n21.97% in terms of precision and F1 score on the Synthea dataset, respectively.\nThe results also demonstrate that our approach is more efficient in end-to-end\nschema matching, and scales to retrieve from large KGs. Our case studies on the\ndataset from the real-world schema matching scenario exhibit that the\nhallucination problem of LLMs for schema matching is well mitigated by our\nsolution.\n","authors":["Chuangtao Ma","Sriom Chakrabarti","Arijit Khan","Bálint Molnár"],"pdf_url":"https://arxiv.org/pdf/2501.08686v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2408.15512v3","updated":"2025-01-15T09:12:02Z","published":"2024-08-28T03:48:05Z","title":"Toward Automated Simulation Research Workflow through LLM Prompt\n  Engineering Design","summary":"  The advent of Large Language Models (LLMs) has created new opportunities for\nthe automation of scientific research spanning both experimental processes and\ncomputational simulations. This study explores the feasibility of constructing\nan autonomous simulation agent (ASA) powered by LLMs through prompt engineering\nand automated program design to automate the entire simulation research process\naccording to a human-provided research plan. This process includes experimental\ndesign, remote upload and simulation execution, data analysis, and report\ncompilation. Using a well-studied simulation problem of polymer chain\nconformations as a test case, we assessed the long-task completion and\nreliability of ASAs powered by different LLMs, including GPT-4o, Claude-3.5,\netc. Our findings revealed that ASA-GPT-4o achieved near-flawless execution on\ndesignated research missions, underscoring the potential of methods like ASA to\nachieve automation in simulation research processes to enhance research\nefficiency. The outlined automation can be iteratively performed for up to 20\ncycles without human intervention, illustrating the potential of ASA for\nlong-task workflow automation. Additionally, we discussed the intrinsic traits\nof ASA in managing extensive tasks, focusing on self-validation mechanisms, and\nthe balance between local attention and global oversight.\n","authors":["Zhihan Liu","Yubo Chai","Jianfeng Li"],"pdf_url":"https://arxiv.org/pdf/2408.15512v3.pdf","comment":"The source code and example results of ASA can be found at\n  https://github.com/zokaraa/autonomous_simulation_agent"},{"id":"http://arxiv.org/abs/2501.08284v2","updated":"2025-01-15T08:55:50Z","published":"2025-01-14T18:00:07Z","title":"AfriHate: A Multilingual Collection of Hate Speech and Abusive Language\n  Datasets for African Languages","summary":"  Hate speech and abusive language are global phenomena that need\nsocio-cultural background knowledge to be understood, identified, and\nmoderated. However, in many regions of the Global South, there have been\nseveral documented occurrences of (1) absence of moderation and (2) censorship\ndue to the reliance on keyword spotting out of context. Further, high-profile\nindividuals have frequently been at the center of the moderation process, while\nlarge and targeted hate speech campaigns against minorities have been\noverlooked. These limitations are mainly due to the lack of high-quality data\nin the local languages and the failure to include local communities in the\ncollection, annotation, and moderation processes. To address this issue, we\npresent AfriHate: a multilingual collection of hate speech and abusive language\ndatasets in 15 African languages. Each instance in AfriHate is annotated by\nnative speakers familiar with the local culture. We report the challenges\nrelated to the construction of the datasets and present various classification\nbaseline results with and without using LLMs. The datasets, individual\nannotations, and hate speech and offensive language lexicons are available on\nhttps://github.com/AfriHate/AfriHate\n","authors":["Shamsuddeen Hassan Muhammad","Idris Abdulmumin","Abinew Ali Ayele","David Ifeoluwa Adelani","Ibrahim Said Ahmad","Saminu Mohammad Aliyu","Nelson Odhiambo Onyango","Lilian D. A. Wanzare","Samuel Rutunda","Lukman Jibril Aliyu","Esubalew Alemneh","Oumaima Hourrane","Hagos Tesfahun Gebremichael","Elyas Abdi Ismail","Meriem Beloucif","Ebrahim Chekol Jibril","Andiswa Bukula","Rooweither Mabuya","Salomey Osei","Abigail Oppong","Tadesse Destaw Belay","Tadesse Kebede Guge","Tesfa Tegegne Asfaw","Chiamaka Ijeoma Chukwuneke","Paul Röttger","Seid Muhie Yimam","Nedjma Ousidhoum"],"pdf_url":"https://arxiv.org/pdf/2501.08284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08648v1","updated":"2025-01-15T08:24:03Z","published":"2025-01-15T08:24:03Z","title":"MAGNET: Augmenting Generative Decoders with Representation Learning and\n  Infilling Capabilities","summary":"  While originally designed for unidirectional generative modeling,\ndecoder-only large language models (LLMs) are increasingly being adapted for\nbidirectional modeling. However, unidirectional and bidirectional models are\ntypically trained separately with distinct objectives (generation and\nrepresentation learning, respectively). This separation overlooks the\nopportunity for developing a more versatile language model and for these\nobjectives to complement each other. In this work, we introduce MAGNET, an\nadaptation of decoder-only LLMs that enhances their ability to generate robust\nrepresentations and infill missing text spans, while preserving their knowledge\nand text generation capabilities. MAGNET employs three self-supervised training\nobjectives and introduces an attention mechanism that combines bidirectional\nand causal attention, enabling unified training across all objectives. Our\nresults demonstrate that LLMs adapted with MAGNET (1) surpass strong text\nencoders on token-level and sentence-level representation learning tasks, (2)\ngenerate contextually appropriate text infills by leveraging future context,\n(3) retain the ability for open-ended text generation without exhibiting\nrepetition problem, and (4) preserve the knowledge gained by the LLM during\npretraining.\n","authors":["Savya Khosla","Kushal Kafle","Simon Jenni","Handong Zhao","John Collomosse","Jing Shi"],"pdf_url":"https://arxiv.org/pdf/2501.08648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16705v2","updated":"2025-01-15T08:20:19Z","published":"2024-02-26T16:21:53Z","title":"SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware\n  Self-Reflection","summary":"  Instruction tuning (IT) is crucial to tailoring large language models (LLMs)\ntowards human-centric interactions. Recent advancements have shown that the\ncareful selection of a small, high-quality subset of IT data can significantly\nenhance the performance of LLMs. Despite this, common approaches often rely on\nadditional models or data, which increases costs and limits widespread\nadoption. In this work, we propose a novel approach, termed SelectIT, that\ncapitalizes on the foundational capabilities of the LLM itself. Specifically,\nwe exploit the intrinsic uncertainty present in LLMs to more effectively select\nhigh-quality IT data, without the need for extra resources. Furthermore, we\nintroduce a curated IT dataset, the Selective Alpaca, created by applying\nSelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT\nusing Selective Alpaca leads to substantial model ability enhancement. The\nrobustness of SelectIT has also been corroborated in various foundation models\nand domain-specific tasks. Our findings suggest that longer and more\ncomputationally intensive IT data may serve as superior sources of IT, offering\nvaluable insights for future research in this area. Data, code, and scripts are\nfreely available at https://github.com/Blue-Raincoat/SelectIT.\n","authors":["Liangxin Liu","Xuebo Liu","Derek F. Wong","Dongfang Li","Ziyi Wang","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.16705v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2501.08641v1","updated":"2025-01-15T08:07:22Z","published":"2025-01-15T08:07:22Z","title":"Reassessing the Role of Chain-of-Thought in Sentiment Analysis: Insights\n  and Limitations","summary":"  The relationship between language and thought remains an unresolved\nphilosophical issue. Existing viewpoints can be broadly categorized into two\nschools: one asserting their independence, and another arguing that language\nconstrains thought. In the context of large language models, this debate raises\na crucial question: Does a language model's grasp of semantic meaning depend on\nthought processes? To explore this issue, we investigate whether reasoning\ntechniques can facilitate semantic understanding. Specifically, we\nconceptualize thought as reasoning, employ chain-of-thought prompting as a\nreasoning technique, and examine its impact on sentiment analysis tasks. The\nexperiments show that chain-of-thought has a minimal impact on sentiment\nanalysis tasks. Both the standard and chain-of-thought prompts focus on aspect\nterms rather than sentiment in the generated content. Furthermore,\ncounterfactual experiments reveal that the model's handling of sentiment tasks\nprimarily depends on information from demonstrations. The experimental results\nsupport the first viewpoint.\n","authors":["Kaiyuan Zheng","Qinghua Zhao","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2501.08641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11344v3","updated":"2025-01-15T07:46:15Z","published":"2024-11-18T07:33:10Z","title":"Mitigating Knowledge Conflicts in Language Model-Driven Question\n  Answering","summary":"  In the context of knowledge-driven seq-to-seq generation tasks, such as\ndocument-based question answering and document summarization systems, two\nfundamental knowledge sources play crucial roles: the inherent knowledge\nembedded within model parameters and the external knowledge obtained through\ncontext. Recent studies revealed a significant challenge: when there exists a\nmisalignment between the model's inherent knowledge and the ground truth\nanswers in training data, the system may exhibit problematic behaviors during\ninference, such as ignoring input context, or generating unfaithful content.\nOur investigation proposes a strategy to minimize hallucination by building\nexplicit connection between source inputs and generated outputs. We\nspecifically target a common hallucination pattern in question answering,\nexamining how the correspondence between entities and their contexts during\nmodel training influences the system's performance at inference time.\n","authors":["Han Cao","Zhaoyang Zhang","Xiangtian Li","Chufan Wu","Hansong Zhang","Wenqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.11344v3.pdf","comment":"revised version, more figures"},{"id":"http://arxiv.org/abs/2501.08631v1","updated":"2025-01-15T07:36:19Z","published":"2025-01-15T07:36:19Z","title":"SWSC: Shared Weight for Similar Channel in LLM","summary":"  Large language models (LLMs) have spurred development in multiple industries.\nHowever, the growing number of their parameters brings substantial storage and\ncomputing burdens, making it essential to explore model compression techniques\nfor parameter reduction and easier deployment. We propose SWSC, an LLM\ncompression method based on the concept of Shared Weight for Similar Channel.\nIt uses the K-Means clustering algorithm to cluster model weights\nchannel-by-channel, generating clusters with highly similar vectors within\neach. A representative vector from each cluster is selected to approximately\nreplace all vectors in the cluster, significantly reducing the number of model\nweight parameters. However, approximate restoration will inevitably cause\ndamage to the performance of the model. To tackle this issue, we perform\nsingular value decomposition on the weight error values before and after\ncompression and retain the larger singular values and their corresponding\nsingular vectors to compensate for the accuracy. The experimental results show\nthat our method can effectively ensure the performance of the compressed LLM\neven under low-precision conditions.\n","authors":["Binrui Zeng","Yongtao Tang","Xiaodong Liu","Xiaopeng Li"],"pdf_url":"https://arxiv.org/pdf/2501.08631v1.pdf","comment":"5pages, 3 figures, work in progress"},{"id":"http://arxiv.org/abs/2405.07765v2","updated":"2025-01-15T07:29:20Z","published":"2024-05-13T14:07:20Z","title":"TANQ: An open domain dataset of table answered questions","summary":"  Language models, potentially augmented with tool usage such as retrieval are\nbecoming the go-to means of answering questions. Understanding and answering\nquestions in real-world settings often requires retrieving information from\ndifferent sources, processing and aggregating data to extract insights, and\npresenting complex findings in form of structured artifacts such as novel\ntables, charts, or infographics. In this paper, we introduce TANQ, the first\nopen domain question answering dataset where the answers require building\ntables from information across multiple sources. We release the full source\nattribution for every cell in the resulting table and benchmark\nstate-of-the-art language models in open, oracle, and closed book setups. Our\nbest-performing baseline, GPT4 reaches an overall F1 score of 29.1, lagging\nbehind human performance by 19.7 points. We analyse baselines' performance\nacross different dataset attributes such as different skills required for this\ntask, including multi-hop reasoning, math operations, and unit conversions. We\nfurther discuss common failures in model-generated answers, suggesting that\nTANQ is a complex task with many challenges ahead.\n","authors":["Mubashara Akhtar","Chenxi Pang","Andreea Marzoca","Yasemin Altun","Julian Martin Eisenschlos"],"pdf_url":"https://arxiv.org/pdf/2405.07765v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2501.08621v1","updated":"2025-01-15T06:40:26Z","published":"2025-01-15T06:40:26Z","title":"ViBidirectionMT-Eval: Machine Translation for Vietnamese-Chinese and\n  Vietnamese-Lao language pair","summary":"  This paper presents an results of the VLSP 2022-2023 Machine Translation\nShared Tasks, focusing on Vietnamese-Chinese and Vietnamese-Lao machine\ntranslation. The tasks were organized as part of the 9th, 10th annual workshop\non Vietnamese Language and Speech Processing (VLSP 2022, VLSP 2023). The\nobjective of the shared task was to build machine translation systems,\nspecifically targeting Vietnamese-Chinese and Vietnamese-Lao translation\n(corresponding to 4 translation directions). The submission were evaluated on\n1,000 pairs for testing (news and general domains) using established metrics\nlike BLEU [11] and SacreBLEU [12]. Additionally, system outputs also were\nevaluated with human judgment provided by experts in Chinese and Lao languages.\nThese human assessments played a crucial role in ranking the performance of the\nmachine translation models, ensuring a more comprehensive evaluation.\n","authors":["Hong-Viet Tran","Minh-Quy Nguyen","Van-Vinh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2501.08621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07437v3","updated":"2025-01-15T06:39:57Z","published":"2024-09-11T17:34:52Z","title":"Salmon: A Suite for Acoustic Language Model Evaluation","summary":"  Speech language models have recently demonstrated great potential as\nuniversal speech processing systems. Such models have the ability to model the\nrich acoustic information existing in audio signals, beyond spoken content,\nsuch as emotion, background noise, etc. Despite this, evaluation benchmarks\nwhich evaluate awareness to a wide range of acoustic aspects, are lacking. To\nhelp bridge this gap, we introduce SALMon, a novel evaluation suite\nencompassing background noise, emotion, speaker identity and room impulse\nresponse. The proposed benchmarks both evaluate the consistency of the\ninspected element and how much it matches the spoken text. We follow a\nmodelling based approach, measuring whether a model gives correct samples\nhigher scores than incorrect ones. This approach makes the benchmark fast to\ncompute even for large models. We evaluated several speech language models on\nSALMon, thus highlighting the strengths and weaknesses of each evaluated\nmethod. We make the code and data publicly available at\nhttps://pages.cs.huji.ac.il/adiyoss-lab/salmon/ .\n","authors":["Gallil Maimon","Amit Roth","Yossi Adi"],"pdf_url":"https://arxiv.org/pdf/2409.07437v3.pdf","comment":"ICASSP 2025, project page -\n  https://pages.cs.huji.ac.il/adiyoss-lab/salmon/"},{"id":"http://arxiv.org/abs/2501.08618v1","updated":"2025-01-15T06:34:34Z","published":"2025-01-15T06:34:34Z","title":"Disjoint Processing Mechanisms of Hierarchical and Linear Grammars in\n  Large Language Models","summary":"  All natural languages are structured hierarchically. In humans, this\nstructural restriction is neurologically coded: when two grammars are presented\nwith identical vocabularies, brain areas responsible for language processing\nare only sensitive to hierarchical grammars. Using large language models\n(LLMs), we investigate whether such functionally distinct hierarchical\nprocessing regions can arise solely from exposure to large-scale language\ndistributions. We generate inputs using English, Italian, Japanese, or nonce\nwords, varying the underlying grammars to conform to either hierarchical or\nlinear/positional rules. Using these grammars, we first observe that language\nmodels show distinct behaviors on hierarchical versus linearly structured\ninputs. Then, we find that the components responsible for processing\nhierarchical grammars are distinct from those that process linear grammars; we\ncausally verify this in ablation experiments. Finally, we observe that\nhierarchy-selective components are also active on nonce grammars; this suggests\nthat hierarchy sensitivity is not tied to meaning, nor in-distribution inputs.\n","authors":["Aruna Sankaranarayanan","Dylan Hadfield-Menell","Aaron Mueller"],"pdf_url":"https://arxiv.org/pdf/2501.08618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08617v1","updated":"2025-01-15T06:33:15Z","published":"2025-01-15T06:33:15Z","title":"RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation","summary":"  Generative AI systems like foundation models (FMs) must align well with human\nvalues to ensure their behavior is helpful and trustworthy. While Reinforcement\nLearning from Human Feedback (RLHF) has shown promise for optimizing model\nperformance using human judgments, existing RLHF pipelines predominantly rely\non immediate feedback, which can fail to accurately reflect the downstream\nimpact of an interaction on users' utility. We demonstrate that feedback based\non evaluators' foresight estimates of downstream consequences systematically\ninduces Goodhart's Law dynamics, incentivizing misaligned behaviors like\nsycophancy and deception and ultimately degrading user outcomes. To alleviate\nthis, we propose decoupling evaluation from prediction by refocusing RLHF on\nhindsight feedback. Our theoretical analysis reveals that conditioning\nevaluator feedback on downstream observations mitigates misalignment and\nimproves expected human utility, even when these observations are simulated by\nthe AI system itself. To leverage this insight in a practical alignment\nalgorithm, we introduce Reinforcement Learning from Hindsight Simulation\n(RLHS), which first simulates plausible consequences and then elicits feedback\nto assess what behaviors were genuinely beneficial in hindsight. We apply RLHS\nto two widely-employed online and offline preference optimization methods --\nProximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) --\nand show empirically that misalignment is significantly reduced with both\nmethods. Through an online human user study, we show that RLHS consistently\noutperforms RLHF in helping users achieve their goals and earns higher\nsatisfaction ratings, despite being trained solely with simulated hindsight\nfeedback. These results underscore the importance of focusing on long-term\nconsequences, even simulated ones, to mitigate misalignment in RLHF.\n","authors":["Kaiqu Liang","Haimin Hu","Ryan Liu","Thomas L. Griffiths","Jaime Fernández Fisac"],"pdf_url":"https://arxiv.org/pdf/2501.08617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06832v4","updated":"2025-01-15T06:30:19Z","published":"2024-03-11T15:48:43Z","title":"Noise-powered Multi-modal Knowledge Graph Representation Framework","summary":"  The rise of Multi-modal Pre-training highlights the necessity for a unified\nMulti-Modal Knowledge Graph (MMKG) representation learning framework. Such a\nframework is essential for embedding structured knowledge into multi-modal\nLarge Language Models effectively, alleviating issues like knowledge\nmisconceptions and multi-modal hallucinations. In this work, we explore the\nefficacy of models in accurately embedding entities within MMKGs through two\npivotal tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal\nEntity Alignment (MMEA). Building on this foundation, we propose a novel SNAG\nmethod that utilizes a Transformer-based architecture equipped with\nmodality-level noise masking to robustly integrate multi-modal entity features\nin KGs. By incorporating specific training objectives for both MKGC and MMEA,\nour approach achieves SOTA performance across a total of ten datasets,\ndemonstrating its versatility. Moreover, SNAG can not only function as a\nstandalone model but also enhance other existing methods, providing stable\nperformance improvements. Code and data are available at\nhttps://github.com/zjukg/SNAG.\n","authors":["Zhuo Chen","Yin Fang","Yichi Zhang","Lingbing Guo","Jiaoyan Chen","Jeff Z. Pan","Huajun Chen","Wen Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06832v4.pdf","comment":"COLING 2025 Accepted, Repo is available at\n  https://github.com/zjukg/SNAG"},{"id":"http://arxiv.org/abs/2501.08613v1","updated":"2025-01-15T06:22:35Z","published":"2025-01-15T06:22:35Z","title":"Assessing the Alignment of FOL Closeness Metrics with Human Judgement","summary":"  The recent successful paradigm of solving logical reasoning problems with\ntool-augmented large language models (LLMs) leverages translation of natural\nlanguage statements into First-Order Logic~(FOL) and external theorem provers.\nHowever, the correctness of FOL statements, comprising operators and text\npredicates, often goes unverified due to the lack of a reliable evaluation\nmetric for comparing generated and ground-truth FOLs. In this paper, we present\na comprehensive study of sensitivity of existing metrics and their alignment\nwith human judgement on FOL evaluation. Using ground-truth FOLs, we carefully\ndesigned various perturbations on the ground-truth to assess metric\nsensitivity. We sample FOL translation candidates for natural language\nstatements and measure the ranking alignment between automatic metrics and\nhuman annotators. Our empirical findings highlight oversensitivity in the\nn-gram metric BLEU for text perturbations, the semantic graph metric Smatch++\nfor structural perturbations, and FOL metric for operator perturbation. We also\nobserve a closer alignment between BertScore and human judgement. Additionally,\nwe show that combining metrics enhances both alignment and sensitivity compared\nto using individual metrics.\n","authors":["Ramya Keerthy Thatikonda","Wray Buntine","Ehsan Shareghi"],"pdf_url":"https://arxiv.org/pdf/2501.08613v1.pdf","comment":"Code: https://github.com/RamyaKeerthy/AlignmentFOL"},{"id":"http://arxiv.org/abs/2412.20061v2","updated":"2025-01-15T06:15:13Z","published":"2024-12-28T07:30:05Z","title":"Comparative Analysis of Listwise Reranking with Large Language Models in\n  Limited-Resource Language Contexts","summary":"  Large Language Models (LLMs) have demonstrated significant effectiveness\nacross various NLP tasks, including text ranking. This study assesses the\nperformance of large language models (LLMs) in listwise reranking for\nlimited-resource African languages. We compare proprietary models RankGPT3.5,\nRank4o-mini, RankGPTo1-mini and RankClaude-sonnet in cross-lingual contexts.\nResults indicate that these LLMs significantly outperform traditional baseline\nmethods such as BM25-DT in most evaluation metrics, particularly in nDCG@10 and\nMRR@100. These findings highlight the potential of LLMs in enhancing reranking\ntasks for low-resource languages and offer insights into cost-effective\nsolutions.\n","authors":["Yanxin Shen","Lun Wang","Chuanqi Shi","Shaoshuai Du","Yiyi Tao","Yixian Shen","Hang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.20061v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08597v1","updated":"2025-01-15T05:45:04Z","published":"2025-01-15T05:45:04Z","title":"Dynamic Knowledge Integration for Enhanced Vision-Language Reasoning","summary":"  Large Vision-Language Models (LVLMs) have demonstrated impressive\ncapabilities in multimodal tasks, but their performance is often constrained by\nthe lack of external knowledge integration, limiting their ability to handle\nknowledge-intensive tasks such as visual question answering and reasoning. To\naddress this challenge, we propose a novel method, Adaptive Knowledge-Guided\nPretraining for Large Vision-Language Models (AKGP-LVLM), which dynamically\nincorporates structured and unstructured knowledge into LVLMs during\npretraining and fine-tuning. Our approach employs a knowledge encoder to\nrepresent external knowledge, a retrieval mechanism to select task-relevant\ninformation, and a dynamic adaptor to align multimodal and knowledge\nrepresentations effectively. We evaluate our method on four benchmark datasets,\ndemonstrating significant performance improvements over state-of-the-art\nmodels. Furthermore, human evaluations highlight the superior correctness and\nrelevance of our model's outputs. Extensive analyses confirm the robustness,\nefficiency, and scalability of AKGP-LVLM, making it a compelling solution for\nreal-world knowledge-intensive tasks.\n","authors":["Julian Perry","Surasakdi Siripong","Thanakorn Phonchai"],"pdf_url":"https://arxiv.org/pdf/2501.08597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08582v1","updated":"2025-01-15T05:07:06Z","published":"2025-01-15T05:07:06Z","title":"LoRS: Efficient Low-Rank Adaptation for Sparse Large Language Model","summary":"  Existing low-rank adaptation (LoRA) methods face challenges on sparse large\nlanguage models (LLMs) due to the inability to maintain sparsity. Recent works\nintroduced methods that maintain sparsity by augmenting LoRA techniques with\nadditional masking mechanisms. Despite these successes, such approaches suffer\nfrom an increased memory and computation overhead, which affects efficiency of\nLoRA methods. In response to this limitation, we introduce LoRS, an innovative\nmethod designed to achieve both memory and computation efficiency when\nfine-tuning sparse LLMs. To mitigate the substantial memory and computation\ndemands associated with preserving sparsity, our approach incorporates\nstrategies of weight recompute and computational graph rearrangement. In\naddition, we also improve the effectiveness of LoRS through better adapter\ninitialization. These innovations lead to a notable reduction in memory and\ncomputation consumption during the fine-tuning phase, all while achieving\nperformance levels that outperform existing LoRA approaches.\n","authors":["Yuxuan Hu","Jing Zhang","Xiaodong Chen","Zhe Zhao","Cuiping Li","Hong Chen"],"pdf_url":"https://arxiv.org/pdf/2501.08582v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2501.08579v1","updated":"2025-01-15T04:59:49Z","published":"2025-01-15T04:59:49Z","title":"What Limits LLM-based Human Simulation: LLMs or Our Design?","summary":"  We argue that advancing LLM-based human simulation requires addressing both\nLLM's inherent limitations and simulation framework design challenges. Recent\nstudies have revealed significant gaps between LLM-based human simulations and\nreal-world observations, highlighting these dual challenges. To address these\ngaps, we present a comprehensive analysis of LLM limitations and our design\nissues, proposing targeted solutions for both aspects. Furthermore, we explore\nfuture directions that address both challenges simultaneously, particularly in\ndata collection, LLM generation, and evaluation. To support further research in\nthis field, we provide a curated collection of LLM-based human simulation\nresources.\\footnote{https://github.com/Persdre/llm-human-simulation}\n","authors":["Qian Wang","Jiaying Wu","Zhenheng Tang","Bingqiao Luo","Nuo Chen","Wei Chen","Bingsheng He"],"pdf_url":"https://arxiv.org/pdf/2501.08579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18023v3","updated":"2025-01-15T04:47:36Z","published":"2024-02-28T03:38:20Z","title":"Do Large Language Models Mirror Cognitive Language Processing?","summary":"  Large Language Models (LLMs) have demonstrated remarkable abilities in text\ncomprehension and logical reasoning, indicating that the text representations\nlearned by LLMs can facilitate their language processing capabilities. In\nneuroscience, brain cognitive processing signals are typically utilized to\nstudy human language processing. Therefore, it is natural to ask how well the\ntext embeddings from LLMs align with the brain cognitive processing signals,\nand how training strategies affect the LLM-brain alignment? In this paper, we\nemploy Representational Similarity Analysis (RSA) to measure the alignment\nbetween 23 mainstream LLMs and fMRI signals of the brain to evaluate how\neffectively LLMs simulate cognitive language processing. We empirically\ninvestigate the impact of various factors (e.g., pre-training data size, model\nscaling, alignment training, and prompts) on such LLM-brain alignment.\nExperimental results indicate that pre-training data size and model scaling are\npositively correlated with LLM-brain similarity, and alignment training can\nsignificantly improve LLM-brain similarity. Explicit prompts contribute to the\nconsistency of LLMs with brain cognitive language processing, while nonsensical\nnoisy prompts may attenuate such alignment. Additionally, the performance of a\nwide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated\nwith the LLM-brain similarity.\n","authors":["Yuqi Ren","Renren Jin","Tongxuan Zhang","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2402.18023v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08570v1","updated":"2025-01-15T04:32:41Z","published":"2025-01-15T04:32:41Z","title":"Information Entropy Invariance: Enhancing Length Extrapolation in\n  Attention Mechanisms","summary":"  Improving the length extrapolation capabilities of Large Language Models\n(LLMs) remains a critical challenge in natural language processing. Many recent\nefforts have focused on modifying the scaled dot-product attention mechanism,\nand often introduce scaled temperatures without rigorous theoretical\njustification. To fill this gap, we introduce a novel approach based on\ninformation entropy invariance. We propose two new scaled temperatures to\nenhance length extrapolation. First, a training-free method InfoScale is\ndesigned for dot-product attention, and preserves focus on original tokens\nduring length extrapolation by ensuring information entropy remains consistent.\nSecond, we theoretically analyze the impact of scaling (CosScale) on cosine\nattention. Experimental data demonstrates that combining InfoScale and CosScale\nachieves state-of-the-art performance on the GAU-{\\alpha} model with a context\nwindow extended to 64 times the training length, and outperforms seven existing\nmethods. Our analysis reveals that significantly increasing CosScale\napproximates windowed attention, and highlights the significance of attention\nscore dilution as a key challenge in long-range context handling. The code and\ndata are available at https://github.com/HT-NEKO/InfoScale.\n","authors":["Kewei Li","Yanwen Kong","Yiping Xu","Lan Huang","Ruochi Zhang","Fengfeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.08570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04820v2","updated":"2025-01-15T03:43:22Z","published":"2024-08-09T02:22:51Z","title":"Natural Language Outlines for Code: Literate Programming in the LLM Era","summary":"  We propose using natural language outlines as a novel modality and\ninteraction surface for providing AI assistance to developers throughout the\nsoftware development process. An NL outline for a code function comprises\nmultiple statements written in concise prose, which partition the code and\nsummarize its main ideas in the style of literate programming. Crucially, we\nfind that modern LLMs can generate accurate and high-quality NL outlines in\npractice. Moreover, NL outlines enable a bidirectional sync between code and\nNL, allowing changes in one to be automatically reflected in the other. We\ndiscuss many use cases for NL outlines: they can accelerate understanding and\nnavigation of code and diffs, simplify code maintenance, augment code search,\nsteer code generation, and more. We then propose and compare multiple LLM\nprompting techniques for generating outlines and ask professional developers to\njudge outline quality. Finally, we present two case studies applying NL\noutlines toward code review and malware detection.\n","authors":["Kensen Shi","Deniz Altınbüken","Saswat Anand","Mihai Christodorescu","Katja Grünwedel","Alexa Koenings","Sai Naidu","Anurag Pathak","Marc Rasi","Fredde Ribeiro","Brandon Ruffin","Siddhant Sanyam","Maxim Tabachnyk","Sara Toth","Roy Tu","Tobias Welp","Pengcheng Yin","Manzil Zaheer","Satish Chandra","Charles Sutton"],"pdf_url":"https://arxiv.org/pdf/2408.04820v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11514v2","updated":"2025-01-15T03:20:24Z","published":"2024-06-17T13:21:23Z","title":"Counterfactual Debating with Preset Stances for Hallucination\n  Elimination of LLMs","summary":"  Large Language Models (LLMs) excel in various natural language processing\ntasks but struggle with hallucination issues. Existing solutions have\nconsidered utilizing LLMs' inherent reasoning abilities to alleviate\nhallucination, such as self-correction and diverse sampling methods. However,\nthese methods often overtrust LLMs' initial answers due to inherent biases. The\nkey to alleviating this issue lies in overriding LLMs' inherent biases for\nanswer inspection. To this end, we propose a CounterFactual Multi-Agent Debate\n(CFMAD) framework. CFMAD presets the stances of LLMs to override their inherent\nbiases by compelling LLMs to generate justifications for a predetermined\nanswer's correctness. The LLMs with different predetermined stances are engaged\nwith a skeptical critic for counterfactual debate on the rationality of\ngenerated justifications. Finally, the debate process is evaluated by a\nthird-party judge to determine the final answer. Extensive experiments on four\ndatasets of three tasks demonstrate the superiority of CFMAD over existing\nmethods.\n","authors":["Yi Fang","Moxin Li","Wenjie Wang","Hui Lin","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2406.11514v2.pdf","comment":"accepted by COLING 2025"},{"id":"http://arxiv.org/abs/2501.01028v4","updated":"2025-01-15T03:02:22Z","published":"2025-01-02T03:17:51Z","title":"KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model","summary":"  As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters.\n","authors":["Xinshuo Hu","Zifei Shan","Xinping Zhao","Zetian Sun","Zhenyu Liu","Dongfang Li","Shaolin Ye","Xinyuan Wei","Qian Chen","Baotian Hu","Haofen Wang","Jun Yu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.01028v4.pdf","comment":"Technical Report. 23 pages, 6 figures, 10 tables"},{"id":"http://arxiv.org/abs/2501.08540v1","updated":"2025-01-15T03:00:57Z","published":"2025-01-15T03:00:57Z","title":"Knowledge prompt chaining for semantic modeling","summary":"  The task of building semantics for structured data such as CSV, JSON, and XML\nfiles is highly relevant in the knowledge representation field. Even though we\nhave a vast of structured data on the internet, mapping them to domain\nontologies to build semantics for them is still very challenging as it requires\nthe construction model to understand and learn graph-structured knowledge.\nOtherwise, the task will require human beings' effort and cost. In this paper,\nwe proposed a novel automatic semantic modeling framework: Knowledge Prompt\nChaining. It can serialize the graph-structured knowledge and inject it into\nthe LLMs properly in a Prompt Chaining architecture. Through this knowledge\ninjection and prompting chaining, the model in our framework can learn the\nstructure information and latent space of the graph and generate the semantic\nlabels and semantic graphs following the chains' insturction naturally. Based\non experimental results, our method achieves better performance than existing\nleading techniques, despite using reduced structured input data.\n","authors":["Ning Pei Ding","Jingge Du","Zaiwen Feng"],"pdf_url":"https://arxiv.org/pdf/2501.08540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08187v2","updated":"2025-01-15T02:59:32Z","published":"2025-01-14T15:12:19Z","title":"A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction\n  Following","summary":"  Large language models excel at interpreting complex natural language\ninstructions, enabling them to perform a wide range of tasks. In the life\nsciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language\nof cellular biology\", capturing intricate gene expression patterns at the\nsingle-cell level. However, interacting with this \"language\" through\nconventional tools is often inefficient and unintuitive, posing challenges for\nresearchers. To address these limitations, we present InstructCell, a\nmulti-modal AI copilot that leverages natural language as a medium for more\ndirect and flexible single-cell analysis. We construct a comprehensive\nmulti-modal instruction dataset that pairs text-based instructions with\nscRNA-seq profiles from diverse tissues and species. Building on this, we\ndevelop a multi-modal cell language architecture capable of simultaneously\ninterpreting and processing both modalities. InstructCell empowers researchers\nto accomplish critical tasks-such as cell type annotation, conditional\npseudo-cell generation, and drug sensitivity prediction-using straightforward\nnatural language commands. Extensive evaluations demonstrate that InstructCell\nconsistently meets or exceeds the performance of existing single-cell\nfoundation models, while adapting to diverse experimental conditions. More\nimportantly, InstructCell provides an accessible and intuitive tool for\nexploring complex single-cell data, lowering technical barriers and enabling\ndeeper biological insights.\n","authors":["Yin Fang","Xinle Deng","Kangwei Liu","Ningyu Zhang","Jingyang Qian","Penghui Yang","Xiaohui Fan","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2501.08187v2.pdf","comment":"37 pages; 13 figures; Code: https://github.com/zjunlp/Instructcell,\n  Models: https://huggingface.co/zjunlp/Instructcell-chat,\n  https://huggingface.co/zjunlp/InstructCell-instruct"},{"id":"http://arxiv.org/abs/2501.08537v1","updated":"2025-01-15T02:54:52Z","published":"2025-01-15T02:54:52Z","title":"Complexity Control Facilitates Reasoning-Based Compositional\n  Generalization in Transformers","summary":"  Transformers have demonstrated impressive capabilities across various tasks,\nyet their performance on compositional problems remains a subject of debate. In\nthis study, we investigate the internal mechanisms underlying Transformers'\nbehavior in compositional tasks. We find that complexity control strategies\nsignificantly influence whether the model learns primitive-level rules that\ngeneralize out-of-distribution (reasoning-based solutions) or relies solely on\nmemorized mappings (memory-based solutions). By applying masking strategies to\nthe model's information circuits and employing multiple complexity metrics, we\nreveal distinct internal working mechanisms associated with different solution\ntypes. Further analysis reveals that reasoning-based solutions exhibit a lower\ncomplexity bias, which aligns with the well-studied neuron condensation\nphenomenon. This lower complexity bias is hypothesized to be the key factor\nenabling these solutions to learn reasoning rules. We validate these\nconclusions across multiple real-world datasets, including image generation and\nnatural language processing tasks, confirming the broad applicability of our\nfindings.\n","authors":["Zhongwang Zhang","Pengxiao Lin","Zhiwei Wang","Yaoyu Zhang","Zhi-Qin John Xu"],"pdf_url":"https://arxiv.org/pdf/2501.08537v1.pdf","comment":"Mistakenly submitted as a replacement to 2405.05409v4"},{"id":"http://arxiv.org/abs/2403.15796v3","updated":"2025-01-15T02:48:59Z","published":"2024-03-23T11:03:31Z","title":"Understanding Emergent Abilities of Language Models from the Loss\n  Perspective","summary":"  Recent studies have put into question the belief that emergent abilities in\nlanguage models are exclusive to large models. This skepticism arises from two\nobservations: 1) smaller models can also exhibit high performance on emergent\nabilities and 2) there is doubt on the discontinuous metrics used to measure\nthese abilities. In this paper, we propose to study emergent abilities in the\nlens of pre-training loss, instead of model size or training compute. We\ndemonstrate that the Transformer models with the same pre-training loss, but\ndifferent model and data sizes, generate the same performance on various\ndownstream tasks, with a fixed data corpus, tokenization, and model\narchitecture. We also discover that a model exhibits emergent abilities on\ncertain tasks -- regardless of the continuity of metrics -- when its\npre-training loss falls below a specific threshold. Before reaching this\nthreshold, its performance remains at the level of random guessing. This\ninspires us to redefine emergent abilities as those that manifest in models\nwith lower pre-training losses, highlighting that these abilities cannot be\npredicted by merely extrapolating the performance trends of models with higher\npre-training losses.\n","authors":["Zhengxiao Du","Aohan Zeng","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2403.15796v3.pdf","comment":"23 pages, 8 figures. Accepted in NeurIPS 2024"},{"id":"http://arxiv.org/abs/2501.08523v1","updated":"2025-01-15T02:25:35Z","published":"2025-01-15T02:25:35Z","title":"Doc-Guided Sent2Sent++: A Sent2Sent++ Agent with Doc-Guided memory for\n  Document-level Machine Translation","summary":"  The field of artificial intelligence has witnessed significant advancements\nin natural language processing, largely attributed to the capabilities of Large\nLanguage Models (LLMs). These models form the backbone of Agents designed to\naddress long-context dependencies, particularly in Document-level Machine\nTranslation (DocMT). DocMT presents unique challenges, with quality,\nconsistency, and fluency being the key metrics for evaluation. Existing\napproaches, such as Doc2Doc and Doc2Sent, either omit sentences or compromise\nfluency. This paper introduces Doc-Guided Sent2Sent++, an Agent that employs an\nincremental sentence-level forced decoding strategy \\textbf{to ensure every\nsentence is translated while enhancing the fluency of adjacent sentences.} Our\nAgent leverages a Doc-Guided Memory, focusing solely on the summary and its\ntranslation, which we find to be an efficient approach to maintaining\nconsistency. Through extensive testing across multiple languages and domains,\nwe demonstrate that Sent2Sent++ outperforms other methods in terms of quality,\nconsistency, and fluency. The results indicate that, our approach has achieved\nsignificant improvements in metrics such as s-COMET, d-COMET, LTCR-$1_f$, and\ndocument-level perplexity (d-ppl). The contributions of this paper include a\ndetailed analysis of current DocMT research, the introduction of the\nSent2Sent++ decoding method, the Doc-Guided Memory mechanism, and validation of\nits effectiveness across languages and domains.\n","authors":["Jiaxin Guo","Yuanchang Luo","Daimeng Wei","Ling Zhang","Zongyao Li","Hengchao Shang","Zhiqiang Rao","Shaojun Li","Jinlong Yang","Zhanglin Wu","Hao Yang"],"pdf_url":"https://arxiv.org/pdf/2501.08523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03363v2","updated":"2025-01-15T01:51:55Z","published":"2024-09-05T09:10:38Z","title":"Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding","summary":"  The training data in large language models is key to their success, but it\nalso presents privacy and security risks, as it may contain sensitive\ninformation. Detecting pre-training data is crucial for mitigating these\nconcerns. Existing methods typically analyze target text in isolation or solely\nwith non-member contexts, overlooking potential insights from simultaneously\nconsidering both member and non-member contexts. While previous work suggested\nthat member contexts provide little information due to the minor distributional\nshift they induce, our analysis reveals that these subtle shifts can be\neffectively leveraged when contrasted with non-member contexts. In this paper,\nwe propose Con-ReCall, a novel approach that leverages the asymmetric\ndistributional shifts induced by member and non-member contexts through\ncontrastive decoding, amplifying subtle differences to enhance membership\ninference. Extensive empirical evaluations demonstrate that Con-ReCall achieves\nstate-of-the-art performance on the WikiMIA benchmark and is robust against\nvarious text manipulation techniques.\n","authors":["Cheng Wang","Yiwei Wang","Bryan Hooi","Yujun Cai","Nanyun Peng","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2409.03363v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00205v2","updated":"2025-01-15T01:46:25Z","published":"2024-10-31T20:56:07Z","title":"Compositional Automata Embeddings for Goal-Conditioned Reinforcement\n  Learning","summary":"  Goal-conditioned reinforcement learning is a powerful way to control an AI\nagent's behavior at runtime. That said, popular goal representations, e.g.,\ntarget states or natural language, are either limited to Markovian tasks or\nrely on ambiguous task semantics. We propose representing temporal goals using\ncompositions of deterministic finite automata (cDFAs) and use cDFAs to guide RL\nagents. cDFAs balance the need for formal temporal semantics with ease of\ninterpretation: if one can understand a flow chart, one can understand a cDFA.\nOn the other hand, cDFAs form a countably infinite concept class with Boolean\nsemantics, and subtle changes to the automaton can result in very different\ntasks, making them difficult to condition agent behavior on. To address this,\nwe observe that all paths through a DFA correspond to a series of reach-avoid\ntasks and propose pre-training graph neural network embeddings on \"reach-avoid\nderived\" DFAs. Through empirical evaluation, we demonstrate that the proposed\npre-training method enables zero-shot generalization to various cDFA task\nclasses and accelerated policy specialization without the myopic suboptimality\nof hierarchical methods.\n","authors":["Beyazit Yalcinkaya","Niklas Lauffer","Marcell Vazquez-Chanlatte","Sanjit A. Seshia"],"pdf_url":"https://arxiv.org/pdf/2411.00205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00656v2","updated":"2025-01-15T01:44:16Z","published":"2024-12-31T21:55:10Z","title":"2 OLMo 2 Furious","summary":"  We present OLMo 2, the next generation of our fully open language models.\nOLMo 2 includes dense autoregressive models with improved architecture and\ntraining recipe, pretraining data mixtures, and instruction tuning recipes. Our\nmodified model architecture and training recipe achieve both better training\nstability and improved per-token efficiency. Our updated pretraining data\nmixture introduces a new, specialized data mix called Dolmino Mix 1124, which\nsignificantly improves model capabilities across many downstream task\nbenchmarks when introduced via late-stage curriculum training (i.e. specialized\ndata during the annealing phase of pretraining). Finally, we incorporate best\npractices from T\\\"ulu 3 to develop OLMo 2-Instruct, focusing on permissive data\nand extending our final-stage reinforcement learning with verifiable rewards\n(RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to\ncompute, often matching or outperforming open-weight only models like Llama 3.1\nand Qwen 2.5 while using fewer FLOPs and with fully transparent training data,\ncode, and recipe. Our fully open OLMo 2-Instruct models are competitive with or\nsurpassing open-weight only models of comparable size, including Qwen 2.5,\nLlama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B\nand 13B scales, both pretrained and post-trained, including their full training\ndata, training code and recipes, training logs and thousands of intermediate\ncheckpoints. The final instruction model is available on the Ai2 Playground as\na free research demo.\n","authors":["Team OLMo","Pete Walsh","Luca Soldaini","Dirk Groeneveld","Kyle Lo","Shane Arora","Akshita Bhagia","Yuling Gu","Shengyi Huang","Matt Jordan","Nathan Lambert","Dustin Schwenk","Oyvind Tafjord","Taira Anderson","David Atkinson","Faeze Brahman","Christopher Clark","Pradeep Dasigi","Nouha Dziri","Michal Guerquin","Hamish Ivison","Pang Wei Koh","Jiacheng Liu","Saumya Malik","William Merrill","Lester James V. Miranda","Jacob Morrison","Tyler Murray","Crystal Nam","Valentina Pyatkin","Aman Rangapur","Michael Schmitz","Sam Skjonsberg","David Wadden","Christopher Wilhelm","Michael Wilson","Luke Zettlemoyer","Ali Farhadi","Noah A. Smith","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2501.00656v2.pdf","comment":"Model demo available at playground.allenai.org"},{"id":"http://arxiv.org/abs/2501.08502v1","updated":"2025-01-15T00:39:21Z","published":"2025-01-15T00:39:21Z","title":"Adapting Whisper for Regional Dialects: Enhancing Public Services for\n  Vulnerable Populations in the United Kingdom","summary":"  We collect novel data in the public service domain to evaluate the capability\nof the state-of-the-art automatic speech recognition (ASR) models in capturing\nregional differences in accents in the United Kingdom (UK), specifically\nfocusing on two accents from Scotland with distinct dialects. This study\naddresses real-world problems where biased ASR models can lead to\nmiscommunication in public services, disadvantaging individuals with regional\naccents particularly those in vulnerable populations. We first examine the\nout-of-the-box performance of the Whisper large-v3 model on a baseline dataset\nand our data. We then explore the impact of fine-tuning Whisper on the\nperformance in the two UK regions and investigate the effectiveness of existing\nmodel evaluation techniques for our real-world application through manual\ninspection of model errors. We observe that the Whisper model has a higher word\nerror rate (WER) on our test datasets compared to the baseline data and\nfine-tuning on a given data improves performance on the test dataset with the\nsame domain and accent. The fine-tuned models also appear to show improved\nperformance when applied to the test data outside of the region it was trained\non suggesting that fine-tuned models may be transferable within parts of the\nUK. Our manual analysis of model outputs reveals the benefits and drawbacks of\nusing WER as an evaluation metric and fine-tuning to adapt to regional\ndialects.\n","authors":["Melissa Torgbi","Andrew Clayman","Jordan J. Speight","Harish Tayyar Madabushi"],"pdf_url":"https://arxiv.org/pdf/2501.08502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20906v4","updated":"2025-01-15T00:10:57Z","published":"2024-07-30T15:26:36Z","title":"Automated Review Generation Method Based on Large Language Models","summary":"  Literature research, vital for scientific work, faces the challenge of\nsurging information volumes exceeding researchers' processing capabilities. We\npresent an automated review generation method based on large language models\n(LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our\nstatistically validated evaluation framework demonstrates that the generated\nreviews match or exceed manual quality, offering broad applicability across\nresearch fields without requiring users' domain knowledge. Applied to propane\ndehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles,\naveraging seconds per article per LLM account, producing comprehensive reviews\nspanning 35 topics, with extended analysis of 1041 articles providing insights\ninto catalysts' properties. Through multi-layered quality control, we\neffectively mitigated LLMs' hallucinations, with expert verification confirming\naccuracy and citation integrity while demonstrating hallucination risks reduced\nto below 0.5\\% with 95\\% confidence. Released Windows application enables\none-click review generation, enhancing research productivity and literature\nrecommendation efficiency while setting the stage for broader scientific\nexplorations.\n","authors":["Shican Wu","Xiao Ma","Dehui Luo","Lulu Li","Xiangcheng Shi","Xin Chang","Xiaoyun Lin","Ran Luo","Chunlei Pei","Changying Du","Zhi-Jian Zhao","Jinlong Gong"],"pdf_url":"https://arxiv.org/pdf/2407.20906v4.pdf","comment":"21 pages, 5 figures, 1 tables Code:\n  https://github.com/TJU-ECAT-AI/AutomaticReviewGeneration Data:\n  https://github.com/TJU-ECAT-AI/AutomaticReviewGenerationData This research\n  has been invited for a Short Oral presentation at the 18th ICC -\n  International Congress on Catalysis, taking place in Lyon, France from July\n  14-19, 2024"},{"id":"http://arxiv.org/abs/2406.17967v3","updated":"2025-01-15T22:20:15Z","published":"2024-06-25T22:49:17Z","title":"Unmasking the Imposters: How Censorship and Domain Adaptation Affect the\n  Detection of Machine-Generated Tweets","summary":"  The rapid development of large language models (LLMs) has significantly\nimproved the generation of fluent and convincing text, raising concerns about\ntheir potential misuse on social media platforms. We present a comprehensive\nmethodology for creating nine Twitter datasets to examine the generative\ncapabilities of four prominent LLMs: Llama 3, Mistral, Qwen2, and GPT4o. These\ndatasets encompass four censored and five uncensored model configurations,\nincluding 7B and 8B parameter base-instruction models of the three open-source\nLLMs. Additionally, we perform a data quality analysis to assess the\ncharacteristics of textual outputs from human, \"censored,\" and \"uncensored\"\nmodels, employing semantic meaning, lexical richness, structural patterns,\ncontent characteristics, and detector performance metrics to identify\ndifferences and similarities. Our evaluation demonstrates that \"uncensored\"\nmodels significantly undermine the effectiveness of automated detection\nmethods. This study addresses a critical gap by exploring smaller open-source\nmodels and the ramifications of \"uncensoring,\" providing valuable insights into\nhow domain adaptation and content moderation strategies influence both the\ndetectability and structural characteristics of machine-generated text.\n","authors":["Bryan E. Tuck","Rakesh M. Verma"],"pdf_url":"https://arxiv.org/pdf/2406.17967v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09164v1","updated":"2025-01-15T21:30:03Z","published":"2025-01-15T21:30:03Z","title":"The Veln(ia)s is in the Details: Evaluating LLM Judgment on Latvian and\n  Lithuanian Short Answer Matching","summary":"  In this work, we address the challenge of evaluating large language models\n(LLMs) on the short answer matching task for Latvian and Lithuanian languages.\nWe introduce novel datasets consisting of 502 Latvian and 690 Lithuanian\nquestion-answer pairs. For each question-answer pair, we generated matched and\nnon-matched answers using a set of alteration rules specifically designed to\nintroduce small but meaningful changes in the text. These generated answers\nserve as test cases to assess the ability of LLMs to detect subtle differences\nin matching of the original answers. A subset of the datasets was manually\nverified for quality and accuracy. Our results show that while larger LLMs,\nsuch as QWEN2.5 72b and LLaMa3.1 70b, demonstrate near-perfect performance in\ndistinguishing matched and non-matched answers, smaller models show more\nvariance. For instance, LLaMa3.1 8b and EuroLLM 9b benefited from few-shot\nexamples, while Mistral Nemo 12b underperformed on detection of subtle text\nalteration, particularly in Lithuanian, even with additional examples. QWEN2.5\n7b and Mistral 7b were able to obtain a strong and comparable performance to\nthe larger 70b models in zero and few shot experiments. Moreover, the\nperformance of Mistral 7b was weaker in few shot experiments.\n","authors":["Yevhen Kostiuk","Oxana Vitman","Łukasz Gagała","Artur Kiulian"],"pdf_url":"https://arxiv.org/pdf/2501.09164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09158v1","updated":"2025-01-15T21:19:01Z","published":"2025-01-15T21:19:01Z","title":"Evaluating GenAI for Simplifying Texts for Education: Improving Accuracy\n  and Consistency for Enhanced Readability","summary":"  Generative artificial intelligence (GenAI) holds great promise as a tool to\nsupport personalized learning. Teachers need tools to efficiently and\neffectively enhance content readability of educational texts so that they are\nmatched to individual students reading levels, while retaining key details.\nLarge Language Models (LLMs) show potential to fill this need, but previous\nresearch notes multiple shortcomings in current approaches. In this study, we\nintroduced a generalized approach and metrics for the systematic evaluation of\nthe accuracy and consistency in which LLMs, prompting techniques, and a novel\nmulti-agent architecture to simplify sixty informational reading passages,\nreducing each from the twelfth grade level down to the eighth, sixth, and\nfourth grade levels. We calculated the degree to which each LLM and prompting\ntechnique accurately achieved the targeted grade level for each passage,\npercentage change in word count, and consistency in maintaining keywords and\nkey phrases (semantic similarity). One-sample t-tests and multiple regression\nmodels revealed significant differences in the best performing LLM and prompt\ntechnique for each of the four metrics. Both LLMs and prompting techniques\ndemonstrated variable utility in grade level accuracy and consistency of\nkeywords and key phrases when attempting to level content down to the fourth\ngrade reading level. These results demonstrate the promise of the application\nof LLMs for efficient and precise automated text simplification, the\nshortcomings of current models and prompting methods in attaining an ideal\nbalance across various evaluation criteria, and a generalizable method to\nevaluate future systems.\n","authors":["Stephanie L. Day","Jacapo Cirica","Steven R. Clapp","Veronika Penkova","Amy E. Giroux","Abbey Banta","Catherine Bordeau","Poojitha Mutteneni","Ben D. Sawyer"],"pdf_url":"https://arxiv.org/pdf/2501.09158v1.pdf","comment":"64 pages, 9 tables, 6 figures, and supplemental materials"},{"id":"http://arxiv.org/abs/2501.09155v1","updated":"2025-01-15T21:14:36Z","published":"2025-01-15T21:14:36Z","title":"VCRScore: Image captioning metric based on V\\&L Transformers, CLIP, and\n  precision-recall","summary":"  Image captioning has become an essential Vision & Language research task. It\nis about predicting the most accurate caption given a specific image or video.\nThe research community has achieved impressive results by continuously\nproposing new models and approaches to improve the overall model's performance.\nNevertheless, despite increasing proposals, the performance metrics used to\nmeasure their advances have remained practically untouched through the years. A\nprobe of that, nowadays metrics like BLEU, METEOR, CIDEr, and ROUGE are still\nvery used, aside from more sophisticated metrics such as BertScore and\nClipScore.\n  Hence, it is essential to adjust how are measure the advances, limitations,\nand scopes of the new image captioning proposals, as well as to adapt new\nmetrics to these new advanced image captioning approaches.\n  This work proposes a new evaluation metric for the image captioning problem.\nTo do that, first, it was generated a human-labeled dataset to assess to which\ndegree the captions correlate with the image's content. Taking these human\nscores as ground truth, we propose a new metric, and compare it with several\nwell-known metrics, from classical to newer ones. Outperformed results were\nalso found, and interesting insights were presented and discussed.\n","authors":["Guillermo Ruiz","Tania Ramírez","Daniela Moctezuma"],"pdf_url":"https://arxiv.org/pdf/2501.09155v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2501.09154v1","updated":"2025-01-15T21:14:09Z","published":"2025-01-15T21:14:09Z","title":"Towards Multilingual LLM Evaluation for Baltic and Nordic languages: A\n  study on Lithuanian History","summary":"  In this work, we evaluated Lithuanian and general history knowledge of\nmultilingual Large Language Models (LLMs) on a multiple-choice\nquestion-answering task. The models were tested on a dataset of Lithuanian\nnational and general history questions translated into Baltic, Nordic, and\nother languages (English, Ukrainian, Arabic) to assess the knowledge sharing\nfrom culturally and historically connected groups. We evaluated GPT-4o,\nLLaMa3.1 8b and 70b, QWEN2.5 7b and 72b, Mistral Nemo 12b, LLaMa3 8b, Mistral\n7b, LLaMa3.2 3b, and Nordic fine-tuned models (GPT-SW3 and LLaMa3 8b).\n  Our results show that GPT-4o consistently outperformed all other models\nacross language groups, with slightly better results for Baltic and Nordic\nlanguages. Larger open-source models like QWEN2.5 72b and LLaMa3.1 70b\nperformed well but showed weaker alignment with Baltic languages. Smaller\nmodels (Mistral Nemo 12b, LLaMa3.2 3b, QWEN 7B, LLaMa3.1 8B, and LLaMa3 8b)\ndemonstrated gaps with LT-related alignment with Baltic languages while\nperforming better on Nordic and other languages. The Nordic fine-tuned models\ndid not surpass multilingual models, indicating that shared cultural or\nhistorical context alone does not guarantee better performance.\n","authors":["Yevhen Kostiuk","Oxana Vitman","Łukasz Gagała","Artur Kiulian"],"pdf_url":"https://arxiv.org/pdf/2501.09154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06497v2","updated":"2025-01-15T20:43:44Z","published":"2025-01-11T10:22:04Z","title":"PASS: Presentation Automation for Slide Generation and Speech","summary":"  In today's fast-paced world, effective presentations have become an essential\ntool for communication in both online and offline meetings. The crafting of a\ncompelling presentation requires significant time and effort, from gathering\nkey insights to designing slides that convey information clearly and concisely.\nHowever, despite the wealth of resources available, people often find\nthemselves manually extracting crucial points, analyzing data, and organizing\ncontent in a way that ensures clarity and impact. Furthermore, a successful\npresentation goes beyond just the slides; it demands rehearsal and the ability\nto weave a captivating narrative to fully engage the audience. Although there\nhas been some exploration of automating document-to-slide generation, existing\nresearch is largely centered on converting research papers. In addition,\nautomation of the delivery of these presentations has yet to be addressed. We\nintroduce PASS, a pipeline used to generate slides from general Word documents,\ngoing beyond just research papers, which also automates the oral delivery of\nthe generated slides. PASS analyzes user documents to create a dynamic,\nengaging presentation with an AI-generated voice. Additionally, we developed an\nLLM-based evaluation metric to assess our pipeline across three critical\ndimensions of presentations: relevance, coherence, and redundancy. The data and\ncodes are available at https://github.com/AggarwalTushar/PASS.\n","authors":["Tushar Aggarwal","Aarohi Bhand"],"pdf_url":"https://arxiv.org/pdf/2501.06497v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09136v1","updated":"2025-01-15T20:40:25Z","published":"2025-01-15T20:40:25Z","title":"Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG","summary":"  Large Language Models (LLMs) have revolutionized artificial intelligence (AI)\nby enabling human like text generation and natural language understanding.\nHowever, their reliance on static training data limits their ability to respond\nto dynamic, real time queries, resulting in outdated or inaccurate outputs.\nRetrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs\nby integrating real time data retrieval to provide contextually relevant and\nup-to-date responses. Despite its promise, traditional RAG systems are\nconstrained by static workflows and lack the adaptability required for\nmultistep reasoning and complex task management.\n  Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these\nlimitations by embedding autonomous AI agents into the RAG pipeline. These\nagents leverage agentic design patterns reflection, planning, tool use, and\nmultiagent collaboration to dynamically manage retrieval strategies,\niteratively refine contextual understanding, and adapt workflows to meet\ncomplex task requirements. This integration enables Agentic RAG systems to\ndeliver unparalleled flexibility, scalability, and context awareness across\ndiverse applications.\n  This survey provides a comprehensive exploration of Agentic RAG, beginning\nwith its foundational principles and the evolution of RAG paradigms. It\npresents a detailed taxonomy of Agentic RAG architectures, highlights key\napplications in industries such as healthcare, finance, and education, and\nexamines practical implementation strategies. Additionally, it addresses\nchallenges in scaling these systems, ensuring ethical decision making, and\noptimizing performance for real-world applications, while providing detailed\ninsights into frameworks and tools for implementing Agentic RAG\n","authors":["Aditi Singh","Abul Ehtesham","Saket Kumar","Tala Talaei Khoei"],"pdf_url":"https://arxiv.org/pdf/2501.09136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09134v1","updated":"2025-01-15T20:37:04Z","published":"2025-01-15T20:37:04Z","title":"Benchmarking Robustness of Contrastive Learning Models for Medical\n  Image-Report Retrieval","summary":"  Medical images and reports offer invaluable insights into patient health. The\nheterogeneity and complexity of these data hinder effective analysis. To bridge\nthis gap, we investigate contrastive learning models for cross-domain\nretrieval, which associates medical images with their corresponding clinical\nreports. This study benchmarks the robustness of four state-of-the-art\ncontrastive learning models: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. We\nintroduce an occlusion retrieval task to evaluate model performance under\nvarying levels of image corruption. Our findings reveal that all evaluated\nmodels are highly sensitive to out-of-distribution data, as evidenced by the\nproportional decrease in performance with increasing occlusion levels. While\nMedCLIP exhibits slightly more robustness, its overall performance remains\nsignificantly behind CXR-CLIP and CXR-RePaiR. CLIP, trained on a\ngeneral-purpose dataset, struggles with medical image-report retrieval,\nhighlighting the importance of domain-specific training data. The evaluation of\nthis work suggests that more effort needs to be spent on improving the\nrobustness of these models. By addressing these limitations, we can develop\nmore reliable cross-domain retrieval models for medical applications.\n","authors":["Demetrio Deanda","Yuktha Priya Masupalli","Jeong Yang","Young Lee","Zechun Cao","Gongbo Liang"],"pdf_url":"https://arxiv.org/pdf/2501.09134v1.pdf","comment":"This work is accepted to AAAI 2025 Workshop -- the 9th International\n  Workshop on Health Intelligence"},{"id":"http://arxiv.org/abs/2501.09127v1","updated":"2025-01-15T20:22:35Z","published":"2025-01-15T20:22:35Z","title":"Multilingual LLMs Struggle to Link Orthography and Semantics in\n  Bilingual Word Processing","summary":"  Bilingual lexical processing is shaped by the complex interplay of\nphonological, orthographic, and semantic features of two languages within an\nintegrated mental lexicon. In humans, this is evident in the ease with which\ncognate words - words similar in both orthographic form and meaning (e.g.,\nblind, meaning \"sightless\" in both English and German) - are processed,\ncompared to the challenges posed by interlingual homographs, which share\northographic form but differ in meaning (e.g., gift, meaning \"present\" in\nEnglish but \"poison\" in German). We investigate how multilingual Large Language\nModels (LLMs) handle such phenomena, focusing on English-Spanish,\nEnglish-French, and English-German cognates, non-cognate, and interlingual\nhomographs. Specifically, we evaluate their ability to disambiguate meanings\nand make semantic judgments, both when these word types are presented in\nisolation or within sentence contexts. Our findings reveal that while certain\nLLMs demonstrate strong performance in recognizing cognates and non-cognates in\nisolation, they exhibit significant difficulty in disambiguating interlingual\nhomographs, often performing below random baselines. This suggests LLMs tend to\nrely heavily on orthographic similarities rather than semantic understanding\nwhen interpreting interlingual homographs. Further, we find LLMs exhibit\ndifficulty in retrieving word meanings, with performance in isolative\ndisambiguation tasks having no correlation with semantic understanding.\nFinally, we study how the LLM processes interlingual homographs in incongruent\nsentences. We find models to opt for different strategies in understanding\nEnglish and non-English homographs, highlighting a lack of a unified approach\nto handling cross-lingual ambiguities.\n","authors":["Eshaan Tanwar","Gayatri Oke","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2501.09127v1.pdf","comment":"Code available at:\n  https://github.com/EshaanT/Bilingual_processing_LLMs"},{"id":"http://arxiv.org/abs/2501.09126v1","updated":"2025-01-15T20:13:46Z","published":"2025-01-15T20:13:46Z","title":"Augmenting Human-Annotated Training Data with Large Language Model\n  Generation and Distillation in Open-Response Assessment","summary":"  Large Language Models (LLMs) like GPT-4o can help automate text\nclassification tasks at low cost and scale. However, there are major concerns\nabout the validity and reliability of LLM outputs. By contrast, human coding is\ngenerally more reliable but expensive to procure at scale. In this study, we\npropose a hybrid solution to leverage the strengths of both. We combine\nhuman-coded data and synthetic LLM-produced data to fine-tune a classical\nmachine learning classifier, distilling both into a smaller BERT model. We\nevaluate our method on a human-coded test set as a validity measure for LLM\noutput quality. In three experiments, we systematically vary LLM-generated\nsamples' size, variety, and consistency, informed by best practices in LLM\ntuning. Our findings indicate that augmenting datasets with synthetic samples\nimproves classifier performance, with optimal results achieved at an 80%\nsynthetic to 20% human-coded data ratio. Lower temperature settings of 0.3,\ncorresponding to less variability in LLM generations, produced more stable\nimprovements but also limited model learning from augmented samples. In\ncontrast, higher temperature settings (0.7 and above) introduced greater\nvariability in performance estimates and, at times, lower performance. Hence,\nLLMs may produce more uniform output that classifiers overfit to earlier or\nproduce more diverse output that runs the risk of deteriorating model\nperformance through information irrelevant to the prediction task. Filtering\nout inconsistent synthetic samples did not enhance performance. We conclude\nthat integrating human and LLM-generated data to improve text classification\nmodels in assessment offers a scalable solution that leverages both the\naccuracy of human coding and the variety of LLM outputs.\n","authors":["Conrad Borchers","Danielle R. Thomas","Jionghao Lin","Ralph Abboud","Kenneth R. Koedinger"],"pdf_url":"https://arxiv.org/pdf/2501.09126v1.pdf","comment":"Manuscript accepted to the Second Workshop on Generative AI for\n  Learning Analytics (GenAI-LA) at LAK25"},{"id":"http://arxiv.org/abs/2501.09092v1","updated":"2025-01-15T19:24:48Z","published":"2025-01-15T19:24:48Z","title":"SteLLA: A Structured Grading System Using LLMs with RAG","summary":"  Large Language Models (LLMs) have shown strong general capabilities in many\napplications. However, how to make them reliable tools for some specific tasks\nsuch as automated short answer grading (ASAG) remains a challenge. We present\nSteLLA (Structured Grading System Using LLMs with RAG) in which a) Retrieval\nAugmented Generation (RAG) approach is used to empower LLMs specifically on the\nASAG task by extracting structured information from the highly relevant and\nreliable external knowledge based on the instructor-provided reference answer\nand rubric, b) an LLM performs a structured and question-answering-based\nevaluation of student answers to provide analytical grades and feedback. A\nreal-world dataset that contains students' answers in an exam was collected\nfrom a college-level Biology course. Experiments show that our proposed system\ncan achieve substantial agreement with the human grader while providing\nbreak-down grades and feedback on all the knowledge points examined in the\nproblem. A qualitative and error analysis of the feedback generated by GPT4\nshows that GPT4 is good at capturing facts while may be prone to inferring too\nmuch implication from the given text in the grading task which provides\ninsights into the usage of LLMs in the ASAG system.\n","authors":["Hefei Qiu","Brian White","Ashley Ding","Reinaldo Costa","Ali Hachem","Wei Ding","Ping Chen"],"pdf_url":"https://arxiv.org/pdf/2501.09092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09201v3","updated":"2025-01-15T18:52:52Z","published":"2024-09-13T21:28:54Z","title":"Contextual Evaluation of Large Language Models for Classifying Tropical\n  and Infectious Diseases","summary":"  While large language models (LLMs) have shown promise for medical question\nanswering, there is limited work focused on tropical and infectious\ndisease-specific exploration. We build on an opensource tropical and infectious\ndiseases (TRINDs) dataset, expanding it to include demographic and semantic\nclinical and consumer augmentations yielding 11000+ prompts. We evaluate LLM\nperformance on these, comparing generalist and medical LLMs, as well as LLM\noutcomes to human experts. We demonstrate through systematic experimentation,\nthe benefit of contextual information such as demographics, location, gender,\nrisk factors for optimal LLM response. Finally we develop a prototype of\nTRINDs-LM, a research tool that provides a playground to navigate how context\nimpacts LLM outputs for health.\n","authors":["Mercy Asiedu","Nenad Tomasev","Chintan Ghate","Tiya Tiyasirichokchai","Awa Dieng","Oluwatosin Akande","Geoffrey Siwo","Steve Adudans","Sylvanus Aitkins","Odianosen Ehiakhamen","Eric Ndombi","Katherine Heller"],"pdf_url":"https://arxiv.org/pdf/2409.09201v3.pdf","comment":"Accepted at 2 NeurIPS 2024 workshops: Generative AI for Health\n  Workshop and Workshop on Advancements In Medical Foundation Models:\n  Explainability, Robustness, Security, and Beyond"},{"id":"http://arxiv.org/abs/2501.09056v1","updated":"2025-01-15T18:44:01Z","published":"2025-01-15T18:44:01Z","title":"Decompose-ToM: Enhancing Theory of Mind Reasoning in Large Language\n  Models through Simulation and Task Decomposition","summary":"  Theory of Mind (ToM) is the ability to understand and reflect on the mental\nstates of others. Although this capability is crucial for human interaction,\ntesting on Large Language Models (LLMs) reveals that they possess only a\nrudimentary understanding of it. Although the most capable closed-source LLMs\nhave come close to human performance on some ToM tasks, they still perform\npoorly on complex variations of the task that involve more structured\nreasoning. In this work, we utilize the concept of \"pretend-play\", or\n``Simulation Theory'' from cognitive psychology to propose ``Decompose-ToM'':\nan LLM-based inference algorithm that improves model performance on complex ToM\ntasks. We recursively simulate user perspectives and decompose the ToM task\ninto a simpler set of functions: subject identification, question-reframing,\nworld model updation, and knowledge availability. We test the algorithm on\nhigher-order ToM tasks and a task testing for ToM capabilities in a\nconversational setting, demonstrating that our approach shows significant\nimprovement across models compared to baseline methods while requiring minimal\nprompt tuning across tasks and no additional model training.\n","authors":["Sneheel Sarangi","Maha Elgarf","Hanan Salam"],"pdf_url":"https://arxiv.org/pdf/2501.09056v1.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2501.09041v1","updated":"2025-01-15T04:00:36Z","published":"2025-01-15T04:00:36Z","title":"Generative Visual Commonsense Answering and Explaining with Generative\n  Scene Graph Constructing","summary":"  Visual Commonsense Reasoning, which is regarded as one challenging task to\npursue advanced visual scene comprehension, has been used to diagnose the\nreasoning ability of AI systems. However, reliable reasoning requires a good\ngrasp of the scene's details. Existing work fails to effectively exploit the\nreal-world object relationship information present within the scene, and\ninstead overly relies on knowledge from training memory. Based on these\nobservations, we propose a novel scene-graph-enhanced visual commonsense\nreasoning generation method named \\textit{\\textbf{G2}}, which first utilizes\nthe image patches and LLMs to construct a location-free scene graph, and then\nanswer and explain based on the scene graph's information. We also propose\nautomatic scene graph filtering and selection strategies to absorb valuable\nscene graph information during training. Extensive experiments are conducted on\nthe tasks and datasets of scene graph constructing and visual commonsense\nanswering and explaining, respectively. Experimental results and ablation\nanalysis demonstrate the effectiveness of our proposed framework.\n","authors":["Fan Yuan","Xiaoyuan Fang","Rong Quan","Jing Li","Wei Bi","Xiaogang Xu","Piji Li"],"pdf_url":"https://arxiv.org/pdf/2501.09041v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2501.09019v1","updated":"2025-01-15T18:59:15Z","published":"2025-01-15T18:59:15Z","title":"Ouroboros-Diffusion: Exploring Consistent Content Generation in\n  Tuning-free Long Video Diffusion","summary":"  The first-in-first-out (FIFO) video diffusion, built on a pre-trained\ntext-to-video model, has recently emerged as an effective approach for\ntuning-free long video generation. This technique maintains a queue of video\nframes with progressively increasing noise, continuously producing clean frames\nat the queue's head while Gaussian noise is enqueued at the tail. However,\nFIFO-Diffusion often struggles to keep long-range temporal consistency in the\ngenerated videos due to the lack of correspondence modeling across frames. In\nthis paper, we propose Ouroboros-Diffusion, a novel video denoising framework\ndesigned to enhance structural and content (subject) consistency, enabling the\ngeneration of consistent videos of arbitrary length. Specifically, we introduce\na new latent sampling technique at the queue tail to improve structural\nconsistency, ensuring perceptually smooth transitions among frames. To enhance\nsubject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA)\nmechanism, which aligns subjects across frames within short segments to achieve\nbetter visual coherence. Furthermore, we introduce self-recurrent guidance.\nThis technique leverages information from all previous cleaner frames at the\nfront of the queue to guide the denoising of noisier frames at the end,\nfostering rich and contextual global information interaction. Extensive\nexperiments of long video generation on the VBench benchmark demonstrate the\nsuperiority of our Ouroboros-Diffusion, particularly in terms of subject\nconsistency, motion smoothness, and temporal consistency.\n","authors":["Jingyuan Chen","Fuchen Long","Jie An","Zhaofan Qiu","Ting Yao","Jiebo Luo","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2501.09019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14505v2","updated":"2025-01-15T18:57:31Z","published":"2024-07-19T17:58:36Z","title":"T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video\n  Generation","summary":"  Text-to-video (T2V) generative models have advanced significantly, yet their\nability to compose different objects, attributes, actions, and motions into a\nvideo remains unexplored. Previous text-to-video benchmarks also neglect this\nimportant ability for evaluation. In this work, we conduct the first systematic\nstudy on compositional text-to-video generation. We propose T2V-CompBench, the\nfirst benchmark tailored for compositional text-to-video generation.\nT2V-CompBench encompasses diverse aspects of compositionality, including\nconsistent attribute binding, dynamic attribute binding, spatial relationships,\nmotion binding, action binding, object interactions, and generative numeracy.\nWe further carefully design evaluation metrics of multimodal large language\nmodel (MLLM)-based, detection-based, and tracking-based metrics, which can\nbetter reflect the compositional text-to-video generation quality of seven\nproposed categories with 1400 text prompts. The effectiveness of the proposed\nmetrics is verified by correlation with human evaluations. We also benchmark\nvarious text-to-video generative models and conduct in-depth analysis across\ndifferent models and various compositional categories. We find that\ncompositional text-to-video generation is highly challenging for current\nmodels, and we hope our attempt could shed light on future research in this\ndirection.\n","authors":["Kaiyue Sun","Kaiyi Huang","Xian Liu","Yue Wu","Zihan Xu","Zhenguo Li","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2407.14505v2.pdf","comment":"Project page: https://t2v-compbench-2025.github.io/ Code:\n  https://github.com/KaiyueSun98/T2V-CompBench/tree/V2"},{"id":"http://arxiv.org/abs/2501.09012v1","updated":"2025-01-15T18:56:22Z","published":"2025-01-15T18:56:22Z","title":"Multimodal LLMs Can Reason about Aesthetics in Zero-Shot","summary":"  We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability\nshall be elicited to evaluate the aesthetics of artworks. To facilitate this\ninvestigation, we construct MM-StyleBench, a novel high-quality dataset for\nbenchmarking artistic stylization. We then develop a principled method for\nhuman preference modeling and perform a systematic correlation analysis between\nMLLMs' responses and human preference. Our experiments reveal an inherent\nhallucination issue of MLLMs in art evaluation, associated with response\nsubjectivity. ArtCoT is proposed, demonstrating that art-specific task\ndecomposition and the use of concrete language boost MLLMs' reasoning ability\nfor aesthetics. Our findings offer valuable insights into MLLMs for art and can\nbenefit a wide range of downstream applications, such as style transfer and\nartistic image generation. Code available at\nhttps://github.com/songrise/MLLM4Art.\n","authors":["Ruixiang Jiang","Changwen Chen"],"pdf_url":"https://arxiv.org/pdf/2501.09012v1.pdf","comment":"WIP, Homepage https://github.com/songrise/MLLM4Art"},{"id":"http://arxiv.org/abs/2501.09008v1","updated":"2025-01-15T18:48:38Z","published":"2025-01-15T18:48:38Z","title":"SimGen: A Diffusion-Based Framework for Simultaneous Surgical Image and\n  Segmentation Mask Generation","summary":"  Acquiring and annotating surgical data is often resource-intensive, ethical\nconstraining, and requiring significant expert involvement. While generative AI\nmodels like text-to-image can alleviate data scarcity, incorporating spatial\nannotations, such as segmentation masks, is crucial for precision-driven\nsurgical applications, simulation, and education. This study introduces both a\nnovel task and method, SimGen, for Simultaneous Image and Mask Generation.\nSimGen is a diffusion model based on the DDPM framework and Residual U-Net,\ndesigned to jointly generate high-fidelity surgical images and their\ncorresponding segmentation masks. The model leverages cross-correlation priors\nto capture dependencies between continuous image and discrete mask\ndistributions. Additionally, a Canonical Fibonacci Lattice (CFL) is employed to\nenhance class separability and uniformity in the RGB space of the masks. SimGen\ndelivers high-fidelity images and accurate segmentation masks, outperforming\nbaselines across six public datasets assessed on image and semantic inception\ndistance metrics. Ablation study shows that the CFL improves mask quality and\nspatial separation. Downstream experiments suggest generated image-mask pairs\nare usable if regulations limit human data release for research. This work\noffers a cost-effective solution for generating paired surgical images and\ncomplex labels, advancing surgical AI development by reducing the need for\nexpensive manual annotations.\n","authors":["Aditya Bhat","Rupak Bose","Chinedu Innocent Nwoye","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2501.09008v1.pdf","comment":"12 pages, 17 figures, 4 tables, project page at\n  https://camma-public.github.io/endogen/"},{"id":"http://arxiv.org/abs/2403.13163v5","updated":"2025-01-15T18:45:15Z","published":"2024-03-19T21:31:31Z","title":"DeblurDiNAT: A Compact Model with Exceptional Generalization and Visual\n  Fidelity on Unseen Domains","summary":"  Recent deblurring networks have effectively restored clear images from the\nblurred ones. However, they often struggle with generalization to unknown\ndomains. Moreover, these models typically focus on distortion metrics such as\nPSNR and SSIM, neglecting the critical aspect of metrics aligned with human\nperception. To address these limitations, we propose DeblurDiNAT, a deblurring\nTransformer based on Dilated Neighborhood Attention. First, DeblurDiNAT employs\nan alternating dilation factor paradigm to capture both local and global\nblurred patterns, enhancing generalization and perceptual clarity. Second, a\nlocal cross-channel learner aids the Transformer block to understand the\nshort-range relationships between adjacent channels. Additionally, we present a\nlinear feed-forward network with a simple while effective design. Finally, a\ndual-stage feature fusion module is introduced as an alternative to the\nexisting approach, which efficiently process multi-scale visual information\nacross network levels. Compared to state-of-the-art models, our compact\nDeblurDiNAT demonstrates superior generalization capabilities and achieves\nremarkable performance in perceptual metrics, while maintaining a favorable\nmodel size.\n","authors":["Hanzhou Liu","Binghan Li","Chengkai Liu","Mi Lu"],"pdf_url":"https://arxiv.org/pdf/2403.13163v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09001v1","updated":"2025-01-15T18:30:58Z","published":"2025-01-15T18:30:58Z","title":"Vision Foundation Models for Computed Tomography","summary":"  Foundation models (FMs) have shown transformative potential in radiology by\nperforming diverse, complex tasks across imaging modalities. Here, we developed\nCT-FM, a large-scale 3D image-based pre-trained model designed explicitly for\nvarious radiological tasks. CT-FM was pre-trained using 148,000 computed\ntomography (CT) scans from the Imaging Data Commons through label-agnostic\ncontrastive learning. We evaluated CT-FM across four categories of tasks,\nnamely, whole-body and tumor segmentation, head CT triage, medical image\nretrieval, and semantic understanding, showing superior performance against\nstate-of-the-art models. Beyond quantitative success, CT-FM demonstrated the\nability to cluster regions anatomically and identify similar anatomical and\nstructural concepts across scans. Furthermore, it remained robust across\ntest-retest settings and indicated reasonable salient regions attached to its\nembeddings. This study demonstrates the value of large-scale medical imaging\nfoundation models and by open-sourcing the model weights, code, and data, aims\nto support more adaptable, reliable, and interpretable AI solutions in\nradiology.\n","authors":["Suraj Pai","Ibrahim Hadzic","Dennis Bontempi","Keno Bressem","Benjamin H. Kann","Andriy Fedorov","Raymond H. Mak","Hugo J. W. L. Aerts"],"pdf_url":"https://arxiv.org/pdf/2501.09001v1.pdf","comment":"6 figures, followed by 9 Extended Data Figures and a Supplementary\n  Information document"},{"id":"http://arxiv.org/abs/2501.01557v2","updated":"2025-01-15T18:29:56Z","published":"2025-01-02T22:24:13Z","title":"Click-Calib: A Robust Extrinsic Calibration Method for Surround-View\n  Systems","summary":"  Surround-View System (SVS) is an essential component in Advanced Driver\nAssistance System (ADAS) and requires precise calibrations. However,\nconventional offline extrinsic calibration methods are cumbersome and\ntime-consuming as they rely heavily on physical patterns. Additionally, these\nmethods primarily focus on short-range areas surrounding the vehicle, resulting\nin lower calibration quality in more distant zones. To address these\nlimitations, we propose Click-Calib, a pattern-free approach for offline SVS\nextrinsic calibration. Without requiring any special setup, the user only needs\nto click a few keypoints on the ground in natural scenes. Unlike other offline\ncalibration approaches, Click-Calib optimizes camera poses over a wide range by\nminimizing reprojection distance errors of keypoints, thereby achieving\naccurate calibrations at both short and long distances. Furthermore,\nClick-Calib supports both single-frame and multiple-frame modes, with the\nlatter offering even better results. Evaluations on our in-house dataset and\nthe public WoodScape dataset demonstrate its superior accuracy and robustness\ncompared to baseline methods. Code is available at\nhttps://github.com/lwangvaleo/click_calib.\n","authors":["Lihao Wang"],"pdf_url":"https://arxiv.org/pdf/2501.01557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06848v2","updated":"2025-01-15T18:28:37Z","published":"2025-01-12T15:34:24Z","title":"A General Framework for Inference-time Scaling and Steering of Diffusion\n  Models","summary":"  Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we propose Feynman Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models, even\nwith off-the-shelf rewards, can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering .\n","authors":["Raghav Singhal","Zachary Horvitz","Ryan Teehan","Mengye Ren","Zhou Yu","Kathleen McKeown","Rajesh Ranganath"],"pdf_url":"https://arxiv.org/pdf/2501.06848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08994v1","updated":"2025-01-15T18:20:37Z","published":"2025-01-15T18:20:37Z","title":"RepVideo: Rethinking Cross-Layer Representation for Video Generation","summary":"  Video generation has achieved remarkable progress with the introduction of\ndiffusion models, which have significantly improved the quality of generated\nvideos. However, recent research has primarily focused on scaling up model\ntraining, while offering limited insights into the direct impact of\nrepresentations on the video generation process. In this paper, we initially\ninvestigate the characteristics of features in intermediate layers, finding\nsubstantial variations in attention maps across different layers. These\nvariations lead to unstable semantic representations and contribute to\ncumulative differences between features, which ultimately reduce the similarity\nbetween adjacent frames and negatively affect temporal coherence. To address\nthis, we propose RepVideo, an enhanced representation framework for\ntext-to-video diffusion models. By accumulating features from neighboring\nlayers to form enriched representations, this approach captures more stable\nsemantic information. These enhanced representations are then used as inputs to\nthe attention mechanism, thereby improving semantic expressiveness while\nensuring feature consistency across adjacent frames. Extensive experiments\ndemonstrate that our RepVideo not only significantly enhances the ability to\ngenerate accurate spatial appearances, such as capturing complex spatial\nrelationships between multiple objects, but also improves temporal consistency\nin video generation.\n","authors":["Chenyang Si","Weichen Fan","Zhengyao Lv","Ziqi Huang","Yu Qiao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08994v1.pdf","comment":"Project page: https://vchitect.github.io/RepVid-Webpage"},{"id":"http://arxiv.org/abs/2409.01998v2","updated":"2025-01-15T18:07:13Z","published":"2024-09-03T15:43:44Z","title":"SA-MLP: A Low-Power Multiplication-Free Deep Network for 3D Point Cloud\n  Classification in Resource-Constrained Environments","summary":"  Point cloud classification plays a crucial role in the processing and\nanalysis of data from 3D sensors such as LiDAR, which are commonly used in\napplications like autonomous vehicles, robotics, and environmental monitoring.\nHowever, traditional neural networks, which rely heavily on multiplication\noperations, often face challenges in terms of high computational costs and\nenergy consumption. This study presents a novel family of efficient MLP-based\narchitectures designed to improve the computational efficiency of point cloud\nclassification tasks in sensor systems. The baseline model, Mul-MLP, utilizes\nconventional multiplication operations, while Add-MLP and Shift-MLP replace\nmultiplications with addition and shift operations, respectively. These\nreplacements leverage more sensor-friendly operations that can significantly\nreduce computational overhead, making them particularly suitable for\nresource-constrained sensor platforms. To further enhance performance, we\npropose SA-MLP, a hybrid architecture that alternates between shift and adder\nlayers, preserving the network depth while optimizing computational efficiency.\nUnlike previous approaches such as ShiftAddNet, which increase the layer count\nand limit representational capacity by freezing shift weights, SA-MLP fully\nexploits the complementary advantages of shift and adder layers by employing\ndistinct learning rates and optimizers. Experimental results show that Add-MLP\nand Shift-MLP achieve competitive performance compared to Mul-MLP, while SA-MLP\nsurpasses the baseline, delivering results comparable to state-of-the-art MLP\nmodels in terms of both classification accuracy and computational efficiency.\nThis work offers a promising, energy-efficient solution for sensor-driven\napplications requiring real-time point cloud classification, particularly in\nenvironments with limited computational resources.\n","authors":["Qiang Zheng","Chao Zhang","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2409.01998v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08983v1","updated":"2025-01-15T17:59:56Z","published":"2025-01-15T17:59:56Z","title":"CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities","summary":"  3D scene generation has garnered growing attention in recent years and has\nmade significant progress. Generating 4D cities is more challenging than 3D\nscenes due to the presence of structurally complex, visually diverse objects\nlike buildings and vehicles, and heightened human sensitivity to distortions in\nurban environments. To tackle these issues, we propose CityDreamer4D, a\ncompositional generative model specifically tailored for generating unbounded\n4D cities. Our main insights are 1) 4D city generation should separate dynamic\nobjects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2)\nall objects in the 4D scene should be composed of different types of neural\nfields for buildings, vehicles, and background stuff. Specifically, we propose\nTraffic Scenario Generator and Unbounded Layout Generator to produce dynamic\ntraffic scenarios and static city layouts using a highly compact BEV\nrepresentation. Objects in 4D cities are generated by combining stuff-oriented\nand instance-oriented neural fields for background stuff, buildings, and\nvehicles. To suit the distinct characteristics of background stuff and\ninstances, the neural fields employ customized generative hash grids and\nperiodic positional embeddings as scene parameterizations. Furthermore, we\noffer a comprehensive suite of datasets for city generation, including OSM,\nGoogleEarth, and CityTopia. The OSM dataset provides a variety of real-world\ncity layouts, while the Google Earth and CityTopia datasets deliver\nlarge-scale, high-quality city imagery complete with 3D instance annotations.\nLeveraging its compositional design, CityDreamer4D supports a range of\ndownstream applications, such as instance editing, city stylization, and urban\nsimulation, while delivering state-of-the-art performance in generating\nrealistic 4D cities.\n","authors":["Haozhe Xie","Zhaoxi Chen","Fangzhou Hong","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08982v1","updated":"2025-01-15T17:59:32Z","published":"2025-01-15T17:59:32Z","title":"CityLoc: 6 DoF Localization of Text Descriptions in Large-Scale Scenes\n  with Gaussian Representation","summary":"  Localizing text descriptions in large-scale 3D scenes is inherently an\nambiguous task. This nonetheless arises while describing general concepts, e.g.\nall traffic lights in a city.\n  To facilitate reasoning based on such concepts, text localization in the form\nof distribution is required. In this paper, we generate the distribution of the\ncamera poses conditioned upon the textual description.\n  To facilitate such generation, we propose a diffusion-based architecture that\nconditionally diffuses the noisy 6DoF camera poses to their plausible\nlocations.\n  The conditional signals are derived from the text descriptions, using the\npre-trained text encoders. The connection between text descriptions and pose\ndistribution is established through pretrained Vision-Language-Model, i.e.\nCLIP. Furthermore, we demonstrate that the candidate poses for the distribution\ncan be further refined by rendering potential poses using 3D Gaussian\nsplatting, guiding incorrectly posed samples towards locations that better\nalign with the textual description, through visual reasoning.\n  We demonstrate the effectiveness of our method by comparing it with both\nstandard retrieval methods and learning-based approaches. Our proposed method\nconsistently outperforms these baselines across all five large-scale datasets.\nOur source code and dataset will be made publicly available.\n","authors":["Qi Ma","Runyi Yang","Bin Ren","Ender Konukoglu","Luc Van Gool","Danda Pani Paudel"],"pdf_url":"https://arxiv.org/pdf/2501.08982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06394v5","updated":"2025-01-15T17:56:35Z","published":"2023-11-10T20:50:36Z","title":"A design of Convolutional Neural Network model for the Diagnosis of the\n  COVID-19","summary":"  With the spread of COVID-19 around the globe over the past year, the usage of\nartificial intelligence (AI) algorithms and image processing methods to analyze\nthe X-ray images of patients' chest with COVID-19 has become essential. The\nCOVID-19 virus recognition in the lung area of a patient is one of the basic\nand essential needs of clicical centers and hospitals. Most research in this\nfield has been devoted to papers on the basis of deep learning methods\nutilizing CNNs (Convolutional Neural Network), which mainly deal with the\nscreening of sick and healthy people.In this study, a new structure of a\n19-layer CNN has been recommended for accurately recognition of the COVID-19\nfrom the X-ray pictures of chest. The offered CNN is developed to serve as a\nprecise diagnosis system for a three class (viral pneumonia, Normal, COVID) and\na four classclassification (Lung opacity, Normal, COVID-19, and pneumonia). A\ncomparison is conducted among the outcomes of the offered procedure and some\npopular pretrained networks, including Inception, Alexnet, ResNet50,\nSqueezenet, and VGG19 and based on Specificity, Accuracy, Precision,\nSensitivity, Confusion Matrix, and F1-score. The experimental results of the\noffered CNN method specify its dominance over the existing published\nprocedures. This method can be a useful tool for clinicians in deciding\nproperly about COVID-19.\n","authors":["Xinyuan Song"],"pdf_url":"https://arxiv.org/pdf/2311.06394v5.pdf","comment":"Important mistakes found. There's no new version currently. Also\n  contradiction with authorship"},{"id":"http://arxiv.org/abs/2501.05179v2","updated":"2025-01-15T17:34:26Z","published":"2025-01-09T11:57:58Z","title":"Compression with Global Guidance: Towards Training-free High-Resolution\n  MLLMs Acceleration","summary":"  Multimodal large language models (MLLMs) have attracted considerable\nattention due to their exceptional performance in visual content understanding\nand reasoning. However, their inference efficiency has been a notable concern,\nas the increasing length of multimodal contexts leads to quadratic complexity.\nToken compression techniques, which reduce the number of visual tokens, have\ndemonstrated their effectiveness in reducing computational costs. Yet, these\napproaches have struggled to keep pace with the rapid advancements in MLLMs,\nespecially the AnyRes strategy in the context of high-resolution image\nunderstanding. In this paper, we propose a novel token compression method,\nGlobalCom$^2$, tailored for high-resolution MLLMs that receive both the\nthumbnail and multiple crops. GlobalCom$^2$ treats the tokens derived from the\nthumbnail as the \"commander\" of the entire token compression process, directing\nthe allocation of retention ratios and the specific compression for each crop.\nIn this way, redundant tokens are eliminated while important local details are\nadaptively preserved to the highest extent feasible. Empirical results across\n10 benchmarks reveal that GlobalCom$^2$ achieves an optimal balance between\nperformance and efficiency, and consistently outperforms state-of-the-art token\ncompression methods with LLaVA-NeXT-7B/13B models. Our code is released at\nhttps://github.com/xuyang-liu16/GlobalCom2.\n","authors":["Xuyang Liu","Ziming Wang","Yuhang Han","Yingyao Wang","Jiale Yuan","Jun Song","Bo Zheng","Linfeng Zhang","Siteng Huang","Honggang Chen"],"pdf_url":"https://arxiv.org/pdf/2501.05179v2.pdf","comment":"Our code is released at\n  \\url{https://github.com/xuyang-liu16/GlobalCom2}"},{"id":"http://arxiv.org/abs/2501.08962v1","updated":"2025-01-15T17:18:46Z","published":"2025-01-15T17:18:46Z","title":"An analysis of data variation and bias in image-based dermatological\n  datasets for machine learning classification","summary":"  AI algorithms have become valuable in aiding professionals in healthcare. The\nincreasing confidence obtained by these models is helpful in critical decision\ndemands. In clinical dermatology, classification models can detect malignant\nlesions on patients' skin using only RGB images as input. However, most\nlearning-based methods employ data acquired from dermoscopic datasets on\ntraining, which are large and validated by a gold standard. Clinical models aim\nto deal with classification on users' smartphone cameras that do not contain\nthe corresponding resolution provided by dermoscopy. Also, clinical\napplications bring new challenges. It can contain captures from uncontrolled\nenvironments, skin tone variations, viewpoint changes, noises in data and\nlabels, and unbalanced classes. A possible alternative would be to use transfer\nlearning to deal with the clinical images. However, as the number of samples is\nlow, it can cause degradations on the model's performance; the source\ndistribution used in training differs from the test set. This work aims to\nevaluate the gap between dermoscopic and clinical samples and understand how\nthe dataset variations impact training. It assesses the main differences\nbetween distributions that disturb the model's prediction. Finally, from\nexperiments on different architectures, we argue how to combine the data from\ndivergent distributions, decreasing the impact on the model's final accuracy.\n","authors":["Francisco Mauro","Emanoel Thyago","Othon Vinicius","Rodrigo Abreu","Kelvin Cunha","José Gabriel","Rafael Barros","Thales Bezerra","Manoel Henriques","Natalia Lopes","Érico Moutinho","Jéssica Guido","Tsang Ing Ren","Paulo Borba"],"pdf_url":"https://arxiv.org/pdf/2501.08962v1.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2312.02186v3","updated":"2025-01-15T17:11:20Z","published":"2023-12-01T20:16:02Z","title":"Identifying Spurious Correlations using Counterfactual Alignment","summary":"  Models driven by spurious correlations often yield poor generalization\nperformance. We propose the counterfactual (CF) alignment method to detect and\nquantify spurious correlations of black box classifiers. Our methodology is\nbased on counterfactual images generated with respect to one classifier being\ninput into other classifiers to see if they also induce changes in the outputs\nof these classifiers. The relationship between these responses can be\nquantified and used to identify specific instances where a spurious correlation\nexists. This is validated by observing intuitive trends in face-attribute and\nwaterbird classifiers, as well as by fabricating spurious correlations and\ndetecting their presence, both visually and quantitatively. Furthermore,\nutilizing the CF alignment method, we demonstrate that we can evaluate robust\noptimization methods (GroupDRO, JTT, and FLAC) by detecting a reduction in\nspurious correlations.\n","authors":["Joseph Paul Cohen","Louis Blankemeier","Akshay Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2312.02186v3.pdf","comment":"Accepted to Transactions on Machine Learning Research (TMLR), Code:\n  https://github.com/ieee8023/latentshift"},{"id":"http://arxiv.org/abs/2409.17137v4","updated":"2025-01-15T16:56:26Z","published":"2024-09-25T17:56:00Z","title":"PACE: Marrying generalization in PArameter-efficient fine-tuning with\n  Consistency rEgularization","summary":"  Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained\ntransformers to downstream tasks. However, the optimization of tasks\nperformance often comes at the cost of generalizability in fine-tuned models.\nTo address this issue, we theoretically connect smaller weight gradient norms\nduring training and larger datasets to the improvements in model\ngeneralization. Motivated by this connection, we propose reducing gradient\nnorms for enhanced generalization and aligning fine-tuned model with the\npre-trained counterpart to retain knowledge from large-scale pre-training data.\nYet, naive alignment does not guarantee gradient reduction and can potentially\ncause gradient explosion, complicating efforts to manage gradients. To address\nsuch an issue, we propose PACE, marrying generalization of PArameter-efficient\nfine-tuning with Consistency rEgularization. We perturb features learned from\nthe adapter with the multiplicative noise and ensure the fine-tuned model\nremains consistent for same sample under different perturbations. Theoretical\nanalysis shows that PACE not only implicitly regularizes gradients for enhanced\ngeneralization, but also implicitly aligns the fine-tuned and pre-trained\nmodels to retain knowledge. Experimental evidence supports our theories. PACE\nsurpasses existing PEFT methods in visual adaptation tasks (VTAB-1k, FGVC,\nfew-shot learning, domain adaptation) showcasing its potential for\nresource-efficient fine-tuning. It also improves LoRA in text classification\n(GLUE) and mathematical reasoning (GSM-8K). The code is available at\nhttps://github.com/MaxwellYaoNi/PACE\n","authors":["Yao Ni","Shan Zhang","Piotr Koniusz"],"pdf_url":"https://arxiv.org/pdf/2409.17137v4.pdf","comment":"Accepted by NeurIPS 2024 as a spotlight"},{"id":"http://arxiv.org/abs/2412.14816v3","updated":"2025-01-15T16:54:36Z","published":"2024-12-19T13:10:03Z","title":"TextSleuth: Towards Explainable Tampered Text Detection","summary":"  Recently, tampered text detection has attracted increasing attention due to\nits essential role in information security. Although existing methods can\ndetect the tampered text region, the interpretation of such detection remains\nunclear, making the prediction unreliable. To address this problem, we propose\nto explain the basis of tampered text detection with natural language via large\nmultimodal models. To fill the data gap for this task, we propose a\nlarge-scale, comprehensive dataset, ETTD, which contains both pixel-level\nannotations for tampered text region and natural language annotations\ndescribing the anomaly of the tampered text. Multiple methods are employed to\nimprove the quality of the proposed data. For example, elaborate queries are\nintroduced to generate high-quality anomaly descriptions with GPT4o. A fused\nmask prompt is proposed to reduce confusion when querying GPT4o to generate\nanomaly descriptions. To automatically filter out low-quality annotations, we\nalso propose to prompt GPT4o to recognize tampered texts before describing the\nanomaly, and to filter out the responses with low OCR accuracy. To further\nimprove explainable tampered text detection, we propose a simple yet effective\nmodel called TextSleuth, which achieves improved fine-grained perception and\ncross-domain generalization by focusing on the suspected region, with a\ntwo-stage analysis paradigm and an auxiliary grounding prompt. Extensive\nexperiments on both the ETTD dataset and the public dataset have verified the\neffectiveness of the proposed methods. In-depth analysis is also provided to\ninspire further research. Our dataset and code will be open-source.\n","authors":["Chenfan Qu","Jian Liu","Haoxing Chen","Baihan Yu","Jingjing Liu","Weiqiang Wang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2412.14816v3.pdf","comment":"The first work for explainable tampered text detection"},{"id":"http://arxiv.org/abs/2501.08931v1","updated":"2025-01-15T16:34:20Z","published":"2025-01-15T16:34:20Z","title":"Visual WetlandBirds Dataset: Bird Species Identification and Behavior\n  Recognition in Videos","summary":"  The current biodiversity loss crisis makes animal monitoring a relevant field\nof study. In light of this, data collected through monitoring can provide\nessential insights, and information for decision-making aimed at preserving\nglobal biodiversity. Despite the importance of such data, there is a notable\nscarcity of datasets featuring videos of birds, and none of the existing\ndatasets offer detailed annotations of bird behaviors in video format. In\nresponse to this gap, our study introduces the first fine-grained video dataset\nspecifically designed for bird behavior detection and species classification.\nThis dataset addresses the need for comprehensive bird video datasets and\nprovides detailed data on bird actions, facilitating the development of deep\nlearning models to recognize these, similar to the advancements made in human\naction recognition. The proposed dataset comprises 178 videos recorded in\nSpanish wetlands, capturing 13 different bird species performing 7 distinct\nbehavior classes. In addition, we also present baseline results using state of\nthe art models on two tasks: bird behavior recognition and species\nclassification.\n","authors":["Javier Rodriguez-Juan","David Ortiz-Perez","Manuel Benavent-Lledo","David Mulero-Pérez","Pablo Ruiz-Ponce","Adrian Orihuela-Torres","Jose Garcia-Rodriguez","Esther Sebastián-González"],"pdf_url":"https://arxiv.org/pdf/2501.08931v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08924v1","updated":"2025-01-15T16:30:05Z","published":"2025-01-15T16:30:05Z","title":"Learning Joint Denoising, Demosaicing, and Compression from the Raw\n  Natural Image Noise Dataset","summary":"  This paper introduces the Raw Natural Image Noise Dataset (RawNIND), a\ndiverse collection of paired raw images designed to support the development of\ndenoising models that generalize across sensors, image development workflows,\nand styles. Two denoising methods are proposed: one operates directly on raw\nBayer data, leveraging computational efficiency, while the other processes\nlinear RGB images for improved generalization to different sensors, with both\npreserving flexibility for subsequent development. Both methods outperform\ntraditional approaches which rely on developed images. Additionally, the\nintegration of denoising and compression at the raw data level significantly\nenhances rate-distortion performance and computational efficiency. These\nfindings suggest a paradigm shift toward raw data workflows for efficient and\nflexible image processing.\n","authors":["Benoit Brummer","Christophe De Vleeschouwer"],"pdf_url":"https://arxiv.org/pdf/2501.08924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08912v1","updated":"2025-01-15T16:20:26Z","published":"2025-01-15T16:20:26Z","title":"Empowering Agricultural Insights: RiceLeafBD - A Novel Dataset and\n  Optimal Model Selection for Rice Leaf Disease Diagnosis through Transfer\n  Learning Technique","summary":"  The number of people living in this agricultural nation of ours, which is\nsurrounded by lush greenery, is growing on a daily basis. As a result of this,\nthe level of arable land is decreasing, as well as residential houses and\nindustrial factories. The food crisis is becoming the main threat for us in the\nupcoming days. Because on the one hand, the population is increasing, and on\nthe other hand, the amount of food crop production is decreasing due to the\nattack of diseases. Rice is one of the most significant cultivated crops since\nit provides food for more than half of the world's population. Bangladesh is\ndependent on rice (Oryza sativa) as a vital crop for its agriculture, but it\nfaces a significant problem as a result of the ongoing decline in rice yield\nbrought on by common diseases. Early disease detection is the main difficulty\nin rice crop cultivation. In this paper, we proposed our own dataset, which was\ncollected from the Bangladesh field, and also applied deep learning and\ntransfer learning models for the evaluation of the datasets. We elaborately\nexplain our dataset and also give direction for further research work to serve\nsociety using this dataset. We applied a light CNN model and pre-trained\nInceptionNet-V2, EfficientNet-V2, and MobileNet-V2 models, which achieved 91.5%\nperformance for the EfficientNet-V2 model of this work. The results obtained\nassaulted other models and even exceeded approaches that are considered to be\npart of the state of the art. It has been demonstrated by this study that it is\npossible to precisely and effectively identify diseases that affect rice leaves\nusing this unbiased datasets. After analysis of the performance of different\nmodels, the proposed datasets are significant for the society for research work\nto provide solutions for decreasing rice leaf disease.\n","authors":["Sadia Afrin Rimi","Md. Jalal Uddin Chowdhury","Rifat Abdullah","Iftekhar Ahmed","Mahrima Akter Mim","Mohammad Shoaib Rahman"],"pdf_url":"https://arxiv.org/pdf/2501.08912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08910v1","updated":"2025-01-15T16:19:37Z","published":"2025-01-15T16:19:37Z","title":"Lights, Camera, Matching: The Role of Image Illumination in Fair Face\n  Recognition","summary":"  Facial brightness is a key image quality factor impacting face recognition\naccuracy differentials across demographic groups. In this work, we aim to\ndecrease the accuracy gap between the similarity score distributions for\nCaucasian and African American female mated image pairs, as measured by d'\nbetween distributions. To balance brightness across demographic groups, we\nconduct three experiments, interpreting brightness in the face skin region\neither as median pixel value or as the distribution of pixel values. Balancing\nbased on median brightness alone yields up to a 46.8% decrease in d', while\nbalancing based on brightness distribution yields up to a 57.6% decrease. In\nall three cases, the similarity scores of the individual distributions improve,\nwith mean scores maximally improving 5.9% for Caucasian females and 3.7% for\nAfrican American females.\n","authors":["Gabriella Pangelinan","Grace Bezold","Haiyu Wu","Michael C. King","Kevin W. Bowyer"],"pdf_url":"https://arxiv.org/pdf/2501.08910v1.pdf","comment":"14 pages, 11 figures, Conference submission"},{"id":"http://arxiv.org/abs/2308.07898v2","updated":"2025-01-15T16:19:36Z","published":"2023-08-15T17:39:52Z","title":"A Foundation Language-Image Model of the Retina (FLAIR): Encoding Expert\n  Knowledge in Text Supervision","summary":"  Foundation vision-language models are currently transforming computer vision,\nand are on the rise in medical imaging fueled by their very promising\ngeneralization capabilities. However, the initial attempts to transfer this new\nparadigm to medical imaging have shown less impressive performances than those\nobserved in other domains, due to the significant domain shift and the complex,\nexpert domain knowledge inherent to medical-imaging tasks. Motivated by the\nneed for domain-expert foundation models, we present FLAIR, a pre-trained\nvision-language model for universal retinal fundus image understanding. To this\nend, we compiled 38 open-access, mostly categorical fundus imaging datasets\nfrom various sources, with up to 101 different target conditions and 288,307\nimages. We integrate the expert's domain knowledge in the form of descriptive\ntextual prompts, during both pre-training and zero-shot inference, enhancing\nthe less-informative categorical supervision of the data. Such a textual\nexpert's knowledge, which we compiled from the relevant clinical literature and\ncommunity standards, describes the fine-grained features of the pathologies as\nwell as the hierarchies and dependencies between them. We report comprehensive\nevaluations, which illustrate the benefit of integrating expert knowledge and\nthe strong generalization capabilities of FLAIR under difficult scenarios with\ndomain shifts or unseen categories. When adapted with a lightweight linear\nprobe, FLAIR outperforms fully-trained, dataset-focused models, more so in the\nfew-shot regimes. Interestingly, FLAIR outperforms by a wide margin\nlarger-scale generalist image-language models and retina domain-specific\nself-supervised networks, which emphasizes the potential of embedding experts'\ndomain knowledge and the limitations of generalist models in medical imaging.\n","authors":["Julio Silva-Rodríguez","Hadi Chakor","Riadh Kobbi","Jose Dolz","Ismail Ben Ayed"],"pdf_url":"https://arxiv.org/pdf/2308.07898v2.pdf","comment":"Accepted in Medical Image Analysis. The pre-trained model is\n  available at: https://github.com/jusiro/FLAIR"},{"id":"http://arxiv.org/abs/2501.08902v1","updated":"2025-01-15T16:11:24Z","published":"2025-01-15T16:11:24Z","title":"Multi-View Transformers for Airway-To-Lung Ratio Inference on Cardiac CT\n  Scans: The C4R Study","summary":"  The ratio of airway tree lumen to lung size (ALR), assessed at full\ninspiration on high resolution full-lung computed tomography (CT), is a major\nrisk factor for chronic obstructive pulmonary disease (COPD). There is growing\ninterest to infer ALR from cardiac CT images, which are widely available in\nepidemiological cohorts, to investigate the relationship of ALR to severe\nCOVID-19 and post-acute sequelae of SARS-CoV-2 infection (PASC). Previously,\ncardiac scans included approximately 2/3 of the total lung volume with 5-6x\ngreater slice thickness than high-resolution (HR) full-lung (FL) CT. In this\nstudy, we present a novel attention-based Multi-view Swin Transformer to infer\nFL ALR values from segmented cardiac CT scans. For the supervised training we\nexploit paired full-lung and cardiac CTs acquired in the Multi-Ethnic Study of\nAtherosclerosis (MESA). Our network significantly outperforms a proxy direct\nALR inference on segmented cardiac CT scans and achieves accuracy and\nreproducibility comparable with a scan-rescan reproducibility of the FL ALR\nground-truth.\n","authors":["Sneha N. Naik","Elsa D. Angelini","Eric A. Hoffman","Elizabeth C. Oelsner","R. Graham Barr","Benjamin M. Smith","Andrew F. Laine"],"pdf_url":"https://arxiv.org/pdf/2501.08902v1.pdf","comment":"Accepted to appear in Proceedings of International Symposium on\n  Biomedical Imaging (ISBI), 2025"},{"id":"http://arxiv.org/abs/2501.08900v1","updated":"2025-01-15T16:08:25Z","published":"2025-01-15T16:08:25Z","title":"Enhanced Multi-Scale Cross-Attention for Person Image Generation","summary":"  In this paper, we propose a novel cross-attention-based generative\nadversarial network (GAN) for the challenging person image generation task.\nCross-attention is a novel and intuitive multi-modal fusion method in which an\nattention/correlation matrix is calculated between two feature maps of\ndifferent modalities. Specifically, we propose the novel XingGAN (or\nCrossingGAN), which consists of two generation branches that capture the\nperson's appearance and shape, respectively. Moreover, we propose two novel\ncross-attention blocks to effectively transfer and update the person's shape\nand appearance embeddings for mutual improvement. This has not been considered\nby any other existing GAN-based image generation work. To further learn the\nlong-range correlations between different person poses at different scales and\nsub-regions, we propose two novel multi-scale cross-attention blocks. To tackle\nthe issue of independent correlation computations within the cross-attention\nmechanism leading to noisy and ambiguous attention weights, which hinder\nperformance improvements, we propose a module called enhanced attention (EA).\nLastly, we introduce a novel densely connected co-attention module to fuse\nappearance and shape features at different stages effectively. Extensive\nexperiments on two public datasets demonstrate that the proposed method\noutperforms current GAN-based methods and performs on par with diffusion-based\nmethods. However, our method is significantly faster than diffusion-based\nmethods in both training and inference.\n","authors":["Hao Tang","Ling Shao","Nicu Sebe","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2501.08900v1.pdf","comment":"Accepted to TPAMI, an extended version of a paper published in\n  ECCV2020. arXiv admin note: substantial text overlap with arXiv:2007.09278"},{"id":"http://arxiv.org/abs/2501.08885v1","updated":"2025-01-15T15:56:06Z","published":"2025-01-15T15:56:06Z","title":"Feature-based One-For-All: A Universal Framework for Heterogeneous\n  Knowledge Distillation","summary":"  Knowledge distillation (KD) involves transferring knowledge from a\npre-trained heavy teacher model to a lighter student model, thereby reducing\nthe inference cost while maintaining comparable effectiveness. Prior KD\ntechniques typically assume homogeneity between the teacher and student models.\nHowever, as technology advances, a wide variety of architectures have emerged,\nranging from initial Convolutional Neural Networks (CNNs) to Vision\nTransformers (ViTs), and Multi-Level Perceptrons (MLPs). Consequently,\ndeveloping a universal KD framework compatible with any architecture has become\nan important research topic. In this paper, we introduce a feature-based\none-for-all (FOFA) KD framework to enable feature distillation across diverse\narchitecture. Our framework comprises two key components. First, we design\nprompt tuning blocks that incorporate student feedback, allowing teacher\nfeatures to adapt to the student model's learning process. Second, we propose\nregion-aware attention to mitigate the view mismatch problem between\nheterogeneous architecture. By leveraging these two modules, effective\ndistillation of intermediate features can be achieved across heterogeneous\narchitectures. Extensive experiments on CIFAR, ImageNet, and COCO demonstrate\nthe superiority of the proposed method.\n","authors":["Jhe-Hao Lin","Yi Yao","Chan-Feng Hsu","Hongxia Xie","Hong-Han Shuai","Wen-Huang Cheng"],"pdf_url":"https://arxiv.org/pdf/2501.08885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20062v2","updated":"2025-01-15T15:53:13Z","published":"2024-12-28T07:34:49Z","title":"MADiff: Text-Guided Fashion Image Editing with Mask Prediction and\n  Attention-Enhanced Diffusion","summary":"  Text-guided image editing model has achieved great success in general domain.\nHowever, directly applying these models to the fashion domain may encounter two\nissues: (1) Inaccurate localization of editing region; (2) Weak editing\nmagnitude. To address these issues, the MADiff model is proposed. Specifically,\nto more accurately identify editing region, the MaskNet is proposed, in which\nthe foreground region, densepose and mask prompts from large language model are\nfed into a lightweight UNet to predict the mask for editing region. To\nstrengthen the editing magnitude, the Attention-Enhanced Diffusion Model is\nproposed, where the noise map, attention map, and the mask from MaskNet are fed\ninto the proposed Attention Processor to produce a refined noise map. By\nintegrating the refined noise map into the diffusion model, the edited image\ncan better align with the target prompt. Given the absence of benchmarks in\nfashion image editing, we constructed a dataset named Fashion-E, comprising\n28390 image-text pairs in the training set, and 2639 image-text pairs for four\ntypes of fashion tasks in the evaluation set. Extensive experiments on\nFashion-E demonstrate that our proposed method can accurately predict the mask\nof editing region and significantly enhance editing magnitude in fashion image\nediting compared to the state-of-the-art methods.\n","authors":["Zechao Zhan","Dehong Gao","Jinxia Zhang","Jiale Huang","Yang Hu","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.20062v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03492v6","updated":"2025-01-15T15:26:03Z","published":"2023-06-06T08:19:30Z","title":"Industrial Anomaly Detection and Localization Using Weakly-Supervised\n  Residual Transformers","summary":"  Recent advancements in industrial anomaly detection (AD) have demonstrated\nthat incorporating a small number of anomalous samples during training can\nsignificantly enhance accuracy. However, this improvement often comes at the\ncost of extensive annotation efforts, which are impractical for many real-world\napplications. In this paper, we introduce a novel framework, Weak}ly-supervised\nRESidual Transformer (WeakREST), designed to achieve high anomaly detection\naccuracy while minimizing the reliance on manual annotations. First, we\nreformulate the pixel-wise anomaly localization task into a block-wise\nclassification problem. Second, we introduce a residual-based feature\nrepresentation called Positional Fast Anomaly Residuals (PosFAR) which captures\nanomalous patterns more effectively. To leverage this feature, we adapt the\nSwin Transformer for enhanced anomaly detection and localization. Additionally,\nwe propose a weak annotation approach, utilizing bounding boxes and image tags\nto define anomalous regions. This approach establishes a semi-supervised\nlearning context that reduces the dependency on precise pixel-level labels. To\nfurther improve the learning process, we develop a novel ResMixMatch algorithm,\ncapable of handling the interplay between weak labels and residual-based\nrepresentations.\n  On the benchmark dataset MVTec-AD, our method achieves an Average Precision\n(AP) of $83.0\\%$, surpassing the previous best result of $82.7\\%$ in the\nunsupervised setting. In the supervised AD setting, WeakREST attains an AP of\n$87.6\\%$, outperforming the previous best of $86.0\\%$. Notably, even when using\nweaker annotations such as bounding boxes, WeakREST exceeds the performance of\nleading methods relying on pixel-wise supervision, achieving an AP of $87.1\\%$\ncompared to the prior best of $86.0\\%$ on MVTec-AD.\n","authors":["Hanxi Li","Jingqi Wu","Deyin Liu","Lin Wu","Hao Chen","Mingwen Wang","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2306.03492v6.pdf","comment":"13 pages,7 figures"},{"id":"http://arxiv.org/abs/2411.10175v2","updated":"2025-01-15T15:24:32Z","published":"2024-11-15T13:21:26Z","title":"The Surprising Ineffectiveness of Pre-Trained Visual Representations for\n  Model-Based Reinforcement Learning","summary":"  Visual Reinforcement Learning (RL) methods often require extensive amounts of\ndata. As opposed to model-free RL, model-based RL (MBRL) offers a potential\nsolution with efficient data utilization through planning. Additionally, RL\nlacks generalization capabilities for real-world tasks. Prior work has shown\nthat incorporating pre-trained visual representations (PVRs) enhances sample\nefficiency and generalization. While PVRs have been extensively studied in the\ncontext of model-free RL, their potential in MBRL remains largely unexplored.\nIn this paper, we benchmark a set of PVRs on challenging control tasks in a\nmodel-based RL setting. We investigate the data efficiency, generalization\ncapabilities, and the impact of different properties of PVRs on the performance\nof model-based agents. Our results, perhaps surprisingly, reveal that for MBRL\ncurrent PVRs are not more sample efficient than learning representations from\nscratch, and that they do not generalize better to out-of-distribution (OOD)\nsettings. To explain this, we analyze the quality of the trained dynamics\nmodel. Furthermore, we show that data diversity and network architecture are\nthe most important contributors to OOD generalization performance.\n","authors":["Moritz Schneider","Robert Krug","Narunas Vaskevicius","Luigi Palmieri","Joschka Boedecker"],"pdf_url":"https://arxiv.org/pdf/2411.10175v2.pdf","comment":"Published at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024). Project page: https://schneimo.com/pvr4mbrl/"},{"id":"http://arxiv.org/abs/2412.18977v2","updated":"2025-01-15T15:22:45Z","published":"2024-12-25T19:38:32Z","title":"CGCOD: Class-Guided Camouflaged Object Detection","summary":"  Camouflaged Object Detection (COD) aims to identify objects that blend\nseamlessly into their surroundings. The inherent visual complexity of\ncamouflaged objects, including their low contrast with the background, diverse\ntextures, and subtle appearance variations, often obscures semantic cues,\nmaking accurate segmentation highly challenging. Existing methods primarily\nrely on visual features, which are insufficient to handle the variability and\nintricacy of camouflaged objects, leading to unstable object perception and\nambiguous segmentation results. To tackle these limitations, we introduce a\nnovel task, class-guided camouflaged object detection (CGCOD), which extends\ntraditional COD task by incorporating object-specific class knowledge to\nenhance detection robustness and accuracy. To facilitate this task, we present\na new dataset, CamoClass, comprising real-world camouflaged objects with class\nannotations. Furthermore, we propose a multi-stage framework, CGNet, which\nincorporates a plug-and-play class prompt generator and a simple yet effective\nclass-guided detector. This establishes a new paradigm for COD, bridging the\ngap between contextual understanding and class-guided detection. Extensive\nexperimental results demonstrate the effectiveness of our flexible framework in\nimproving the performance of proposed and existing detectors by leveraging\nclass-level textual information.\n","authors":["Chenxi Zhang","Qing Zhang","Jiayun Wu","Youwei Pang"],"pdf_url":"https://arxiv.org/pdf/2412.18977v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08861v1","updated":"2025-01-15T15:20:46Z","published":"2025-01-15T15:20:46Z","title":"Generative Planning with 3D-vision Language Pre-training for End-to-End\n  Autonomous Driving","summary":"  Autonomous driving is a challenging task that requires perceiving and\nunderstanding the surrounding environment for safe trajectory planning. While\nexisting vision-based end-to-end models have achieved promising results, these\nmethods are still facing the challenges of vision understanding, decision\nreasoning and scene generalization. To solve these issues, a generative\nplanning with 3D-vision language pre-training model named GPVL is proposed for\nend-to-end autonomous driving. The proposed paradigm has two significant\naspects. On one hand, a 3D-vision language pre-training module is designed to\nbridge the gap between visual perception and linguistic understanding in the\nbird's eye view. On the other hand, a cross-modal language model is introduced\nto generate holistic driving decisions and fine-grained trajectories with\nperception and navigation information in an auto-regressive manner. Experiments\non the challenging nuScenes dataset demonstrate that the proposed scheme\nachieves excellent performances compared with state-of-the-art methods.\nBesides, the proposed GPVL presents strong generalization ability and real-time\npotential when handling high-level commands in various scenarios. It is\nbelieved that the effective, robust and efficient performance of GPVL is\ncrucial for the practical application of future autonomous driving systems.\nCode is available at https://github.com/ltp1995/GPVL\n","authors":["Tengpeng Li","Hanli Wang","Xianfei Li","Wenlong Liao","Tao He","Pai Peng"],"pdf_url":"https://arxiv.org/pdf/2501.08861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08841v1","updated":"2025-01-15T14:52:20Z","published":"2025-01-15T14:52:20Z","title":"Exploring Task-Level Optimal Prompts for Visual In-Context Learning","summary":"  With the development of Vision Foundation Models (VFMs) in recent years,\nVisual In-Context Learning (VICL) has become a better choice compared to\nmodifying models in most scenarios. Different from retraining or fine-tuning\nmodel, VICL does not require modifications to the model's weights or\narchitecture, and only needs a prompt with demonstrations to teach VFM how to\nsolve tasks. Currently, significant computational cost for finding optimal\nprompts for every test sample hinders the deployment of VICL, as determining\nwhich demonstrations to use for constructing prompts is very costly. In this\npaper, however, we find a counterintuitive phenomenon that most test samples\nactually achieve optimal performance under the same prompts, and searching for\nsample-level prompts only costs more time but results in completely identical\nprompts. Therefore, we propose task-level prompting to reduce the cost of\nsearching for prompts during the inference stage and introduce two time-saving\nyet effective task-level prompt search strategies. Extensive experimental\nresults show that our proposed method can identify near-optimal prompts and\nreach the best VICL performance with a minimal cost that prior work has never\nachieved.\n","authors":["Yan Zhu","Huan Ma","Changqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.08841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08837v1","updated":"2025-01-15T14:46:44Z","published":"2025-01-15T14:46:44Z","title":"MANTA: Diffusion Mamba for Efficient and Effective Stochastic Long-Term\n  Dense Anticipation","summary":"  Our work addresses the problem of stochastic long-term dense anticipation.\nThe goal of this task is to predict actions and their durations several minutes\ninto the future based on provided video observations. Anticipation over\nextended horizons introduces high uncertainty, as a single observation can lead\nto multiple plausible future outcomes. To address this uncertainty, stochastic\nmodels are designed to predict several potential future action sequences.\nRecent work has further proposed to incorporate uncertainty modelling for\nobserved frames by simultaneously predicting per-frame past and future actions\nin a unified manner. While such joint modelling of actions is beneficial, it\nrequires long-range temporal capabilities to connect events across distant past\nand future time points. However, the previous work struggles to achieve such a\nlong-range understanding due to its limited and/or sparse receptive field. To\nalleviate this issue, we propose a novel MANTA (MAmba for ANTicipation)\nnetwork. Our model enables effective long-term temporal modelling even for very\nlong sequences while maintaining linear complexity in sequence length. We\ndemonstrate that our approach achieves state-of-the-art results on three\ndatasets - Breakfast, 50Salads, and Assembly101 - while also significantly\nimproving computational and memory efficiency.\n","authors":["Olga Zatsarynna","Emad Bahrami","Yazan Abu Farha","Gianpiero Francesca","Juergen Gall"],"pdf_url":"https://arxiv.org/pdf/2501.08837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16758v2","updated":"2025-01-15T14:35:11Z","published":"2024-12-21T20:16:37Z","title":"Evaluation of radiomic feature harmonization techniques for benign and\n  malignant pulmonary nodules","summary":"  BACKGROUND: Radiomics provides quantitative features of pulmonary nodules\n(PNs) which could aid lung cancer diagnosis, but medical image acquisition\nvariability is an obstacle to clinical application. Acquisition effects may\ndiffer between radiomic features from benign vs. malignant PNs. PURPOSE: We\nevaluated how to account for differences between benign and malignant PNs when\ncorrecting radiomic features' acquisition dependency. METHODS: We used 567\nchest CT scans grouped as benign, malignant, or lung cancer screening (mixed\nbenign, malignant). ComBat harmonization was applied to extracted features for\nvariation in 4 acquisition parameters. We compared: harmonizing without\ndistinction, harmonizing with a covariate to preserve distinctions between\nsubgroups, and harmonizing subgroups separately. Significant ($p\\le0.05$)\nKruskal-Wallis tests showed whether harmonization removed acquisition\ndependency. A LASSO-SVM pipeline was trained on successfully harmonized\nfeatures to predict malignancy. To evaluate predictive information in these\nfeatures, the trained harmonization estimators and predictive model were\napplied to unseen test sets. Harmonization and predictive performance were\nassessed for 10 trials of 5-fold cross-validation. RESULTS: An average 2.1% of\nfeatures (95% CI:1.9-2.4%) were acquisition-independent when harmonized without\ndistinction, 27.3% (95% CI:25.7-28.9%) when harmonized with a covariate, and\n90.9% (95% CI:90.4-91.5%) when harmonized separately. Data harmonized\nseparately or with a covariate trained models with higher ROC-AUC for screening\nscans than data harmonized without distinction between benign and malignant PNs\n(Delong test, adjusted $p\\le0.05$). CONCLUSIONS: Radiomic features of benign\nand malignant PNs need different corrective transformations to recover\nacquisition-independent distributions. This can be done by harmonizing\nseparately or with a covariate.\n","authors":["Claire Huchthausen","Menglin Shi","Gabriel L. A. de Sousa","Jonathan Colen","Emery Shelley","James Larner","Einsley Janowski","Krishni Wijesooriya"],"pdf_url":"https://arxiv.org/pdf/2412.16758v2.pdf","comment":"15 pages, 3 figures, plus supplemental material; updated author list,\n  corrected result in paragraph 3 of Discussion, updated Figure S1"},{"id":"http://arxiv.org/abs/2501.08828v1","updated":"2025-01-15T14:30:13Z","published":"2025-01-15T14:30:13Z","title":"MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents","summary":"  Multi-modal document retrieval is designed to identify and retrieve various\nforms of multi-modal content, such as figures, tables, charts, and layout\ninformation from extensive documents. Despite its significance, there is a\nnotable lack of a robust benchmark to effectively evaluate the performance of\nsystems in multi-modal document retrieval. To address this gap, this work\nintroduces a new benchmark, named as MMDocIR, encompassing two distinct tasks:\npage-level and layout-level retrieval. The former focuses on localizing the\nmost relevant pages within a long document, while the latter targets the\ndetection of specific layouts, offering a more fine-grained granularity than\nwhole-page analysis. A layout can refer to a variety of elements such as\ntextual paragraphs, equations, figures, tables, or charts. The MMDocIR\nbenchmark comprises a rich dataset featuring expertly annotated labels for\n1,685 questions and bootstrapped labels for 173,843 questions, making it a\npivotal resource for advancing multi-modal document retrieval for both training\nand evaluation. Through rigorous experiments, we reveal that (i) visual\nretrievers significantly outperform their text counterparts, (ii) MMDocIR train\nset can effectively benefit the training process of multi-modal document\nretrieval and (iii) text retrievers leveraging on VLM-text perform much better\nthan those using OCR-text. These findings underscores the potential advantages\nof integrating visual elements for multi-modal document retrieval.\n","authors":["Kuicai Dong","Yujing Chang","Xin Deik Goh","Dexun Li","Ruiming Tang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08828v1.pdf","comment":"https://huggingface.co/MMDocIR"},{"id":"http://arxiv.org/abs/2501.08819v1","updated":"2025-01-15T14:17:13Z","published":"2025-01-15T14:17:13Z","title":"Boosting Diffusion Guidance via Learning Degradation-Aware Models for\n  Blind Super Resolution","summary":"  Recently, diffusion-based blind super-resolution (SR) methods have shown\ngreat ability to generate high-resolution images with abundant high-frequency\ndetail, but the detail is often achieved at the expense of fidelity. Meanwhile,\nanother line of research focusing on rectifying the reverse process of\ndiffusion models (i.e., diffusion guidance), has demonstrated the power to\ngenerate high-fidelity results for non-blind SR. However, these methods rely on\nknown degradation kernels, making them difficult to apply to blind SR. To\naddress these issues, we introduce degradation-aware models that can be\nintegrated into the diffusion guidance framework, eliminating the need to know\ndegradation kernels. Additionally, we propose two novel techniques input\nperturbation and guidance scalar to further improve our performance. Extensive\nexperimental results show that our proposed method has superior performance\nover state-of-the-art methods on blind SR benchmarks\n","authors":["Shao-Hao Lu","Ren Wang","Ching-Chun Huang","Wei-Chen Chiu"],"pdf_url":"https://arxiv.org/pdf/2501.08819v1.pdf","comment":"To appear in WACV 2025. Code is available at:\n  https://github.com/ryanlu2240/Boosting-Diffusion-Guidance-via-Learning-Degradation-Aware-Models-for-Blind-Super-Resolution"},{"id":"http://arxiv.org/abs/2501.08816v1","updated":"2025-01-15T14:12:59Z","published":"2025-01-15T14:12:59Z","title":"IDEA: Image Description Enhanced CLIP-Adapter","summary":"  CLIP (Contrastive Language-Image Pre-training) has attained great success in\npattern recognition and computer vision. Transferring CLIP to downstream tasks\n(e.g. zero- or few-shot classification) is a hot topic in multimodal learning.\nHowever, current studies primarily focus on either prompt learning for text or\nadapter tuning for vision, without fully exploiting the complementary\ninformation and correlations among image-text pairs. In this paper, we propose\nan Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to\nfew-shot image classification tasks. This method captures fine-grained features\nby leveraging both visual features and textual descriptions of images. IDEA is\na training-free method for CLIP, and it can be comparable to or even exceeds\nstate-of-the-art models on multiple tasks. Furthermore, we introduce\nTrainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable\ncomponents (i.e., a projector and a learnable latent space), further enhancing\nthe model's performance and achieving SOTA results on 11 datasets. As one\nimportant contribution, we employ the Llama model and design a comprehensive\npipeline to generate textual descriptions for images of 11 datasets, resulting\nin a total of 1,637,795 image-text pairs, named \"IMD-11\". Our code and data are\nreleased at https://github.com/FourierAI/IDEA.\n","authors":["Zhipeng Ye","Feng Jiang","Qiufeng Wang","Kaizhu Huang","Jiaqi Huang"],"pdf_url":"https://arxiv.org/pdf/2501.08816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08815v1","updated":"2025-01-15T14:12:55Z","published":"2025-01-15T14:12:55Z","title":"Human Pose-Constrained UV Map Estimation","summary":"  UV map estimation is used in computer vision for detailed analysis of human\nposture or activity. Previous methods assign pixels to body model vertices by\ncomparing pixel descriptors independently, without enforcing global coherence\nor plausibility in the UV map. We propose Pose-Constrained Continuous Surface\nEmbeddings (PC-CSE), which integrates estimated 2D human pose into the\npixel-to-vertex assignment process. The pose provides global anatomical\nconstraints, ensuring that UV maps remain coherent while preserving local\nprecision. Evaluation on DensePose COCO demonstrates consistent improvement,\nregardless of the chosen 2D human pose model. Whole-body poses offer better\nconstraints by incorporating additional details about the hands and feet.\nConditioning UV maps with human pose reduces invalid mappings and enhances\nanatomical plausibility. In addition, we highlight inconsistencies in the\nground-truth annotations.\n","authors":["Matej Suchanek","Miroslav Purkrabek","Jiri Matas"],"pdf_url":"https://arxiv.org/pdf/2501.08815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08807v1","updated":"2025-01-15T14:03:27Z","published":"2025-01-15T14:03:27Z","title":"Multi-visual modality micro drone-based structural damage detection","summary":"  Accurate detection and resilience of object detectors in structural damage\ndetection are important in ensuring the continuous use of civil infrastructure.\nHowever, achieving robustness in object detectors remains a persistent\nchallenge, impacting their ability to generalize effectively. This study\nproposes DetectorX, a robust framework for structural damage detection coupled\nwith a micro drone. DetectorX addresses the challenges of object detector\nrobustness by incorporating two innovative modules: a stem block and a spiral\npooling technique. The stem block introduces a dynamic visual modality by\nleveraging the outputs of two Deep Convolutional Neural Network (DCNN) models.\nThe framework employs the proposed event-based reward reinforcement learning to\nconstrain the actions of a parent and child DCNN model leading to a reward.\nThis results in the induction of two dynamic visual modalities alongside the\nRed, Green, and Blue (RGB) data. This enhancement significantly augments\nDetectorX's perception and adaptability in diverse environmental situations.\nFurther, a spiral pooling technique, an online image augmentation method,\nstrengthens the framework by increasing feature representations by\nconcatenating spiraled and average/max pooled features. In three extensive\nexperiments: (1) comparative and (2) robustness, which use the Pacific\nEarthquake Engineering Research Hub ImageNet dataset, and (3) field-experiment,\nDetectorX performed satisfactorily across varying metrics, including precision\n(0.88), recall (0.84), average precision (0.91), mean average precision (0.76),\nand mean average recall (0.73), compared to the competing detectors including\nYou Only Look Once X-medium (YOLOX-m) and others. The study's findings indicate\nthat DetectorX can provide satisfactory results and demonstrate resilience in\nchallenging environments.\n","authors":["Isaac Osei Agyemanga","Liaoyuan Zeng","Jianwen Chena","Isaac Adjei-Mensah","Daniel Acheampong"],"pdf_url":"https://arxiv.org/pdf/2501.08807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19694v2","updated":"2025-01-15T13:53:29Z","published":"2024-07-29T04:33:04Z","title":"Structural damage detection via hierarchical damage information with\n  volumetric assessment","summary":"  Structural health monitoring (SHM) is essential for ensuring the safety and\nlongevity of infrastructure, but complex image environments, noisy labels, and\nreliance on manual damage assessments often hinder its effectiveness. This\nstudy introduces the Guided Detection Network (Guided-DetNet), a framework\ndesigned to address these challenges. Guided-DetNet is characterized by a\nGenerative Attention Module (GAM), Hierarchical Elimination Algorithm (HEA),\nand Volumetric Contour Visual Assessment (VCVA). GAM leverages cross-horizontal\nand cross-vertical patch merging and cross-foreground-background feature fusion\nto generate varied features to mitigate complex image environments. HEA\naddresses noisy labeling using hierarchical relationships among classes to\nrefine instances given an image by eliminating unlikely class instances. VCVA\nassesses the severity of detected damages via volumetric representation and\nquantification leveraging the Dirac delta distribution. A comprehensive\nquantitative study and two robustness tests were conducted using the PEER Hub\ndataset, and a drone-based application, which involved a field experiment, was\nconducted to substantiate Guided-DetNet's promising performances. In triple\nclassification tasks, the framework achieved 96% accuracy, surpassing\nstate-of-the-art classifiers by up to 3%. In dual detection tasks, it\noutperformed competitive detectors with a precision of 94% and a mean average\nprecision (mAP) of 79% while maintaining a frame rate of 57.04fps, suitable for\nreal-time applications. Additionally, robustness tests demonstrated resilience\nunder adverse conditions, with precision scores ranging from 79% to 91%.\nGuided-DetNet is established as a robust and efficient framework for SHM,\noffering advancements in automation and precision, with the potential for\nwidespread application in drone-based infrastructure inspections.\n","authors":["Isaac Osei Agyemang","Isaac Adjei-Mensah","Daniel Acheampong","Gordon Owusu Boateng","Adu Asare Baffour"],"pdf_url":"https://arxiv.org/pdf/2407.19694v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08799v1","updated":"2025-01-15T13:46:33Z","published":"2025-01-15T13:46:33Z","title":"Exploring ChatGPT for Face Presentation Attack Detection in Zero and\n  Few-Shot in-Context Learning","summary":"  This study highlights the potential of ChatGPT (specifically GPT-4o) as a\ncompetitive alternative for Face Presentation Attack Detection (PAD),\noutperforming several PAD models, including commercial solutions, in specific\nscenarios. Our results show that GPT-4o demonstrates high consistency,\nparticularly in few-shot in-context learning, where its performance improves as\nmore examples are provided (reference data). We also observe that detailed\nprompts enable the model to provide scores reliably, a behavior not observed\nwith concise prompts. Additionally, explanation-seeking prompts slightly\nenhance the model's performance by improving its interpretability. Remarkably,\nthe model exhibits emergent reasoning capabilities, correctly predicting the\nattack type (print or replay) with high accuracy in few-shot scenarios, despite\nnot being explicitly instructed to classify attack types. Despite these\nstrengths, GPT-4o faces challenges in zero-shot tasks, where its performance is\nlimited compared to specialized PAD systems. Experiments were conducted on a\nsubset of the SOTERIA dataset, ensuring compliance with data privacy\nregulations by using only data from consenting individuals. These findings\nunderscore GPT-4o's promise in PAD applications, laying the groundwork for\nfuture research to address broader data privacy concerns and improve\ncross-dataset generalization. Code available here:\nhttps://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad\n","authors":["Alain Komaty","Hatef Otroshi Shahreza","Anjith George","Sebastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2501.08799v1.pdf","comment":"Accepted in WACV workshop 2025"},{"id":"http://arxiv.org/abs/2412.16563v2","updated":"2025-01-15T13:34:12Z","published":"2024-12-21T10:16:07Z","title":"SemTalk: Holistic Co-speech Motion Generation with Frame-level Semantic\n  Emphasis","summary":"  A good co-speech motion generation cannot be achieved without a careful\nintegration of common rhythmic motion and rare yet essential semantic motion.\nIn this work, we propose SemTalk for holistic co-speech motion generation with\nframe-level semantic emphasis. Our key insight is to separately learn general\nmotions and sparse motions, and then adaptively fuse them. In particular,\nrhythmic consistency learning is explored to establish rhythm-related base\nmotion, ensuring a coherent foundation that synchronizes gestures with the\nspeech rhythm. Subsequently, textit{semantic emphasis learning is designed to\ngenerate semantic-aware sparse motion, focusing on frame-level semantic cues.\nFinally, to integrate sparse motion into the base motion and generate\nsemantic-emphasized co-speech gestures, we further leverage a learned semantic\nscore for adaptive synthesis. Qualitative and quantitative comparisons on two\npublic datasets demonstrate that our method outperforms the state-of-the-art,\ndelivering high-quality co-speech motion with enhanced semantic richness over a\nstable base motion.\n","authors":["Xiangyue Zhang","Jianfang Li","Jiaxu Zhang","Ziqiang Dang","Jianqiang Ren","Liefeng Bo","Zhigang Tu"],"pdf_url":"https://arxiv.org/pdf/2412.16563v2.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2501.02487v3","updated":"2025-01-15T13:07:56Z","published":"2025-01-05T09:40:58Z","title":"ACE++: Instruction-Based Image Creation and Editing via Context-Aware\n  Content Filling","summary":"  We report ACE++, an instruction-based diffusion framework that tackles\nvarious image generation and editing tasks. Inspired by the input format for\nthe inpainting task proposed by FLUX.1-Fill-dev, we improve the Long-context\nCondition Unit (LCU) introduced in ACE and extend this input paradigm to any\nediting and generation tasks. To take full advantage of image generative\npriors, we develop a two-stage training scheme to minimize the efforts of\nfinetuning powerful text-to-image diffusion models like FLUX.1-dev. In the\nfirst stage, we pre-train the model using task data with the 0-ref tasks from\nthe text-to-image model. There are many models in the community based on the\npost-training of text-to-image foundational models that meet this training\nparadigm of the first stage. For example, FLUX.1-Fill-dev deals primarily with\npainting tasks and can be used as an initialization to accelerate the training\nprocess. In the second stage, we finetune the above model to support the\ngeneral instructions using all tasks defined in ACE. To promote the widespread\napplication of ACE++ in different scenarios, we provide a comprehensive set of\nmodels that cover both full finetuning and lightweight finetuning, while\nconsidering general applicability and applicability in vertical scenarios. The\nqualitative analysis showcases the superiority of ACE++ in terms of generating\nimage quality and prompt following ability. Code and models will be available\non the project page: https://ali-vilab. github.io/ACE_plus_page/.\n","authors":["Chaojie Mao","Jingfeng Zhang","Yulin Pan","Zeyinzi Jiang","Zhen Han","Yu Liu","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.02487v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07047v2","updated":"2025-01-15T12:47:35Z","published":"2024-05-11T16:30:39Z","title":"Solving Energy-Independent Density for CT Metal Artifact Reduction via\n  Neural Representation","summary":"  X-ray CT often suffers from shadowing and streaking artifacts in the presence\nof metallic materials, which severely degrade imaging quality. Physically, the\nlinear attenuation coefficients (LACs) of metals vary significantly with X-ray\nenergy, causing a nonlinear beam hardening effect (BHE) in CT measurements.\nReconstructing CT images from metal-corrupted measurements consequently becomes\na challenging nonlinear inverse problem. Existing state-of-the-art (SOTA) metal\nartifact reduction (MAR) algorithms rely on supervised learning with numerous\npaired CT samples. While promising, these supervised methods often assume that\nthe unknown LACs are energy-independent, ignoring the energy-induced BHE, which\nresults in limited generalization. Moreover, the requirement for large datasets\nalso limits their applications in real-world scenarios. In this work, we\npropose Density neural representation (Diner), a novel unsupervised MAR method.\nOur key innovation lies in formulating MAR as an energy-independent density\nreconstruction problem that strictly adheres to the photon-tissue absorption\nphysical model. This model is inherently nonlinear and complex, making it a\nrarely considered approach in inverse imaging problems. By introducing the\nwater-equivalent tissues approximation and a new polychromatic model to\ncharacterize the nonlinear CT acquisition process, we directly learn the neural\nrepresentation of the density map from raw measurements without using external\ntraining data. This energy-independent density reconstruction framework\nfundamentally resolves the nonlinear BHE, enabling superior MAR performance\nacross a wide range of scanning scenarios. Extensive experiments on both\nsimulated and real-world datasets demonstrate the superiority of our\nunsupervised Diner over popular supervised methods in terms of MAR performance\nand robustness.\n","authors":["Qing Wu","Xu Guo","Lixuan Chen","Yanyan Liu","Dongming He","Xudong Wang","Xueli Chen","Yifeng Zhang","S. Kevin Zhou","Jingyi Yu","Yuyao Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.07047v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2312.17345v2","updated":"2025-01-15T12:46:07Z","published":"2023-12-28T20:26:03Z","title":"3VL: Using Trees to Improve Vision-Language Models' Interpretability","summary":"  Vision-Language models (VLMs) have proven to be effective at aligning image\nand text representations, producing superior zero-shot results when transferred\nto many downstream tasks. However, these representations suffer from some key\nshortcomings in understanding Compositional Language Concepts (CLC), such as\nrecognizing objects' attributes, states, and relations between different\nobjects. Moreover, VLMs typically have poor interpretability, making it\nchallenging to debug and mitigate compositional-understanding failures. In this\nwork, we introduce the architecture and training technique of Tree-augmented\nVision-Language (3VL) model accompanied by our proposed Anchor inference method\nand Differential Relevance (DiRe) interpretability tool. By expanding the text\nof an arbitrary image-text pair into a hierarchical tree structure using\nlanguage analysis tools, 3VL allows the induction of this structure into the\nvisual representation learned by the model, enhancing its interpretability and\ncompositional reasoning. Additionally, we show how Anchor, a simple technique\nfor text unification, can be used to filter nuisance factors while increasing\nCLC understanding performance, e.g., on the fundamental VL-Checklist benchmark.\nWe also show how DiRe, which performs a differential comparison between VLM\nrelevancy maps, enables us to generate compelling visualizations of the reasons\nfor a model's success or failure. Our code is available at:\nhttps://github.com/niryellinek/3VL.\n","authors":["Nir Yellinek","Leonid Karlinsky","Raja Giryes"],"pdf_url":"https://arxiv.org/pdf/2312.17345v2.pdf","comment":"accepted to IEEE TIP"},{"id":"http://arxiv.org/abs/2501.08771v1","updated":"2025-01-15T12:44:52Z","published":"2025-01-15T12:44:52Z","title":"Admitting Ignorance Helps the Video Question Answering Models to Answer","summary":"  Significant progress has been made in the field of video question answering\n(VideoQA) thanks to deep learning and large-scale pretraining. Despite the\npresence of sophisticated model structures and powerful video-text foundation\nmodels, most existing methods focus solely on maximizing the correlation\nbetween answers and video-question pairs during training. We argue that these\nmodels often establish shortcuts, resulting in spurious correlations between\nquestions and answers, especially when the alignment between video and text\ndata is suboptimal. To address these spurious correlations, we propose a novel\ntraining framework in which the model is compelled to acknowledge its ignorance\nwhen presented with an intervened question, rather than making guesses solely\nbased on superficial question-answer correlations. We introduce methodologies\nfor intervening in questions, utilizing techniques such as displacement and\nperturbation, and design frameworks for the model to admit its lack of\nknowledge in both multi-choice VideoQA and open-ended settings. In practice, we\nintegrate a state-of-the-art model into our framework to validate its\neffectiveness. The results clearly demonstrate that our framework can\nsignificantly enhance the performance of VideoQA models with minimal structural\nmodifications.\n","authors":["Haopeng Li","Tom Drummond","Mingming Gong","Mohammed Bennamoun","Qiuhong Ke"],"pdf_url":"https://arxiv.org/pdf/2501.08771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06406v2","updated":"2025-01-15T12:36:24Z","published":"2024-03-11T03:35:41Z","title":"When No-Reference Image Quality Models Meet MAP Estimation in Diffusion\n  Latents","summary":"  Contemporary no-reference image quality assessment (NR-IQA) models can\neffectively quantify perceived image quality, often achieving strong\ncorrelations with human perceptual scores on standard IQA benchmarks. Yet,\nlimited efforts have been devoted to treating NR-IQA models as natural image\npriors for real-world image enhancement, and consequently comparing them from a\nperceptual optimization standpoint. In this work, we show -- for the first time\n-- that NR-IQA models can be plugged into the maximum a posteriori (MAP)\nestimation framework for image enhancement. This is achieved by performing\ngradient ascent in the diffusion latent space rather than in the raw pixel\ndomain, leveraging a pretrained differentiable and bijective diffusion process.\nLikely, different NR-IQA models lead to different enhanced outputs, which in\nturn provides a new computational means of comparing them. Unlike conventional\ncorrelation-based measures, our comparison method offers complementary insights\ninto the respective strengths and weaknesses of the competing NR-IQA models in\nperceptual optimization scenarios. Additionally, we aim to improve the\nbest-performing NR-IQA model in diffusion latent MAP estimation by\nincorporating the advantages of other top-performing methods. The resulting\nmodel delivers noticeably better results in enhancing real-world images\nafflicted by unknown and complex distortions, all preserving a high degree of\nimage fidelity.\n","authors":["Weixia Zhang","Dingquan Li","Guangtao Zhai","Xiaokang Yang","Kede Ma"],"pdf_url":"https://arxiv.org/pdf/2403.06406v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08763v1","updated":"2025-01-15T12:33:11Z","published":"2025-01-15T12:33:11Z","title":"Few-Shot Learner Generalizes Across AI-Generated Image Detection","summary":"  Current fake image detectors trained on large synthetic image datasets\nperform satisfactorily on limited studied generative models. However, they\nsuffer a notable performance decline over unseen models. Besides, collecting\nadequate training data from online generative models is often expensive or\ninfeasible. To overcome these issues, we propose Few-Shot Detector (FSD), a\nnovel AI-generated image detector which learns a specialized metric space to\neffectively distinguish unseen fake images by utilizing very few samples.\nExperiments show FSD achieves state-of-the-art performance by $+7.4\\%$ average\nACC on GenImage dataset. More importantly, our method is better capable of\ncapturing the intra-category common features in unseen images without further\ntraining.\n","authors":["Shiyu Wu","Jing Liu","Jing Li","Yequan Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08763v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.01505v4","updated":"2025-01-15T12:31:57Z","published":"2024-01-03T02:22:34Z","title":"Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex\n  and Professional Sports","summary":"  Reasoning over sports videos for question answering is an important task with\nnumerous applications, such as player training and information retrieval.\nHowever, this task has not been explored due to the lack of relevant datasets\nand the challenging nature it presents. Most datasets for video question\nanswering (VideoQA) focus mainly on general and coarse-grained understanding of\ndaily-life videos, which is not applicable to sports scenarios requiring\nprofessional action understanding and fine-grained motion analysis. In this\npaper, we introduce the first dataset, named Sports-QA, specifically designed\nfor the sports VideoQA task. The Sports-QA dataset includes various types of\nquestions, such as descriptions, chronologies, causalities, and counterfactual\nconditions, covering multiple sports. Furthermore, to address the\ncharacteristics of the sports VideoQA task, we propose a new Auto-Focus\nTransformer (AFT) capable of automatically focusing on particular scales of\ntemporal information for question answering. We conduct extensive experiments\non Sports-QA, including baseline studies and the evaluation of different\nmethods. The results demonstrate that our AFT achieves state-of-the-art\nperformance.\n","authors":["Haopeng Li","Andong Deng","Jun Liu","Hossein Rahmani","Yulan Guo","Bernt Schiele","Mohammed Bennamoun","Qiuhong Ke"],"pdf_url":"https://arxiv.org/pdf/2401.01505v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08002v2","updated":"2025-01-15T11:52:29Z","published":"2025-01-14T10:46:41Z","title":"Maximizing Uncertainty for Federated learning via Bayesian\n  Optimisation-based Model Poisoning","summary":"  As we transition from Narrow Artificial Intelligence towards Artificial Super\nIntelligence, users are increasingly concerned about their privacy and the\ntrustworthiness of machine learning (ML) technology. A common denominator for\nthe metrics of trustworthiness is the quantification of uncertainty inherent in\nDL algorithms, and specifically in the model parameters, input data, and model\npredictions. One of the common approaches to address privacy-related issues in\nDL is to adopt distributed learning such as federated learning (FL), where\nprivate raw data is not shared among users. Despite the privacy-preserving\nmechanisms in FL, it still faces challenges in trustworthiness. Specifically,\nthe malicious users, during training, can systematically create malicious model\nparameters to compromise the models predictive and generative capabilities,\nresulting in high uncertainty about their reliability. To demonstrate malicious\nbehaviour, we propose a novel model poisoning attack method named Delphi which\naims to maximise the uncertainty of the global model output. We achieve this by\ntaking advantage of the relationship between the uncertainty and the model\nparameters of the first hidden layer of the local model. Delphi employs two\ntypes of optimisation , Bayesian Optimisation and Least Squares Trust Region,\nto search for the optimal poisoned model parameters, named as Delphi-BO and\nDelphi-LSTR. We quantify the uncertainty using the KL Divergence to minimise\nthe distance of the predictive probability distribution towards an uncertain\ndistribution of model output. Furthermore, we establish a mathematical proof\nfor the attack effectiveness demonstrated in FL. Numerical results demonstrate\nthat Delphi-BO induces a higher amount of uncertainty than Delphi-LSTR\nhighlighting vulnerability of FL systems to model poisoning attacks.\n","authors":["Marios Aristodemou","Xiaolan Liu","Yuan Wang","Konstantinos G. Kyriakopoulos","Sangarapillai Lambotharan","Qingsong Wei"],"pdf_url":"https://arxiv.org/pdf/2501.08002v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2402.12238v2","updated":"2025-01-15T11:52:13Z","published":"2024-02-19T15:48:55Z","title":"MGF: Mixed Gaussian Flow for Diverse Trajectory Prediction","summary":"  To predict future trajectories, the normalizing flow with a standard Gaussian\nprior suffers from weak diversity. The ineffectiveness comes from the conflict\nbetween the fact of asymmetric and multi-modal distribution of likely outcomes\nand symmetric and single-modal original distribution and supervision losses.\nInstead, we propose constructing a mixed Gaussian prior for a normalizing flow\nmodel for trajectory prediction. The prior is constructed by analyzing the\ntrajectory patterns in the training samples without requiring extra annotations\nwhile showing better expressiveness and being multi-modal and asymmetric.\nBesides diversity, it also provides better controllability for probabilistic\ntrajectory generation. We name our method Mixed Gaussian Flow (MGF). It\nachieves state-of-the-art performance in the evaluation of both trajectory\nalignment and diversity on the popular UCY/ETH and SDD datasets. Code is\navailable at https://github.com/mulplue/MGF.\n","authors":["Jiahe Chen","Jinkun Cao","Dahua Lin","Kris Kitani","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2402.12238v2.pdf","comment":"Accepted by Neurips 2024. Code: https://github.com/mulplue/MGF"},{"id":"http://arxiv.org/abs/2407.11664v3","updated":"2025-01-15T11:51:19Z","published":"2024-07-16T12:36:26Z","title":"Mask-guided cross-image attention for zero-shot in-silico\n  histopathologic image generation with a diffusion model","summary":"  Creating in-silico data with generative AI promises a cost-effective\nalternative to staining, imaging, and annotating whole slide images in\ncomputational pathology. Diffusion models are the state-of-the-art solution for\ngenerating in-silico images, offering unparalleled fidelity and realism. Using\nappearance transfer diffusion models allows for zero-shot image generation,\nfacilitating fast application and making model training unnecessary. However\ncurrent appearance transfer diffusion models are designed for natural images,\nwhere the main task is to transfer the foreground object from an origin to a\ntarget domain, while the background is of insignificant importance. In\ncomputational pathology, specifically in oncology, it is however not\nstraightforward to define which objects in an image should be classified as\nforeground and background, as all objects in an image may be of critical\nimportance for the detailed understanding the tumor micro-environment. We\ncontribute to the applicability of appearance transfer diffusion models to\nimmunohistochemistry-stained images by modifying the appearance transfer\nguidance to alternate between class-specific AdaIN feature statistics matchings\nusing existing segmentation masks. The performance of the proposed method is\ndemonstrated on the downstream task of supervised epithelium segmentation,\nshowing that the number of manual annotations required for model training can\nbe reduced by 75%, outperforming the baseline approach. Additionally, we\nconsulted with a certified pathologist to investigate future improvements. We\nanticipate this work to inspire the application of zero-shot diffusion models\nin computational pathology, providing an efficient method to generate in-silico\nimages with unmatched fidelity and realism, which prove meaningful for\ndownstream tasks, such as training existing deep learning models or finetuning\nfoundation models.\n","authors":["Dominik Winter","Nicolas Triltsch","Marco Rosati","Anatoliy Shumilov","Ziya Kokaragac","Yuri Popov","Thomas Padel","Laura Sebastian Monasor","Ross Hill","Markus Schick","Nicolas Brieu"],"pdf_url":"https://arxiv.org/pdf/2407.11664v3.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2501.08717v1","updated":"2025-01-15T10:58:32Z","published":"2025-01-15T10:58:32Z","title":"$\\texttt{InfoHier}$: Hierarchical Information Extraction via Encoding\n  and Embedding","summary":"  Analyzing large-scale datasets, especially involving complex and\nhigh-dimensional data like images, is particularly challenging. While\nself-supervised learning (SSL) has proven effective for learning\nrepresentations from unlabelled data, it typically focuses on flat,\nnon-hierarchical structures, missing the multi-level relationships present in\nmany real-world datasets. Hierarchical clustering (HC) can uncover these\nrelationships by organizing data into a tree-like structure, but it often\nrelies on rigid similarity metrics that struggle to capture the complexity of\ndiverse data types. To address these we envision $\\texttt{InfoHier}$, a\nframework that combines SSL with HC to jointly learn robust latent\nrepresentations and hierarchical structures. This approach leverages SSL to\nprovide adaptive representations, enhancing HC's ability to capture complex\npatterns. Simultaneously, it integrates HC loss to refine SSL training,\nresulting in representations that are more attuned to the underlying\ninformation hierarchy. $\\texttt{InfoHier}$ has the potential to improve the\nexpressiveness and performance of both clustering and representation learning,\noffering significant benefits for data analysis, management, and information\nretrieval.\n","authors":["Tianru Zhang","Li Ju","Prashant Singh","Salman Toor"],"pdf_url":"https://arxiv.org/pdf/2501.08717v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2501.08712v1","updated":"2025-01-15T10:54:21Z","published":"2025-01-15T10:54:21Z","title":"Self-supervised Transformation Learning for Equivariant Representations","summary":"  Unsupervised representation learning has significantly advanced various\nmachine learning tasks. In the computer vision domain, state-of-the-art\napproaches utilize transformations like random crop and color jitter to achieve\ninvariant representations, embedding semantically the same inputs despite\ntransformations. However, this can degrade performance in tasks requiring\nprecise features, such as localization or flower classification. To address\nthis, recent research incorporates equivariant representation learning, which\ncaptures transformation-sensitive information. However, current methods depend\non transformation labels and thus struggle with interdependency and complex\ntransformations. We propose Self-supervised Transformation Learning (STL),\nreplacing transformation labels with transformation representations derived\nfrom image pairs. The proposed method ensures transformation representation is\nimage-invariant and learns corresponding equivariant transformations, enhancing\nperformance without increased batch complexity. We demonstrate the approach's\neffectiveness across diverse classification and detection tasks, outperforming\nexisting methods in 7 out of 11 benchmarks and excelling in detection. By\nintegrating complex transformations like AugMix, unusable by prior equivariant\nmethods, this approach enhances performance across tasks, underscoring its\nadaptability and resilience. Additionally, its compatibility with various base\nmodels highlights its flexibility and broad applicability. The code is\navailable at https://github.com/jaemyung-u/stl.\n","authors":["Jaemyung Yu","Jaehyun Choi","Dong-Jae Lee","HyeongGwon Hong","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2501.08712v1.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)"},{"id":"http://arxiv.org/abs/2501.08115v2","updated":"2025-01-15T10:05:39Z","published":"2025-01-14T13:46:07Z","title":"RoHan: Robust Hand Detection in Operation Room","summary":"  Hand-specific localization has garnered significant interest within the\ncomputer vision community. Although there are numerous datasets with hand\nannotations from various angles and settings, domain transfer techniques\nfrequently struggle in surgical environments. This is mainly due to the limited\navailability of gloved hand instances and the unique challenges of operating\nrooms (ORs). Thus, hand-detection models tailored to OR settings require\nextensive training and expensive annotation processes. To overcome these\nchallenges, we present \"RoHan\" - a novel approach for robust hand detection in\nthe OR, leveraging advanced semi-supervised domain adaptation techniques to\ntackle the challenges of varying recording conditions, diverse glove colors,\nand occlusions common in surgical settings. Our methodology encompasses two\nmain stages: (1) data augmentation strategy that utilizes \"Artificial Gloves,\"\na method for augmenting publicly available hand datasets with synthetic images\nof hands-wearing gloves; (2) semi-supervised domain adaptation pipeline that\nimproves detection performance in real-world OR settings through iterative\nprediction refinement and efficient frame filtering. We evaluate our method\nusing two datasets: simulated enterotomy repair and saphenous vein graft\nharvesting. \"RoHan\" substantially reduces the need for extensive labeling and\nmodel training, paving the way for the practical implementation of hand\ndetection technologies in medical settings.\n","authors":["Roi Papo","Sapir Gershov","Tom Friedman","Itay Or","Gil Bolotin","Shlomi Laufer"],"pdf_url":"https://arxiv.org/pdf/2501.08115v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2410.05301v2","updated":"2025-01-15T09:42:42Z","published":"2024-10-04T12:22:54Z","title":"Diffusion-based Unsupervised Audio-visual Speech Enhancement","summary":"  This paper proposes a new unsupervised audio-visual speech enhancement (AVSE)\napproach that combines a diffusion-based audio-visual speech generative model\nwith a non-negative matrix factorization (NMF) noise model. First, the\ndiffusion model is pre-trained on clean speech conditioned on corresponding\nvideo data to simulate the speech generative distribution. This pre-trained\nmodel is then paired with the NMF-based noise model to estimate clean speech\niteratively. Specifically, a diffusion-based posterior sampling approach is\nimplemented within the reverse diffusion process, where after each iteration, a\nspeech estimate is obtained and used to update the noise parameters.\nExperimental results confirm that the proposed AVSE approach not only\noutperforms its audio-only counterpart but also generalizes better than a\nrecent supervised-generative AVSE method. Additionally, the new inference\nalgorithm offers a better balance between inference speed and performance\ncompared to the previous diffusion-based method. Code and demo available at:\nhttps://jeaneudesayilo.github.io/fast_UdiffSE\n","authors":["Jean-Eudes Ayilo","Mostafa Sadeghi","Romain Serizel","Xavier Alameda-Pineda"],"pdf_url":"https://arxiv.org/pdf/2410.05301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06787v2","updated":"2025-01-15T09:39:03Z","published":"2025-01-12T11:54:46Z","title":"Improving Pain Classification using Spatio-Temporal Deep Learning\n  Approaches with Facial Expressions","summary":"  Pain management and severity detection are crucial for effective treatment,\nyet traditional self-reporting methods are subjective and may be unsuitable for\nnon-verbal individuals (people with limited speaking skills). To address this\nlimitation, we explore automated pain detection using facial expressions. Our\nstudy leverages deep learning techniques to improve pain assessment by\nanalyzing facial images from the Pain Emotion Faces Database (PEMF). We propose\ntwo novel approaches1: (1) a hybrid ConvNeXt model combined with Long\nShort-Term Memory (LSTM) blocks to analyze video frames and predict pain\npresence, and (2) a Spatio-Temporal Graph Convolution Network (STGCN)\nintegrated with LSTM to process landmarks from facial images for pain\ndetection. Our work represents the first use of the PEMF dataset for binary\npain classification and demonstrates the effectiveness of these models through\nextensive experimentation. The results highlight the potential of combining\nspatial and temporal features for enhanced pain detection, offering a promising\nadvancement in objective pain assessment methodologies.\n","authors":["Aafaf Ridouan","Amine Bohi","Youssef Mourchid"],"pdf_url":"https://arxiv.org/pdf/2501.06787v2.pdf","comment":"8 pages, 3 figures, 3 tables. Accepted and presented at the 18th\n  International Conference on Machine Vision (ICMV 2024), Edinburgh, UK"},{"id":"http://arxiv.org/abs/2501.08682v1","updated":"2025-01-15T09:22:38Z","published":"2025-01-15T09:22:38Z","title":"RealVVT: Towards Photorealistic Video Virtual Try-on via Spatio-Temporal\n  Consistency","summary":"  Virtual try-on has emerged as a pivotal task at the intersection of computer\nvision and fashion, aimed at digitally simulating how clothing items fit on the\nhuman body. Despite notable progress in single-image virtual try-on (VTO),\ncurrent methodologies often struggle to preserve a consistent and authentic\nappearance of clothing across extended video sequences. This challenge arises\nfrom the complexities of capturing dynamic human pose and maintaining target\nclothing characteristics. We leverage pre-existing video foundation models to\nintroduce RealVVT, a photoRealistic Video Virtual Try-on framework tailored to\nbolster stability and realism within dynamic video contexts. Our methodology\nencompasses a Clothing & Temporal Consistency strategy, an Agnostic-guided\nAttention Focus Loss mechanism to ensure spatial consistency, and a Pose-guided\nLong Video VTO technique adept at handling extended video sequences.Extensive\nexperiments across various datasets confirms that our approach outperforms\nexisting state-of-the-art models in both single-image and video VTO tasks,\noffering a viable solution for practical applications within the realms of\nfashion e-commerce and virtual fitting environments.\n","authors":["Siqi Li","Zhengkai Jiang","Jiawei Zhou","Zhihong Liu","Xiaowei Chi","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08682v1.pdf","comment":"10 pages (8 pages main text, 2 pages references), 5 figures in the\n  main text, and 4 pages supplementary materials with 3 additional figures"},{"id":"http://arxiv.org/abs/2501.08676v1","updated":"2025-01-15T09:07:12Z","published":"2025-01-15T09:07:12Z","title":"FlexiClip: Locality-Preserving Free-Form Character Animation","summary":"  Animating clipart images with seamless motion while maintaining visual\nfidelity and temporal coherence presents significant challenges. Existing\nmethods, such as AniClipart, effectively model spatial deformations but often\nfail to ensure smooth temporal transitions, resulting in artifacts like abrupt\nmotions and geometric distortions. Similarly, text-to-video (T2V) and\nimage-to-video (I2V) models struggle to handle clipart due to the mismatch in\nstatistical properties between natural video and clipart styles. This paper\nintroduces FlexiClip, a novel approach designed to overcome these limitations\nby addressing the intertwined challenges of temporal consistency and geometric\nintegrity. FlexiClip extends traditional B\\'ezier curve-based trajectory\nmodeling with key innovations: temporal Jacobians to correct motion dynamics\nincrementally, continuous-time modeling via probability flow ODEs (pfODEs) to\nmitigate temporal noise, and a flow matching loss inspired by GFlowNet\nprinciples to optimize smooth motion transitions. These enhancements ensure\ncoherent animations across complex scenarios involving rapid movements and\nnon-rigid deformations. Extensive experiments validate the effectiveness of\nFlexiClip in generating animations that are not only smooth and natural but\nalso structurally consistent across diverse clipart types, including humans and\nanimals. By integrating spatial and temporal modeling with pre-trained video\ndiffusion models, FlexiClip sets a new standard for high-quality clipart\nanimation, offering robust performance across a wide range of visual content.\nProject Page: https://creative-gen.github.io/flexiclip.github.io/\n","authors":["Anant Khandelwal"],"pdf_url":"https://arxiv.org/pdf/2501.08676v1.pdf","comment":"13 pages, 4 figures, 7 tables"},{"id":"http://arxiv.org/abs/2501.08672v1","updated":"2025-01-15T09:04:56Z","published":"2025-01-15T09:04:56Z","title":"GS-LIVO: Real-Time LiDAR, Inertial, and Visual Multi-sensor Fused\n  Odometry with Gaussian Mapping","summary":"  In recent years, 3D Gaussian splatting (3D-GS) has emerged as a novel scene\nrepresentation approach. However, existing vision-only 3D-GS methods often rely\non hand-crafted heuristics for point-cloud densification and face challenges in\nhandling occlusions and high GPU memory and computation consumption.\nLiDAR-Inertial-Visual (LIV) sensor configuration has demonstrated superior\nperformance in localization and dense mapping by leveraging complementary\nsensing characteristics: rich texture information from cameras, precise\ngeometric measurements from LiDAR, and high-frequency motion data from IMU.\nInspired by this, we propose a novel real-time Gaussian-based simultaneous\nlocalization and mapping (SLAM) system. Our map system comprises a global\nGaussian map and a sliding window of Gaussians, along with an IESKF-based\nodometry. The global Gaussian map consists of hash-indexed voxels organized in\na recursive octree, effectively covering sparse spatial volumes while adapting\nto different levels of detail and scales. The Gaussian map is initialized\nthrough multi-sensor fusion and optimized with photometric gradients. Our\nsystem incrementally maintains a sliding window of Gaussians, significantly\nreducing GPU computation and memory consumption by only optimizing the map\nwithin the sliding window. Moreover, we implement a tightly coupled\nmulti-sensor fusion odometry with an iterative error state Kalman filter\n(IESKF), leveraging real-time updating and rendering of the Gaussian map. Our\nsystem represents the first real-time Gaussian-based SLAM framework deployable\non resource-constrained embedded systems, demonstrated on the NVIDIA Jetson\nOrin NX platform. The framework achieves real-time performance while\nmaintaining robust multi-sensor fusion capabilities. All implementation\nalgorithms, hardware designs, and CAD models will be publicly available.\n","authors":["Sheng Hong","Chunran Zheng","Yishu Shen","Changze Li","Fu Zhang","Tong Qin","Shaojie Shen"],"pdf_url":"https://arxiv.org/pdf/2501.08672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08667v1","updated":"2025-01-15T09:02:04Z","published":"2025-01-15T09:02:04Z","title":"TimeFlow: Longitudinal Brain Image Registration and Aging Progression\n  Analysis","summary":"  Predicting future brain states is crucial for understanding healthy aging and\nneurodegenerative diseases. Longitudinal brain MRI registration, a cornerstone\nfor such analyses, has long been limited by its inability to forecast future\ndevelopments, reliance on extensive, dense longitudinal data, and the need to\nbalance registration accuracy with temporal smoothness. In this work, we\npresent \\emph{TimeFlow}, a novel framework for longitudinal brain MRI\nregistration that overcomes all these challenges. Leveraging a U-Net\narchitecture with temporal conditioning inspired by diffusion models, TimeFlow\nenables accurate longitudinal registration and facilitates prospective analyses\nthrough future image prediction. Unlike traditional methods that depend on\nexplicit smoothness regularizers and dense sequential data, TimeFlow achieves\ntemporal consistency and continuity without these constraints. Experimental\nresults highlight its superior performance in both future timepoint prediction\nand registration accuracy compared to state-of-the-art methods. Additionally,\nTimeFlow supports novel biological brain aging analyses, effectively\ndifferentiating neurodegenerative conditions from healthy aging. It eliminates\nthe need for segmentation, thereby avoiding the challenges of non-trivial\nannotation and inconsistent segmentation errors. TimeFlow paves the way for\naccurate, data-efficient, and annotation-free prospective analyses of brain\naging and chronic diseases.\n","authors":["Bailiang Jian","Jiazhen Pan","Yitong Li","Fabian Bongratz","Ruochen Li","Daniel Rueckert","Benedikt Wiestler","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2501.08667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08665v1","updated":"2025-01-15T09:00:32Z","published":"2025-01-15T09:00:32Z","title":"A Survey on Facial Image Privacy Preservation in Cloud-Based Services","summary":"  Facial recognition models are increasingly employed by commercial\nenterprises, government agencies, and cloud service providers for identity\nverification, consumer services, and surveillance. These models are often\ntrained using vast amounts of facial data processed and stored in cloud-based\nplatforms, raising significant privacy concerns. Users' facial images may be\nexploited without their consent, leading to potential data breaches and misuse.\nThis survey presents a comprehensive review of current methods aimed at\npreserving facial image privacy in cloud-based services. We categorize these\nmethods into two primary approaches: image obfuscation-based protection and\nadversarial perturbation-based protection. We provide an in-depth analysis of\nboth categories, offering qualitative and quantitative comparisons of their\neffectiveness. Additionally, we highlight unresolved challenges and propose\nfuture research directions to improve privacy preservation in cloud computing\nenvironments.\n","authors":["Chen Chen","Mengyuan Sun","Xueluan Gong","Yanjiao Chen","Qian Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08662v1","updated":"2025-01-15T08:57:41Z","published":"2025-01-15T08:57:41Z","title":"Product of Gaussian Mixture Diffusion Model for non-linear MRI Inversion","summary":"  Diffusion models have recently shown remarkable results in magnetic resonance\nimaging reconstruction. However, the employed networks typically are black-box\nestimators of the (smoothed) prior score with tens of millions of parameters,\nrestricting interpretability and increasing reconstruction time. Furthermore,\nparallel imaging reconstruction algorithms either rely on off-line coil\nsensitivity estimation, which is prone to misalignment and restricting sampling\ntrajectories, or perform per-coil reconstruction, making the computational cost\nproportional to the number of coils. To overcome this, we jointly reconstruct\nthe image and the coil sensitivities using the lightweight,\nparameter-efficient, and interpretable product of Gaussian mixture diffusion\nmodel as an image prior and a classical smoothness priors on the coil\nsensitivities. The proposed method delivers promising results while allowing\nfor fast inference and demonstrating robustness to contrast out-of-distribution\ndata and sampling trajectories, comparable to classical variational penalties\nsuch as total variation. Finally, the probabilistic formulation allows the\ncalculation of the posterior expectation and pixel-wise variance.\n","authors":["Laurenz Nagler","Martin Zach","Thomas Pock"],"pdf_url":"https://arxiv.org/pdf/2501.08662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08659v1","updated":"2025-01-15T08:50:52Z","published":"2025-01-15T08:50:52Z","title":"BRIGHT-VO: Brightness-Guided Hybrid Transformer for Visual Odometry with\n  Multi-modality Refinement Module","summary":"  Visual odometry (VO) plays a crucial role in autonomous driving, robotic\nnavigation, and other related tasks by estimating the position and orientation\nof a camera based on visual input. Significant progress has been made in\ndata-driven VO methods, particularly those leveraging deep learning techniques\nto extract image features and estimate camera poses. However, these methods\noften struggle in low-light conditions because of the reduced visibility of\nfeatures and the increased difficulty of matching keypoints. To address this\nlimitation, we introduce BrightVO, a novel VO model based on Transformer\narchitecture, which not only performs front-end visual feature extraction, but\nalso incorporates a multi-modality refinement module in the back-end that\nintegrates Inertial Measurement Unit (IMU) data. Using pose graph optimization,\nthis module iteratively refines pose estimates to reduce errors and improve\nboth accuracy and robustness. Furthermore, we create a synthetic low-light\ndataset, KiC4R, which includes a variety of lighting conditions to facilitate\nthe training and evaluation of VO frameworks in challenging environments.\nExperimental results demonstrate that BrightVO achieves state-of-the-art\nperformance on both the KiC4R dataset and the KITTI benchmarks. Specifically,\nit provides an average improvement of 20% in pose estimation accuracy in normal\noutdoor environments and 259% in low-light conditions, outperforming existing\nmethods. For widespread use and further development, the research work is fully\nopen-source at https://github.com/Anastasiawd/BrightVO.\n","authors":["Dongzhihan Wang","Yang Yang","Liang Xu"],"pdf_url":"https://arxiv.org/pdf/2501.08659v1.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2501.08654v1","updated":"2025-01-15T08:43:48Z","published":"2025-01-15T08:43:48Z","title":"StereoGen: High-quality Stereo Image Generation from a Single Image","summary":"  State-of-the-art supervised stereo matching methods have achieved amazing\nresults on various benchmarks. However, these data-driven methods suffer from\ngeneralization to real-world scenarios due to the lack of real-world annotated\ndata. In this paper, we propose StereoGen, a novel pipeline for high-quality\nstereo image generation. This pipeline utilizes arbitrary single images as left\nimages and pseudo disparities generated by a monocular depth estimation model\nto synthesize high-quality corresponding right images. Unlike previous methods\nthat fill the occluded area in warped right images using random backgrounds or\nusing convolutions to take nearby pixels selectively, we fine-tune a diffusion\ninpainting model to recover the background. Images generated by our model\npossess better details and undamaged semantic structures. Besides, we propose\nTraining-free Confidence Generation and Adaptive Disparity Selection. The\nformer suppresses the negative effect of harmful pseudo ground truth during\nstereo training, while the latter helps generate a wider disparity distribution\nand better synthetic images. Experiments show that models trained under our\npipeline achieve state-of-the-art zero-shot generalization results among all\npublished methods. The code will be available upon publication of the paper.\n","authors":["Xianqi Wang","Hao Yang","Gangwei Xu","Junda Cheng","Min Lin","Yong Deng","Jinliang Zang","Yurui Chen","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2501.08654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02640v3","updated":"2025-01-15T08:41:38Z","published":"2025-01-05T20:05:10Z","title":"Multispectral Pedestrian Detection with Sparsely Annotated Label","summary":"  Although existing Sparsely Annotated Object Detection (SAOD) approches have\nmade progress in handling sparsely annotated environments in multispectral\ndomain, where only some pedestrians are annotated, they still have the\nfollowing limitations: (i) they lack considerations for improving the quality\nof pseudo-labels for missing annotations, and (ii) they rely on fixed ground\ntruth annotations, which leads to learning only a limited range of pedestrian\nvisual appearances in the multispectral domain. To address these issues, we\npropose a novel framework called Sparsely Annotated Multispectral Pedestrian\nDetection (SAMPD). For limitation (i), we introduce Multispectral\nPedestrian-aware Adaptive Weight (MPAW) and Positive Pseudo-label Enhancement\n(PPE) module. Utilizing multispectral knowledge, these modules ensure the\ngeneration of high-quality pseudo-labels and enable effective learning by\nincreasing weights for high-quality pseudo-labels based on modality\ncharacteristics. To address limitation (ii), we propose an Adaptive Pedestrian\nRetrieval Augmentation (APRA) module, which adaptively incorporates pedestrian\npatches from ground-truth and dynamically integrates high-quality pseudo-labels\nwith the ground-truth, facilitating a more diverse learning pool of\npedestrians. Extensive experimental results demonstrate that our SAMPD\nsignificantly enhances performance in sparsely annotated environments within\nthe multispectral domain.\n","authors":["Chan Lee","Seungho Shin","Gyeong-Moon Park","Jung Uk Kim"],"pdf_url":"https://arxiv.org/pdf/2501.02640v3.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2501.08649v1","updated":"2025-01-15T08:24:35Z","published":"2025-01-15T08:24:35Z","title":"Joint Learning of Depth and Appearance for Portrait Image Animation","summary":"  2D portrait animation has experienced significant advancements in recent\nyears. Much research has utilized the prior knowledge embedded in large\ngenerative diffusion models to enhance high-quality image manipulation.\nHowever, most methods only focus on generating RGB images as output, and the\nco-generation of consistent visual plus 3D output remains largely\nunder-explored. In our work, we propose to jointly learn the visual appearance\nand depth simultaneously in a diffusion-based portrait image generator. Our\nmethod embraces the end-to-end diffusion paradigm and introduces a new\narchitecture suitable for learning this conditional joint distribution,\nconsisting of a reference network and a channel-expanded diffusion backbone.\nOnce trained, our framework can be efficiently adapted to various downstream\napplications, such as facial depth-to-image and image-to-depth generation,\nportrait relighting, and audio-driven talking head animation with consistent 3D\noutput.\n","authors":["Xinya Ji","Gaspard Zoss","Prashanth Chandran","Lingchen Yang","Xun Cao","Barbara Solenthaler","Derek Bradley"],"pdf_url":"https://arxiv.org/pdf/2501.08649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08643v1","updated":"2025-01-15T08:11:24Z","published":"2025-01-15T08:11:24Z","title":"MonSter: Marry Monodepth to Stereo Unleashes Power","summary":"  Stereo matching recovers depth from image correspondences. Existing methods\nstruggle to handle ill-posed regions with limited matching cues, such as\nocclusions and textureless areas. To address this, we propose MonSter, a novel\nmethod that leverages the complementary strengths of monocular depth estimation\nand stereo matching. MonSter integrates monocular depth and stereo matching\ninto a dual-branch architecture to iteratively improve each other.\nConfidence-based guidance adaptively selects reliable stereo cues for monodepth\nscale-shift recovery. The refined monodepth is in turn guides stereo\neffectively at ill-posed regions. Such iterative mutual enhancement enables\nMonSter to evolve monodepth priors from coarse object-level structures to\npixel-level geometry, fully unlocking the potential of stereo matching. As\nshown in Fig.1, MonSter ranks 1st across five most commonly used leaderboards\n-- SceneFlow, KITTI 2012, KITTI 2015, Middlebury, and ETH3D. Achieving up to\n49.5% improvements (Bad 1.0 on ETH3D) over the previous best method.\nComprehensive analysis verifies the effectiveness of MonSter in ill-posed\nregions. In terms of zero-shot generalization, MonSter significantly and\nconsistently outperforms state-of-the-art across the board. The code is\npublicly available at: https://github.com/Junda24/MonSter.\n","authors":["Junda Cheng","Longliang Liu","Gangwei Xu","Xianqi Wang","Zhaoxing Zhang","Yong Deng","Jinliang Zang","Yurui Chen","Zhipeng Cai","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2501.08643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08639v1","updated":"2025-01-15T08:04:44Z","published":"2025-01-15T08:04:44Z","title":"Detecting Wildfire Flame and Smoke through Edge Computing using Transfer\n  Learning Enhanced Deep Learning Models","summary":"  Autonomous unmanned aerial vehicles (UAVs) integrated with edge computing\ncapabilities empower real-time data processing directly on the device,\ndramatically reducing latency in critical scenarios such as wildfire detection.\nThis study underscores Transfer Learning's (TL) significance in boosting the\nperformance of object detectors for identifying wildfire smoke and flames,\nespecially when trained on limited datasets, and investigates the impact TL has\non edge computing metrics. With the latter focusing how TL-enhanced You Only\nLook Once (YOLO) models perform in terms of inference time, power usage, and\nenergy consumption when using edge computing devices. This study utilizes the\nAerial Fire and Smoke Essential (AFSE) dataset as the target, with the Flame\nand Smoke Detection Dataset (FASDD) and the Microsoft Common Objects in Context\n(COCO) dataset serving as source datasets. We explore a two-stage cascaded TL\nmethod, utilizing D-Fire or FASDD as initial stage target datasets and AFSE as\nthe subsequent stage. Through fine-tuning, TL significantly enhances detection\nprecision, achieving up to 79.2% mean Average Precision (mAP@0.5), reduces\ntraining time, and increases model generalizability across the AFSE dataset.\nHowever, cascaded TL yielded no notable improvements and TL alone did not\nbenefit the edge computing metrics evaluated. Lastly, this work found that\nYOLOv5n remains a powerful model when lacking hardware acceleration, finding\nthat YOLOv5n can process images nearly twice as fast as its newer counterpart,\nYOLO11n. Overall, the results affirm TL's role in augmenting the accuracy of\nobject detectors while also illustrating that additional enhancements are\nneeded to improve edge computing performance.\n","authors":["Giovanny Vazquez","Shengjie Zhai","Mei Yang"],"pdf_url":"https://arxiv.org/pdf/2501.08639v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2405.05095v5","updated":"2025-01-15T07:37:21Z","published":"2024-05-08T14:44:34Z","title":"Approximation properties relative to continuous scale space for hybrid\n  discretizations of Gaussian derivative operators","summary":"  This paper presents an analysis of properties of two hybrid discretization\nmethods for Gaussian derivatives, based on convolutions with either the\nnormalized sampled Gaussian kernel or the integrated Gaussian kernel followed\nby central differences. The motivation for studying these discretization\nmethods is that in situations when multiple spatial derivatives of different\norder are needed at the same scale level, they can be computed significantly\nmore efficiently compared to more direct derivative approximations based on\nexplicit convolutions with either sampled Gaussian kernels or integrated\nGaussian kernels.\n  While these computational benefits do also hold for the genuinely discrete\napproach for computing discrete analogues of Gaussian derivatives, based on\nconvolution with the discrete analogue of the Gaussian kernel followed by\ncentral differences, the underlying mathematical primitives for the discrete\nanalogue of the Gaussian kernel, in terms of modified Bessel functions of\ninteger order, may not be available in certain frameworks for image processing,\nsuch as when performing deep learning based on scale-parameterized filters in\nterms of Gaussian derivatives, with learning of the scale levels.\n  In this paper, we present a characterization of the properties of these\nhybrid discretization methods, in terms of quantitative performance measures\nconcerning the amount of spatial smoothing that they imply, as well as the\nrelative consistency of scale estimates obtained from scale-invariant feature\ndetectors with automatic scale selection, with an emphasis on the behaviour for\nvery small values of the scale parameter, which may differ significantly from\ncorresponding results obtained from the fully continuous scale-space theory, as\nwell as between different types of discretization methods.\n","authors":["Tony Lindeberg"],"pdf_url":"https://arxiv.org/pdf/2405.05095v5.pdf","comment":"23 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2311.11317"},{"id":"http://arxiv.org/abs/2411.15098v4","updated":"2025-01-15T07:30:29Z","published":"2024-11-22T17:55:15Z","title":"OminiControl: Minimal and Universal Control for Diffusion Transformer","summary":"  In this paper, we introduce OminiControl, a highly versatile and\nparameter-efficient framework that integrates image conditions into pre-trained\nDiffusion Transformer (DiT) models. At its core, OminiControl leverages a\nparameter reuse mechanism, enabling the DiT to encode image conditions using\nitself as a powerful backbone and process them with its flexible multi-modal\nattention processors. Unlike existing methods, which rely heavily on additional\nencoder modules with complex architectures, OminiControl (1) effectively and\nefficiently incorporates injected image conditions with only ~0.1% additional\nparameters, and (2) addresses a wide range of image conditioning tasks in a\nunified manner, including subject-driven generation and spatially-aligned\nconditions such as edges, depth, and more. Remarkably, these capabilities are\nachieved by training on images generated by the DiT itself, which is\nparticularly beneficial for subject-driven generation. Extensive evaluations\ndemonstrate that OminiControl outperforms existing UNet-based and DiT-adapted\nmodels in both subject-driven and spatially-aligned conditional generation.\nAdditionally, we release our training dataset, Subjects200K, a diverse\ncollection of over 200,000 identity-consistent images, along with an efficient\ndata synthesis pipeline to advance research in subject-consistent generation.\n","authors":["Zhenxiong Tan","Songhua Liu","Xingyi Yang","Qiaochu Xue","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15098v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08629v1","updated":"2025-01-15T07:24:15Z","published":"2025-01-15T07:24:15Z","title":"Self-Organizing Edge Computing Distribution Framework for Visual SLAM","summary":"  Localization within a known environment is a crucial capability for mobile\nrobots. Simultaneous Localization and Mapping (SLAM) is a prominent solution to\nthis problem. SLAM is a framework that consists of a diverse set of\ncomputational tasks ranging from real-time tracking to computation-intensive\nmap optimization. This combination can present a challenge for resource-limited\nmobile robots. Previously, edge-assisted SLAM methods have demonstrated\npromising real-time execution capabilities by offloading heavy computations\nwhile performing real-time tracking onboard. However, the common approach of\nutilizing a client-server architecture for offloading is sensitive to server\nand network failures. In this article, we propose a novel edge-assisted SLAM\nframework capable of self-organizing fully distributed SLAM execution across a\nnetwork of devices or functioning on a single device without connectivity. The\narchitecture consists of three layers and is designed to be device-agnostic,\nresilient to network failures, and minimally invasive to the core SLAM system.\nWe have implemented and demonstrated the framework for monocular ORB SLAM3 and\nevaluated it in both fully distributed and standalone SLAM configurations\nagainst the ORB SLAM3. The experiment results demonstrate that the proposed\ndesign matches the accuracy and resource utilization of the monolithic approach\nwhile enabling collaborative execution.\n","authors":["Jussi Kalliola","Lauri Suomela","Sergio Moreschini","David Hästbacka"],"pdf_url":"https://arxiv.org/pdf/2501.08629v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.10919v3","updated":"2025-01-15T07:17:58Z","published":"2024-08-20T15:04:14Z","title":"CrossFi: A Cross Domain Wi-Fi Sensing Framework Based on Siamese Network","summary":"  In recent years, Wi-Fi sensing has garnered significant attention due to its\nnumerous benefits, such as privacy protection, low cost, and penetration\nability. Extensive research has been conducted in this field, focusing on areas\nsuch as gesture recognition, people identification, and fall detection.\nHowever, many data-driven methods encounter challenges related to domain shift,\nwhere the model fails to perform well in environments different from the\ntraining data. One major factor contributing to this issue is the limited\navailability of Wi-Fi sensing datasets, which makes models learn excessive\nirrelevant information and over-fit to the training set. Unfortunately,\ncollecting large-scale Wi-Fi sensing datasets across diverse scenarios is a\nchallenging task. To address this problem, we propose CrossFi, a siamese\nnetwork-based approach that excels in both in-domain scenario and cross-domain\nscenario, including few-shot, zero-shot scenarios, and even works in few-shot\nnew-class scenario where testing set contains new categories. The core\ncomponent of CrossFi is a sample-similarity calculation network called CSi-Net,\nwhich improves the structure of the siamese network by using an attention\nmechanism to capture similarity information, instead of simply calculating the\ndistance or cosine similarity. Based on it, we develop an extra Weight-Net that\ncan generate a template for each class, so that our CrossFi can work in\ndifferent scenarios. Experimental results demonstrate that our CrossFi achieves\nstate-of-the-art performance across various scenarios. In gesture recognition\ntask, our CrossFi achieves an accuracy of 98.17% in in-domain scenario, 91.72%\nin one-shot cross-domain scenario, 64.81% in zero-shot cross-domain scenario,\nand 84.75% in one-shot new-class scenario. The code for our model is publicly\navailable at https://github.com/RS2002/CrossFi.\n","authors":["Zijian Zhao","Tingwei Chen","Zhijie Cai","Xiaoyang Li","Hang Li","Qimei Chen","Guangxu Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.10919v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00330v2","updated":"2025-01-15T06:57:25Z","published":"2024-11-01T03:08:10Z","title":"Multiple Information Prompt Learning for Cloth-Changing Person\n  Re-Identification","summary":"  Cloth-changing person re-identification is a subject closer to the real\nworld, which focuses on solving the problem of person re-identification after\npedestrians change clothes. The primary challenge in this field is to overcome\nthe complex interplay between intra-class and inter-class variations and to\nidentify features that remain unaffected by changes in appearance. Sufficient\ndata collection for model training would significantly aid in addressing this\nproblem. However, it is challenging to gather diverse datasets in practice.\nCurrent methods focus on implicitly learning identity information from the\noriginal image or introducing additional auxiliary models, which are largely\nlimited by the quality of the image and the performance of the additional\nmodel. To address these issues, inspired by prompt learning, we propose a novel\nmultiple information prompt learning (MIPL) scheme for cloth-changing person\nReID, which learns identity robust features through the common prompt guidance\nof multiple messages. Specifically, the clothing information stripping (CIS)\nmodule is designed to decouple the clothing information from the original RGB\nimage features to counteract the influence of clothing appearance. The\nBio-guided attention (BGA) module is proposed to increase the learning\nintensity of the model for key information. A dual-length hybrid patch (DHP)\nmodule is employed to make the features have diverse coverage to minimize the\nimpact of feature bias. Extensive experiments demonstrate that the proposed\nmethod outperforms all state-of-the-art methods on the LTCC, Celeb-reID,\nCeleb-reID-light, and CSCC datasets, achieving rank-1 scores of 74.8%, 73.3%,\n66.0%, and 88.1%, respectively. When compared to AIM (CVPR23), ACID (TIP23),\nand SCNet (MM23), MIPL achieves rank-1 improvements of 11.3%, 13.8%, and 7.9%,\nrespectively, on the PRCC dataset.\n","authors":["Shengxun Wei","Zan Gao","Chunjie Ma","Yibo Zhao","Weili Guan","Shengyong Chen"],"pdf_url":"https://arxiv.org/pdf/2411.00330v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00961v2","updated":"2025-01-15T06:46:51Z","published":"2025-01-01T21:45:00Z","title":"The Silent Majority: Demystifying Memorization Effect in the Presence of\n  Spurious Correlations","summary":"  Machine learning models often rely on simple spurious features -- patterns in\ntraining data that correlate with targets but are not causally related to them,\nlike image backgrounds in foreground classification. This reliance typically\nleads to imbalanced test performance across minority and majority groups. In\nthis work, we take a closer look at the fundamental cause of such imbalanced\nperformance through the lens of memorization, which refers to the ability to\npredict accurately on \\textit{atypical} examples (minority groups) in the\ntraining set but failing in achieving the same accuracy in the testing set.\nThis paper systematically shows the ubiquitous existence of spurious features\nin a small set of neurons within the network, providing the first-ever evidence\nthat memorization may contribute to imbalanced group performance. Through three\nexperimental sources of converging empirical evidence, we find the property of\na small subset of neurons or channels in memorizing minority group information.\nInspired by these findings, we articulate the hypothesis: the imbalanced group\nperformance is a byproduct of ``noisy'' spurious memorization confined to a\nsmall set of neurons. To further substantiate this hypothesis, we show that\neliminating these unnecessary spurious memorization patterns via a novel\nframework during training can significantly affect the model performance on\nminority groups. Our experimental results across various architectures and\nbenchmarks offer new insights on how neural networks encode core and spurious\nknowledge, laying the groundwork for future research in demystifying robustness\nto spurious correlation.\n","authors":["Chenyu You","Haocheng Dai","Yifei Min","Jasjeet S. Sekhon","Sarang Joshi","James S. Duncan"],"pdf_url":"https://arxiv.org/pdf/2501.00961v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19599v3","updated":"2025-01-15T06:40:31Z","published":"2024-09-29T07:32:14Z","title":"DATransNet: Dynamic Attention Transformer Network for Infrared Small\n  Target Detection","summary":"  Infrared small target detection (ISTD) is widely used in civilian and\nmilitary applications. However, ISTD encounters several challenges, including\nthe tendency for small and dim targets to be obscured by complex backgrounds.To\naddress this issue, we propose the Dynamic Attention Transformer Network\n(DATransNet), which aims to extract and preserve edge information of small\ntargets.DATransNet employs the Dynamic Attention Transformer (DATrans),\nsimulating central difference convolutions (CDC) to extract and integrate\ngradient features with deeper features.Furthermore, we propose a global feature\nextraction module (GFEM) that offers a comprehensive perspective to prevent the\nnetwork from focusing solely on details while neglecting the background\ninformation. We compare the network with state-of-the-art (SOTA) approaches,\nand the results demonstrate that our method performs effectively. Our source\ncode is available at https://github.com/greekinRoma/DATransNet.\n","authors":["Chen Hu","Yian Huang","Kexuan Li","Luping Zhang","Chang Long","Yiming Zhu","Tian Pu","Zhenming Peng"],"pdf_url":"https://arxiv.org/pdf/2409.19599v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.03678v2","updated":"2025-01-15T06:32:05Z","published":"2022-06-08T05:04:43Z","title":"Ultra-High-Definition Image Deblurring via Multi-scale Cubic-Mixer","summary":"  Currently, transformer-based algorithms are making a splash in the domain of\nimage deblurring. Their achievement depends on the self-attention mechanism\nwith CNN stem to model long range dependencies between tokens. Unfortunately,\nthis ear-pleasing pipeline introduces high computational complexity and makes\nit difficult to run an ultra-high-definition image on a single GPU in real\ntime. To trade-off accuracy and efficiency, the input degraded image is\ncomputed cyclically over three dimensional ($C$, $W$, and $H$) signals without\na self-attention mechanism. We term this deep network as Multi-scale\nCubic-Mixer, which is acted on both the real and imaginary components after\nfast Fourier transform to estimate the Fourier coefficients and thus obtain a\ndeblurred image. Furthermore, we combine the multi-scale cubic-mixer with a\nslicing strategy to generate high-quality results at a much lower computational\ncost. Experimental results demonstrate that the proposed algorithm performs\nfavorably against the state-of-the-art deblurring approaches on the several\nbenchmarks and a new ultra-high-definition dataset in terms of accuracy and\nspeed.\n","authors":["Xingchi Chen","Xiuyi Jia","Zhuoran Zheng"],"pdf_url":"https://arxiv.org/pdf/2206.03678v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2501.08609v1","updated":"2025-01-15T06:15:15Z","published":"2025-01-15T06:15:15Z","title":"Computerized Assessment of Motor Imitation for Distinguishing Autism in\n  Video (CAMI-2DNet)","summary":"  Motor imitation impairments are commonly reported in individuals with autism\nspectrum conditions (ASCs), suggesting that motor imitation could be used as a\nphenotype for addressing autism heterogeneity. Traditional methods for\nassessing motor imitation are subjective, labor-intensive, and require\nextensive human training. Modern Computerized Assessment of Motor Imitation\n(CAMI) methods, such as CAMI-3D for motion capture data and CAMI-2D for video\ndata, are less subjective. However, they rely on labor-intensive data\nnormalization and cleaning techniques, and human annotations for algorithm\ntraining. To address these challenges, we propose CAMI-2DNet, a scalable and\ninterpretable deep learning-based approach to motor imitation assessment in\nvideo data, which eliminates the need for data normalization, cleaning and\nannotation. CAMI-2DNet uses an encoder-decoder architecture to map a video to a\nmotion encoding that is disentangled from nuisance factors such as body shape\nand camera views. To learn a disentangled representation, we employ synthetic\ndata generated by motion retargeting of virtual characters through the\nreshuffling of motion, body shape, and camera views, as well as real\nparticipant data. To automatically assess how well an individual imitates an\nactor, we compute a similarity score between their motion encodings, and use it\nto discriminate individuals with ASCs from neurotypical (NT) individuals. Our\ncomparative analysis demonstrates that CAMI-2DNet has a strong correlation with\nhuman scores while outperforming CAMI-2D in discriminating ASC vs NT children.\nMoreover, CAMI-2DNet performs comparably to CAMI-3D while offering greater\npracticality by operating directly on video data and without the need for\nad-hoc data normalization and human annotations.\n","authors":["Kaleab A. Kinfu","Carolina Pacheco","Alice D. Sperry","Deana Crocetti","Bahar Tunçgenç","Stewart H. Mostofsky","René Vidal"],"pdf_url":"https://arxiv.org/pdf/2501.08609v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2407.01960v2","updated":"2025-01-15T06:06:31Z","published":"2024-07-02T05:31:59Z","title":"Zero-shot Video Restoration and Enhancement Using Pre-Trained Image\n  Diffusion Model","summary":"  Diffusion-based zero-shot image restoration and enhancement models have\nachieved great success in various tasks of image restoration and enhancement.\nHowever, directly applying them to video restoration and enhancement results in\nsevere temporal flickering artifacts. In this paper, we propose the first\nframework for zero-shot video restoration and enhancement based on the\npre-trained image diffusion model. By replacing the spatial self-attention\nlayer with the proposed short-long-range (SLR) temporal attention layer, the\npre-trained image diffusion model can take advantage of the temporal\ncorrelation between frames. We further propose temporal consistency guidance,\nspatial-temporal noise sharing, and an early stopping sampling strategy to\nimprove temporally consistent sampling. Our method is a plug-and-play module\nthat can be inserted into any diffusion-based image restoration or enhancement\nmethods to further improve their performance. Experimental results demonstrate\nthe superiority of our proposed method. Our code is available at\nhttps://github.com/cao-cong/ZVRD.\n","authors":["Cong Cao","Huanjing Yue","Xin Liu","Jingyu Yang"],"pdf_url":"https://arxiv.org/pdf/2407.01960v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2501.08605v1","updated":"2025-01-15T06:05:57Z","published":"2025-01-15T06:05:57Z","title":"PACF: Prototype Augmented Compact Features for Improving Domain Adaptive\n  Object Detection","summary":"  In recent years, there has been significant advancement in object detection.\nHowever, applying off-the-shelf detectors to a new domain leads to significant\nperformance drop, caused by the domain gap. These detectors exhibit\nhigher-variance class-conditional distributions in the target domain than that\nin the source domain, along with mean shift. To address this problem, we\npropose the Prototype Augmented Compact Features (PACF) framework to regularize\nthe distribution of intra-class features. Specifically, we provide an in-depth\ntheoretical analysis on the lower bound of the target features-related\nlikelihood and derive the prototype cross entropy loss to further calibrate the\ndistribution of target RoI features. Furthermore, a mutual regularization\nstrategy is designed to enable the linear and prototype-based classifiers to\nlearn from each other, promoting feature compactness while enhancing\ndiscriminability. Thanks to this PACF framework, we have obtained a more\ncompact cross-domain feature space, within which the variance of the target\nfeatures' class-conditional distributions has significantly decreased, and the\nclass-mean shift between the two domains has also been further reduced. The\nresults on different adaptation settings are state-of-the-art, which\ndemonstrate the board applicability and effectiveness of the proposed approach.\n","authors":["Chenguang Liu","Yongchao Feng","Yanan Zhang","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08604v1","updated":"2025-01-15T06:04:18Z","published":"2025-01-15T06:04:18Z","title":"Watermarking in Diffusion Model: Gaussian Shading with Exact Diffusion\n  Inversion via Coupled Transformations (EDICT)","summary":"  This paper introduces a novel approach to enhance the performance of Gaussian\nShading, a prevalent watermarking technique, by integrating the Exact Diffusion\nInversion via Coupled Transformations (EDICT) framework. While Gaussian Shading\ntraditionally embeds watermarks in a noise latent space, followed by iterative\ndenoising for image generation and noise addition for watermark recovery, its\ninversion process is not exact, leading to potential watermark distortion. We\npropose to leverage EDICT's ability to derive exact inverse mappings to refine\nthis process. Our method involves duplicating the watermark-infused noisy\nlatent and employing a reciprocal, alternating denoising and noising scheme\nbetween the two latents, facilitated by EDICT. This allows for a more precise\nreconstruction of both the image and the embedded watermark. Empirical\nevaluation on standard datasets demonstrates that our integrated approach\nyields a slight, yet statistically significant improvement in watermark\nrecovery fidelity. These results highlight the potential of EDICT to enhance\nexisting diffusion-based watermarking techniques by providing a more accurate\nand robust inversion mechanism. To the best of our knowledge, this is the first\nwork to explore the synergy between EDICT and Gaussian Shading for digital\nwatermarking, opening new avenues for research in robust and high-fidelity\nwatermark embedding and extraction.\n","authors":["Krishna Panthi"],"pdf_url":"https://arxiv.org/pdf/2501.08604v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2501.08593v1","updated":"2025-01-15T05:36:41Z","published":"2025-01-15T05:36:41Z","title":"Image-to-Force Estimation for Soft Tissue Interaction in\n  Robotic-Assisted Surgery Using Structured Light","summary":"  For Minimally Invasive Surgical (MIS) robots, accurate haptic interaction\nforce feedback is essential for ensuring the safety of interacting with soft\ntissue. However, most existing MIS robotic systems cannot facilitate direct\nmeasurement of the interaction force with hardware sensors due to space\nlimitations. This letter introduces an effective vision-based scheme that\nutilizes a One-Shot structured light projection with a designed pattern on soft\ntissue coupled with haptic information processing through a trained\nimage-to-force neural network. The images captured from the endoscopic stereo\ncamera are analyzed to reconstruct high-resolution 3D point clouds for soft\ntissue deformation. Based on this, a modified PointNet-based force estimation\nmethod is proposed, which excels in representing the complex mechanical\nproperties of soft tissue. Numerical force interaction experiments are\nconducted on three silicon materials with different stiffness. The results\nvalidate the effectiveness of the proposed scheme.\n","authors":["Jiayin Wang","Mingfeng Yao","Yanran Wei","Xiaoyu Guo","Ayong Zheng","Weidong Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.08593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00580v2","updated":"2025-01-15T05:30:24Z","published":"2024-11-30T20:40:10Z","title":"Continuous Concepts Removal in Text-to-image Diffusion Models","summary":"  Text-to-image diffusion models have shown an impressive ability to generate\nhigh-quality images from input textual descriptions. However, concerns have\nbeen raised about the potential for these models to create content that\ninfringes on copyrights or depicts disturbing subject matter. Removing specific\nconcepts from these models is a promising potential solution to this problem.\nHowever, existing methods for concept removal do not work well in practical but\nchallenging scenarios where concepts need to be continuously removed.\nSpecifically, these methods lead to poor alignment between the text prompts and\nthe generated image after the continuous removal process. To address this\nissue, we propose a novel approach called CCRT that includes a designed\nknowledge distillation paradigm. It constrains the text-image alignment\nbehavior during the continuous concept removal process by using a set of text\nprompts generated through our genetic algorithm, which employs a designed\nfuzzing strategy. We conduct extensive experiments involving the removal of\nvarious concepts. The results evaluated through both algorithmic metrics and\nhuman studies demonstrate that our CCRT can effectively remove the targeted\nconcepts in a continuous manner while maintaining the high generation quality\n(e.g., text-image alignment) of the model.\n","authors":["Tingxu Han","Weisong Sun","Yanrong Hu","Chunrong Fang","Yonglong Zhang","Shiqing Ma","Tao Zheng","Zhenyu Chen","Zhenting Wang"],"pdf_url":"https://arxiv.org/pdf/2412.00580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08580v1","updated":"2025-01-15T05:00:03Z","published":"2025-01-15T05:00:03Z","title":"Densely Connected Parameter-Efficient Tuning for Referring Image\n  Segmentation","summary":"  In the domain of computer vision, Parameter-Efficient Tuning (PET) is\nincreasingly replacing the traditional paradigm of pre-training followed by\nfull fine-tuning. PET is particularly favored for its effectiveness in large\nfoundation models, as it streamlines transfer learning costs and optimizes\nhardware utilization. However, the current PET methods are mainly designed for\nsingle-modal optimization. While some pioneering studies have undertaken\npreliminary explorations, they still remain at the level of aligned encoders\n(e.g., CLIP) and lack exploration of misaligned encoders. These methods show\nsub-optimal performance with misaligned encoders, as they fail to effectively\nalign the multimodal features during fine-tuning. In this paper, we introduce\nDETRIS, a parameter-efficient tuning framework designed to enhance low-rank\nvisual feature propagation by establishing dense interconnections between each\nlayer and all preceding layers, which enables effective cross-modal feature\ninteraction and adaptation to misaligned encoders. We also suggest using text\nadapters to improve textual features. Our simple yet efficient approach greatly\nsurpasses state-of-the-art methods with 0.9% to 1.8% backbone parameter\nupdates, evaluated on challenging benchmarks. Our project is available at\n\\url{https://github.com/jiaqihuang01/DETRIS}.\n","authors":["Jiaqi Huang","Zunnan Xu","Ting Liu","Yong Liu","Haonan Han","Kehong Yuan","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2501.08580v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2501.08577v1","updated":"2025-01-15T04:56:26Z","published":"2025-01-15T04:56:26Z","title":"Scalable and High-Quality Neural Implicit Representation for 3D\n  Reconstruction","summary":"  Various SDF-based neural implicit surface reconstruction methods have been\nproposed recently, and have demonstrated remarkable modeling capabilities.\nHowever, due to the global nature and limited representation ability of a\nsingle network, existing methods still suffer from many drawbacks, such as\nlimited accuracy and scale of the reconstruction. In this paper, we propose a\nversatile, scalable and high-quality neural implicit representation to address\nthese issues. We integrate a divide-and-conquer approach into the neural\nSDF-based reconstruction. Specifically, we model the object or scene as a\nfusion of multiple independent local neural SDFs with overlapping regions. The\nconstruction of our representation involves three key steps: (1) constructing\nthe distribution and overlap relationship of the local radiance fields based on\nobject structure or data distribution, (2) relative pose registration for\nadjacent local SDFs, and (3) SDF blending. Thanks to the independent\nrepresentation of each local region, our approach can not only achieve\nhigh-fidelity surface reconstruction, but also enable scalable scene\nreconstruction. Extensive experimental results demonstrate the effectiveness\nand practicality of our proposed method.\n","authors":["Leyuan Yang","Bailin Deng","Juyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.08577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02281v2","updated":"2025-01-15T04:51:48Z","published":"2024-11-04T17:09:58Z","title":"Conformal-in-the-Loop for Learning with Imbalanced Noisy Data","summary":"  Class imbalance and label noise are pervasive in large-scale datasets, yet\nmuch of machine learning research assumes well-labeled, balanced data, which\nrarely reflects real world conditions. Existing approaches typically address\neither label noise or class imbalance in isolation, leading to suboptimal\nresults when both issues coexist. In this work, we propose\nConformal-in-the-Loop (CitL), a novel training framework that addresses both\nchallenges with a conformal prediction-based approach. CitL evaluates sample\nuncertainty to adjust weights and prune unreliable examples, enhancing model\nresilience and accuracy with minimal computational cost. Our extensive\nexperiments include a detailed analysis showing how CitL effectively emphasizes\nimpactful data in noisy, imbalanced datasets. Our results show that CitL\nconsistently boosts model performance, achieving up to a 6.1% increase in\nclassification accuracy and a 5.0 mIoU improvement in segmentation. Our code is\npublicly available: CitL.\n","authors":["John Brandon Graham-Knight","Jamil Fayyad","Nourhan Bayasi","Patricia Lasserre","Homayoun Najjaran"],"pdf_url":"https://arxiv.org/pdf/2411.02281v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2501.08575v1","updated":"2025-01-15T04:51:10Z","published":"2025-01-15T04:51:10Z","title":"GOTLoc: General Outdoor Text-based Localization Using Scene Graph\n  Retrieval with OpenStreetMap","summary":"  We propose GOTLoc, a robust localization method capable of operating even in\noutdoor environments where GPS signals are unavailable. The method achieves\nthis robust localization by leveraging comparisons between scene graphs\ngenerated from text descriptions and maps. Existing text-based localization\nstudies typically represent maps as point clouds and identify the most similar\nscenes by comparing embeddings of text and point cloud data. However, point\ncloud maps have limited scalability as it is impractical to pre-generate maps\nfor all outdoor spaces. Furthermore, their large data size makes it challenging\nto store and utilize them directly on actual robots. To address these issues,\nGOTLoc leverages compact data structures, such as scene graphs, to store\nspatial information, enabling individual robots to carry and utilize large\namounts of map data. Additionally, by utilizing publicly available map data,\nsuch as OpenStreetMap, which provides global information on outdoor spaces, we\neliminate the need for additional effort to create custom map data. For\nperformance evaluation, we utilized the KITTI360Pose dataset in conjunction\nwith corresponding OpenStreetMap data to compare the proposed method with\nexisting approaches. Our results demonstrate that the proposed method achieves\naccuracy comparable to algorithms relying on point cloud maps. Moreover, in\ncity-scale tests, GOTLoc required significantly less storage compared to point\ncloud-based methods and completed overall processing within a few seconds,\nvalidating its applicability to real-world robotics. Our code is available at\nhttps://github.com/donghwijung/GOTLoc.\n","authors":["Donghwi Jung","Keonwoo Kim","Seong-Woo Kim"],"pdf_url":"https://arxiv.org/pdf/2501.08575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08562v1","updated":"2025-01-15T04:07:06Z","published":"2025-01-15T04:07:06Z","title":"MIAFEx: An Attention-based Feature Extraction Method for Medical Image\n  Classification","summary":"  Feature extraction techniques are crucial in medical image classification;\nhowever, classical feature extractors in addition to traditional machine\nlearning classifiers often exhibit significant limitations in providing\nsufficient discriminative information for complex image sets. While\nConvolutional Neural Networks (CNNs) and Vision Transformer (ViT) have shown\npromise in feature extraction, they are prone to overfitting due to the\ninherent characteristics of medical imaging data, including small sample sizes\nor high intra-class variance. In this work, the Medical Image Attention-based\nFeature Extractor (MIAFEx) is proposed, a novel method that employs a learnable\nrefinement mechanism to enhance the classification token within the Transformer\nencoder architecture. This mechanism adjusts the token based on learned\nweights, improving the extraction of salient features and enhancing the model's\nadaptability to the challenges presented by medical imaging data. The MIAFEx\noutput features quality is compared against classical feature extractors using\ntraditional and hybrid classifiers. Also, the performance of these features is\ncompared against modern CNN and ViT models in classification tasks,\ndemonstrating its superiority in accuracy and robustness across multiple\ncomplex classification medical imaging datasets. This advantage is particularly\npronounced in scenarios with limited training data, where traditional and\nmodern models often struggle to generalize effectively. The source code of this\nproposal can be found at\nhttps://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx\n","authors":["Oscar Ramos-Soto","Jorge Ramos-Frutos","Ezequiel Perez-Zarate","Diego Oliva","Sandra E. Balderas-Mata"],"pdf_url":"https://arxiv.org/pdf/2501.08562v1.pdf","comment":"In preparation for Journal Submission"},{"id":"http://arxiv.org/abs/2501.08553v1","updated":"2025-01-15T03:28:14Z","published":"2025-01-15T03:28:14Z","title":"DynamicFace: High-Quality and Consistent Video Face Swapping using\n  Composable 3D Facial Priors","summary":"  Face swapping transfers the identity of a source face to a target face while\nretaining the attributes like expression, pose, hair, and background of the\ntarget face. Advanced face swapping methods have achieved attractive results.\nHowever, these methods often inadvertently transfer identity information from\nthe target face, compromising expression-related details and accurate identity.\nWe propose a novel method DynamicFace that leverages the power of diffusion\nmodel and plug-and-play temporal layers for video face swapping. First, we\nintroduce four fine-grained face conditions using 3D facial priors. All\nconditions are designed to be disentangled from each other for precise and\nunique control. Then, we adopt Face Former and ReferenceNet for high-level and\ndetailed identity injection. Through experiments on the FF++ dataset, we\ndemonstrate that our method achieves state-of-the-art results in face swapping,\nshowcasing superior image quality, identity preservation, and expression\naccuracy. Besides, our method could be easily transferred to video domain with\ntemporal attention layer. Our code and results will be available on the project\npage: https://dynamic-face.github.io/\n","authors":["Runqi Wang","Sijie Xu","Tianyao He","Yang Chen","Wei Zhu","Dejia Song","Nemo Chen","Xu Tang","Yao Hu"],"pdf_url":"https://arxiv.org/pdf/2501.08553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08549v1","updated":"2025-01-15T03:17:24Z","published":"2025-01-15T03:17:24Z","title":"The Devil is in Temporal Token: High Quality Video Reasoning\n  Segmentation","summary":"  Existing methods for Video Reasoning Segmentation rely heavily on a single\nspecial token to represent the object in the keyframe or the entire video,\ninadequately capturing spatial complexity and inter-frame motion. To overcome\nthese challenges, we propose VRS-HQ, an end-to-end video reasoning segmentation\napproach that leverages Multimodal Large Language Models (MLLMs) to inject rich\nspatiotemporal features into hierarchical tokens.Our key innovations include a\nTemporal Dynamic Aggregation (TDA) and a Token-driven Keyframe Selection (TKS).\nSpecifically, we design frame-level <SEG> and temporal-level <TAK> tokens that\nutilize MLLM's autoregressive learning to effectively capture both local and\nglobal information. Subsequently, we apply a similarity-based weighted fusion\nand frame selection strategy, then utilize SAM2 to perform keyframe\nsegmentation and propagation. To enhance keyframe localization accuracy, the\nTKS filters keyframes based on SAM2's occlusion scores during inference. VRS-HQ\nachieves state-of-the-art performance on ReVOS, surpassing VISA by\n5.9%/12.5%/9.1% in J&F scores across the three subsets. These results highlight\nthe strong temporal reasoning and segmentation capabilities of our method. Code\nand model weights will be released at VRS-HQ.\n","authors":["Sitong Gong","Yunzhi Zhuge","Lu Zhang","Zongxin Yang","Pingping Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2501.08549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08545v1","updated":"2025-01-15T03:11:33Z","published":"2025-01-15T03:11:33Z","title":"Comprehensive Subjective and Objective Evaluation Method for\n  Text-generated Video","summary":"  Recent text-to-video (T2V) technology advancements, as demonstrated by models\nsuch as Gen3, Pika, and Sora, have significantly broadened its applicability\nand popularity. This progress has created a growing demand for accurate quality\nassessment metrics to evaluate the perceptual quality of text-generated videos\nand optimize video generation models. However, assessing the quality of\ntext-generated videos remains challenging due to the presence of highly complex\ndistortions, such as unnatural actions and phenomena that defy human cognition.\nTo address these challenges, we constructed a large-scale benchmark dataset for\n\\textbf{T}ext-generated \\textbf{V}ideo \\textbf{eval}uation,\n\\textbf{T2VEval-Bench}, comprising 148 textual words and 1,783 videos generated\nby 12 models. During the subjective evaluation, we collected five key scores:\noverall impression, video quality, aesthetic quality, realness, and text-video\nconsistency. For objective evaluation, we developed the \\textbf{T2VEval} model,\nwhich assesses videos across three branches: quality, authenticity, and\nconsistency. Using an attention-based fusion module, T2VEval effectively\nintegrates features from each branch and predicts scores with the aid of a\nlarge oracle model. Additionally, we implemented a progressive training\nstrategy, enabling each branch to learn targeted knowledge while maintaining\nsynergy with the others. Experimental results demonstrate that T2VEval achieves\nstate-of-the-art performance across multiple metrics. The dataset and code will\nbe open-sourced upon completion of the follow-up work.\n","authors":["Zelu Qi","Ping Shi","Shuqi Wang","Zhaoyang Zhang","Zefeng Ying","Da Pan"],"pdf_url":"https://arxiv.org/pdf/2501.08545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19727v2","updated":"2025-01-15T02:29:14Z","published":"2024-09-29T14:57:45Z","title":"Investigating the Effect of Network Pruning on Performance and\n  Interpretability","summary":"  Deep Neural Networks (DNNs) are often over-parameterized for their tasks and\ncan be compressed quite drastically by removing weights, a process called\npruning. We investigate the impact of different pruning techniques on the\nclassification performance and interpretability of GoogLeNet. We systematically\napply unstructured and structured pruning, as well as connection sparsity\n(pruning of input weights) methods to the network and analyze the outcomes\nregarding the network's performance on the validation set of ImageNet. We also\ncompare different retraining strategies, such as iterative pruning and one-shot\npruning. We find that with sufficient retraining epochs, the performance of the\nnetworks can approximate the performance of the default GoogLeNet - and even\nsurpass it in some cases. To assess interpretability, we employ the Mechanistic\nInterpretability Score (MIS) developed by Zimmermann et al. . Our experiments\nreveal that there is no significant relationship between interpretability and\npruning rate when using MIS as a measure. Additionally, we observe that\nnetworks with extremely low accuracy can still achieve high MIS scores,\nsuggesting that the MIS may not always align with intuitive notions of\ninterpretability, such as understanding the basis of correct decisions.\n","authors":["Jonathan von Rad","Florian Seuffert"],"pdf_url":"https://arxiv.org/pdf/2409.19727v2.pdf","comment":"4 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.07870v2","updated":"2025-01-15T02:23:10Z","published":"2025-01-14T06:21:31Z","title":"Make-A-Character 2: Animatable 3D Character Generation From a Single\n  Image","summary":"  This report introduces Make-A-Character 2, an advanced system for generating\nhigh-quality 3D characters from single portrait photographs, ideal for game\ndevelopment and digital human applications. Make-A-Character 2 builds upon its\npredecessor by incorporating several significant improvements for image-based\nhead generation. We utilize the IC-Light method to correct non-ideal\nillumination in input photos and apply neural network-based color correction to\nharmonize skin tones between the photos and game engine renders. We also employ\nthe Hierarchical Representation Network to capture high-frequency facial\nstructures and conduct adaptive skeleton calibration for accurate and\nexpressive facial animations. The entire image-to-3D-character generation\nprocess takes less than 2 minutes. Furthermore, we leverage transformer\narchitecture to generate co-speech facial and gesture actions, enabling\nreal-time conversation with the generated character. These technologies have\nbeen integrated into our conversational AI avatar products.\n","authors":["Lin Liu","Yutong Wang","Jiahao Chen","Jianfang Li","Tangli Xue","Longlong Li","Jianqiang Ren","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2501.07870v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2412.11409v3","updated":"2025-01-15T01:59:02Z","published":"2024-12-16T03:25:23Z","title":"Multi-modal and Multi-scale Spatial Environment Understanding for\n  Immersive Visual Text-to-Speech","summary":"  Visual Text-to-Speech (VTTS) aims to take the environmental image as the\nprompt to synthesize the reverberant speech for the spoken content. The\nchallenge of this task lies in understanding the spatial environment from the\nimage. Many attempts have been made to extract global spatial visual\ninformation from the RGB space of an spatial image. However, local and depth\nimage information are crucial for understanding the spatial environment, which\nprevious works have ignored. To address the issues, we propose a novel\nmulti-modal and multi-scale spatial environment understanding scheme to achieve\nimmersive VTTS, termed M2SE-VTTS. The multi-modal aims to take both the RGB and\nDepth spaces of the spatial image to learn more comprehensive spatial\ninformation, and the multi-scale seeks to model the local and global spatial\nknowledge simultaneously. Specifically, we first split the RGB and Depth images\ninto patches and adopt the Gemini-generated environment captions to guide the\nlocal spatial understanding. After that, the multi-modal and multi-scale\nfeatures are integrated by the local-aware global spatial understanding. In\nthis way, M2SE-VTTS effectively models the interactions between local and\nglobal spatial contexts in the multi-modal spatial environment. Objective and\nsubjective evaluations suggest that our model outperforms the advanced\nbaselines in environmental speech generation. The code and audio samples are\navailable at: https://github.com/AI-S2-Lab/M2SE-VTTS.\n","authors":["Rui Liu","Shuwei He","Yifan Hu","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2412.11409v3.pdf","comment":"9 pages,2 figures, Accepted by AAAI'2025"},{"id":"http://arxiv.org/abs/2501.08514v1","updated":"2025-01-15T01:52:54Z","published":"2025-01-15T01:52:54Z","title":"Multimodal Fake News Video Explanation Generation","summary":"  Multi-modal explanation involves the assessment of the veracity of a variety\nof different content, and relies on multiple information modalities to\ncomprehensively consider the relevance and consistency between modalities. Most\nexisting fake news video detection methods focus on improving accuracy while\nignoring the importance of providing explanations. In this paper, we propose a\nnovel problem - Fake News Video Explanation (FNVE) - Given a multimodal news\ncontaining both video and caption text, we aim to generate natural language\nexplanations to reveal the truth of predictions. To this end, we develop\nFakeNVE, a new dataset of explanations for truthfully multimodal posts, where\neach explanation is a natural language (English) sentence describing the\nattribution of a news thread. We benchmark FakeNVE by using a multimodal\ntransformer-based architecture. Subsequently, a BART-based autoregressive\ndecoder is used as the generator. Empirical results show compelling results for\nvarious baselines (applicable to FNVE) across multiple evaluation metrics. We\nalso perform human evaluation on explanation generation, achieving high scores\nfor both adequacy and fluency.\n","authors":["Lizhi Chen","Zhong Qian","Peifeng Li","Qiaoming Zhu"],"pdf_url":"https://arxiv.org/pdf/2501.08514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04939v2","updated":"2025-01-15T01:12:29Z","published":"2025-01-09T03:04:08Z","title":"Multi-Context Temporal Consistent Modeling for Referring Video Object\n  Segmentation","summary":"  Referring video object segmentation aims to segment objects within a video\ncorresponding to a given text description. Existing transformer-based temporal\nmodeling approaches face challenges related to query inconsistency and the\nlimited consideration of context. Query inconsistency produces unstable masks\nof different objects in the middle of the video. The limited consideration of\ncontext leads to the segmentation of incorrect objects by failing to adequately\naccount for the relationship between the given text and instances. To address\nthese issues, we propose the Multi-context Temporal Consistency Module (MTCM),\nwhich consists of an Aligner and a Multi-Context Enhancer (MCE). The Aligner\nremoves noise from queries and aligns them to achieve query consistency. The\nMCE predicts text-relevant queries by considering multi-context. We applied\nMTCM to four different models, increasing performance across all of them,\nparticularly achieving 47.6 J&F on the MeViS. Code is available at\nhttps://github.com/Choi58/MTCM.\n","authors":["Sun-Hyuk Choi","Hayoung Jo","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2501.04939v2.pdf","comment":"Comment: Accepted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.08506v1","updated":"2025-01-15T00:56:59Z","published":"2025-01-15T00:56:59Z","title":"Exploring the Efficacy of Meta-Learning: Unveiling Superior Data\n  Diversity Utilization of MAML Over Pre-training","summary":"  Currently, data and model size dominate the narrative in the training of\nsuper-large, powerful models. However, there has been a lack of exploration on\nthe effect of other attributes of the training dataset on model performance. We\nhypothesize that dataset diversity can impact the performance of vision models.\nOur study shows positive correlations between test set accuracy and data\ndiversity, providing an argument for furthering the research of dataset\nattributes beyond size. We analyzed pre-training and model-agnostic\nmeta-learning methods on twelve popular visual datasets (e.g., Omniglot,\nCIFAR-FS, Aircraft) and five model configurations, including MAML variants with\ndifferent numbers of inner gradient steps and supervised learning. We show\nmoderate to strong positive correlations (R-squared: 0.15-0.42) between\naccuracy and data diversity and weaker but significant correlations (R-squared:\n~0.2) between loss and diversity. These findings support our hypothesis and\ndemonstrate a promising way for a deeper exploration of how formal data\ndiversity influences model performance. This initial study highlights the\npotential of (Task2Vec) data diversity as a valuable measure in the rapidly\nevolving field of large-scale learning and emphasizes that understanding the\ndataset is key to building more powerful and generalizable models.\n","authors":["Kavita Selva","Satita Vittayaareekul","Brando Miranda"],"pdf_url":"https://arxiv.org/pdf/2501.08506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07163v2","updated":"2025-01-15T00:54:54Z","published":"2025-01-13T09:49:34Z","title":"Adaptive Noise-Tolerant Network for Image Segmentation","summary":"  Unlike image classification and annotation, for which deep network models\nhave achieved dominating superior performances compared to traditional computer\nvision algorithms, deep learning for automatic image segmentation still faces\ncritical challenges. One of such hurdles is to obtain ground-truth\nsegmentations as the training labels for deep network training. Especially when\nwe study biomedical images, such as histopathological images (histo-images), it\nis unrealistic to ask for manual segmentation labels as the ground truth for\ntraining due to the fine image resolution as well as the large image size and\ncomplexity. In this paper, instead of relying on clean segmentation labels, we\nstudy whether and how integrating imperfect or noisy segmentation results from\noff-the-shelf segmentation algorithms may help achieve better segmentation\nresults through a new Adaptive Noise-Tolerant Network (ANTN) model. We extend\nthe noisy label deep learning to image segmentation with two novel aspects: (1)\nmultiple noisy labels can be integrated into one deep learning model; (2) noisy\nsegmentation modeling, including probabilistic parameters, is adaptive,\ndepending on the given testing image appearance. Implementation of the new ANTN\nmodel on both the synthetic data and real-world histo-images demonstrates its\neffectiveness and superiority over off-the-shelf and other existing\ndeep-learning-based image segmentation algorithms.\n","authors":["Weizhi Li"],"pdf_url":"https://arxiv.org/pdf/2501.07163v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08505v1","updated":"2025-01-15T00:54:33Z","published":"2025-01-15T00:54:33Z","title":"Yuan: Yielding Unblemished Aesthetics Through A Unified Network for\n  Visual Imperfections Removal in Generated Images","summary":"  Generative AI presents transformative potential across various domains, from\ncreative arts to scientific visualization. However, the utility of AI-generated\nimagery is often compromised by visual flaws, including anatomical\ninaccuracies, improper object placements, and misplaced textual elements. These\nimperfections pose significant challenges for practical applications. To\novercome these limitations, we introduce \\textit{Yuan}, a novel framework that\nautonomously corrects visual imperfections in text-to-image synthesis.\n\\textit{Yuan} uniquely conditions on both the textual prompt and the segmented\nimage, generating precise masks that identify areas in need of refinement\nwithout requiring manual intervention -- a common constraint in previous\nmethodologies. Following the automated masking process, an advanced inpainting\nmodule seamlessly integrates contextually coherent content into the identified\nregions, preserving the integrity and fidelity of the original image and\nassociated text prompts. Through extensive experimentation on publicly\navailable datasets such as ImageNet100 and Stanford Dogs, along with a\ncustom-generated dataset, \\textit{Yuan} demonstrated superior performance in\neliminating visual imperfections. Our approach consistently achieved higher\nscores in quantitative metrics, including NIQE, BRISQUE, and PI, alongside\nfavorable qualitative evaluations. These results underscore \\textit{Yuan}'s\npotential to significantly enhance the quality and applicability of\nAI-generated images across diverse fields.\n","authors":["Zhenyu Yu","Chee Seng Chan"],"pdf_url":"https://arxiv.org/pdf/2501.08505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08504v1","updated":"2025-01-15T00:54:12Z","published":"2025-01-15T00:54:12Z","title":"SuperSAM: Crafting a SAM Supernetwork via Structured Pruning and\n  Unstructured Parameter Prioritization","summary":"  Neural Architecture Search (NAS) is a powerful approach of automating the\ndesign of efficient neural architectures. In contrast to traditional NAS\nmethods, recently proposed one-shot NAS methods prove to be more efficient in\nperforming NAS. One-shot NAS works by generating a singular weight-sharing\nsupernetwork that acts as a search space (container) of subnetworks. Despite\nits achievements, designing the one-shot search space remains a major\nchallenge. In this work we propose a search space design strategy for Vision\nTransformer (ViT)-based architectures. In particular, we convert the Segment\nAnything Model (SAM) into a weight-sharing supernetwork called SuperSAM. Our\napproach involves automating the search space design via layer-wise structured\npruning and parameter prioritization. While the structured pruning applies\nprobabilistic removal of certain transformer layers, parameter prioritization\nperforms weight reordering and slicing of MLP-blocks in the remaining layers.\nWe train supernetworks on several datasets using the sandwich rule. For\ndeployment, we enhance subnetwork discovery by utilizing a program autotuner to\nidentify efficient subnetworks within the search space. The resulting\nsubnetworks are 30-70% smaller in size compared to the original pre-trained SAM\nViT-B, yet outperform the pretrained model. Our work introduces a new and\neffective method for ViT NAS search-space design.\n","authors":["Waqwoya Abebe","Sadegh Jafari","Sixing Yu","Akash Dutta","Jan Strube","Nathan R. Tallent","Luanzheng Guo","Pablo Munoz","Ali Jannesari"],"pdf_url":"https://arxiv.org/pdf/2501.08504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14762v3","updated":"2025-01-15T00:53:38Z","published":"2024-11-22T06:50:44Z","title":"Efficient Long Video Tokenization via Coordinate-based Patch\n  Reconstruction","summary":"  Efficient tokenization of videos remains a challenge in training vision\nmodels that can process long videos. One promising direction is to develop a\ntokenizer that can encode long video clips, as it would enable the tokenizer to\nleverage the temporal coherence of videos better for tokenization. However,\ntraining existing tokenizers on long videos often incurs a huge training cost\nas they are trained to reconstruct all the frames at once. In this paper, we\nintroduce CoordTok, a video tokenizer that learns a mapping from\ncoordinate-based representations to the corresponding patches of input videos,\ninspired by recent advances in 3D generative models. In particular, CoordTok\nencodes a video into factorized triplane representations and reconstructs\npatches that correspond to randomly sampled $(x,y,t)$ coordinates. This allows\nfor training large tokenizer models directly on long videos without requiring\nexcessive training resources. Our experiments show that CoordTok can\ndrastically reduce the number of tokens for encoding long video clips. For\ninstance, CoordTok can encode a 128-frame video with 128$\\times$128 resolution\ninto 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar\nreconstruction quality. We further show that this efficient video tokenization\nenables memory-efficient training of a diffusion transformer that can generate\n128 frames at once.\n","authors":["Huiwon Jang","Sihyun Yu","Jinwoo Shin","Pieter Abbeel","Younggyo Seo"],"pdf_url":"https://arxiv.org/pdf/2411.14762v3.pdf","comment":"Code is available on the project webpage:\n  https://huiwon-jang.github.io/coordtok/"},{"id":"http://arxiv.org/abs/2412.14340v2","updated":"2025-01-15T00:02:00Z","published":"2024-12-18T21:17:02Z","title":"A Unifying Information-theoretic Perspective on Evaluating Generative\n  Models","summary":"  Considering the difficulty of interpreting generative model output, there is\nsignificant current research focused on determining meaningful evaluation\nmetrics. Several recent approaches utilize \"precision\" and \"recall,\" borrowed\nfrom the classification domain, to individually quantify the output fidelity\n(realism) and output diversity (representation of the real data variation),\nrespectively. With the increase in metric proposals, there is a need for a\nunifying perspective, allowing for easier comparison and clearer explanation of\ntheir benefits and drawbacks. To this end, we unify a class of\nkth-nearest-neighbors (kNN)-based metrics under an information-theoretic lens\nusing approaches from kNN density estimation. Additionally, we propose a\ntri-dimensional metric composed of Precision Cross-Entropy (PCE), Recall\nCross-Entropy (RCE), and Recall Entropy (RE), which separately measure fidelity\nand two distinct aspects of diversity, inter- and intra-class. Our\ndomain-agnostic metric, derived from the information-theoretic concepts of\nentropy and cross-entropy, can be dissected for both sample- and mode-level\nanalysis. Our detailed experimental results demonstrate the sensitivity of our\nmetric components to their respective qualities and reveal undesirable\nbehaviors of other metrics.\n","authors":["Alexis Fox","Samarth Swarup","Abhijin Adiga"],"pdf_url":"https://arxiv.org/pdf/2412.14340v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09203v1","updated":"2025-01-15T23:36:05Z","published":"2025-01-15T23:36:05Z","title":"Unified Few-shot Crack Segmentation and its Precise 3D Automatic\n  Measurement in Concrete Structures","summary":"  Visual-Spatial Systems has become increasingly essential in concrete crack\ninspection. However, existing methods often lacks adaptability to diverse\nscenarios, exhibits limited robustness in image-based approaches, and struggles\nwith curved or complex geometries. To address these limitations, an innovative\nframework for two-dimensional (2D) crack detection, three-dimensional (3D)\nreconstruction, and 3D automatic crack measurement was proposed by integrating\ncomputer vision technologies and multi-modal Simultaneous localization and\nmapping (SLAM) in this study. Firstly, building on a base DeepLabv3+\nsegmentation model, and incorporating specific refinements utilizing foundation\nmodel Segment Anything Model (SAM), we developed a crack segmentation method\nwith strong generalization across unfamiliar scenarios, enabling the generation\nof precise 2D crack masks. To enhance the accuracy and robustness of 3D\nreconstruction, Light Detection and Ranging (LiDAR) point clouds were utilized\ntogether with image data and segmentation masks. By leveraging both image- and\nLiDAR-SLAM, we developed a multi-frame and multi-modal fusion framework that\nproduces dense, colorized point clouds, effectively capturing crack semantics\nat a 3D real-world scale. Furthermore, the crack geometric attributions were\nmeasured automatically and directly within 3D dense point cloud space,\nsurpassing the limitations of conventional 2D image-based measurements. This\nadvancement makes the method suitable for structural components with curved and\ncomplex 3D geometries. Experimental results across various concrete structures\nhighlight the significant improvements and unique advantages of the proposed\nmethod, demonstrating its effectiveness, accuracy, and robustness in real-world\napplications.\n","authors":["Pengru Deng","Jiapeng Yao","Chun Li","Su Wang","Xinrun Li","Varun Ojha","Xuhui He","Takashi Matsumoto"],"pdf_url":"https://arxiv.org/pdf/2501.09203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09520v2","updated":"2025-01-15T23:21:06Z","published":"2024-09-14T20:11:25Z","title":"Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery\n  with SAM","summary":"  Current AI-assisted skin image diagnosis has achieved dermatologist-level\nperformance in classifying skin cancer, driven by rapid advancements in deep\nlearning architectures. However, unlike traditional vision tasks, skin images\nin general present unique challenges due to the limited availability of\nwell-annotated datasets, complex variations in conditions, and the necessity\nfor detailed interpretations to ensure patient safety. Previous segmentation\nmethods have sought to reduce image noise and enhance diagnostic performance,\nbut these techniques require fine-grained, pixel-level ground truth masks for\ntraining. In contrast, with the rise of foundation models, the Segment Anything\nModel (SAM) has been introduced to facilitate promptable segmentation, enabling\nthe automation of the segmentation process with simple yet effective prompts.\nEfforts applying SAM predominantly focus on dermatoscopy images, which present\nmore easily identifiable lesion boundaries than clinical photos taken with\nsmartphones. This limitation constrains the practicality of these approaches to\nreal-world applications. To overcome the challenges posed by noisy clinical\nphotos acquired via non-standardized protocols and to improve diagnostic\naccessibility, we propose a novel Cross-Attentive Fusion framework for\ninterpretable skin lesion diagnosis. Our method leverages SAM to generate\nvisual concepts for skin diseases using prompts, integrating local visual\nconcepts with global image features to enhance model performance. Extensive\nevaluation on two skin disease datasets demonstrates our proposed method's\neffectiveness on lesion diagnosis and interpretability.\n","authors":["Xin Hu","Janet Wang","Jihun Hamm","Rie R Yotsu","Zhengming Ding"],"pdf_url":"https://arxiv.org/pdf/2409.09520v2.pdf","comment":"This paper is accepted by WACV 2025"},{"id":"http://arxiv.org/abs/2501.09194v1","updated":"2025-01-15T22:55:26Z","published":"2025-01-15T22:55:26Z","title":"Grounding Text-To-Image Diffusion Models For Controlled High-Quality\n  Image Generation","summary":"  Large-scale text-to-image (T2I) diffusion models have demonstrated an\noutstanding performance in synthesizing diverse high-quality visuals from\nnatural language text captions. Multiple layout-to-image models have been\ndeveloped to control the generation process by utilizing a broad array of\nlayouts such as segmentation maps, edges, and human keypoints. In this work, we\npresent ObjectDiffusion, a model that takes inspirations from the top\ncutting-edge image generative frameworks to seamlessly condition T2I models\nwith new bounding boxes capabilities. Specifically, we make substantial\nmodifications to the network architecture introduced in ContorlNet to integrate\nit with the condition processing and injection techniques proposed in GLIGEN.\nObjectDiffusion is initialized with pretraining parameters to leverage the\ngeneration knowledge obtained from training on large-scale datasets. We\nfine-tune ObjectDiffusion on the COCO2017 training dataset and evaluate it on\nthe COCO2017 validation dataset. Our model achieves an AP$_{50}$ of 46.6, an AR\nof 44.5, and a FID of 19.8 outperforming the current SOTA model trained on\nopen-source datasets in all of the three metrics. ObjectDiffusion demonstrates\na distinctive capability in synthesizing diverse, high-quality, high-fidelity\nimages that seamlessly conform to the semantic and spatial control layout.\nEvaluated in qualitative and quantitative tests, ObjectDiffusion exhibits\nremarkable grounding abilities on closed-set and open-set settings across a\nwide variety of contexts. The qualitative assessment verifies the ability of\nObjectDiffusion to generate multiple objects of different sizes and locations.\n","authors":["Ahmad Süleyman","Göksel Biricik"],"pdf_url":"https://arxiv.org/pdf/2501.09194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09187v1","updated":"2025-01-15T22:26:26Z","published":"2025-01-15T22:26:26Z","title":"Patch-aware Vector Quantized Codebook Learning for Unsupervised Visual\n  Defect Detection","summary":"  Unsupervised visual defect detection is critical in industrial applications,\nrequiring a representation space that captures normal data features while\ndetecting deviations. Achieving a balance between expressiveness and\ncompactness is challenging; an overly expressive space risks inefficiency and\nmode collapse, impairing detection accuracy. We propose a novel approach using\nan enhanced VQ-VAE framework optimized for unsupervised defect detection. Our\nmodel introduces a patch-aware dynamic code assignment scheme, enabling\ncontext-sensitive code allocation to optimize spatial representation. This\nstrategy enhances normal-defect distinction and improves detection accuracy\nduring inference. Experiments on MVTecAD, BTAD, and MTSD datasets show our\nmethod achieves state-of-the-art performance.\n","authors":["Qisen Cheng","Shuhui Qu","Janghwan Lee"],"pdf_url":"https://arxiv.org/pdf/2501.09187v1.pdf","comment":"7 pages, Accepted to 36th IEEE ICTAI 2024"},{"id":"http://arxiv.org/abs/2501.09185v1","updated":"2025-01-15T22:23:41Z","published":"2025-01-15T22:23:41Z","title":"Cancer-Net PCa-Seg: Benchmarking Deep Learning Models for Prostate\n  Cancer Segmentation Using Synthetic Correlated Diffusion Imaging","summary":"  Prostate cancer (PCa) is the most prevalent cancer among men in the United\nStates, accounting for nearly 300,000 cases, 29% of all diagnoses and 35,000\ntotal deaths in 2024. Traditional screening methods such as prostate-specific\nantigen (PSA) testing and magnetic resonance imaging (MRI) have been pivotal in\ndiagnosis, but have faced limitations in specificity and generalizability. In\nthis paper, we explore the potential of enhancing PCa lesion segmentation using\na novel MRI modality called synthetic correlated diffusion imaging (CDI$^s$).\nWe employ several state-of-the-art deep learning models, including U-Net,\nSegResNet, Swin UNETR, Attention U-Net, and LightM-UNet, to segment PCa lesions\nfrom a 200 CDI$^s$ patient cohort. We find that SegResNet achieved superior\nsegmentation performance with a Dice-Sorensen coefficient (DSC) of $76.68 \\pm\n0.8$. Notably, the Attention U-Net, while slightly less accurate (DSC $74.82\n\\pm 2.0$), offered a favorable balance between accuracy and computational\nefficiency. Our findings demonstrate the potential of deep learning models in\nimproving PCa lesion segmentation using CDI$^s$ to enhance PCa management and\nclinical support.\n","authors":["Jarett Dewbury","Chi-en Amy Tai","Alexander Wong"],"pdf_url":"https://arxiv.org/pdf/2501.09185v1.pdf","comment":"8 pages, 2 figures, to be published in Studies in Computational\n  Intelligence. This paper introduces Cancer-Net PCa-Seg, a comprehensive\n  evaluation of deep learning models for prostate cancer segmentation using\n  synthetic correlated diffusion imaging (CDI$^s$). We benchmark five\n  state-of-the-art architectures: U-Net, SegResNet, Swin UNETR, Attention\n  U-Net, and LightM-UNet"},{"id":"http://arxiv.org/abs/2312.11458v3","updated":"2025-01-15T22:17:24Z","published":"2023-12-18T18:59:03Z","title":"GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View\n  Synthesis","summary":"  We propose a method that achieves state-of-the-art rendering quality and\nefficiency on monocular dynamic scene reconstruction using deformable 3D\nGaussians. Implicit deformable representations commonly model motion with a\ncanonical space and time-dependent backward-warping deformation field. Our\nmethod, GauFRe, uses a forward-warping deformation to explicitly model\nnon-rigid transformations of scene geometry. Specifically, we propose a\ntemplate set of 3D Gaussians residing in a canonical space, and a\ntime-dependent forward-warping deformation field to model dynamic objects.\nAdditionally, we tailor a 3D Gaussian-specific static component supported by an\ninductive bias-aware initialization approach which allows the deformation field\nto focus on moving scene regions, improving the rendering of complex real-world\nmotion. The differentiable pipeline is optimized end-to-end with a\nself-supervised rendering loss. Experiments show our method achieves\ncompetitive results and higher efficiency than both previous state-of-the-art\nNeRF and Gaussian-based methods. For real-world scenes, GauFRe can train in ~20\nmins and offer 96 FPS real-time rendering on an RTX 3090 GPU. Project website:\nhttps://lynl7130.github.io/gaufre/index.html\n","authors":["Yiqing Liang","Numair Khan","Zhengqin Li","Thu Nguyen-Phuoc","Douglas Lanman","James Tompkin","Lei Xiao"],"pdf_url":"https://arxiv.org/pdf/2312.11458v3.pdf","comment":"WACV 2025. 11 pages, 8 figures, 5 tables"},{"id":"http://arxiv.org/abs/2501.09167v1","updated":"2025-01-15T21:36:19Z","published":"2025-01-15T21:36:19Z","title":"Embodied Scene Understanding for Vision Language Models via MetaVQA","summary":"  Vision Language Models (VLMs) demonstrate significant potential as embodied\nAI agents for various mobility applications. However, a standardized,\nclosed-loop benchmark for evaluating their spatial reasoning and sequential\ndecision-making capabilities is lacking. To address this, we present MetaVQA: a\ncomprehensive benchmark designed to assess and enhance VLMs' understanding of\nspatial relationships and scene dynamics through Visual Question Answering\n(VQA) and closed-loop simulations. MetaVQA leverages Set-of-Mark prompting and\ntop-down view ground-truth annotations from nuScenes and Waymo datasets to\nautomatically generate extensive question-answer pairs based on diverse\nreal-world traffic scenarios, ensuring object-centric and context-rich\ninstructions. Our experiments show that fine-tuning VLMs with the MetaVQA\ndataset significantly improves their spatial reasoning and embodied scene\ncomprehension in safety-critical simulations, evident not only in improved VQA\naccuracies but also in emerging safety-aware driving maneuvers. In addition,\nthe learning demonstrates strong transferability from simulation to real-world\nobservation. Code and data will be publicly available at\nhttps://metadriverse.github.io/metavqa .\n","authors":["Weizhen Wang","Chenda Duan","Zhenghao Peng","Yuxin Liu","Bolei Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.09167v1.pdf","comment":"for the project webpage, see https://metadriverse.github.io/metavqa"},{"id":"http://arxiv.org/abs/2501.09162v1","updated":"2025-01-15T21:28:47Z","published":"2025-01-15T21:28:47Z","title":"A Vessel Bifurcation Landmark Pair Dataset for Abdominal CT Deformable\n  Image Registration (DIR) Validation","summary":"  Deformable image registration (DIR) is an enabling technology in many\ndiagnostic and therapeutic tasks. Despite this, DIR algorithms have limited\nclinical use, largely due to a lack of benchmark datasets for quality assurance\nduring development. To support future algorithm development, here we introduce\nour first-of-its-kind abdominal CT DIR benchmark dataset, comprising large\nnumbers of highly accurate landmark pairs on matching blood vessel\nbifurcations. Abdominal CT image pairs of 30 patients were acquired from\nseveral public repositories as well as the authors' institution with IRB\napproval. The two CTs of each pair were originally acquired for the same\npatient on different days. An image processing workflow was developed and\napplied to each image pair: 1) Abdominal organs were segmented with a deep\nlearning model, and image intensity within organ masks was overwritten. 2)\nMatching image patches were manually identified between two CTs of each image\npair 3) Vessel bifurcation landmarks were labeled on one image of each image\npatch pair. 4) Image patches were deformably registered, and landmarks were\nprojected onto the second image. 5) Landmark pair locations were refined\nmanually or with an automated process. This workflow resulted in 1895 total\nlandmark pairs, or 63 per case on average. Estimates of the landmark pair\naccuracy using digital phantoms were 0.7+/-1.2mm. The data is published in\nZenodo at https://doi.org/10.5281/zenodo.14362785. Instructions for use can be\nfound at https://github.com/deshanyang/Abdominal-DIR-QA. This dataset is a\nfirst-of-its-kind for abdominal DIR validation. The number, accuracy, and\ndistribution of landmark pairs will allow for robust validation of DIR\nalgorithms with precision beyond what is currently available.\n","authors":["Edward R Criscuolo","Yao Hao","Zhendong Zhang","Trevor McKeown","Deshan Yang"],"pdf_url":"https://arxiv.org/pdf/2501.09162v1.pdf","comment":"19 pages, 3 figures"},{"id":"http://arxiv.org/abs/2501.09155v1","updated":"2025-01-15T21:14:36Z","published":"2025-01-15T21:14:36Z","title":"VCRScore: Image captioning metric based on V\\&L Transformers, CLIP, and\n  precision-recall","summary":"  Image captioning has become an essential Vision & Language research task. It\nis about predicting the most accurate caption given a specific image or video.\nThe research community has achieved impressive results by continuously\nproposing new models and approaches to improve the overall model's performance.\nNevertheless, despite increasing proposals, the performance metrics used to\nmeasure their advances have remained practically untouched through the years. A\nprobe of that, nowadays metrics like BLEU, METEOR, CIDEr, and ROUGE are still\nvery used, aside from more sophisticated metrics such as BertScore and\nClipScore.\n  Hence, it is essential to adjust how are measure the advances, limitations,\nand scopes of the new image captioning proposals, as well as to adapt new\nmetrics to these new advanced image captioning approaches.\n  This work proposes a new evaluation metric for the image captioning problem.\nTo do that, first, it was generated a human-labeled dataset to assess to which\ndegree the captions correlate with the image's content. Taking these human\nscores as ground truth, we propose a new metric, and compare it with several\nwell-known metrics, from classical to newer ones. Outperformed results were\nalso found, and interesting insights were presented and discussed.\n","authors":["Guillermo Ruiz","Tania Ramírez","Daniela Moctezuma"],"pdf_url":"https://arxiv.org/pdf/2501.09155v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2302.13336v2","updated":"2025-01-15T20:50:17Z","published":"2023-02-26T15:45:19Z","title":"Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early\n  Knee Osteoarthritis Detection","summary":"  Knee Osteoarthritis (KOA) is a common musculoskeletal condition that\nsignificantly affects mobility and quality of life, particularly in elderly\npopulations. However, training deep learning models for early KOA\nclassification is often hampered by the limited availability of annotated\nmedical datasets, owing to the high costs and labour-intensive nature of data\nlabelling. Traditional data augmentation techniques, while useful, rely on\nsimple transformations and fail to introduce sufficient diversity into the\ndataset. To address these challenges, we propose the Key-Exchange Convolutional\nAuto-Encoder (KECAE) as an innovative Artificial Intelligence (AI)-based data\naugmentation strategy for early KOA classification. Our model employs a\nconvolutional autoencoder with a novel key-exchange mechanism that generates\nsynthetic images by selectively exchanging key pathological features between\nX-ray images, which not only diversifies the dataset but also ensures the\nclinical validity of the augmented data. A hybrid loss function is introduced\nto supervise feature learning and reconstruction, integrating multiple\ncomponents, including reconstruction, supervision, and feature separation\nlosses. Experimental results demonstrate that the KECAE-generated data\nsignificantly improve the performance of KOA classification models, with\naccuracy gains of up to 1.98% across various standard and state-of-the-art\narchitectures. Furthermore, a clinical validation study involving expert\nradiologists confirms the anatomical plausibility and diagnostic realism of the\nsynthetic outputs. These findings highlight the potential of KECAE as a robust\ntool for augmenting medical datasets in early KOA detection.\n","authors":["Zhe Wang","Aladine Chetouani","Mohamed Jarraya","Yung Hsin Chen","Yuhua Ru","Fang Chen","Fabian Bauer","Liping Zhang","Didier Hans","Rachid Jennane"],"pdf_url":"https://arxiv.org/pdf/2302.13336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12141v2","updated":"2025-01-15T20:44:23Z","published":"2022-12-23T04:31:20Z","title":"Human Activity Recognition in an Open World","summary":"  Managing novelty in perception-based human activity recognition (HAR) is\ncritical in realistic settings to improve task performance over time and ensure\nsolution generalization outside of prior seen samples. Novelty manifests in HAR\nas unseen samples, activities, objects, environments, and sensor changes, among\nother ways. Novelty may be task-relevant, such as a new class or new features,\nor task-irrelevant resulting in nuisance novelty, such as never before seen\nnoise, blur, or distorted video recordings. To perform HAR optimally,\nalgorithmic solutions must be tolerant to nuisance novelty, and learn over time\nin the face of novelty. This paper 1) formalizes the definition of novelty in\nHAR building upon the prior definition of novelty in classification tasks, 2)\nproposes an incremental open world learning (OWL) protocol and applies it to\nthe Kinetics datasets to generate a new benchmark KOWL-718, 3) analyzes the\nperformance of current state-of-the-art HAR models when novelty is introduced\nover time, 4) provides a containerized and packaged pipeline for reproducing\nthe OWL protocol and for modifying for any future updates to Kinetics. The\nexperimental analysis includes an ablation study of how the different models\nperform under various conditions as annotated by Kinetics-AVA. The protocol as\nan algorithm for reproducing experiments using the KOWL-718 benchmark will be\npublicly released with code and containers at\nhttps://github.com/prijatelj/human-activity-recognition-in-an-open-world. The\ncode may be used to analyze different annotations and subsets of the Kinetics\ndatasets in an incremental open world fashion, as well as be extended as\nfurther updates to Kinetics are released.\n","authors":["Derek S. Prijatelj","Samuel Grieggs","Jin Huang","Dawei Du","Ameya Shringi","Christopher Funk","Adam Kaufman","Eric Robertson","Walter J. Scheirer"],"pdf_url":"https://arxiv.org/pdf/2212.12141v2.pdf","comment":"37 pages, 16 figures, 3 tables. Published in JAIR 81 on Dec 20, 2024.\n  All author affiliations are from during the paper's original funded work.\n  Updated info and current emails are provided in this version's first page"},{"id":"http://arxiv.org/abs/2501.09138v1","updated":"2025-01-15T20:44:21Z","published":"2025-01-15T20:44:21Z","title":"Few-Shot Adaptation of Training-Free Foundation Model for 3D Medical\n  Image Segmentation","summary":"  Vision foundation models have achieved remarkable progress across various\nimage analysis tasks. In the image segmentation task, foundation models like\nthe Segment Anything Model (SAM) enable generalizable zero-shot segmentation\nthrough user-provided prompts. However, SAM primarily trained on natural\nimages, lacks the domain-specific expertise of medical imaging. This limitation\nposes challenges when applying SAM to medical image segmentation, including the\nneed for extensive fine-tuning on specialized medical datasets and a dependency\non manual prompts, which are both labor-intensive and require intervention from\nmedical experts.\n  This work introduces the Few-shot Adaptation of Training-frEe SAM (FATE-SAM),\na novel method designed to adapt the advanced Segment Anything Model 2 (SAM2)\nfor 3D medical image segmentation. FATE-SAM reassembles pre-trained modules of\nSAM2 to enable few-shot adaptation, leveraging a small number of support\nexamples to capture anatomical knowledge and perform prompt-free segmentation,\nwithout requiring model fine-tuning. To handle the volumetric nature of medical\nimages, we incorporate a Volumetric Consistency mechanism that enhances spatial\ncoherence across 3D slices. We evaluate FATE-SAM on multiple medical imaging\ndatasets and compare it with supervised learning methods, zero-shot SAM\napproaches, and fine-tuned medical SAM methods. Results show that FATE-SAM\ndelivers robust and accurate segmentation while eliminating the need for large\nannotated datasets and expert intervention. FATE-SAM provides a practical,\nefficient solution for medical image segmentation, making it more accessible\nfor clinical applications.\n","authors":["Xingxin He","Yifan Hu","Zhaoye Zhou","Mohamed Jarraya","Fang Liu"],"pdf_url":"https://arxiv.org/pdf/2501.09138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13203v2","updated":"2025-01-15T20:41:42Z","published":"2023-03-23T11:57:50Z","title":"Confidence-Driven Deep Learning Framework for Early Detection of Knee\n  Osteoarthritis","summary":"  Knee Osteoarthritis (KOA) is a prevalent musculoskeletal disorder that\nseverely impacts mobility and quality of life, particularly among older adults.\nIts diagnosis often relies on subjective assessments using the\nKellgren-Lawrence (KL) grading system, leading to variability in clinical\nevaluations. To address these challenges, we propose a confidence-driven deep\nlearning framework for early KOA detection, focusing on distinguishing KL-0 and\nKL-2 stages. The Siamese-based framework integrates a novel multi-level feature\nextraction architecture with a hybrid loss strategy. Specifically, multi-level\nGlobal Average Pooling (GAP) layers are employed to extract features from\nvarying network depths, ensuring comprehensive feature representation, while\nthe hybrid loss strategy partitions training samples into high-, medium-, and\nlow-confidence subsets. Tailored loss functions are applied to improve model\nrobustness and effectively handle uncertainty in annotations. Experimental\nresults on the Osteoarthritis Initiative (OAI) dataset demonstrate that the\nproposed framework achieves competitive accuracy, sensitivity, and specificity,\ncomparable to those of expert radiologists. Cohen's kappa values (k > 0.85))\nconfirm substantial agreement, while McNemar's test (p > 0.05) indicates no\nstatistically significant differences between the model and radiologists.\nAdditionally, Confidence distribution analysis reveals that the model emulates\nradiologists' decision-making patterns. These findings highlight the potential\nof the proposed approach to serve as an auxiliary diagnostic tool, enhancing\nearly KOA detection and reducing clinical workload.\n","authors":["Zhe Wang","Aladine Chetouani","Yung Hsin Chen","Yuhua Ru","Fang Chen","Mohamed Jarraya","Fabian Bauer","Liping Zhang","Didier Hans","Rachid Jennane"],"pdf_url":"https://arxiv.org/pdf/2303.13203v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09134v1","updated":"2025-01-15T20:37:04Z","published":"2025-01-15T20:37:04Z","title":"Benchmarking Robustness of Contrastive Learning Models for Medical\n  Image-Report Retrieval","summary":"  Medical images and reports offer invaluable insights into patient health. The\nheterogeneity and complexity of these data hinder effective analysis. To bridge\nthis gap, we investigate contrastive learning models for cross-domain\nretrieval, which associates medical images with their corresponding clinical\nreports. This study benchmarks the robustness of four state-of-the-art\ncontrastive learning models: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. We\nintroduce an occlusion retrieval task to evaluate model performance under\nvarying levels of image corruption. Our findings reveal that all evaluated\nmodels are highly sensitive to out-of-distribution data, as evidenced by the\nproportional decrease in performance with increasing occlusion levels. While\nMedCLIP exhibits slightly more robustness, its overall performance remains\nsignificantly behind CXR-CLIP and CXR-RePaiR. CLIP, trained on a\ngeneral-purpose dataset, struggles with medical image-report retrieval,\nhighlighting the importance of domain-specific training data. The evaluation of\nthis work suggests that more effort needs to be spent on improving the\nrobustness of these models. By addressing these limitations, we can develop\nmore reliable cross-domain retrieval models for medical applications.\n","authors":["Demetrio Deanda","Yuktha Priya Masupalli","Jeong Yang","Young Lee","Zechun Cao","Gongbo Liang"],"pdf_url":"https://arxiv.org/pdf/2501.09134v1.pdf","comment":"This work is accepted to AAAI 2025 Workshop -- the 9th International\n  Workshop on Health Intelligence"},{"id":"http://arxiv.org/abs/2501.09129v1","updated":"2025-01-15T20:24:18Z","published":"2025-01-15T20:24:18Z","title":"Deep Self-Supervised Disturbance Mapping with the OPERA Sentinel-1\n  Radiometric Terrain Corrected SAR Backscatter Product","summary":"  Mapping land surface disturbances supports disaster response, resource and\necosystem management, and climate adaptation efforts. Synthetic aperture radar\n(SAR) is an invaluable tool for disturbance mapping, providing consistent\ntime-series images of the ground regardless of weather or illumination\nconditions. Despite SAR's potential for disturbance mapping, processing SAR\ndata to an analysis-ready format requires expertise and significant compute\nresources, particularly for large-scale global analysis. In October 2023,\nNASA's Observational Products for End-Users from Remote Sensing Analysis\n(OPERA) project released the near-global Radiometric Terrain Corrected SAR\nbackscatter from Sentinel-1 (RTC-S1) dataset, providing publicly available,\nanalysis-ready SAR imagery. In this work, we utilize this new dataset to\nsystematically analyze land surface disturbances. As labeling SAR data is often\nprohibitively time-consuming, we train a self-supervised vision transformer -\nwhich requires no labels to train - on OPERA RTC-S1 data to estimate a\nper-pixel distribution from the set of baseline imagery and assess disturbances\nwhen there is significant deviation from the modeled distribution. To test our\nmodel's capability and generality, we evaluate three different natural\ndisasters - which represent high-intensity, abrupt disturbances - from three\ndifferent regions of the world. Across events, our approach yields high quality\ndelineations: F1 scores exceeding 0.6 and Areas Under the Precision-Recall\nCurve exceeding 0.65, consistently outperforming existing SAR disturbance\nmethods. Our findings suggest that a self-supervised vision transformer is\nwell-suited for global disturbance mapping and can be a valuable tool for\noperational, near-global disturbance monitoring, particularly when labeled data\ndoes not exist.\n","authors":["Harris Hardiman-Mostow","Charles Marshak","Alexander L. Handwerger"],"pdf_url":"https://arxiv.org/pdf/2501.09129v1.pdf","comment":"19 pages, 18 figures, 5 tables. Preprint. Submitted to JSTARS"},{"id":"http://arxiv.org/abs/2501.09116v1","updated":"2025-01-15T19:52:02Z","published":"2025-01-15T19:52:02Z","title":"Deep Distance Map Regression Network with Shape-aware Loss for\n  Imbalanced Medical Image Segmentation","summary":"  Small object segmentation, like tumor segmentation, is a difficult and\ncritical task in the field of medical image analysis. Although deep learning\nbased methods have achieved promising performance, they are restricted to the\nuse of binary segmentation mask. Inspired by the rigorous mapping between\nbinary segmentation mask and distance map, we adopt distance map as a novel\nground truth and employ a network to fulfill the computation of distance map.\nSpecially, we propose a new segmentation framework that incorporates the\nexisting binary segmentation network and a light weight regression network\n(dubbed as LR-Net). Thus, the LR-Net can convert the distance map computation\ninto a regression task and leverage the rich information of distance maps.\nAdditionally, we derive a shape-aware loss by employing distance maps as\npenalty map to infer the complete shape of an object. We evaluated our approach\non MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge dataset and a clinical\ndataset. Experimental results show that our approach outperforms the\nclassification-based methods as well as other existing state-of-the-arts.\n","authors":["Huiyu Li","Xiabi Liu","Said Boumaraf","Xiaopeng Gong","Donghai Liao","Xiaohong Ma"],"pdf_url":"https://arxiv.org/pdf/2501.09116v1.pdf","comment":"Conference"},{"id":"http://arxiv.org/abs/2501.09114v1","updated":"2025-01-15T19:50:56Z","published":"2025-01-15T19:50:56Z","title":"Generative Medical Image Anonymization Based on Latent Code Projection\n  and Optimization","summary":"  Medical image anonymization aims to protect patient privacy by removing\nidentifying information, while preserving the data utility to solve downstream\ntasks. In this paper, we address the medical image anonymization problem with a\ntwo-stage solution: latent code projection and optimization. In the projection\nstage, we design a streamlined encoder to project input images into a latent\nspace and propose a co-training scheme to enhance the projection process. In\nthe optimization stage, we refine the latent code using two deep loss functions\ndesigned to address the trade-off between identity protection and data utility\ndedicated to medical images. Through a comprehensive set of qualitative and\nquantitative experiments, we showcase the effectiveness of our approach on the\nMIMIC-CXR chest X-ray dataset by generating anonymized synthetic images that\ncan serve as training set for detecting lung pathologies. Source codes are\navailable at https://github.com/Huiyu-Li/GMIA.\n","authors":["Huiyu Li","Nicholas Ayache","Hervé Delingette"],"pdf_url":"https://arxiv.org/pdf/2501.09114v1.pdf","comment":"Conference"},{"id":"http://arxiv.org/abs/2501.09101v1","updated":"2025-01-15T19:37:18Z","published":"2025-01-15T19:37:18Z","title":"Relation U-Net","summary":"  Towards clinical interpretations, this paper presents a new\n''output-with-confidence'' segmentation neural network with multiple input\nimages and multiple output segmentation maps and their pairwise relations. A\nconfidence score of the test image without ground-truth can be estimated from\nthe difference among the estimated relation maps. We evaluate the method based\non the widely used vanilla U-Net for segmentation and our new model is named\nRelation U-Net which can output segmentation maps of the input images as well\nas an estimated confidence score of the test image without ground-truth.\nExperimental results on four public datasets show that Relation U-Net can not\nonly provide better accuracy than vanilla U-Net but also estimate a confidence\nscore which is linearly correlated to the segmentation accuracy on test images.\n","authors":["Sheng He","Rina Bao","P. Ellen Grant","Yangming Ou"],"pdf_url":"https://arxiv.org/pdf/2501.09101v1.pdf","comment":"ISIB 2025"},{"id":"http://arxiv.org/abs/2501.09096v1","updated":"2025-01-15T19:29:31Z","published":"2025-01-15T19:29:31Z","title":"Self Pre-training with Adaptive Mask Autoencoders for Variable-Contrast\n  3D Medical Imaging","summary":"  The Masked Autoencoder (MAE) has recently demonstrated effectiveness in\npre-training Vision Transformers (ViT) for analyzing natural images. By\nreconstructing complete images from partially masked inputs, the ViT encoder\ngathers contextual information to predict the missing regions. This capability\nto aggregate context is especially important in medical imaging, where\nanatomical structures are functionally and mechanically linked to surrounding\nregions. However, current methods do not consider variations in the number of\ninput images, which is typically the case in real-world Magnetic Resonance (MR)\nstudies. To address this limitation, we propose a 3D Adaptive Masked\nAutoencoders (AMAE) architecture that accommodates a variable number of 3D\ninput contrasts per subject. A magnetic resonance imaging (MRI) dataset of\n45,364 subjects was used for pretraining and a subset of 1648 training, 193\nvalidation and 215 test subjects were used for finetuning. The performance\ndemonstrates that self pre-training of this adaptive masked autoencoders can\nenhance the infarct segmentation performance by 2.8%-3.7% for ViT-based\nsegmentation models.\n","authors":["Badhan Kumar Das","Gengyan Zhao","Han Liu","Thomas J. Re","Dorin Comaniciu","Eli Gibson","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2501.09096v1.pdf","comment":"5 pages, ISBI 2025 accepted"},{"id":"http://arxiv.org/abs/2311.12068v4","updated":"2025-01-15T19:28:27Z","published":"2023-11-19T17:28:28Z","title":"Enhancing Novel Object Detection via Cooperative Foundational Models","summary":"  In this work, we address the challenging and emergent problem of novel object\ndetection (NOD), focusing on the accurate detection of both known and novel\nobject categories during inference. Traditional object detection algorithms are\ninherently closed-set, limiting their capability to handle NOD. We present a\nnovel approach to transform existing closed-set detectors into open-set\ndetectors. This transformation is achieved by leveraging the complementary\nstrengths of pre-trained foundational models, specifically CLIP and SAM,\nthrough our cooperative mechanism. Furthermore, by integrating this mechanism\nwith state-of-the-art open-set detectors such as GDINO, we establish new\nbenchmarks in object detection performance. Our method achieves 17.42 mAP in\nnovel object detection and 42.08 mAP for known objects on the challenging LVIS\ndataset. Adapting our approach to the COCO OVD split, we surpass the current\nstate-of-the-art by a margin of 7.2 $ \\text{AP}_{50} $ for novel classes. Our\ncode is available at https://rohit901.github.io/coop-foundation-models/ .\n","authors":["Rohit Bharadwaj","Muzammal Naseer","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2311.12068v4.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2501.09086v1","updated":"2025-01-15T19:12:59Z","published":"2025-01-15T19:12:59Z","title":"Salient Information Preserving Adversarial Training Improves Clean and\n  Robust Accuracy","summary":"  In this work we introduce Salient Information Preserving Adversarial Training\n(SIP-AT), an intuitive method for relieving the robustness-accuracy trade-off\nincurred by traditional adversarial training. SIP-AT uses salient image regions\nto guide the adversarial training process in such a way that fragile features\ndeemed meaningful by an annotator remain unperturbed during training, allowing\nmodels to learn highly predictive non-robust features without sacrificing\noverall robustness. This technique is compatible with both human-based and\nautomatically generated salience estimates, allowing SIP-AT to be used as a\npart of human-driven model development without forcing SIP-AT to be reliant\nupon additional human data. We perform experiments across multiple datasets and\narchitectures and demonstrate that SIP-AT is able to boost the clean accuracy\nof models while maintaining a high degree of robustness against attacks at\nmultiple epsilon levels. We complement our central experiments with an\nobservational study measuring the rate at which human subjects successfully\nidentify perturbed images. This study helps build a more intuitive\nunderstanding of adversarial attack strength and demonstrates the heightened\nimportance of low-epsilon robustness. Our results demonstrate the efficacy of\nSIP-AT and provide valuable insight into the risks posed by adversarial samples\nof various strengths.\n","authors":["Timothy Redgrave","Adam Czajka"],"pdf_url":"https://arxiv.org/pdf/2501.09086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19794v3","updated":"2025-01-15T19:04:48Z","published":"2024-12-27T18:47:05Z","title":"MVTamperBench: Evaluating Robustness of Vision-Language Models","summary":"  Recent advancements in Vision-Language Models (VLMs) have enabled significant\nprogress in complex video understanding tasks. However, their robustness to\nreal-world manipulations remains underexplored, limiting their reliability in\ncritical applications. To address this gap, we introduce MVTamperBench, a\ncomprehensive benchmark designed to evaluate VLM's resilience to video\ntampering effects, including rotation, dropping, masking, substitution, and\nrepetition. By systematically assessing state-of-the-art models, MVTamperBench\nreveals substantial variability in robustness, with models like InternVL2-8B\nachieving high performance, while others, such as Llama-VILA1.5-8B, exhibit\nsevere vulnerabilities. To foster broader adoption and reproducibility,\nMVTamperBench is integrated into VLMEvalKit, a modular evaluation toolkit,\nenabling streamlined testing and facilitating advancements in model robustness.\nOur benchmark represents a critical step towards developing tamper-resilient\nVLMs, ensuring their dependability in real-world scenarios.\n  Project Page: https://amitbcp.github.io/MVTamperBench/\n","authors":["Amit Agarwal","Srikant Panda","Angeline Charles","Bhargava Kumar","Hitesh Patel","Priyaranjan Pattnayak","Taki Hasan Rafi","Tejaswini Kumar","Dong-Kyu Chae"],"pdf_url":"https://arxiv.org/pdf/2412.19794v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09055v1","updated":"2025-01-15T18:39:03Z","published":"2025-01-15T18:39:03Z","title":"SHYI: Action Support for Contrastive Learning in High-Fidelity\n  Text-to-Image Generation","summary":"  In this project, we address the issue of infidelity in text-to-image\ngeneration, particularly for actions involving multiple objects. For this we\nbuild on top of the CONFORM framework which uses Contrastive Learning to\nimprove the accuracy of the generated image for multiple objects. However the\ndepiction of actions which involves multiple different object has still large\nroom for improvement. To improve, we employ semantically hypergraphic\ncontrastive adjacency learning, a comprehension of enhanced contrastive\nstructure and \"contrast but link\" technique. We further amend Stable\nDiffusion's understanding of actions by InteractDiffusion. As evaluation\nmetrics we use image-text similarity CLIP and TIFA. In addition, we conducted a\nuser study.\n  Our method shows promising results even with verbs that Stable Diffusion\nunderstands mediocrely. We then provide future directions by analyzing the\nresults.\n  Our codebase can be found on polybox under the link:\nhttps://polybox.ethz.ch/index.php/s/dJm3SWyRohUrFxn\n","authors":["Tianxiang Xia","Lin Xiao","Yannick Montorfani","Francesco Pavia","Enis Simsar","Thomas Hofmann"],"pdf_url":"https://arxiv.org/pdf/2501.09055v1.pdf","comment":"Main content 4 pages"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2501.08927v1","updated":"2025-01-15T16:30:45Z","published":"2025-01-15T16:30:45Z","title":"Continuous Approach to Phase (Norm) Retrieval Frames","summary":"  This paper investigates the properties of continuous frames, with a\nparticular focus on phase retrieval and norm retrieval in the context of\nHilbert spaces. We introduce the concept of continuous near-Riesz bases and\nprove their invariance under invertible operators. Some equivalent conditions\nfor phase and norm retrieval property of continuous frames are presented. We\nstudy the stability of phase retrieval under perturbations. Furthermore, tensor\nproduct frames for separable Hilbert spaces are studied, and we establish the\nequivalence of phase retrieval and norm retrieval properties between components\nand their tensor products.\n","authors":["Ramin Farshchian","Rajab Ali Kamyabi-Gol","Fahimeh Arabyani-Neyshaburi","Fatemeh Esmaeelzadeh"],"pdf_url":"https://arxiv.org/pdf/2501.08927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08828v1","updated":"2025-01-15T14:30:13Z","published":"2025-01-15T14:30:13Z","title":"MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents","summary":"  Multi-modal document retrieval is designed to identify and retrieve various\nforms of multi-modal content, such as figures, tables, charts, and layout\ninformation from extensive documents. Despite its significance, there is a\nnotable lack of a robust benchmark to effectively evaluate the performance of\nsystems in multi-modal document retrieval. To address this gap, this work\nintroduces a new benchmark, named as MMDocIR, encompassing two distinct tasks:\npage-level and layout-level retrieval. The former focuses on localizing the\nmost relevant pages within a long document, while the latter targets the\ndetection of specific layouts, offering a more fine-grained granularity than\nwhole-page analysis. A layout can refer to a variety of elements such as\ntextual paragraphs, equations, figures, tables, or charts. The MMDocIR\nbenchmark comprises a rich dataset featuring expertly annotated labels for\n1,685 questions and bootstrapped labels for 173,843 questions, making it a\npivotal resource for advancing multi-modal document retrieval for both training\nand evaluation. Through rigorous experiments, we reveal that (i) visual\nretrievers significantly outperform their text counterparts, (ii) MMDocIR train\nset can effectively benefit the training process of multi-modal document\nretrieval and (iii) text retrievers leveraging on VLM-text perform much better\nthan those using OCR-text. These findings underscores the potential advantages\nof integrating visual elements for multi-modal document retrieval.\n","authors":["Kuicai Dong","Yujing Chang","Xin Deik Goh","Dexun Li","Ruiming Tang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08828v1.pdf","comment":"https://huggingface.co/MMDocIR"},{"id":"http://arxiv.org/abs/2501.08717v1","updated":"2025-01-15T10:58:32Z","published":"2025-01-15T10:58:32Z","title":"$\\texttt{InfoHier}$: Hierarchical Information Extraction via Encoding\n  and Embedding","summary":"  Analyzing large-scale datasets, especially involving complex and\nhigh-dimensional data like images, is particularly challenging. While\nself-supervised learning (SSL) has proven effective for learning\nrepresentations from unlabelled data, it typically focuses on flat,\nnon-hierarchical structures, missing the multi-level relationships present in\nmany real-world datasets. Hierarchical clustering (HC) can uncover these\nrelationships by organizing data into a tree-like structure, but it often\nrelies on rigid similarity metrics that struggle to capture the complexity of\ndiverse data types. To address these we envision $\\texttt{InfoHier}$, a\nframework that combines SSL with HC to jointly learn robust latent\nrepresentations and hierarchical structures. This approach leverages SSL to\nprovide adaptive representations, enhancing HC's ability to capture complex\npatterns. Simultaneously, it integrates HC loss to refine SSL training,\nresulting in representations that are more attuned to the underlying\ninformation hierarchy. $\\texttt{InfoHier}$ has the potential to improve the\nexpressiveness and performance of both clustering and representation learning,\noffering significant benefits for data analysis, management, and information\nretrieval.\n","authors":["Tianru Zhang","Li Ju","Prashant Singh","Salman Toor"],"pdf_url":"https://arxiv.org/pdf/2501.08717v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2501.08695v1","updated":"2025-01-15T10:09:15Z","published":"2025-01-15T10:09:15Z","title":"Real-time Indexing for Large-scale Recommendation by Streaming Vector\n  Quantization Retriever","summary":"  Retrievers, which form one of the most important recommendation stages, are\nresponsible for efficiently selecting possible positive samples to the later\nstages under strict latency limitations. Because of this, large-scale systems\nalways rely on approximate calculations and indexes to roughly shrink candidate\nscale, with a simple ranking model. Considering simple models lack the ability\nto produce precise predictions, most of the existing methods mainly focus on\nincorporating complicated ranking models. However, another fundamental problem\nof index effectiveness remains unresolved, which also bottlenecks complication.\nIn this paper, we propose a novel index structure: streaming Vector\nQuantization model, as a new generation of retrieval paradigm. Streaming VQ\nattaches items with indexes in real time, granting it immediacy. Moreover,\nthrough meticulous verification of possible variants, it achieves additional\nbenefits like index balancing and reparability, enabling it to support\ncomplicated ranking models as existing approaches. As a lightweight and\nimplementation-friendly architecture, streaming VQ has been deployed and\nreplaced all major retrievers in Douyin and Douyin Lite, resulting in\nremarkable user engagement gain.\n","authors":["Xingyan Bin","Jianfei Cui","Wujie Yan","Zhichen Zhao","Xintian Han","Chongyang Yan","Feng Zhang","Xun Zhou","Qi Wu","Zuotao Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08686v1","updated":"2025-01-15T09:32:37Z","published":"2025-01-15T09:32:37Z","title":"Knowledge Graph-based Retrieval-Augmented Generation for Schema Matching","summary":"  Traditional similarity-based schema matching methods are incapable of\nresolving semantic ambiguities and conflicts in domain-specific complex mapping\nscenarios due to missing commonsense and domain-specific knowledge. The\nhallucination problem of large language models (LLMs) also makes it challenging\nfor LLM-based schema matching to address the above issues. Therefore, we\npropose a Knowledge Graph-based Retrieval-Augmented Generation model for Schema\nMatching, referred to as the KG-RAG4SM. In particular, KG-RAG4SM introduces\nnovel vector-based, graph traversal-based, and query-based graph retrievals, as\nwell as a hybrid approach and ranking schemes that identify the most relevant\nsubgraphs from external large knowledge graphs (KGs). We showcase that KG-based\nretrieval-augmented LLMs are capable of generating more accurate results for\ncomplex matching cases without any re-training. Our experimental results show\nthat KG-RAG4SM outperforms the LLM-based state-of-the-art (SOTA) methods (e.g.,\nJellyfish-8B) by 35.89% and 30.50% in terms of precision and F1 score on the\nMIMIC dataset, respectively; KG-RAG4SM with GPT-4o-mini outperforms the\npre-trained language model (PLM)-based SOTA methods (e.g., SMAT) by 69.20% and\n21.97% in terms of precision and F1 score on the Synthea dataset, respectively.\nThe results also demonstrate that our approach is more efficient in end-to-end\nschema matching, and scales to retrieve from large KGs. Our case studies on the\ndataset from the real-world schema matching scenario exhibit that the\nhallucination problem of LLMs for schema matching is well mitigated by our\nsolution.\n","authors":["Chuangtao Ma","Sriom Chakrabarti","Arijit Khan","Bálint Molnár"],"pdf_url":"https://arxiv.org/pdf/2501.08686v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2401.15299v3","updated":"2025-01-15T09:23:55Z","published":"2024-01-27T05:14:17Z","title":"SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph\n  Neural Networks","summary":"  Graph Neural Networks (GNNs) have gained traction across different domains\nsuch as transportation, bio-informatics, language processing, and computer\nvision. However, there is a noticeable absence of research on applying GNNs to\nsupply chain networks. Supply chain networks are inherently graph-like in\nstructure, making them prime candidates for applying GNN methodologies. This\nopens up a world of possibilities for optimizing, predicting, and solving even\nthe most complex supply chain problems. A major setback in this approach lies\nin the absence of real-world benchmark datasets to facilitate the research and\nresolution of supply chain problems using GNNs. To address the issue, we\npresent a real-world benchmark dataset for temporal tasks, obtained from one of\nthe leading FMCG companies in Bangladesh, focusing on supply chain planning for\nproduction purposes. The dataset includes temporal data as node features to\nenable sales predictions, production planning, and the identification of\nfactory issues. By utilizing this dataset, researchers can employ GNNs to\naddress numerous supply chain problems, thereby advancing the field of supply\nchain analytics and planning. Source: https://github.com/CIOL-SUST/SupplyGraph\n","authors":["Azmine Toushik Wasi","MD Shafikul Islam","Adipto Raihan Akib"],"pdf_url":"https://arxiv.org/pdf/2401.15299v3.pdf","comment":"Accepted to 4th workshop on Graphs and more Complex structures for\n  Learning and Reasoning, colocated with AAAI 2024"},{"id":"http://arxiv.org/abs/2407.19692v3","updated":"2025-01-15T05:04:59Z","published":"2024-07-29T04:30:38Z","title":"Fusion Self-supervised Learning for Recommendation","summary":"  Recommender systems are widely deployed in various web environments, and\nself-supervised learning (SSL) has recently attracted significant attention in\nthis field. Contrastive learning (CL) stands out as a major SSL paradigm due to\nits robust ability to generate self-supervised signals. Mainstream graph\ncontrastive learning (GCL)-based methods typically implement CL by creating\ncontrastive views through various data augmentation techniques. Despite these\nmethods are effective, we argue that there still exist several challenges. i)\nData augmentation ($e.g.,$ discarding edges or adding noise) necessitates\nadditional graph convolution (GCN) or modeling operations, which are highly\ntime-consuming and potentially harm the embedding quality. ii) Existing\nCL-based methods use traditional CL objectives to capture self-supervised\nsignals. However, few studies have explored obtaining CL objectives from more\nperspectives and have attempted to fuse the varying signals from these CL\nobjectives to enhance recommendation performance.\n  To overcome these challenges, we propose a High-order Fusion Graph\nContrastive Learning (HFGCL) framework for recommendation. Specifically,\ninstead of facilitating data augmentations, we use high-order information from\nGCN process to create contrastive views. Additionally, to integrate\nself-supervised signals from various CL objectives, we propose an advanced CL\nobjective. By ensuring that positive pairs are distanced from negative samples\nderived from both contrastive views, we effectively fuse self-supervised\nsignals from distinct CL objectives, thereby enhancing the mutual information\nbetween positive pairs. Experimental results on three public datasets\ndemonstrate the superior recommendation performance and efficiency of HFGCL\ncompared to the state-of-the-art baselines.\n","authors":["Yu Zhang","Lei Sang","Yi Zhang","Yiwen Zhang","Yun Yang"],"pdf_url":"https://arxiv.org/pdf/2407.19692v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08572v1","updated":"2025-01-15T04:36:55Z","published":"2025-01-15T04:36:55Z","title":"DNMDR: Dynamic Networks and Multi-view Drug Representations for Safe\n  Medication Recommendation","summary":"  Medication Recommendation (MR) is a promising research topic which booms\ndiverse applications in the healthcare and clinical domains. However, existing\nmethods mainly rely on sequential modeling and static graphs for representation\nlearning, which ignore the dynamic correlations in diverse medical events of a\npatient's temporal visits, leading to insufficient global structural\nexploration on nodes. Additionally, mitigating drug-drug interactions (DDIs) is\nanother issue determining the utility of the MR systems. To address the\nchallenges mentioned above, this paper proposes a novel MR method with the\nintegration of dynamic networks and multi-view drug representations (DNMDR).\nSpecifically, weighted snapshot sequences for dynamic heterogeneous networks\nare constructed based on discrete visits in temporal EHRs, and all the dynamic\nnetworks are jointly trained to gain both structural correlations in diverse\nmedical events and temporal dependency in historical health conditions, for\nachieving comprehensive patient representations with both semantic features and\nstructural relationships. Moreover, combining the drug co-occurrences and\nadverse drug-drug interactions (DDIs) in internal view of drug molecule\nstructure and interactive view of drug pairs, the safe drug representations are\navailable to obtain high-quality medication combination recommendation.\nFinally, extensive experiments on real world datasets are conducted for\nperformance evaluation, and the experimental results demonstrate that the\nproposed DNMDR method outperforms the state-of-the-art baseline models with a\nlarge margin on various metrics such as PRAUC, Jaccard, DDI rates and so on.\n","authors":["Guanlin Liu","Xiaomei Yu","Zihao Liu","Xue Li","Xingxu Fan","Xiangwei Zheng"],"pdf_url":"https://arxiv.org/pdf/2501.08572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09186v1","updated":"2025-01-15T22:23:53Z","published":"2025-01-15T22:23:53Z","title":"Guiding Retrieval using LLM-based Listwise Rankers","summary":"  Large Language Models (LLMs) have shown strong promise as rerankers,\nespecially in ``listwise'' settings where an LLM is prompted to rerank several\nsearch results at once. However, this ``cascading'' retrieve-and-rerank\napproach is limited by the bounded recall problem: relevant documents not\nretrieved initially are permanently excluded from the final ranking. Adaptive\nretrieval techniques address this problem, but do not work with listwise\nrerankers because they assume a document's score is computed independently from\nother documents. In this paper, we propose an adaptation of an existing\nadaptive retrieval method that supports the listwise setting and helps guide\nthe retrieval process itself (thereby overcoming the bounded recall problem for\nLLM rerankers). Specifically, our proposed algorithm merges results both from\nthe initial ranking and feedback documents provided by the most relevant\ndocuments seen up to that point. Through extensive experiments across diverse\nLLM rerankers, first stage retrievers, and feedback sources, we demonstrate\nthat our method can improve nDCG@10 by up to 13.23% and recall by 28.02%--all\nwhile keeping the total number of LLM inferences constant and overheads due to\nthe adaptive process minimal. The work opens the door to leveraging LLM-based\nsearch in settings where the initial pool of results is limited, e.g., by\nlegacy systems, or by the cost of deploying a semantic first-stage.\n","authors":["Mandeep Rathee","Sean MacAvaney","Avishek Anand"],"pdf_url":"https://arxiv.org/pdf/2501.09186v1.pdf","comment":"16 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2501.09136v1","updated":"2025-01-15T20:40:25Z","published":"2025-01-15T20:40:25Z","title":"Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG","summary":"  Large Language Models (LLMs) have revolutionized artificial intelligence (AI)\nby enabling human like text generation and natural language understanding.\nHowever, their reliance on static training data limits their ability to respond\nto dynamic, real time queries, resulting in outdated or inaccurate outputs.\nRetrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs\nby integrating real time data retrieval to provide contextually relevant and\nup-to-date responses. Despite its promise, traditional RAG systems are\nconstrained by static workflows and lack the adaptability required for\nmultistep reasoning and complex task management.\n  Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these\nlimitations by embedding autonomous AI agents into the RAG pipeline. These\nagents leverage agentic design patterns reflection, planning, tool use, and\nmultiagent collaboration to dynamically manage retrieval strategies,\niteratively refine contextual understanding, and adapt workflows to meet\ncomplex task requirements. This integration enables Agentic RAG systems to\ndeliver unparalleled flexibility, scalability, and context awareness across\ndiverse applications.\n  This survey provides a comprehensive exploration of Agentic RAG, beginning\nwith its foundational principles and the evolution of RAG paradigms. It\npresents a detailed taxonomy of Agentic RAG architectures, highlights key\napplications in industries such as healthcare, finance, and education, and\nexamines practical implementation strategies. Additionally, it addresses\nchallenges in scaling these systems, ensuring ethical decision making, and\noptimizing performance for real-world applications, while providing detailed\ninsights into frameworks and tools for implementing Agentic RAG\n","authors":["Aditi Singh","Abul Ehtesham","Saket Kumar","Tala Talaei Khoei"],"pdf_url":"https://arxiv.org/pdf/2501.09136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09134v1","updated":"2025-01-15T20:37:04Z","published":"2025-01-15T20:37:04Z","title":"Benchmarking Robustness of Contrastive Learning Models for Medical\n  Image-Report Retrieval","summary":"  Medical images and reports offer invaluable insights into patient health. The\nheterogeneity and complexity of these data hinder effective analysis. To bridge\nthis gap, we investigate contrastive learning models for cross-domain\nretrieval, which associates medical images with their corresponding clinical\nreports. This study benchmarks the robustness of four state-of-the-art\ncontrastive learning models: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. We\nintroduce an occlusion retrieval task to evaluate model performance under\nvarying levels of image corruption. Our findings reveal that all evaluated\nmodels are highly sensitive to out-of-distribution data, as evidenced by the\nproportional decrease in performance with increasing occlusion levels. While\nMedCLIP exhibits slightly more robustness, its overall performance remains\nsignificantly behind CXR-CLIP and CXR-RePaiR. CLIP, trained on a\ngeneral-purpose dataset, struggles with medical image-report retrieval,\nhighlighting the importance of domain-specific training data. The evaluation of\nthis work suggests that more effort needs to be spent on improving the\nrobustness of these models. By addressing these limitations, we can develop\nmore reliable cross-domain retrieval models for medical applications.\n","authors":["Demetrio Deanda","Yuktha Priya Masupalli","Jeong Yang","Young Lee","Zechun Cao","Gongbo Liang"],"pdf_url":"https://arxiv.org/pdf/2501.09134v1.pdf","comment":"This work is accepted to AAAI 2025 Workshop -- the 9th International\n  Workshop on Health Intelligence"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2501.09009v1","updated":"2025-01-15T18:50:52Z","published":"2025-01-15T18:50:52Z","title":"Towards Fast, Specialized Machine Learning Force Fields: Distilling\n  Foundation Models via Energy Hessians","summary":"  The foundation model (FM) paradigm is transforming Machine Learning Force\nFields (MLFFs), leveraging general-purpose representations and scalable\ntraining to perform a variety of computational chemistry tasks. Although MLFF\nFMs have begun to close the accuracy gap relative to first-principles methods,\nthere is still a strong need for faster inference speed. Additionally, while\nresearch is increasingly focused on general-purpose models which transfer\nacross chemical space, practitioners typically only study a small subset of\nsystems at a given time. This underscores the need for fast, specialized MLFFs\nrelevant to specific downstream applications, which preserve test-time physical\nsoundness while maintaining train-time scalability. In this work, we introduce\na method for transferring general-purpose representations from MLFF foundation\nmodels to smaller, faster MLFFs specialized to specific regions of chemical\nspace. We formulate our approach as a knowledge distillation procedure, where\nthe smaller \"student\" MLFF is trained to match the Hessians of the energy\npredictions of the \"teacher\" foundation model. Our specialized MLFFs can be up\nto 20 $\\times$ faster than the original foundation model, while retaining, and\nin some cases exceeding, its performance and that of undistilled models. We\nalso show that distilling from a teacher model with a direct force\nparameterization into a student model trained with conservative forces (i.e.,\ncomputed as derivatives of the potential energy) successfully leverages the\nrepresentations from the large-scale teacher for improved accuracy, while\nmaintaining energy conservation during test-time molecular dynamics\nsimulations. More broadly, our work suggests a new paradigm for MLFF\ndevelopment, in which foundation models are released along with smaller,\nspecialized simulation \"engines\" for common chemical subsets.\n","authors":["Ishan Amin","Sanjeev Raja","Aditi Krishnapriyan"],"pdf_url":"https://arxiv.org/pdf/2501.09009v1.pdf","comment":"Under Review at ICLR 2025"},{"id":"http://arxiv.org/abs/2501.09006v1","updated":"2025-01-15T18:45:05Z","published":"2025-01-15T18:45:05Z","title":"Improving Stability Estimates in Adversarial Explainable AI through\n  Alternate Search Methods","summary":"  Advances in the effectiveness of machine learning models have come at the\ncost of enormous complexity resulting in a poor understanding of how they\nfunction. Local surrogate methods have been used to approximate the workings of\nthese complex models, but recent work has revealed their vulnerability to\nadversarial attacks where the explanation produced is appreciably different\nwhile the meaning and structure of the complex model's output remains similar.\nThis prior work has focused on the existence of these weaknesses but not on\ntheir magnitude. Here we explore using an alternate search method with the goal\nof finding minimum viable perturbations, the fewest perturbations necessary to\nachieve a fixed similarity value between the original and altered text's\nexplanation. Intuitively, a method that requires fewer perturbations to expose\na given level of instability is inferior to one which requires more. This\nnuance allows for superior comparisons of the stability of explainability\nmethods.\n","authors":["Christopher Burger","Charles Walter"],"pdf_url":"https://arxiv.org/pdf/2501.09006v1.pdf","comment":"9 pages, 3 figures, 5 tables. arXiv admin note: text overlap with\n  arXiv:2406.15839"},{"id":"http://arxiv.org/abs/2302.04851v2","updated":"2025-01-15T18:45:04Z","published":"2023-02-09T18:54:02Z","title":"Delay Sensitive Hierarchical Federated Learning with Stochastic Local\n  Updates","summary":"  The impact of local averaging on the performance of federated learning (FL)\nsystems is studied in the presence of communication delay between the clients\nand the parameter server. To minimize the effect of delay, clients are assigned\ninto different groups, each having its own local parameter server (LPS) that\naggregates its clients' models. The groups' models are then aggregated at a\nglobal parameter server (GPS) that only communicates with the LPSs. Such\nsetting is known as hierarchical FL (HFL). Unlike most works in the literature,\nthe number of local and global communication rounds in our work is randomly\ndetermined by the (different) delays experienced by each group of clients.\nSpecifically, the number of local averaging rounds is tied to a wall-clock time\nperiod coined the sync time $S$, after which the LPSs synchronize their models\nby sharing them with the GPS. Such sync time $S$ is then reapplied until a\nglobal wall-clock time is exhausted.\n  First, an upper bound on the deviation between the updated model at each LPS\nwith respect to that available at the GPS is derived. This is then used as a\ntool to derive the convergence analysis of our proposed delay-sensitive HFL\nalgorithm, first at each LPS individually, and then at the GPS. Our theoretical\nconvergence bound showcases the effects of the whole system's parameters,\nincluding the number of groups, the number of clients per group, and the value\nof $S$. Our results show that the value of $S$ should be carefully chosen,\nespecially since it implicitly governs how the delay statistics affect the\nperformance of HFL in situations where training time is restricted.\n","authors":["Abdulmoneam Ali","Ahmed Arafa"],"pdf_url":"https://arxiv.org/pdf/2302.04851v2.pdf","comment":"To appear in the IEEE Transactions on Cognitive Communications and\n  Networking"},{"id":"http://arxiv.org/abs/2406.00120v4","updated":"2025-01-15T18:30:12Z","published":"2024-05-31T18:22:09Z","title":"Reward Machines for Deep RL in Noisy and Uncertain Environments","summary":"  Reward Machines provide an automaton-inspired structure for specifying\ninstructions, safety constraints, and other temporally extended reward-worthy\nbehaviour. By exposing the underlying structure of a reward function, they\nenable the decomposition of an RL task, leading to impressive gains in sample\nefficiency. Although Reward Machines and similar formal specifications have a\nrich history of application towards sequential decision-making problems, they\ncritically rely on a ground-truth interpretation of the domain-specific\nvocabulary that forms the building blocks of the reward function--such\nground-truth interpretations are elusive in the real world due in part to\npartial observability and noisy sensing. In this work, we explore the use of\nReward Machines for Deep RL in noisy and uncertain environments. We\ncharacterize this problem as a POMDP and propose a suite of RL algorithms that\nexploit task structure under uncertain interpretation of the domain-specific\nvocabulary. Through theory and experiments, we expose pitfalls in naive\napproaches to this problem while simultaneously demonstrating how task\nstructure can be successfully leveraged under noisy interpretations of the\nvocabulary.\n","authors":["Andrew C. Li","Zizhao Chen","Toryn Q. Klassen","Pashootan Vaezipoor","Rodrigo Toro Icarte","Sheila A. McIlraith"],"pdf_url":"https://arxiv.org/pdf/2406.00120v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06848v2","updated":"2025-01-15T18:28:37Z","published":"2025-01-12T15:34:24Z","title":"A General Framework for Inference-time Scaling and Steering of Diffusion\n  Models","summary":"  Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we propose Feynman Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models, even\nwith off-the-shelf rewards, can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering .\n","authors":["Raghav Singhal","Zachary Horvitz","Ryan Teehan","Mengye Ren","Zhou Yu","Kathleen McKeown","Rajesh Ranganath"],"pdf_url":"https://arxiv.org/pdf/2501.06848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08998v1","updated":"2025-01-15T18:26:35Z","published":"2025-01-15T18:26:35Z","title":"CrystalGRW: Generative Modeling of Crystal Structures with Targeted\n  Properties via Geodesic Random Walks","summary":"  Determining whether a candidate crystalline material is thermodynamically\nstable depends on identifying its true ground-state structure, a central\nchallenge in computational materials science. We introduce CrystalGRW, a\ndiffusion-based generative model on Riemannian manifolds that proposes novel\ncrystal configurations and can predict stable phases validated by density\nfunctional theory. The crystal properties, such as fractional coordinates,\natomic types, and lattice matrices, are represented on suitable Riemannian\nmanifolds, ensuring that new predictions generated through the diffusion\nprocess preserve the periodicity of crystal structures. We incorporate an\nequivariant graph neural network to also account for rotational and\ntranslational symmetries during the generation process. CrystalGRW demonstrates\nthe ability to generate realistic crystal structures that are close to their\nground states with accuracy comparable to existing models, while also enabling\nconditional control, such as specifying a desired crystallographic point group.\nThese features help accelerate materials discovery and inverse design by\noffering stable, symmetry-consistent crystal candidates for experimental\nvalidation.\n","authors":["Krit Tangsongcharoen","Teerachote Pakornchote","Chayanon Atthapak","Natthaphon Choomphon-anomakhun","Annop Ektarawong","Björn Alling","Christopher Sutton","Thiti Bovornratanaraks","Thiparat Chotibut"],"pdf_url":"https://arxiv.org/pdf/2501.08998v1.pdf","comment":"10+12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2501.08995v1","updated":"2025-01-15T18:23:33Z","published":"2025-01-15T18:23:33Z","title":"VECT-GAN: A variationally encoded generative model for overcoming data\n  scarcity in pharmaceutical science","summary":"  Data scarcity in pharmaceutical research has led to reliance on\nlabour-intensive trial and error approaches for development rather than data\ndriven methods. While Machine Learning offers a solution, existing datasets are\noften small and noisy, limiting their utility. To address this, we developed a\nVariationally Encoded Conditional Tabular Generative Adversarial Network (VECT\nGAN), a novel generative model specifically designed for augmenting small,\nnoisy datasets. We introduce a pipeline where data is augmented before\nregression model development and demonstrate that this consistently and\nsignificantly improves performance over other state of the art tabular\ngenerative models. We apply this pipeline across six pharmaceutical datasets,\nand highlight its real-world applicability by developing novel polymers with\nmedically desirable mucoadhesive properties, which we made and experimentally\ncharacterised. Additionally, we pre-train the model on the ChEMBL database of\ndrug-like molecules, leveraging knowledge distillation to enhance its\ngeneralisability, making it readily available for use on pharmaceutical\ndatasets containing small molecules, which is an extremely common\npharmaceutical task. We demonstrate the power of synthetic data for\nregularising small tabular datasets, highlighting its potential to become\nstandard practice in pharmaceutical model development, and make our method,\nincluding VECT GAN pretrained on ChEMBL available as a pip package.\n","authors":["Youssef Abdalla","Marrisa Taub","Eleanor Hilton","Priya Akkaraju","Alexander Milanovic","Mine Orlu","Abdul W. Basit","Michael T Cook","Tapabrata Chakraborty","David Shorthouse"],"pdf_url":"https://arxiv.org/pdf/2501.08995v1.pdf","comment":"30 pages, 6 primary figures, 3 supplementary figures"},{"id":"http://arxiv.org/abs/2412.18992v2","updated":"2025-01-15T18:07:15Z","published":"2024-12-25T22:06:12Z","title":"Optimal Federated Learning for Functional Mean Estimation under\n  Heterogeneous Privacy Constraints","summary":"  Federated learning (FL) is a distributed machine learning technique designed\nto preserve data privacy and security, and it has gained significant importance\ndue to its broad range of applications. This paper addresses the problem of\noptimal functional mean estimation from discretely sampled data in a federated\nsetting.\n  We consider a heterogeneous framework where the number of individuals,\nmeasurements per individual, and privacy parameters vary across one or more\nservers, under both common and independent design settings. In the common\ndesign setting, the same design points are measured for each individual,\nwhereas in the independent design, each individual has their own random\ncollection of design points. Within this framework, we establish minimax upper\nand lower bounds for the estimation error of the underlying mean function,\nhighlighting the nuanced differences between common and independent designs\nunder distributed privacy constraints.\n  We propose algorithms that achieve the optimal trade-off between privacy and\naccuracy and provide optimality results that quantify the fundamental limits of\nprivate functional mean estimation across diverse distributed settings. These\nresults characterize the cost of privacy and offer practical insights into the\npotential for privacy-preserving statistical analysis in federated\nenvironments.\n","authors":["Tony Cai","Abhinav Chakraborty","Lasse Vuursteen"],"pdf_url":"https://arxiv.org/pdf/2412.18992v2.pdf","comment":"54 pages: 25 page article and 29 pages of appendix"},{"id":"http://arxiv.org/abs/2411.04216v2","updated":"2025-01-15T17:47:22Z","published":"2024-11-06T19:24:34Z","title":"Debiasing Synthetic Data Generated by Deep Generative Models","summary":"  While synthetic data hold great promise for privacy protection, their\nstatistical analysis poses significant challenges that necessitate innovative\nsolutions. The use of deep generative models (DGMs) for synthetic data\ngeneration is known to induce considerable bias and imprecision into synthetic\ndata analyses, compromising their inferential utility as opposed to original\ndata analyses. This bias and uncertainty can be substantial enough to impede\nstatistical convergence rates, even in seemingly straightforward analyses like\nmean calculation. The standard errors of such estimators then exhibit slower\nshrinkage with sample size than the typical 1 over root-$n$ rate. This\ncomplicates fundamental calculations like p-values and confidence intervals,\nwith no straightforward remedy currently available. In response to these\nchallenges, we propose a new strategy that targets synthetic data created by\nDGMs for specific data analyses. Drawing insights from debiased and targeted\nmachine learning, our approach accounts for biases, enhances convergence rates,\nand facilitates the calculation of estimators with easily approximated large\nsample variances. We exemplify our proposal through a simulation study on toy\ndata and two case studies on real-world data, highlighting the importance of\ntailoring DGMs for targeted data analysis. This debiasing strategy contributes\nto advancing the reliability and applicability of synthetic data in statistical\ninference.\n","authors":["Alexander Decruyenaere","Heidelinde Dehaene","Paloma Rabaey","Christiaan Polet","Johan Decruyenaere","Thomas Demeester","Stijn Vansteelandt"],"pdf_url":"https://arxiv.org/pdf/2411.04216v2.pdf","comment":"Accepted for the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024), joint first authors"},{"id":"http://arxiv.org/abs/2501.08970v1","updated":"2025-01-15T17:28:53Z","published":"2025-01-15T17:28:53Z","title":"Trusted Machine Learning Models Unlock Private Inference for Problems\n  Currently Infeasible with Cryptography","summary":"  We often interact with untrusted parties. Prioritization of privacy can limit\nthe effectiveness of these interactions, as achieving certain goals\nnecessitates sharing private data. Traditionally, addressing this challenge has\ninvolved either seeking trusted intermediaries or constructing cryptographic\nprotocols that restrict how much data is revealed, such as multi-party\ncomputations or zero-knowledge proofs. While significant advances have been\nmade in scaling cryptographic approaches, they remain limited in terms of the\nsize and complexity of applications they can be used for. In this paper, we\nargue that capable machine learning models can fulfill the role of a trusted\nthird party, thus enabling secure computations for applications that were\npreviously infeasible. In particular, we describe Trusted Capable Model\nEnvironments (TCMEs) as an alternative approach for scaling secure computation,\nwhere capable machine learning model(s) interact under input/output\nconstraints, with explicit information flow control and explicit statelessness.\nThis approach aims to achieve a balance between privacy and computational\nefficiency, enabling private inference where classical cryptographic solutions\nare currently infeasible. We describe a number of use cases that are enabled by\nTCME, and show that even some simple classic cryptographic problems can already\nbe solved with TCME. Finally, we outline current limitations and discuss the\npath forward in implementing them.\n","authors":["Ilia Shumailov","Daniel Ramage","Sarah Meiklejohn","Peter Kairouz","Florian Hartmann","Borja Balle","Eugene Bagdasarian"],"pdf_url":"https://arxiv.org/pdf/2501.08970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05541v2","updated":"2025-01-15T17:23:23Z","published":"2025-01-09T19:27:28Z","title":"Customizable LLM-Powered Chatbot for Behavioral Science Research","summary":"  The rapid advancement of Artificial Intelligence has resulted in the advent\nof Large Language Models (LLMs) with the capacity to produce text that closely\nresembles human communication. These models have been seamlessly integrated\ninto diverse applications, enabling interactive and responsive communication\nacross multiple platforms. The potential utility of chatbots transcends these\ntraditional applications, particularly in research contexts, wherein they can\noffer valuable insights and facilitate the design of innovative experiments. In\nthis study, we present a Customizable LLM-Powered Chatbot (CLPC), a web-based\nchatbot system designed to assist in behavioral science research. The system is\nmeticulously designed to function as an experimental instrument rather than a\nconventional chatbot, necessitating users to input a username and experiment\ncode upon access. This setup facilitates precise data cross-referencing,\nthereby augmenting the integrity and applicability of the data collected for\nresearch purposes. It can be easily expanded to accommodate new basic events as\nneeded; and it allows researchers to integrate their own logging events without\nthe necessity of implementing a separate logging mechanism. It is worth noting\nthat our system was built to assist primarily behavioral science research but\nis not limited to it, it can easily be adapted to assist information retrieval\nresearch or interacting with chat bot agents in general.\n","authors":["Zenon Lamprou","Yashar Moshfeghi"],"pdf_url":"https://arxiv.org/pdf/2501.05541v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08963v1","updated":"2025-01-15T17:19:51Z","published":"2025-01-15T17:19:51Z","title":"Training-Aware Risk Control for Intensity Modulated Radiation Therapies\n  Quality Assurance with Conformal Prediction","summary":"  Measurement quality assurance (QA) practices play a key role in the safe use\nof Intensity Modulated Radiation Therapies (IMRT) for cancer treatment. These\npractices have reduced measurement-based IMRT QA failure below 1%. However,\nthese practices are time and labor intensive which can lead to delays in\npatient care. In this study, we examine how conformal prediction methodologies\ncan be used to robustly triage plans. We propose a new training-aware conformal\nrisk control method by combining the benefit of conformal risk control and\nconformal training. We incorporate the decision making thresholds based on the\ngamma passing rate, along with the risk functions used in clinical evaluation,\ninto the design of the risk control framework. Our method achieves high\nsensitivity and specificity and significantly reduces the number of plans\nneeding measurement without generating a huge confidence interval. Our results\ndemonstrate the validity and applicability of conformal prediction methods for\nimproving efficiency and reducing the workload of the IMRT QA process.\n","authors":["Kevin He","David Adam","Sarah Han-Oh","Anqi Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08963v1.pdf","comment":"2024 Machine Learning for Health Symposium"},{"id":"http://arxiv.org/abs/2411.13951v3","updated":"2025-01-15T17:16:22Z","published":"2024-11-21T09:03:12Z","title":"A Discrete-sequence Dataset for Evaluating Online Unsupervised Anomaly\n  Detection Approaches for Multivariate Time Series","summary":"  Benchmarking anomaly detection approaches for multivariate time series is\nchallenging due to the lack of high-quality datasets. Current publicly\navailable datasets are too small, not diverse and feature trivial anomalies,\nwhich hinders measurable progress in this research area. We propose a solution:\na diverse, extensive, and non-trivial dataset generated via state-of-the-art\nsimulation tools that reflects realistic behaviour of an automotive powertrain,\nincluding its multivariate, dynamic and variable-state properties. To cater for\nboth unsupervised and semi-supervised anomaly detection settings, as well as\ntime series generation and forecasting, we make different versions of the\ndataset available, where training and test subsets are offered in contaminated\nand clean versions, depending on the task. We also provide baseline results\nfrom a small selection of approaches based on deterministic and variational\nautoencoders, as well as a non-parametric approach. As expected, the baseline\nexperimentation shows that the approaches trained on the semi-supervised\nversion of the dataset outperform their unsupervised counterparts, highlighting\na need for approaches more robust to contaminated training data.\n","authors":["Lucas Correia","Jan-Christoph Goos","Thomas Bäck","Anna V. Kononova"],"pdf_url":"https://arxiv.org/pdf/2411.13951v3.pdf","comment":"Submitted to the IEEE Transactions on Reliability journal"},{"id":"http://arxiv.org/abs/2312.02186v3","updated":"2025-01-15T17:11:20Z","published":"2023-12-01T20:16:02Z","title":"Identifying Spurious Correlations using Counterfactual Alignment","summary":"  Models driven by spurious correlations often yield poor generalization\nperformance. We propose the counterfactual (CF) alignment method to detect and\nquantify spurious correlations of black box classifiers. Our methodology is\nbased on counterfactual images generated with respect to one classifier being\ninput into other classifiers to see if they also induce changes in the outputs\nof these classifiers. The relationship between these responses can be\nquantified and used to identify specific instances where a spurious correlation\nexists. This is validated by observing intuitive trends in face-attribute and\nwaterbird classifiers, as well as by fabricating spurious correlations and\ndetecting their presence, both visually and quantitatively. Furthermore,\nutilizing the CF alignment method, we demonstrate that we can evaluate robust\noptimization methods (GroupDRO, JTT, and FLAC) by detecting a reduction in\nspurious correlations.\n","authors":["Joseph Paul Cohen","Louis Blankemeier","Akshay Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2312.02186v3.pdf","comment":"Accepted to Transactions on Machine Learning Research (TMLR), Code:\n  https://github.com/ieee8023/latentshift"},{"id":"http://arxiv.org/abs/2501.08958v1","updated":"2025-01-15T17:09:07Z","published":"2025-01-15T17:09:07Z","title":"Kolmogorov-Arnold Networks for Time Series Granger Causality Inference","summary":"  We introduce Granger Causality Kolmogorov-Arnold Networks (GCKAN), an\ninnovative architecture that extends the recently proposed Kolmogorov-Arnold\nNetworks (KAN) to the domain of causal inference. By extracting base weights\nfrom KAN layers and incorporating the sparsity-inducing penalty along with\nridge regularization, GCKAN infers the Granger causality from time series while\nenabling automatic time lag selection. Additionally, we propose an algorithm\nleveraging time-reversed Granger causality to enhance inference accuracy. The\nalgorithm compares prediction and sparse-inducing losses derived from the\noriginal and time-reversed series, automatically selecting the casual\nrelationship with the higher score or integrating the results to mitigate\nspurious connectivities. Comprehensive experiments conducted on Lorenz-96, gene\nregulatory networks, fMRI BOLD signals, and VAR datasets demonstrate that the\nproposed model achieves competitive performance to state-of-the-art methods in\ninferring Granger causality from nonlinear, high-dimensional, and\nlimited-sample time series.\n","authors":["Meiliang Liu","Yunfang Xu","Zijin Li","Zhengye Si","Xiaoxiao Yang","Xinyue Yang","Zhiwen Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.08958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17137v4","updated":"2025-01-15T16:56:26Z","published":"2024-09-25T17:56:00Z","title":"PACE: Marrying generalization in PArameter-efficient fine-tuning with\n  Consistency rEgularization","summary":"  Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained\ntransformers to downstream tasks. However, the optimization of tasks\nperformance often comes at the cost of generalizability in fine-tuned models.\nTo address this issue, we theoretically connect smaller weight gradient norms\nduring training and larger datasets to the improvements in model\ngeneralization. Motivated by this connection, we propose reducing gradient\nnorms for enhanced generalization and aligning fine-tuned model with the\npre-trained counterpart to retain knowledge from large-scale pre-training data.\nYet, naive alignment does not guarantee gradient reduction and can potentially\ncause gradient explosion, complicating efforts to manage gradients. To address\nsuch an issue, we propose PACE, marrying generalization of PArameter-efficient\nfine-tuning with Consistency rEgularization. We perturb features learned from\nthe adapter with the multiplicative noise and ensure the fine-tuned model\nremains consistent for same sample under different perturbations. Theoretical\nanalysis shows that PACE not only implicitly regularizes gradients for enhanced\ngeneralization, but also implicitly aligns the fine-tuned and pre-trained\nmodels to retain knowledge. Experimental evidence supports our theories. PACE\nsurpasses existing PEFT methods in visual adaptation tasks (VTAB-1k, FGVC,\nfew-shot learning, domain adaptation) showcasing its potential for\nresource-efficient fine-tuning. It also improves LoRA in text classification\n(GLUE) and mathematical reasoning (GSM-8K). The code is available at\nhttps://github.com/MaxwellYaoNi/PACE\n","authors":["Yao Ni","Shan Zhang","Piotr Koniusz"],"pdf_url":"https://arxiv.org/pdf/2409.17137v4.pdf","comment":"Accepted by NeurIPS 2024 as a spotlight"},{"id":"http://arxiv.org/abs/2501.08950v1","updated":"2025-01-15T16:52:21Z","published":"2025-01-15T16:52:21Z","title":"Computing Approximated Fixpoints via Dampened Mann Iteration","summary":"  Fixpoints are ubiquitous in computer science and when dealing with\nquantitative semantics and verification one is commonly led to consider least\nfixpoints of (higher-dimensional) functions over the nonnegative reals. We show\nhow to approximate the least fixpoint of such functions, focusing on the case\nin which they are not known precisely, but represented by a sequence of\napproximating functions that converge to them. We concentrate on monotone and\nnon-expansive functions, for which uniqueness of fixpoints is not guaranteed\nand standard fixpoint iteration schemes might get stuck at a fixpoint that is\nnot the least. Our main contribution is the identification of an iteration\nscheme, a variation of Mann iteration with a dampening factor, which, under\nsuitable conditions, is shown to guarantee convergence to the least fixpoint of\nthe function of interest. We then argue that these results are relevant in the\ncontext of model-based reinforcement learning for Markov decision processes\n(MDPs), showing that the proposed iteration scheme instantiates to MDPs and\nallows us to derive convergence to the optimal expected return. More generally,\nwe show that our results can be used to iterate to the least fixpoint almost\nsurely for systems where the function of interest can be approximated with\ngiven probabilistic error bounds, as it happens for probabilistic systems, such\nas simple stochastic games, that can be explored via sampling.\n","authors":["Paolo Baldan","Sebastian Gurke","Barbara König","Tommaso Padoan","Florian Wittbold"],"pdf_url":"https://arxiv.org/pdf/2501.08950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13749v2","updated":"2025-01-15T16:50:11Z","published":"2024-10-17T16:48:51Z","title":"Supervised Kernel Thinning","summary":"  The kernel thinning algorithm of Dwivedi & Mackey (2024) provides a\nbetter-than-i.i.d. compression of a generic set of points. By generating\nhigh-fidelity coresets of size significantly smaller than the input points, KT\nis known to speed up unsupervised tasks like Monte Carlo integration,\nuncertainty quantification, and non-parametric hypothesis testing, with minimal\nloss in statistical accuracy. In this work, we generalize the KT algorithm to\nspeed up supervised learning problems involving kernel methods. Specifically,\nwe combine two classical algorithms--Nadaraya-Watson (NW) regression or kernel\nsmoothing, and kernel ridge regression (KRR)--with KT to provide a quadratic\nspeed-up in both training and inference times. We show how distribution\ncompression with KT in each setting reduces to constructing an appropriate\nkernel, and introduce the Kernel-Thinned NW and Kernel-Thinned KRR estimators.\nWe prove that KT-based regression estimators enjoy significantly superior\ncomputational efficiency over the full-data estimators and improved statistical\nefficiency over i.i.d. subsampling of the training data. En route, we also\nprovide a novel multiplicative error guarantee for compressing with KT. We\nvalidate our design choices with both simulations and real data experiments.\n","authors":["Albert Gong","Kyuseong Choi","Raaz Dwivedi"],"pdf_url":"https://arxiv.org/pdf/2410.13749v2.pdf","comment":"Published at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2501.08941v1","updated":"2025-01-15T16:44:35Z","published":"2025-01-15T16:44:35Z","title":"A Reinforcement Learning Approach to Quiet and Safe UAM Traffic\n  Management","summary":"  Urban air mobility (UAM) is a transformative system that operates various\nsmall aerial vehicles in urban environments to reshape urban transportation.\nHowever, integrating UAM into existing urban environments presents a variety of\ncomplex challenges. Recent analyses of UAM's operational constraints highlight\naircraft noise and system safety as key hurdles to UAM system implementation.\nFuture UAM air traffic management schemes must ensure that the system is both\nquiet and safe. We propose a multi-agent reinforcement learning approach to\nmanage UAM traffic, aiming at both vertical separation assurance and noise\nmitigation. Through extensive training, the reinforcement learning agent learns\nto balance the two primary objectives by employing altitude adjustments in a\nmulti-layer UAM network. The results reveal the tradeoffs among noise impact,\ntraffic congestion, and separation. Overall, our findings demonstrate the\npotential of reinforcement learning in mitigating UAM's noise impact while\nmaintaining safe separation using altitude adjustments\n","authors":["Surya Murthy","John-Paul Clarke","Ufuk Topcu","Zhenyu Gao"],"pdf_url":"https://arxiv.org/pdf/2501.08941v1.pdf","comment":"Paper presented at SciTech 2025"},{"id":"http://arxiv.org/abs/2501.08925v1","updated":"2025-01-15T16:30:29Z","published":"2025-01-15T16:30:29Z","title":"Disentangling Exploration of Large Language Models by Optimal\n  Exploitation","summary":"  Exploration is a crucial skill for self-improvement and open-ended\nproblem-solving. However, it remains uncertain whether large language models\ncan effectively explore the state-space. Existing evaluations predominantly\nfocus on the trade-off between exploration and exploitation, often assessed in\nmulti-armed bandit problems. In contrast, this work isolates exploration as the\nsole objective, tasking the agent with delivering information that enhances\nfuture returns. For the evaluation, we propose to decompose missing rewards\ninto exploration and exploitation components by measuring the optimal\nachievable return for the states already explored. Our experiments with various\nLLMs reveal that most models struggle to sufficiently explore the state-space\nand that weak exploration is insufficient. We observe a positive correlation\nbetween model size and exploration performance, with larger models\ndemonstrating superior capabilities. Furthermore, we show that our\ndecomposition provides insights into differences in behaviors driven by agent\ninstructions during prompt engineering, offering a valuable tool for refining\nLLM performance in exploratory tasks.\n","authors":["Tim Grams","Patrick Betz","Christian Bartelt"],"pdf_url":"https://arxiv.org/pdf/2501.08925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07823v2","updated":"2025-01-15T16:29:38Z","published":"2024-05-13T15:08:02Z","title":"Integrating Multi-Physics Simulations and Machine Learning to Define the\n  Spatter Mechanism and Process Window in Laser Powder Bed Fusion","summary":"  Laser powder bed fusion (LPBF) has shown promise for wide range of\napplications due to its ability to fabricate freeform geometries and generate a\ncontrolled microstructure. However, components generated by LPBF still possess\nsub-optimal mechanical properties due to the defects that are created during\nlaser-material interactions. In this work, we investigate mechanism of spatter\nformation, using a high-fidelity modelling tool that was built to simulate the\nmulti-physics phenomena in LPBF. The modelling tool have the capability to\ncapture the 3D resolution of the meltpool and the spatter behavior. To\nunderstand spatter behavior and formation, we reveal its properties at ejection\nand evaluate its variation from the meltpool, the source where it is formed.\nThe dataset of the spatter and the meltpool collected consist of 50 % spatter\nand 50 % melt pool samples, with features that include position components,\nvelocity components, velocity magnitude, temperature, density and pressure. The\nrelationship between the spatter and the meltpool were evaluated via\ncorrelation analysis and machine learning (ML) algorithms for classification\ntasks. Upon screening different ML algorithms on the dataset, a high accuracy\nwas observed for all the ML models, with ExtraTrees having the highest at 96 %\nand KNN having the lowest at 94 %.\n","authors":["Olabode T. Ajenifujah","Francis Ogoke","Florian Wirth","Jack Beuth","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2405.07823v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08922v1","updated":"2025-01-15T16:26:01Z","published":"2025-01-15T16:26:01Z","title":"Modeling Melt Pool Features and Spatter Using Symbolic Regression and\n  Machine Learning","summary":"  Additive manufacturing (AM) is a rapidly evolving technology that has\nattracted applications across a wide range of fields due to its ability to\nfabricate complex geometries. However, one of the key challenges in AM is\nachieving consistent print quality. This inconsistency is often attributed to\nuncontrolled melt pool dynamics, partly caused by spatter which can lead to\ndefects. Therefore, capturing and controlling the evolution of the melt pool is\ncrucial for enhancing process stability and part quality. In this study, we\ndeveloped a framework to support decision-making in AM operations, facilitating\nquality control and minimizing defects via machine learning (ML) and polynomial\nsymbolic regression models. We implemented experimentally validated\ncomputational tools as a cost-effective approach to collect large datasets from\nlaser powder bed fusion (LPBF) processes. For a dataset consisting of 281\nprocess conditions, parameters such as melt pool dimensions (length, width,\ndepth), melt pool geometry (area, volume), and volume indicated as spatter were\nextracted. Using machine learning (ML) and polynomial symbolic regression\nmodels, a high R2 of over 95 % was achieved in predicting the melt pool\ndimensions and geometry features for both the training and testing datasets,\nwith either process conditions (power and velocity) or melt pool dimensions as\nthe model inputs. In the case of volume indicated as spatter, R2 improved after\nlogarithmic transforming the model inputs, which was either the process\nconditions or the melt pool dimensions. Among the investigated ML models, the\nExtraTree model achieved the highest R2 values of 96.7 % and 87.5 %.\n","authors":["Olabode T. Ajenifujah","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2501.08922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08913v1","updated":"2025-01-15T16:21:09Z","published":"2025-01-15T16:21:09Z","title":"GenAI Content Detection Task 3: Cross-Domain Machine-Generated Text\n  Detection Challenge","summary":"  Recently there have been many shared tasks targeting the detection of\ngenerated text from Large Language Models (LLMs). However, these shared tasks\ntend to focus either on cases where text is limited to one particular domain or\ncases where text can be from many domains, some of which may not be seen during\ntest time. In this shared task, using the newly released RAID benchmark, we aim\nto answer whether or not models can detect generated text from a large, yet\nfixed, number of domains and LLMs, all of which are seen during training. Over\nthe course of three months, our task was attempted by 9 teams with 23 detector\nsubmissions. We find that multiple participants were able to obtain accuracies\nof over 99% on machine-generated text from RAID while maintaining a 5% False\nPositive Rate -- suggesting that detectors are able to robustly detect text\nfrom many domains and models simultaneously. We discuss potential\ninterpretations of this result and provide directions for future research.\n","authors":["Liam Dugan","Andrew Zhu","Firoj Alam","Preslav Nakov","Marianna Apidianaki","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2501.08913v1.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2501.08907v1","updated":"2025-01-15T16:17:02Z","published":"2025-01-15T16:17:02Z","title":"Projection Implicit Q-Learning with Support Constraint for Offline\n  Reinforcement Learning","summary":"  Offline Reinforcement Learning (RL) faces a critical challenge of\nextrapolation errors caused by out-of-distribution (OOD) actions. Implicit\nQ-Learning (IQL) algorithm employs expectile regression to achieve in-sample\nlearning, effectively mitigating the risks associated with OOD actions.\nHowever, the fixed hyperparameter in policy evaluation and density-based policy\nimprovement method limit its overall efficiency. In this paper, we propose\nProj-IQL, a projective IQL algorithm enhanced with the support constraint. In\nthe policy evaluation phase, Proj-IQL generalizes the one-step approach to a\nmulti-step approach through vector projection, while maintaining in-sample\nlearning and expectile regression framework. In the policy improvement phase,\nProj-IQL introduces support constraint that is more aligned with the policy\nevaluation approach. Furthermore, we theoretically demonstrate that Proj-IQL\nguarantees monotonic policy improvement and enjoys a progressively more\nrigorous criterion for superior actions. Empirical results demonstrate the\nProj-IQL achieves state-of-the-art performance on D4RL benchmarks, especially\nin challenging navigation domains.\n","authors":["Xinchen Han","Hossam Afifi","Michel Marot"],"pdf_url":"https://arxiv.org/pdf/2501.08907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08902v1","updated":"2025-01-15T16:11:24Z","published":"2025-01-15T16:11:24Z","title":"Multi-View Transformers for Airway-To-Lung Ratio Inference on Cardiac CT\n  Scans: The C4R Study","summary":"  The ratio of airway tree lumen to lung size (ALR), assessed at full\ninspiration on high resolution full-lung computed tomography (CT), is a major\nrisk factor for chronic obstructive pulmonary disease (COPD). There is growing\ninterest to infer ALR from cardiac CT images, which are widely available in\nepidemiological cohorts, to investigate the relationship of ALR to severe\nCOVID-19 and post-acute sequelae of SARS-CoV-2 infection (PASC). Previously,\ncardiac scans included approximately 2/3 of the total lung volume with 5-6x\ngreater slice thickness than high-resolution (HR) full-lung (FL) CT. In this\nstudy, we present a novel attention-based Multi-view Swin Transformer to infer\nFL ALR values from segmented cardiac CT scans. For the supervised training we\nexploit paired full-lung and cardiac CTs acquired in the Multi-Ethnic Study of\nAtherosclerosis (MESA). Our network significantly outperforms a proxy direct\nALR inference on segmented cardiac CT scans and achieves accuracy and\nreproducibility comparable with a scan-rescan reproducibility of the FL ALR\nground-truth.\n","authors":["Sneha N. Naik","Elsa D. Angelini","Eric A. Hoffman","Elizabeth C. Oelsner","R. Graham Barr","Benjamin M. Smith","Andrew F. Laine"],"pdf_url":"https://arxiv.org/pdf/2501.08902v1.pdf","comment":"Accepted to appear in Proceedings of International Symposium on\n  Biomedical Imaging (ISBI), 2025"},{"id":"http://arxiv.org/abs/2407.04491v3","updated":"2025-01-15T16:02:08Z","published":"2024-07-05T13:29:30Z","title":"Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular\n  Data","summary":"  For classification and regression on tabular data, the dominance of\ngradient-boosted decision trees (GBDTs) has recently been challenged by often\nmuch slower deep learning methods with extensive hyperparameter tuning. We\naddress this discrepancy by introducing (a) RealMLP, an improved multilayer\nperceptron (MLP), and (b) strong meta-tuned default parameters for GBDTs and\nRealMLP. We tune RealMLP and the default parameters on a meta-train benchmark\nwith 118 datasets and compare them to hyperparameter-optimized versions on a\ndisjoint meta-test benchmark with 90 datasets, as well as the GBDT-friendly\nbenchmark by Grinsztajn et al. (2022). Our benchmark results on medium-to-large\ntabular datasets (1K--500K samples) show that RealMLP offers a favorable\ntime-accuracy tradeoff compared to other neural baselines and is competitive\nwith GBDTs in terms of benchmark scores. Moreover, a combination of RealMLP and\nGBDTs with improved default parameters can achieve excellent results without\nhyperparameter tuning. Finally, we demonstrate that some of RealMLP's\nimprovements can also considerably improve the performance of TabR with default\nparameters.\n","authors":["David Holzmüller","Léo Grinsztajn","Ingo Steinwart"],"pdf_url":"https://arxiv.org/pdf/2407.04491v3.pdf","comment":"NeurIPS 2024. Changes in v3: mention bug in XGBoost results, mention\n  original name of he+5 method. Code is available at\n  github.com/dholzmueller/pytabkit"},{"id":"http://arxiv.org/abs/2501.08888v1","updated":"2025-01-15T15:58:16Z","published":"2025-01-15T15:58:16Z","title":"A Two-Stage Pretraining-Finetuning Framework for Treatment Effect\n  Estimation with Unmeasured Confounding","summary":"  Estimating the conditional average treatment effect (CATE) from observational\ndata plays a crucial role in areas such as e-commerce, healthcare, and\neconomics. Existing studies mainly rely on the strong ignorability assumption\nthat there are no unmeasured confounders, whose presence cannot be tested from\nobservational data and can invalidate any causal conclusion. In contrast, data\ncollected from randomized controlled trials (RCT) do not suffer from\nconfounding, but are usually limited by a small sample size. In this paper, we\npropose a two-stage pretraining-finetuning (TSPF) framework using both\nlarge-scale observational data and small-scale RCT data to estimate the CATE in\nthe presence of unmeasured confounding. In the first stage, a foundational\nrepresentation of covariates is trained to estimate counterfactual outcomes\nthrough large-scale observational data. In the second stage, we propose to\ntrain an augmented representation of the covariates, which is concatenated to\nthe foundational representation obtained in the first stage to adjust for the\nunmeasured confounding. To avoid overfitting caused by the small-scale RCT data\nin the second stage, we further propose a partial parameter initialization\napproach, rather than training a separate network. The superiority of our\napproach is validated on two public datasets with extensive experiments. The\ncode is available at https://github.com/zhouchuanCN/KDD25-TSPF.\n","authors":["Chuan Zhou","Yaxuan Li","Chunyuan Zheng","Haiteng Zhang","Min Zhang","Haoxuan Li","Mingming Gong"],"pdf_url":"https://arxiv.org/pdf/2501.08888v1.pdf","comment":"KDD 25 Research Track"},{"id":"http://arxiv.org/abs/2501.08887v1","updated":"2025-01-15T15:57:13Z","published":"2025-01-15T15:57:13Z","title":"PAC Learnability of Scenario Decision-Making Algorithms: Necessary and\n  Sufficient Conditions","summary":"  We study the PAC property of scenario decision-making algorithms, that is,\nthe ability to make a decision that has an arbitrarily low risk of violating an\nunknown safety constraint, provided sufficiently many realizations (called\nscenarios) of the safety constraint are sampled. Sufficient conditions for\nscenario decision-making algorithms to be PAC are available in the literature,\nsuch as finiteness of the VC dimension of its associated classifier and\nexistence of a compression scheme. We study the question of whether these\nsufficient conditions are also necessary. We show with counterexamples that\nthis is not the case in general. This contrasts with binary classification\nlearning, for which the analogous conditions are sufficient and necessary.\nPopular scenario decision-making algorithms, such as scenario optimization,\nenjoy additional properties, such as stability and consistency. We show that\neven under these additional assumptions the above conclusions hold. Finally, we\nderive a necessary condition for scenario decision-making algorithms to be PAC,\ninspired by the VC dimension and the so-called no-free-lunch theorem.\n","authors":["Guillaume O. Berger","Raphaël M. Jungers"],"pdf_url":"https://arxiv.org/pdf/2501.08887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08884v1","updated":"2025-01-15T15:53:34Z","published":"2025-01-15T15:53:34Z","title":"Improved Compression Bounds for Scenario Decision Making","summary":"  Scenario decision making offers a flexible way of making decision in an\nuncertain environment while obtaining probabilistic guarantees on the risk of\nfailure of the decision. The idea of this approach is to draw samples of the\nuncertainty and make a decision based on the samples, called \"scenarios\". The\nprobabilistic guarantees take the form of a bound on the probability of\nsampling a set of scenarios that will lead to a decision whose risk of failure\nis above a given maximum tolerance. This bound can be expressed as a function\nof the number of sampled scenarios, the maximum tolerated risk, and some\nintrinsic property of the problem called the \"compression size\". Several such\nbounds have been proposed in the literature under various assumptions on the\nproblem. We propose new bounds that improve upon the existing ones without\nrequiring stronger assumptions on the problem.\n","authors":["Guillaume O. Berger","Raphaël M. Jungers"],"pdf_url":"https://arxiv.org/pdf/2501.08884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08883v1","updated":"2025-01-15T15:53:27Z","published":"2025-01-15T15:53:27Z","title":"Increasing Batch Size Improves Convergence of Stochastic Gradient\n  Descent with Momentum","summary":"  Stochastic gradient descent with momentum (SGDM), which is defined by adding\na momentum term to SGD, has been well studied in both theory and practice.\nTheoretically investigated results showed that the settings of the learning\nrate and momentum weight affect the convergence of SGDM. Meanwhile, practical\nresults showed that the setting of batch size strongly depends on the\nperformance of SGDM. In this paper, we focus on mini-batch SGDM with constant\nlearning rate and constant momentum weight, which is frequently used to train\ndeep neural networks in practice. The contribution of this paper is showing\ntheoretically that using a constant batch size does not always minimize the\nexpectation of the full gradient norm of the empirical loss in training a deep\nneural network, whereas using an increasing batch size definitely minimizes it,\nthat is, increasing batch size improves convergence of mini-batch SGDM. We also\nprovide numerical results supporting our analyses, indicating specifically that\nmini-batch SGDM with an increasing batch size converges to stationary points\nfaster than with a constant batch size. Python implementations of the\noptimizers used in the numerical experiments are available at\nhttps://anonymous.4open.science/r/momentum-increasing-batch-size-888C/.\n","authors":["Keisuke Kamo","Hideaki Iiduka"],"pdf_url":"https://arxiv.org/pdf/2501.08883v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2501.08878v1","updated":"2025-01-15T15:49:46Z","published":"2025-01-15T15:49:46Z","title":"Incrementally Learning Multiple Diverse Data Domains via Multi-Source\n  Dynamic Expansion Model","summary":"  Continual Learning seeks to develop a model capable of incrementally\nassimilating new information while retaining prior knowledge. However, current\nresearch predominantly addresses a straightforward learning context, wherein\nall data samples originate from a singular data domain. This paper shifts focus\nto a more complex and realistic learning environment, characterized by data\nsamples sourced from multiple distinct domains. We tackle this intricate\nlearning challenge by introducing a novel methodology, termed the Multi-Source\nDynamic Expansion Model (MSDEM), which leverages various pre-trained models as\nbackbones and progressively establishes new experts based on them to adapt to\nemerging tasks. Additionally, we propose an innovative dynamic expandable\nattention mechanism designed to selectively harness knowledge from multiple\nbackbones, thereby accelerating the new task learning. Moreover, we introduce a\ndynamic graph weight router that strategically reuses all previously acquired\nparameters and representations for new task learning, maximizing the positive\nknowledge transfer effect, which further improves generalization performance.\nWe conduct a comprehensive series of experiments, and the empirical findings\nindicate that our proposed approach achieves state-of-the-art performance.\n","authors":["Runqing Wu","Fei Ye","Qihe Liu","Guoxi Huang","Jinyu Guo","Rongyao Hu"],"pdf_url":"https://arxiv.org/pdf/2501.08878v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.08376v4","updated":"2025-01-15T15:41:09Z","published":"2023-11-14T18:41:28Z","title":"Ensemble sampling for linear bandits: small ensembles suffice","summary":"  We provide the first useful and rigorous analysis of ensemble sampling for\nthe stochastic linear bandit setting. In particular, we show that, under\nstandard assumptions, for a $d$-dimensional stochastic linear bandit with an\ninteraction horizon $T$, ensemble sampling with an ensemble of size of order $d\n\\log T$ incurs regret at most of the order $(d \\log T)^{5/2} \\sqrt{T}$. Ours is\nthe first result in any structured setting not to require the size of the\nensemble to scale linearly with $T$ -- which defeats the purpose of ensemble\nsampling -- while obtaining near $\\smash{\\sqrt{T}}$ order regret. Our result is\nalso the first to allow for infinite action sets.\n","authors":["David Janz","Alexander E. Litvak","Csaba Szepesvári"],"pdf_url":"https://arxiv.org/pdf/2311.08376v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16749v4","updated":"2025-01-15T15:40:12Z","published":"2024-06-24T15:57:49Z","title":"Inferring stochastic low-rank recurrent neural networks from neural data","summary":"  A central aim in computational neuroscience is to relate the activity of\nlarge populations of neurons to an underlying dynamical system. Models of these\nneural dynamics should ideally be both interpretable and fit the observed data\nwell. Low-rank recurrent neural networks (RNNs) exhibit such interpretability\nby having tractable dynamics. However, it is unclear how to best fit low-rank\nRNNs to data consisting of noisy observations of an underlying stochastic\nsystem. Here, we propose to fit stochastic low-rank RNNs with variational\nsequential Monte Carlo methods. We validate our method on several datasets\nconsisting of both continuous and spiking neural data, where we obtain lower\ndimensional latent dynamics than current state of the art methods.\nAdditionally, for low-rank models with piecewise linear nonlinearities, we show\nhow to efficiently identify all fixed points in polynomial rather than\nexponential cost in the number of units, making analysis of the inferred\ndynamics tractable for large RNNs. Our method both elucidates the dynamical\nsystems underlying experimental recordings and provides a generative model\nwhose trajectories match observed variability.\n","authors":["Matthijs Pals","A Erdem Sağtekin","Felix Pei","Manuel Gloeckler","Jakob H Macke"],"pdf_url":"https://arxiv.org/pdf/2406.16749v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14970v4","updated":"2025-01-15T15:35:22Z","published":"2024-10-19T04:28:44Z","title":"Taming the Long Tail in Human Mobility Prediction","summary":"  With the popularity of location-based services, human mobility prediction\nplays a key role in enhancing personalized navigation, optimizing\nrecommendation systems, and facilitating urban mobility and planning. This\ninvolves predicting a user's next POI (point-of-interest) visit using their\npast visit history. However, the uneven distribution of visitations over time\nand space, namely the long-tail problem in spatial distribution, makes it\ndifficult for AI models to predict those POIs that are less visited by humans.\nIn light of this issue, we propose the Long-Tail Adjusted Next POI Prediction\n(LoTNext) framework for mobility prediction, combining a Long-Tailed Graph\nAdjustment module to reduce the impact of the long-tailed nodes in the user-POI\ninteraction graph and a novel Long-Tailed Loss Adjustment module to adjust loss\nby logit score and sample weight adjustment strategy. Also, we employ the\nauxiliary prediction task to enhance generalization and accuracy. Our\nexperiments with two real-world trajectory datasets demonstrate that LoTNext\nsignificantly surpasses existing state-of-the-art works.\n","authors":["Xiaohang Xu","Renhe Jiang","Chuang Yang","Zipei Fan","Kaoru Sezaki"],"pdf_url":"https://arxiv.org/pdf/2410.14970v4.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.10175v2","updated":"2025-01-15T15:24:32Z","published":"2024-11-15T13:21:26Z","title":"The Surprising Ineffectiveness of Pre-Trained Visual Representations for\n  Model-Based Reinforcement Learning","summary":"  Visual Reinforcement Learning (RL) methods often require extensive amounts of\ndata. As opposed to model-free RL, model-based RL (MBRL) offers a potential\nsolution with efficient data utilization through planning. Additionally, RL\nlacks generalization capabilities for real-world tasks. Prior work has shown\nthat incorporating pre-trained visual representations (PVRs) enhances sample\nefficiency and generalization. While PVRs have been extensively studied in the\ncontext of model-free RL, their potential in MBRL remains largely unexplored.\nIn this paper, we benchmark a set of PVRs on challenging control tasks in a\nmodel-based RL setting. We investigate the data efficiency, generalization\ncapabilities, and the impact of different properties of PVRs on the performance\nof model-based agents. Our results, perhaps surprisingly, reveal that for MBRL\ncurrent PVRs are not more sample efficient than learning representations from\nscratch, and that they do not generalize better to out-of-distribution (OOD)\nsettings. To explain this, we analyze the quality of the trained dynamics\nmodel. Furthermore, we show that data diversity and network architecture are\nthe most important contributors to OOD generalization performance.\n","authors":["Moritz Schneider","Robert Krug","Narunas Vaskevicius","Luigi Palmieri","Joschka Boedecker"],"pdf_url":"https://arxiv.org/pdf/2411.10175v2.pdf","comment":"Published at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024). Project page: https://schneimo.com/pvr4mbrl/"},{"id":"http://arxiv.org/abs/2501.08862v1","updated":"2025-01-15T15:22:57Z","published":"2025-01-15T15:22:57Z","title":"ARMOR: Shielding Unlearnable Examples against Data Augmentation","summary":"  Private data, when published online, may be collected by unauthorized parties\nto train deep neural networks (DNNs). To protect privacy, defensive noises can\nbe added to original samples to degrade their learnability by DNNs. Recently,\nunlearnable examples are proposed to minimize the training loss such that the\nmodel learns almost nothing. However, raw data are often pre-processed before\nbeing used for training, which may restore the private information of protected\ndata. In this paper, we reveal the data privacy violation induced by data\naugmentation, a commonly used data pre-processing technique to improve model\ngeneralization capability, which is the first of its kind as far as we are\nconcerned. We demonstrate that data augmentation can significantly raise the\naccuracy of the model trained on unlearnable examples from 21.3% to 66.1%. To\naddress this issue, we propose a defense framework, dubbed ARMOR, to protect\ndata privacy from potential breaches of data augmentation. To overcome the\ndifficulty of having no access to the model training process, we design a\nnon-local module-assisted surrogate model that better captures the effect of\ndata augmentation. In addition, we design a surrogate augmentation selection\nstrategy that maximizes distribution alignment between augmented and\nnon-augmented samples, to choose the optimal augmentation strategy for each\nclass. We also use a dynamic step size adjustment algorithm to enhance the\ndefensive noise generation process. Extensive experiments are conducted on 4\ndatasets and 5 data augmentation methods to verify the performance of ARMOR.\nComparisons with 6 state-of-the-art defense methods have demonstrated that\nARMOR can preserve the unlearnability of protected private data under data\naugmentation. ARMOR reduces the test accuracy of the model trained on augmented\nprotected samples by as much as 60% more than baselines.\n","authors":["Xueluan Gong","Yuji Wang","Yanjiao Chen","Haocheng Dong","Yiming Li","Mengyuan Sun","Shuaike Li","Qian Wang","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2501.08862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18977v2","updated":"2025-01-15T15:22:45Z","published":"2024-12-25T19:38:32Z","title":"CGCOD: Class-Guided Camouflaged Object Detection","summary":"  Camouflaged Object Detection (COD) aims to identify objects that blend\nseamlessly into their surroundings. The inherent visual complexity of\ncamouflaged objects, including their low contrast with the background, diverse\ntextures, and subtle appearance variations, often obscures semantic cues,\nmaking accurate segmentation highly challenging. Existing methods primarily\nrely on visual features, which are insufficient to handle the variability and\nintricacy of camouflaged objects, leading to unstable object perception and\nambiguous segmentation results. To tackle these limitations, we introduce a\nnovel task, class-guided camouflaged object detection (CGCOD), which extends\ntraditional COD task by incorporating object-specific class knowledge to\nenhance detection robustness and accuracy. To facilitate this task, we present\na new dataset, CamoClass, comprising real-world camouflaged objects with class\nannotations. Furthermore, we propose a multi-stage framework, CGNet, which\nincorporates a plug-and-play class prompt generator and a simple yet effective\nclass-guided detector. This establishes a new paradigm for COD, bridging the\ngap between contextual understanding and class-guided detection. Extensive\nexperimental results demonstrate the effectiveness of our flexible framework in\nimproving the performance of proposed and existing detectors by leveraging\nclass-level textual information.\n","authors":["Chenxi Zhang","Qing Zhang","Jiayun Wu","Youwei Pang"],"pdf_url":"https://arxiv.org/pdf/2412.18977v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06403v4","updated":"2025-01-15T15:21:46Z","published":"2023-12-11T14:24:24Z","title":"RoME: A Robust Mixed-Effects Bandit Algorithm for Optimizing Mobile\n  Health Interventions","summary":"  Mobile health leverages personalized and contextually tailored interventions\noptimized through bandit and reinforcement learning algorithms. In practice,\nhowever, challenges such as participant heterogeneity, nonstationarity, and\nnonlinear relationships hinder algorithm performance. We propose RoME, a Robust\nMixed-Effects contextual bandit algorithm that simultaneously addresses these\nchallenges via (1) modeling the differential reward with user- and\ntime-specific random effects, (2) network cohesion penalties, and (3) debiased\nmachine learning for flexible estimation of baseline rewards. We establish a\nhigh-probability regret bound that depends solely on the dimension of the\ndifferential-reward model, enabling us to achieve robust regret bounds even\nwhen the baseline reward is highly complex. We demonstrate the superior\nperformance of the RoME algorithm in a simulation and two off-policy evaluation\nstudies.\n","authors":["Easton K. Huch","Jieru Shi","Madeline R. Abbott","Jessica R. Golbus","Alexander Moreno","Walter H. Dempsey"],"pdf_url":"https://arxiv.org/pdf/2312.06403v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11316v2","updated":"2025-01-15T15:07:59Z","published":"2024-06-17T08:26:51Z","title":"Improved Algorithms for Contextual Dynamic Pricing","summary":"  In contextual dynamic pricing, a seller sequentially prices goods based on\ncontextual information. Buyers will purchase products only if the prices are\nbelow their valuations. The goal of the seller is to design a pricing strategy\nthat collects as much revenue as possible. We focus on two different valuation\nmodels. The first assumes that valuations linearly depend on the context and\nare further distorted by noise. Under minor regularity assumptions, our\nalgorithm achieves an optimal regret bound of $\\tilde{\\mathcal{O}}(T^{2/3})$,\nimproving the existing results. The second model removes the linearity\nassumption, requiring only that the expected buyer valuation is\n$\\beta$-H\\\"older in the context. For this model, our algorithm obtains a regret\n$\\tilde{\\mathcal{O}}(T^{d+2\\beta/d+3\\beta})$, where $d$ is the dimension of the\ncontext space.\n","authors":["Matilde Tullii","Solenne Gaucher","Nadav Merlis","Vianney Perchet"],"pdf_url":"https://arxiv.org/pdf/2406.11316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04195v2","updated":"2025-01-15T15:06:56Z","published":"2023-03-07T19:32:13Z","title":"PRIMO: Private Regression in Multiple Outcomes","summary":"  We introduce a new private regression setting we call Private Regression in\nMultiple Outcomes (PRIMO), inspired by the common situation where a data\nanalyst wants to perform a set of $l$ regressions while preserving privacy,\nwhere the features $X$ are shared across all $l$ regressions, and each\nregression $i \\in [l]$ has a different vector of outcomes $y_i$. Naively\napplying existing private linear regression techniques $l$ times leads to a\n$\\sqrt{l}$ multiplicative increase in error over the standard linear regression\nsetting. We apply a variety of techniques including sufficient statistics\nperturbation (SSP) and geometric projection-based methods to develop scalable\nalgorithms that outperform this baseline across a range of parameter regimes.\nIn particular, we obtain no dependence on l in the asymptotic error when $l$ is\nsufficiently large. Empirically, on the task of genomic risk prediction with\nmultiple phenotypes we find that even for values of $l$ far smaller than the\ntheory would predict, our projection-based method improves the accuracy\nrelative to the variant that doesn't use the projection.\n","authors":["Seth Neel"],"pdf_url":"https://arxiv.org/pdf/2303.04195v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08851v1","updated":"2025-01-15T15:05:49Z","published":"2025-01-15T15:05:49Z","title":"Digital Phenotyping for Adolescent Mental Health: A Feasibility Study\n  Employing Machine Learning to Predict Mental Health Risk From Active and\n  Passive Smartphone Data","summary":"  Background: Adolescents are particularly vulnerable to mental disorders, with\nover 75% of cases manifesting before the age of 25. Research indicates that\nonly 18 to 34% of young people experiencing high levels of depression or\nanxiety symptoms seek support. Digital tools leveraging smartphones offer\nscalable and early intervention opportunities. Objective: Using a novel machine\nlearning framework, this study evaluated the feasibility of integrating active\nand passive smartphone data to predict mental disorders in non-clinical\nadolescents. Specifically, we investigated the utility of the Mindcraft app in\npredicting risks for internalising and externalising disorders, eating\ndisorders, insomnia and suicidal ideation. Methods: Participants (N=103; mean\nage 16.1 years) were recruited from three London schools. Participants\ncompleted the Strengths and Difficulties Questionnaire, the Eating Disorders-15\nQuestionnaire, Sleep Condition Indicator Questionnaire and indicated the\npresence/absence of suicidal ideation. They used the Mindcraft app for 14 days,\ncontributing active data via self-reports and passive data from smartphone\nsensors. A contrastive pretraining phase was applied to enhance user-specific\nfeature stability, followed by supervised fine-tuning. The model evaluation\nemployed leave-one-subject-out cross-validation using balanced accuracy as the\nprimary metric. Results: The integration of active and passive data achieved\nsuperior performance compared to individual data sources, with mean balanced\naccuracies of 0.71 for SDQ-High risk, 0.67 for insomnia, 0.77 for suicidal\nideation and 0.70 for eating disorders. The contrastive learning framework\nstabilised daily behavioural representations, enhancing predictive robustness.\nThis study demonstrates the potential of integrating active and passive\nsmartphone data with advanced machine-learning techniques for predicting mental\nhealth risks.\n","authors":["Balasundaram Kadirvelu","Teresa Bellido Bel","Aglaia Freccero","Martina Di Simplicio","Dasha Nicholls","A Aldo Faisal"],"pdf_url":"https://arxiv.org/pdf/2501.08851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08850v1","updated":"2025-01-15T15:04:10Z","published":"2025-01-15T15:04:10Z","title":"Graph Counterfactual Explainable AI via Latent Space Traversal","summary":"  Explaining the predictions of a deep neural network is a nontrivial task, yet\nhigh-quality explanations for predictions are often a prerequisite for\npractitioners to trust these models. Counterfactual explanations aim to explain\npredictions by finding the ''nearest'' in-distribution alternative input whose\nprediction changes in a pre-specified way. However, it remains an open question\nhow to define this nearest alternative input, whose solution depends on both\nthe domain (e.g. images, graphs, tabular data, etc.) and the specific\napplication considered. For graphs, this problem is complicated i) by their\ndiscrete nature, as opposed to the continuous nature of state-of-the-art graph\nclassifiers; and ii) by the node permutation group acting on the graphs. We\npropose a method to generate counterfactual explanations for any differentiable\nblack-box graph classifier, utilizing a case-specific permutation equivariant\ngraph variational autoencoder. We generate counterfactual explanations in a\ncontinuous fashion by traversing the latent space of the autoencoder across the\nclassification boundary of the classifier, allowing for seamless integration of\ndiscrete graph structure and continuous graph attributes. We empirically\nvalidate the approach on three graph datasets, showing that our model is\nconsistently high-performing and more robust than the baselines.\n","authors":["Andreas Abildtrup Hansen","Paraskevas Pegios","Anna Calissano","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2501.08850v1.pdf","comment":"Published at Northern Lights Deep Learning Conference 2025"},{"id":"http://arxiv.org/abs/2501.08848v1","updated":"2025-01-15T15:00:11Z","published":"2025-01-15T15:00:11Z","title":"RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning","summary":"  Network simulation is pivotal in network modeling, assisting with tasks\nranging from capacity planning to performance estimation. Traditional\napproaches such as Discrete Event Simulation (DES) face limitations in terms of\ncomputational cost and accuracy. This paper introduces RouteNet-Gauss, a novel\nintegration of a testbed network with a Machine Learning (ML) model to address\nthese challenges. By using the testbed as a hardware accelerator,\nRouteNet-Gauss generates training datasets rapidly and simulates network\nscenarios with high fidelity to real-world conditions. Experimental results\nshow that RouteNet-Gauss significantly reduces prediction errors by up to 95%\nand achieves a 488x speedup in inference time compared to state-of-the-art\nDES-based methods. RouteNet-Gauss's modular architecture is dynamically\nconstructed based on the specific characteristics of the network scenario, such\nas topology and routing. This enables it to understand and generalize to\ndifferent network configurations beyond those seen during training, including\nnetworks up to 10x larger. Additionally, it supports Temporal Aggregated\nPerformance Estimation (TAPE), providing configurable temporal granularity and\nmaintaining high accuracy in flow performance metrics. This approach shows\npromise in improving both simulation efficiency and accuracy, offering a\nvaluable tool for network operators.\n","authors":["Carlos Güemes-Palau","Miquel Ferriol-Galmés","Jordi Paillisse-Vilanova","Albert López-Brescó","Pere Barlet-Ros","Albert Cabellos-Aparicio"],"pdf_url":"https://arxiv.org/pdf/2501.08848v1.pdf","comment":"13 pages, 11 figures"},{"id":"http://arxiv.org/abs/2501.08822v1","updated":"2025-01-15T14:19:20Z","published":"2025-01-15T14:19:20Z","title":"Deep Learning Meets Queue-Reactive: A Framework for Realistic Limit\n  Order Book Simulation","summary":"  The Queue-Reactive model introduced by Huang et al. (2015) has become a\nstandard tool for limit order book modeling, widely adopted by both researchers\nand practitioners for its simplicity and effectiveness. We present the\nMultidimensional Deep Queue-Reactive (MDQR) model, which extends this framework\nin three ways: it relaxes the assumption of queue independence, enriches the\nstate space with market features, and models the distribution of order sizes.\nThrough a neural network architecture, the model learns complex dependencies\nbetween different price levels and adapts to varying market conditions, while\npreserving the interpretable point-process foundation of the original\nframework. Using data from the Bund futures market, we show that MDQR captures\nkey market properties including the square-root law of market impact,\ncross-queue correlations, and realistic order size patterns. The model\ndemonstrates particular strength in reproducing both conditional and stationary\ndistributions of order sizes, as well as various stylized facts of market\nmicrostructure. The model achieves this while maintaining the computational\nefficiency needed for practical applications such as strategy development\nthrough reinforcement learning or realistic backtesting.\n","authors":["Hamza Bodor","Laurent Carlier"],"pdf_url":"https://arxiv.org/pdf/2501.08822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08821v1","updated":"2025-01-15T14:19:03Z","published":"2025-01-15T14:19:03Z","title":"A Closer Look at the Learnability of Out-of-Distribution (OOD) Detection","summary":"  Machine learning algorithms often encounter different or\n\"out-of-distribution\" (OOD) data at deployment time, and OOD detection is\nfrequently employed to detect these examples. While it works reasonably well in\npractice, existing theoretical results on OOD detection are highly pessimistic.\nIn this work, we take a closer look at this problem, and make a distinction\nbetween uniform and non-uniform learnability, following PAC learning theory. We\ncharacterize under what conditions OOD detection is uniformly and non-uniformly\nlearnable, and we show that in several cases, non-uniform learnability turns a\nnumber of negative results into positive. In all cases where OOD detection is\nlearnable, we provide concrete learning algorithms and a sample-complexity\nanalysis.\n","authors":["Konstantin Garov","Kamalika Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2501.08821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08816v1","updated":"2025-01-15T14:12:59Z","published":"2025-01-15T14:12:59Z","title":"IDEA: Image Description Enhanced CLIP-Adapter","summary":"  CLIP (Contrastive Language-Image Pre-training) has attained great success in\npattern recognition and computer vision. Transferring CLIP to downstream tasks\n(e.g. zero- or few-shot classification) is a hot topic in multimodal learning.\nHowever, current studies primarily focus on either prompt learning for text or\nadapter tuning for vision, without fully exploiting the complementary\ninformation and correlations among image-text pairs. In this paper, we propose\nan Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to\nfew-shot image classification tasks. This method captures fine-grained features\nby leveraging both visual features and textual descriptions of images. IDEA is\na training-free method for CLIP, and it can be comparable to or even exceeds\nstate-of-the-art models on multiple tasks. Furthermore, we introduce\nTrainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable\ncomponents (i.e., a projector and a learnable latent space), further enhancing\nthe model's performance and achieving SOTA results on 11 datasets. As one\nimportant contribution, we employ the Llama model and design a comprehensive\npipeline to generate textual descriptions for images of 11 datasets, resulting\nin a total of 1,637,795 image-text pairs, named \"IMD-11\". Our code and data are\nreleased at https://github.com/FourierAI/IDEA.\n","authors":["Zhipeng Ye","Feng Jiang","Qiufeng Wang","Kaizhu Huang","Jiaqi Huang"],"pdf_url":"https://arxiv.org/pdf/2501.08816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.06471v2","updated":"2025-01-15T14:12:04Z","published":"2023-08-12T05:28:49Z","title":"Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model\n  Deforestation: An Exemplification from the Amazon Rainforest","summary":"  Intelligent automation supports us against cyclones, droughts, and seismic\nevents with recent technology advancements. Algorithmic learning has advanced\nfields like neuroscience, genetics, and human-computer interaction. Time-series\ndata boosts progress. Challenges persist in adopting these approaches in\ntraditional fields. Neural networks face comprehension and bias issues. AI's\nexpansion across scientific areas is due to adaptable descriptors and\ncombinatorial argumentation. This article focuses on modeling Forest loss using\nthe VANYA Model, incorporating Prey Predator Dynamics. VANYA predicts forest\ncover, demonstrated on Amazon Rainforest data against other forecasters like\nLong Short-Term Memory, N-BEATS, RCN.\n","authors":["Karthik R.","Ramamoorthy A"],"pdf_url":"https://arxiv.org/pdf/2308.06471v2.pdf","comment":"The experimental data used in this article has given wrong practical\n  interpretation. The data has to be updated to improve this"},{"id":"http://arxiv.org/abs/2402.07437v2","updated":"2025-01-15T14:02:51Z","published":"2024-02-12T06:32:53Z","title":"Learning Optimal Tax Design in Nonatomic Congestion Games","summary":"  In multiplayer games, self-interested behavior among the players can harm the\nsocial welfare. Tax mechanisms are a common method to alleviate this issue and\ninduce socially optimal behavior. In this work, we take the initial step of\nlearning the optimal tax that can maximize social welfare with limited feedback\nin congestion games. We propose a new type of feedback named \\emph{equilibrium\nfeedback}, where the tax designer can only observe the Nash equilibrium after\ndeploying a tax plan. Existing algorithms are not applicable due to the\nexponentially large tax function space, nonexistence of the gradient, and\nnonconvexity of the objective. To tackle these challenges, we design a\ncomputationally efficient algorithm that leverages several novel components:\n(1) a piece-wise linear tax to approximate the optimal tax; (2) extra linear\nterms to guarantee a strongly convex potential function; (3) an efficient\nsubroutine to find the exploratory tax that can provide critical information\nabout the game. The algorithm can find an $\\epsilon$-optimal tax with $O(\\beta\nF^2/\\epsilon)$ sample complexity, where $\\beta$ is the smoothness of the cost\nfunction and $F$ is the number of facilities.\n","authors":["Qiwen Cui","Maryam Fazel","Simon S. Du"],"pdf_url":"https://arxiv.org/pdf/2402.07437v2.pdf","comment":"23 pages. Accepted by Conference on Neural Information Processing\n  Systems (NeurIPS) 2024"},{"id":"http://arxiv.org/abs/2501.07317v3","updated":"2025-01-15T14:01:15Z","published":"2025-01-13T13:28:03Z","title":"Evaluation of Artificial Intelligence Methods for Lead Time Prediction\n  in Non-Cycled Areas of Automotive Production","summary":"  The present study examines the effectiveness of applying Artificial\nIntelligence methods in an automotive production environment to predict unknown\nlead times in a non-cycle-controlled production area. Data structures are\nanalyzed to identify contextual features and then preprocessed using one-hot\nencoding. Methods selection focuses on supervised machine learning techniques.\nIn supervised learning methods, regression and classification methods are\nevaluated. Continuous regression based on target size distribution is not\nfeasible. Classification methods analysis shows that Ensemble Learning and\nSupport Vector Machines are the most suitable. Preliminary study results\nindicate that gradient boosting algorithms LightGBM, XGBoost, and CatBoost\nyield the best results. After further testing and extensive hyperparameter\noptimization, the final method choice is the LightGBM algorithm. Depending on\nfeature availability and prediction interval granularity, relative prediction\naccuracies of up to 90% can be achieved. Further tests highlight the importance\nof periodic retraining of AI models to accurately represent complex production\nprocesses using the database. The research demonstrates that AI methods can be\neffectively applied to highly variable production data, adding business value\nby providing an additional metric for various control tasks while outperforming\ncurrent non AI-based systems.\n","authors":["Cornelius Hake","Jonas Weigele","Frederik Reichert","Christian Friedrich"],"pdf_url":"https://arxiv.org/pdf/2501.07317v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04562v2","updated":"2025-01-15T13:24:49Z","published":"2024-11-07T09:35:22Z","title":"Constrained Latent Action Policies for Model-Based Offline Reinforcement\n  Learning","summary":"  In offline reinforcement learning, a policy is learned using a static dataset\nin the absence of costly feedback from the environment. In contrast to the\nonline setting, only using static datasets poses additional challenges, such as\npolicies generating out-of-distribution samples. Model-based offline\nreinforcement learning methods try to overcome these by learning a model of the\nunderlying dynamics of the environment and using it to guide policy search. It\nis beneficial but, with limited datasets, errors in the model and the issue of\nvalue overestimation among out-of-distribution states can worsen performance.\nCurrent model-based methods apply some notion of conservatism to the Bellman\nupdate, often implemented using uncertainty estimation derived from model\nensembles. In this paper, we propose Constrained Latent Action Policies (C-LAP)\nwhich learns a generative model of the joint distribution of observations and\nactions. We cast policy learning as a constrained objective to always stay\nwithin the support of the latent action distribution, and use the generative\ncapabilities of the model to impose an implicit constraint on the generated\nactions. Thereby eliminating the need to use additional uncertainty penalties\non the Bellman update and significantly decreasing the number of gradient steps\nrequired to learn a policy. We empirically evaluate C-LAP on the D4RL and\nV-D4RL benchmark, and show that C-LAP is competitive to state-of-the-art\nmethods, especially outperforming on datasets with visual observations.\n","authors":["Marvin Alles","Philip Becker-Ehmck","Patrick van der Smagt","Maximilian Karl"],"pdf_url":"https://arxiv.org/pdf/2411.04562v2.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)"},{"id":"http://arxiv.org/abs/2501.08780v1","updated":"2025-01-15T13:01:47Z","published":"2025-01-15T13:01:47Z","title":"Deep learning for temporal super-resolution 4D Flow MRI","summary":"  4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive technique\nfor volumetric, time-resolved blood flow quantification. However, apparent\ntrade-offs between acquisition time, image noise, and resolution limit clinical\napplicability. In particular, in regions of highly transient flow, coarse\ntemporal resolution can hinder accurate capture of physiologically relevant\nflow variations. To overcome these issues, post-processing techniques using\ndeep learning have shown promising results to enhance resolution post-scan\nusing so-called super-resolution networks. However, while super-resolution has\nbeen focusing on spatial upsampling, temporal super-resolution remains largely\nunexplored. The aim of this study was therefore to implement and evaluate a\nresidual network for temporal super-resolution 4D Flow MRI. To achieve this, an\nexisting spatial network (4DFlowNet) was re-designed for temporal upsampling,\nadapting input dimensions, and optimizing internal layer structures. Training\nand testing were performed using synthetic 4D Flow MRI data originating from\npatient-specific in-silico models, as well as using in-vivo datasets. Overall,\nexcellent performance was achieved with input velocities effectively denoised\nand temporally upsampled, with a mean absolute error (MAE) of 1.0 cm/s in an\nunseen in-silico setting, outperforming deterministic alternatives (linear\ninterpolation MAE = 2.3 cm/s, sinc interpolation MAE = 2.6 cm/s). Further, the\nnetwork synthesized high-resolution temporal information from unseen\nlow-resolution in-vivo data, with strong correlation observed at peak flow\nframes. As such, our results highlight the potential of utilizing data-driven\nneural networks for temporal super-resolution 4D Flow MRI, enabling\nhigh-frame-rate flow quantification without extending acquisition times beyond\nclinically acceptable limits.\n","authors":["Pia Callmer","Mia Bonini","Edward Ferdian","David Nordsletten","Daniel Giese","Alistair A. Young","Alexander Fyrdahl","David Marlevi"],"pdf_url":"https://arxiv.org/pdf/2501.08780v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2501.08779v1","updated":"2025-01-15T13:01:34Z","published":"2025-01-15T13:01:34Z","title":"Nesterov Acceleration for Ensemble Kalman Inversion and Variants","summary":"  Ensemble Kalman inversion (EKI) is a derivative-free, particle-based\noptimization method for solving inverse problems. It can be shown that EKI\napproximates a gradient flow, which allows the application of methods for\naccelerating gradient descent. Here, we show that Nesterov acceleration is\neffective in speeding up the reduction of the EKI cost function on a variety of\ninverse problems. We also implement Nesterov acceleration for two EKI variants,\nunscented Kalman inversion and ensemble transform Kalman inversion. Our\nspecific implementation takes the form of a particle-level nudge that is\ndemonstrably simple to couple in a black-box fashion with any existing EKI\nvariant algorithms, comes with no additional computational expense, and with no\nadditional tuning hyperparameters. This work shows a pathway for future\nresearch to translate advances in gradient-based optimization into advances in\ngradient-free Kalman optimization.\n","authors":["Sydney Vernon","Eviatar Bach","Oliver R. A. Dunbar"],"pdf_url":"https://arxiv.org/pdf/2501.08779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08778v1","updated":"2025-01-15T13:01:32Z","published":"2025-01-15T13:01:32Z","title":"Networked Agents in the Dark: Team Value Learning under Partial\n  Observability","summary":"  We propose a novel cooperative multi-agent reinforcement learning (MARL)\napproach for networked agents. In contrast to previous methods that rely on\ncomplete state information or joint observations, our agents must learn how to\nreach shared objectives under partial observability. During training, they\ncollect individual rewards and approximate a team value function through local\ncommunication, resulting in cooperative behavior. To describe our problem, we\nintroduce the networked dynamic partially observable Markov game framework,\nwhere agents communicate over a switching topology communication network. Our\ndistributed method, DNA-MARL, uses a consensus mechanism for local\ncommunication and gradient descent for local computation. DNA-MARL increases\nthe range of the possible applications of networked agents, being well-suited\nfor real world domains that impose privacy and where the messages may not reach\ntheir recipients. We evaluate DNA-MARL across benchmark MARL scenarios. Our\nresults highlight the superior performance of DNA-MARL over previous methods.\n","authors":["Guilherme S. Varela","Alberto Sardinha","Francisco S. Melo"],"pdf_url":"https://arxiv.org/pdf/2501.08778v1.pdf","comment":"18 pages, 7 figures, 5 tables. Accepted as supplemental material at\n  Proceedings of the 24th International Conference on Autonomous Agents and\n  Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA, May 19 - 23, 2025,\n  IFAAMAS"},{"id":"http://arxiv.org/abs/2311.16054v5","updated":"2025-01-15T12:57:47Z","published":"2023-11-27T18:19:07Z","title":"Metric Space Magnitude for Evaluating the Diversity of Latent\n  Representations","summary":"  The magnitude of a metric space is a novel invariant that provides a measure\nof the 'effective size' of a space across multiple scales, while also capturing\nnumerous geometrical properties, such as curvature, density, or entropy. We\ndevelop a family of magnitude-based measures of the intrinsic diversity of\nlatent representations, formalising a novel notion of dissimilarity between\nmagnitude functions of finite metric spaces. Our measures are provably stable\nunder perturbations of the data, can be efficiently calculated, and enable a\nrigorous multi-scale characterisation and comparison of latent representations.\nWe show their utility and superior performance across different domains and\ntasks, including (i) the automated estimation of diversity, (ii) the detection\nof mode collapse, and (iii) the evaluation of generative models for text,\nimage, and graph data.\n","authors":["Katharina Limbeck","Rayna Andreeva","Rik Sarkar","Bastian Rieck"],"pdf_url":"https://arxiv.org/pdf/2311.16054v5.pdf","comment":"Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS) 2024. The code for computing magnitude is available at\n  https://github.com/aidos-lab/magnipy"},{"id":"http://arxiv.org/abs/2501.08760v1","updated":"2025-01-15T12:25:56Z","published":"2025-01-15T12:25:56Z","title":"Leveraging LLM Agents for Translating Network Configurations","summary":"  Configuration translation is a critical and frequent task in network\noperations. When a network device is damaged or outdated, administrators need\nto replace it to maintain service continuity. The replacement devices may\noriginate from different vendors, necessitating configuration translation to\nensure seamless network operation. However, translating configurations manually\nis a labor-intensive and error-prone process. In this paper, we propose an\nintent-based framework for translating network configuration with Large\nLanguage Model (LLM) Agents. The core of our approach is an Intent-based\nRetrieval Augmented Generation (IRAG) module that systematically splits a\nconfiguration file into fragments, extracts intents, and generates accurate\ntranslations. We also design a two-stage verification method to validate the\nsyntax and semantics correctness of the translated configurations. We implement\nand evaluate the proposed method on real-world network configurations.\nExperimental results show that our method achieves 97.74% syntax correctness,\noutperforming state-of-the-art methods in translation accuracy.\n","authors":["Yunze Wei","Xiaohui Xie","Yiwei Zuo","Tianshuo Hu","Xinyi Chen","Kaiwen Chi","Yong Cui"],"pdf_url":"https://arxiv.org/pdf/2501.08760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08002v2","updated":"2025-01-15T11:52:29Z","published":"2025-01-14T10:46:41Z","title":"Maximizing Uncertainty for Federated learning via Bayesian\n  Optimisation-based Model Poisoning","summary":"  As we transition from Narrow Artificial Intelligence towards Artificial Super\nIntelligence, users are increasingly concerned about their privacy and the\ntrustworthiness of machine learning (ML) technology. A common denominator for\nthe metrics of trustworthiness is the quantification of uncertainty inherent in\nDL algorithms, and specifically in the model parameters, input data, and model\npredictions. One of the common approaches to address privacy-related issues in\nDL is to adopt distributed learning such as federated learning (FL), where\nprivate raw data is not shared among users. Despite the privacy-preserving\nmechanisms in FL, it still faces challenges in trustworthiness. Specifically,\nthe malicious users, during training, can systematically create malicious model\nparameters to compromise the models predictive and generative capabilities,\nresulting in high uncertainty about their reliability. To demonstrate malicious\nbehaviour, we propose a novel model poisoning attack method named Delphi which\naims to maximise the uncertainty of the global model output. We achieve this by\ntaking advantage of the relationship between the uncertainty and the model\nparameters of the first hidden layer of the local model. Delphi employs two\ntypes of optimisation , Bayesian Optimisation and Least Squares Trust Region,\nto search for the optimal poisoned model parameters, named as Delphi-BO and\nDelphi-LSTR. We quantify the uncertainty using the KL Divergence to minimise\nthe distance of the predictive probability distribution towards an uncertain\ndistribution of model output. Furthermore, we establish a mathematical proof\nfor the attack effectiveness demonstrated in FL. Numerical results demonstrate\nthat Delphi-BO induces a higher amount of uncertainty than Delphi-LSTR\nhighlighting vulnerability of FL systems to model poisoning attacks.\n","authors":["Marios Aristodemou","Xiaolan Liu","Yuan Wang","Konstantinos G. Kyriakopoulos","Sangarapillai Lambotharan","Qingsong Wei"],"pdf_url":"https://arxiv.org/pdf/2501.08002v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2501.08738v1","updated":"2025-01-15T11:34:56Z","published":"2025-01-15T11:34:56Z","title":"MeshMask: Physics-Based Simulations with Masked Graph Neural Networks","summary":"  We introduce a novel masked pre-training technique for graph neural networks\n(GNNs) applied to computational fluid dynamics (CFD) problems. By randomly\nmasking up to 40\\% of input mesh nodes during pre-training, we force the model\nto learn robust representations of complex fluid dynamics. We pair this masking\nstrategy with an asymmetric encoder-decoder architecture and gated multi-layer\nperceptrons to further enhance performance. The proposed method achieves\nstate-of-the-art results on seven CFD datasets, including a new challenging\ndataset of 3D intracranial aneurysm simulations with over 250,000 nodes per\nmesh. Moreover, it significantly improves model performance and training\nefficiency across such diverse range of fluid simulation tasks. We demonstrate\nimprovements of up to 60\\% in long-term prediction accuracy compared to\nprevious best models, while maintaining similar computational costs. Notably,\nour approach enables effective pre-training on multiple datasets\nsimultaneously, significantly reducing the time and data required to achieve\nhigh performance on new tasks. Through extensive ablation studies, we provide\ninsights into the optimal masking ratio, architectural choices, and training\nstrategies.\n","authors":["Paul Garnier","Vincent Lannelongue","Jonathan Viquerat","Elie Hachem"],"pdf_url":"https://arxiv.org/pdf/2501.08738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08737v1","updated":"2025-01-15T11:33:52Z","published":"2025-01-15T11:33:52Z","title":"Resource-Constrained Federated Continual Learning: What Does Matter?","summary":"  Federated Continual Learning (FCL) aims to enable sequentially\nprivacy-preserving model training on streams of incoming data that vary in edge\ndevices by preserving previous knowledge while adapting to new data. Current\nFCL literature focuses on restricted data privacy and access to previously seen\ndata while imposing no constraints on the training overhead. This is\nunreasonable for FCL applications in real-world scenarios, where edge devices\nare primarily constrained by resources such as storage, computational budget,\nand label rate. We revisit this problem with a large-scale benchmark and\nanalyze the performance of state-of-the-art FCL approaches under different\nresource-constrained settings. Various typical FCL techniques and six datasets\nin two incremental learning scenarios (Class-IL and Domain-IL) are involved in\nour experiments. Through extensive experiments amounting to a total of over\n1,000+ GPU hours, we find that, under limited resource-constrained settings,\nexisting FCL approaches, with no exception, fail to achieve the expected\nperformance. Our conclusions are consistent in the sensitivity analysis. This\nsuggests that most existing FCL methods are particularly too resource-dependent\nfor real-world deployment. Moreover, we study the performance of typical FCL\ntechniques with resource constraints and shed light on future research\ndirections in FCL.\n","authors":["Yichen Li","Yuying Wang","Jiahua Dong","Haozhao Wang","Yining Qi","Rui Zhang","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2501.08737v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2303.11165 by other authors"},{"id":"http://arxiv.org/abs/2412.19217v2","updated":"2025-01-15T11:21:16Z","published":"2024-12-26T13:47:04Z","title":"Applying the maximum entropy principle to neural networks enhances\n  multi-species distribution models","summary":"  The rapid expansion of citizen science initiatives has led to a significant\ngrowth of biodiversity databases, and particularly presence-only (PO)\nobservations. PO data are invaluable for understanding species distributions\nand their dynamics, but their use in a Species Distribution Model (SDM) is\ncurtailed by sampling biases and the lack of information on absences. Poisson\npoint processes are widely used for SDMs, with Maxent being one of the most\npopular methods. Maxent maximises the entropy of a probability distribution\nacross sites as a function of predefined transformations of variables, called\nfeatures. In contrast, neural networks and deep learning have emerged as a\npromising technique for automatic feature extraction from complex input\nvariables. Arbitrarily complex transformations of input variables can be\nlearned from the data efficiently through backpropagation and stochastic\ngradient descent (SGD). In this paper, we propose DeepMaxent, which harnesses\nneural networks to automatically learn shared features among species, using the\nmaximum entropy principle. To do so, it employs a normalised Poisson loss where\nfor each species, presence probabilities across sites are modelled by a neural\nnetwork. We evaluate DeepMaxent on a benchmark dataset known for its spatial\nsampling biases, using PO data for calibration and presence-absence (PA) data\nfor validation across six regions with different biological groups and\ncovariates. Our results indicate that DeepMaxent performs better than Maxent\nand other leading SDMs across all regions and taxonomic groups. The method\nperforms particularly well in regions of uneven sampling, demonstrating\nsubstantial potential to increase SDM performances. In particular, our approach\nyields more accurate predictions than traditional single-species models, which\nopens up new possibilities for methodological enhancement.\n","authors":["Maxime Ryckewaert","Diego Marcos","Christophe Botella","Maximilien Servajean","Pierre Bonnet","Alexis Joly"],"pdf_url":"https://arxiv.org/pdf/2412.19217v2.pdf","comment":"Submitted to Methods in Ecology and Evolution"},{"id":"http://arxiv.org/abs/2407.00956v3","updated":"2025-01-15T11:19:30Z","published":"2024-07-01T04:24:07Z","title":"A Closer Look at Deep Learning Methods on Tabular Datasets","summary":"  Tabular data is prevalent across diverse domains in machine learning. While\nclassical methods like tree-based models have long been effective, Deep Neural\nNetwork (DNN)-based methods have recently demonstrated promising performance.\nHowever, the diverse characteristics of methods and the inherent heterogeneity\nof tabular datasets make understanding and interpreting tabular methods both\nchallenging and prone to unstable observations. In this paper, we conduct\nin-depth evaluations and comprehensive analyses of tabular methods, with a\nparticular focus on DNN-based models, using a benchmark of over 300 tabular\ndatasets spanning a wide range of task types, sizes, and domains. First, we\nperform an extensive comparison of 32 state-of-the-art deep and tree-based\nmethods, evaluating their average performance across multiple criteria.\nAlthough method ranks vary across datasets, we empirically find that\ntop-performing methods tend to concentrate within a small subset of tabular\nmodels, regardless of the criteria used. Next, we investigate whether the\ntraining dynamics of deep tabular models can be predicted based on dataset\nproperties. This approach not only offers insights into the behavior of deep\ntabular methods but also identifies a core set of \"meta-features\" that reflect\ndataset heterogeneity. The other subset includes datasets where method ranks\nare consistent with the overall benchmark, acting as a reliable probe for\nfurther tabular analysis.\n","authors":["Han-Jia Ye","Si-Yang Liu","Hao-Run Cai","Qi-Le Zhou","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2407.00956v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07592v3","updated":"2025-01-15T11:18:10Z","published":"2024-06-11T12:15:47Z","title":"MambaLRP: Explaining Selective State Space Sequence Models","summary":"  Recent sequence modeling approaches using selective state space sequence\nmodels, referred to as Mamba models, have seen a surge of interest. These\nmodels allow efficient processing of long sequences in linear time and are\nrapidly being adopted in a wide range of applications such as language\nmodeling, demonstrating promising performance. To foster their reliable use in\nreal-world scenarios, it is crucial to augment their transparency. Our work\nbridges this critical gap by bringing explainability, particularly Layer-wise\nRelevance Propagation (LRP), to the Mamba architecture. Guided by the axiom of\nrelevance conservation, we identify specific components in the Mamba\narchitecture, which cause unfaithful explanations. To remedy this issue, we\npropose MambaLRP, a novel algorithm within the LRP framework, which ensures a\nmore stable and reliable relevance propagation through these components. Our\nproposed method is theoretically sound and excels in achieving state-of-the-art\nexplanation performance across a diverse range of models and datasets.\nMoreover, MambaLRP facilitates a deeper inspection of Mamba architectures,\nuncovering various biases and evaluating their significance. It also enables\nthe analysis of previous speculations regarding the long-range capabilities of\nMamba models.\n","authors":["Farnoush Rezaei Jafari","Grégoire Montavon","Klaus-Robert Müller","Oliver Eberle"],"pdf_url":"https://arxiv.org/pdf/2406.07592v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08729v1","updated":"2025-01-15T11:11:38Z","published":"2025-01-15T11:11:38Z","title":"GRAPPA - A Hybrid Graph Neural Network for Predicting Pure Component\n  Vapor Pressures","summary":"  Although the pure component vapor pressure is one of the most important\nproperties for designing chemical processes, no broadly applicable,\nsufficiently accurate, and open-source prediction method has been available. To\novercome this, we have developed GRAPPA - a hybrid graph neural network for\npredicting vapor pressures of pure components. GRAPPA enables the prediction of\nthe vapor pressure curve of basically any organic molecule, requiring only the\nmolecular structure as input. The new model consists of three parts: A graph\nattention network for the message passing step, a pooling function that\ncaptures long-range interactions, and a prediction head that yields the\ncomponent-specific parameters of the Antoine equation, from which the vapor\npressure can readily and consistently be calculated for any temperature. We\nhave trained and evaluated GRAPPA on experimental vapor pressure data of almost\n25,000 pure components. We found excellent prediction accuracy for unseen\ncomponents, outperforming state-of-the-art group contribution methods and other\nmachine learning approaches in applicability and accuracy. The trained model\nand its code are fully disclosed, and GRAPPA is directly applicable via the\ninteractive website ml-prop.mv.rptu.de.\n","authors":["Marco Hoffmann","Hans Hasse","Fabian Jirasek"],"pdf_url":"https://arxiv.org/pdf/2501.08729v1.pdf","comment":"38 pages, 12 figures"},{"id":"http://arxiv.org/abs/2501.08727v1","updated":"2025-01-15T11:10:37Z","published":"2025-01-15T11:10:37Z","title":"Transformed Low-rank Adaptation via Tensor Decomposition and Its\n  Applications to Text-to-image Models","summary":"  Parameter-Efficient Fine-Tuning (PEFT) of text-to-image models has become an\nincreasingly popular technique with many applications. Among the various PEFT\nmethods, Low-Rank Adaptation (LoRA) and its variants have gained significant\nattention due to their effectiveness, enabling users to fine-tune models with\nlimited computational resources. However, the approximation gap between the\nlow-rank assumption and desired fine-tuning weights prevents the simultaneous\nacquisition of ultra-parameter-efficiency and better performance. To reduce\nthis gap and further improve the power of LoRA, we propose a new PEFT method\nthat combines two classes of adaptations, namely, transform and residual\nadaptations. In specific, we first apply a full-rank and dense transform to the\npre-trained weight. This learnable transform is expected to align the\npre-trained weight as closely as possible to the desired weight, thereby\nreducing the rank of the residual weight. Then, the residual part can be\neffectively approximated by more compact and parameter-efficient structures,\nwith a smaller approximation error. To achieve ultra-parameter-efficiency in\npractice, we design highly flexible and effective tensor decompositions for\nboth the transform and residual adaptations. Additionally, popular PEFT methods\nsuch as DoRA can be summarized under this transform plus residual adaptation\nscheme. Experiments are conducted on fine-tuning Stable Diffusion models in\nsubject-driven and controllable generation. The results manifest that our\nmethod can achieve better performances and parameter efficiency compared to\nLoRA and several baselines.\n","authors":["Zerui Tao","Yuhta Takida","Naoki Murata","Qibin Zhao","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2501.08727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14378v3","updated":"2025-01-15T11:07:35Z","published":"2024-09-22T09:48:45Z","title":"Sparse Low-Ranked Self-Attention Transformer for Remaining Useful\n  Lifetime Prediction of Optical Fiber Amplifiers","summary":"  Optical fiber amplifiers are key elements in present optical networks.\nFailures of these components result in high financial loss of income of the\nnetwork operator as the communication traffic over an affected link is\ninterrupted. Applying Remaining useful lifetime (RUL) prediction in the context\nof Predictive Maintenance (PdM) to optical fiber amplifiers to predict upcoming\nsystem failures at an early stage, so that network outages can be minimized\nthrough planning of targeted maintenance actions, ensures reliability and\nsafety. Optical fiber amplifier are complex systems, that work under various\noperating conditions, which makes correct forecasting a difficult task.\nIncreased monitoring capabilities of systems results in datasets that\nfacilitate the application of data-driven RUL prediction methods. Deep learning\nmodels in particular have shown good performance, but generalization based on\ncomparatively small datasets for RUL prediction is difficult. In this paper, we\npropose Sparse Low-ranked self-Attention Transformer (SLAT) as a novel RUL\nprediction method. SLAT is based on an encoder-decoder architecture, wherein\ntwo parallel working encoders extract features for sensors and time steps. By\nutilizing the self-attention mechanism, long-term dependencies can be learned\nfrom long sequences. The implementation of sparsity in the attention matrix and\na low-rank parametrization reduce overfitting and increase generalization.\nExperimental application to optical fiber amplifiers exemplified on EDFA, as\nwell as a reference dataset from turbofan engines, shows that SLAT outperforms\nthe state-of-the-art methods.\n","authors":["Dominic Schneider","Lutz Rapp"],"pdf_url":"https://arxiv.org/pdf/2409.14378v3.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2501.08717v1","updated":"2025-01-15T10:58:32Z","published":"2025-01-15T10:58:32Z","title":"$\\texttt{InfoHier}$: Hierarchical Information Extraction via Encoding\n  and Embedding","summary":"  Analyzing large-scale datasets, especially involving complex and\nhigh-dimensional data like images, is particularly challenging. While\nself-supervised learning (SSL) has proven effective for learning\nrepresentations from unlabelled data, it typically focuses on flat,\nnon-hierarchical structures, missing the multi-level relationships present in\nmany real-world datasets. Hierarchical clustering (HC) can uncover these\nrelationships by organizing data into a tree-like structure, but it often\nrelies on rigid similarity metrics that struggle to capture the complexity of\ndiverse data types. To address these we envision $\\texttt{InfoHier}$, a\nframework that combines SSL with HC to jointly learn robust latent\nrepresentations and hierarchical structures. This approach leverages SSL to\nprovide adaptive representations, enhancing HC's ability to capture complex\npatterns. Simultaneously, it integrates HC loss to refine SSL training,\nresulting in representations that are more attuned to the underlying\ninformation hierarchy. $\\texttt{InfoHier}$ has the potential to improve the\nexpressiveness and performance of both clustering and representation learning,\noffering significant benefits for data analysis, management, and information\nretrieval.\n","authors":["Tianru Zhang","Li Ju","Prashant Singh","Salman Toor"],"pdf_url":"https://arxiv.org/pdf/2501.08717v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2501.08712v1","updated":"2025-01-15T10:54:21Z","published":"2025-01-15T10:54:21Z","title":"Self-supervised Transformation Learning for Equivariant Representations","summary":"  Unsupervised representation learning has significantly advanced various\nmachine learning tasks. In the computer vision domain, state-of-the-art\napproaches utilize transformations like random crop and color jitter to achieve\ninvariant representations, embedding semantically the same inputs despite\ntransformations. However, this can degrade performance in tasks requiring\nprecise features, such as localization or flower classification. To address\nthis, recent research incorporates equivariant representation learning, which\ncaptures transformation-sensitive information. However, current methods depend\non transformation labels and thus struggle with interdependency and complex\ntransformations. We propose Self-supervised Transformation Learning (STL),\nreplacing transformation labels with transformation representations derived\nfrom image pairs. The proposed method ensures transformation representation is\nimage-invariant and learns corresponding equivariant transformations, enhancing\nperformance without increased batch complexity. We demonstrate the approach's\neffectiveness across diverse classification and detection tasks, outperforming\nexisting methods in 7 out of 11 benchmarks and excelling in detection. By\nintegrating complex transformations like AugMix, unusable by prior equivariant\nmethods, this approach enhances performance across tasks, underscoring its\nadaptability and resilience. Additionally, its compatibility with various base\nmodels highlights its flexibility and broad applicability. The code is\navailable at https://github.com/jaemyung-u/stl.\n","authors":["Jaemyung Yu","Jaehyun Choi","Dong-Jae Lee","HyeongGwon Hong","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2501.08712v1.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)"},{"id":"http://arxiv.org/abs/2501.08710v1","updated":"2025-01-15T10:50:54Z","published":"2025-01-15T10:50:54Z","title":"Disentangled Interleaving Variational Encoding","summary":"  Conflicting objectives present a considerable challenge in interleaving\nmulti-task learning, necessitating the need for meticulous design and balance\nto ensure effective learning of a representative latent data space across all\ntasks without mutual negative impact. Drawing inspiration from the concept of\nmarginal and conditional probability distributions in probability theory, we\ndesign a principled and well-founded approach to disentangle the original input\ninto marginal and conditional probability distributions in the latent space of\na variational autoencoder. Our proposed model, Deep Disentangled Interleaving\nVariational Encoding (DeepDIVE) learns disentangled features from the original\ninput to form clusters in the embedding space and unifies these features via\nthe cross-attention mechanism in the fusion stage. We theoretically prove that\ncombining the objectives for reconstruction and forecasting fully captures the\nlower bound and mathematically derive a loss function for disentanglement using\nNa\\\"ive Bayes. Under the assumption that the prior is a mixture of log-concave\ndistributions, we also establish that the Kullback-Leibler divergence between\nthe prior and the posterior is upper bounded by a function minimized by the\nminimizer of the cross entropy loss, informing our adoption of radial basis\nfunctions (RBF) and cross entropy with interleaving training for DeepDIVE to\nprovide a justified basis for convergence. Experiments on two public datasets\nshow that DeepDIVE disentangles the original input and yields forecast\naccuracies better than the original VAE and comparable to existing\nstate-of-the-art baselines.\n","authors":["Noelle Y. L. Wong","Eng Yeow Cheu","Zhonglin Chiam"],"pdf_url":"https://arxiv.org/pdf/2501.08710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09495v3","updated":"2025-01-15T10:47:05Z","published":"2024-06-13T17:36:05Z","title":"FADE: Towards Fairness-aware Augmentation for Domain Generalization via\n  Classifier-Guided Score-based Diffusion Models","summary":"  Fairness-aware domain generalization (FairDG) has emerged as a critical\nchallenge for deploying trustworthy AI systems, particularly in scenarios\ninvolving distribution shifts. Traditional methods for addressing fairness have\nfailed in domain generalization due to their lack of consideration for\ndistribution shifts. Although disentanglement has been used to tackle FairDG,\nit is limited by its strong assumptions. To overcome these limitations, we\npropose Fairness-aware Classifier-Guided Score-based Diffusion Models (FADE) as\na novel approach to effectively address the FairDG issue. Specifically, we\nfirst pre-train a score-based diffusion model (SDM) and two classifiers to\nequip the model with strong generalization capabilities across different\ndomains. Then, we guide the SDM using these pre-trained classifiers to\neffectively eliminate sensitive information from the generated data. Finally,\nthe generated fair data is used to train downstream classifiers, ensuring\nrobust performance under new data distributions. Extensive experiments on three\nreal-world datasets demonstrate that FADE not only enhances fairness but also\nimproves accuracy in the presence of distribution shifts. Additionally, FADE\noutperforms existing methods in achieving the best accuracy-fairness\ntrade-offs.\n","authors":["Yujie Lin","Dong Li","Chen Zhao","Minglai Shao","Guihong Wan"],"pdf_url":"https://arxiv.org/pdf/2406.09495v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06484v6","updated":"2025-01-15T10:41:40Z","published":"2024-06-10T17:24:42Z","title":"Parallelizing Linear Transformers with the Delta Rule over Sequence\n  Length","summary":"  Transformers with linear attention (i.e., linear transformers) and\nstate-space models have recently been suggested as a viable linear-time\nalternative to transformers with softmax attention. However, these models still\nunderperform transformers especially on tasks that require in-context\nretrieval. While more expressive variants of linear transformers which replace\nthe additive update in linear transformers with the delta rule (DeltaNet) have\nbeen found to be more effective at associative recall, existing algorithms for\ntraining such models do not parallelize over sequence length and are thus\ninefficient to train on modern hardware. This work describes a\nhardware-efficient algorithm for training linear transformers with the delta\nrule, which exploits a memory-efficient representation for computing products\nof Householder matrices. This algorithm allows us to scale up DeltaNet to\nstandard language modeling settings. We train a 1.3B model for 100B tokens and\nfind that it outperforms recent linear-time baselines such as Mamba and GLA in\nterms of perplexity and zero-shot performance on downstream tasks. We also\nexperiment with two hybrid models which combine DeltaNet layers with (1)\nsliding-window attention layers every other layer or (2) two global attention\nlayers, and find that these hybrids outperform strong transformer baselines.\n","authors":["Songlin Yang","Bailin Wang","Yu Zhang","Yikang Shen","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2406.06484v6.pdf","comment":"Final camera ready"},{"id":"http://arxiv.org/abs/2501.08115v2","updated":"2025-01-15T10:05:39Z","published":"2025-01-14T13:46:07Z","title":"RoHan: Robust Hand Detection in Operation Room","summary":"  Hand-specific localization has garnered significant interest within the\ncomputer vision community. Although there are numerous datasets with hand\nannotations from various angles and settings, domain transfer techniques\nfrequently struggle in surgical environments. This is mainly due to the limited\navailability of gloved hand instances and the unique challenges of operating\nrooms (ORs). Thus, hand-detection models tailored to OR settings require\nextensive training and expensive annotation processes. To overcome these\nchallenges, we present \"RoHan\" - a novel approach for robust hand detection in\nthe OR, leveraging advanced semi-supervised domain adaptation techniques to\ntackle the challenges of varying recording conditions, diverse glove colors,\nand occlusions common in surgical settings. Our methodology encompasses two\nmain stages: (1) data augmentation strategy that utilizes \"Artificial Gloves,\"\na method for augmenting publicly available hand datasets with synthetic images\nof hands-wearing gloves; (2) semi-supervised domain adaptation pipeline that\nimproves detection performance in real-world OR settings through iterative\nprediction refinement and efficient frame filtering. We evaluate our method\nusing two datasets: simulated enterotomy repair and saphenous vein graft\nharvesting. \"RoHan\" substantially reduces the need for extensive labeling and\nmodel training, paving the way for the practical implementation of hand\ndetection technologies in medical settings.\n","authors":["Roi Papo","Sapir Gershov","Tom Friedman","Itay Or","Gil Bolotin","Shlomi Laufer"],"pdf_url":"https://arxiv.org/pdf/2501.08115v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2409.18836v2","updated":"2025-01-15T10:02:02Z","published":"2024-09-27T15:29:32Z","title":"Constructing Confidence Intervals for 'the' Generalization Error -- a\n  Comprehensive Benchmark Study","summary":"  When assessing the quality of prediction models in machine learning,\nconfidence intervals (CIs) for the generalization error, which measures\npredictive performance, are a crucial tool. Luckily, there exist many methods\nfor computing such CIs and new promising approaches are continuously being\nproposed. Typically, these methods combine various resampling procedures, most\npopular among them cross-validation and bootstrapping, with different variance\nestimation techniques. Unfortunately, however, there is currently no consensus\non when any of these combinations may be most reliably employed and how they\ngenerally compare. In this work, we conduct a large-scale study comparing CIs\nfor the generalization error, the first one of such size, where we empirically\nevaluate 13 different CI methods on a total of 19 tabular regression and\nclassification problems, using seven different inducers and a total of eight\nloss functions. We give an overview of the methodological foundations and\ninherent challenges of constructing CIs for the generalization error and\nprovide a concise review of all 13 methods in a unified framework. Finally, the\nCI methods are evaluated in terms of their relative coverage frequency, width,\nand runtime. Based on these findings, we can identify a subset of methods that\nwe would recommend. We also publish the datasets as a benchmarking suite on\nOpenML and our code on GitHub to serve as a basis for further studies.\n","authors":["Hannah Schulz-Kümpel","Sebastian Fischer","Roman Hornung","Anne-Laure Boulesteix","Thomas Nagler","Bernd Bischl"],"pdf_url":"https://arxiv.org/pdf/2409.18836v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05807v2","updated":"2025-01-15T09:53:49Z","published":"2024-10-08T08:40:07Z","title":"Extended convexity and smoothness and their applications in deep\n  learning","summary":"  This paper introduces an optimization framework aimed at providing a\ntheoretical foundation for a class of composite optimization problems,\nparticularly those encountered in deep learning. In this framework, we\nintroduce $\\mathcal{H}(\\phi)$-convexity and $\\mathcal{H}(\\Phi)$-smoothness to\ngeneralize the existing concepts of Lipschitz smoothness and strong convexity.\nFurthermore, we analyze and establish the convergence of both gradient descent\nand stochastic gradient descent methods for objective functions that are\n$\\mathcal{H}(\\Phi)$-smooth. We prove that the optimal convergence rates of\nthese methods depend solely on the homogeneous degree of $\\Phi$. Based on these\nfindings, we construct two types of non-convex and non-smooth optimization\nproblems: deterministic composite and stochastic composite optimization\nproblems, which encompass the majority of optimization problems in deep\nlearning. To address these problems, we develop the gradient structure control\nalgorithm and prove that it can locate approximate global optima. This marks a\nsignificant departure from traditional non-convex analysis framework, which\ntypically settle for stationary points. Therefore, with the introduction of\n$\\mathcal{H}(\\phi)$-convexity and $\\mathcal{H}(\\Phi)$-smoothness, along with\nthe GSC algorithm, the non-convex optimization mechanisms in deep learning can\nbe theoretically explained and supported. Finally, the effectiveness of the\nproposed framework is substantiated through empirical experimentation.\n","authors":["Binchuan Qi","Wei Gong","Li Li"],"pdf_url":"https://arxiv.org/pdf/2410.05807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05301v2","updated":"2025-01-15T09:42:42Z","published":"2024-10-04T12:22:54Z","title":"Diffusion-based Unsupervised Audio-visual Speech Enhancement","summary":"  This paper proposes a new unsupervised audio-visual speech enhancement (AVSE)\napproach that combines a diffusion-based audio-visual speech generative model\nwith a non-negative matrix factorization (NMF) noise model. First, the\ndiffusion model is pre-trained on clean speech conditioned on corresponding\nvideo data to simulate the speech generative distribution. This pre-trained\nmodel is then paired with the NMF-based noise model to estimate clean speech\niteratively. Specifically, a diffusion-based posterior sampling approach is\nimplemented within the reverse diffusion process, where after each iteration, a\nspeech estimate is obtained and used to update the noise parameters.\nExperimental results confirm that the proposed AVSE approach not only\noutperforms its audio-only counterpart but also generalizes better than a\nrecent supervised-generative AVSE method. Additionally, the new inference\nalgorithm offers a better balance between inference speed and performance\ncompared to the previous diffusion-based method. Code and demo available at:\nhttps://jeaneudesayilo.github.io/fast_UdiffSE\n","authors":["Jean-Eudes Ayilo","Mostafa Sadeghi","Romain Serizel","Xavier Alameda-Pineda"],"pdf_url":"https://arxiv.org/pdf/2410.05301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12588v2","updated":"2025-01-15T09:30:18Z","published":"2024-01-23T09:43:30Z","title":"Interpreting Equivariant Representations","summary":"  Latent representations are used extensively for downstream tasks, such as\nvisualization, interpolation or feature extraction of deep learning models.\nInvariant and equivariant neural networks are powerful and well-established\nmodels for enforcing inductive biases. In this paper, we demonstrate that the\ninductive bias imposed on the by an equivariant model must also be taken into\naccount when using latent representations. We show how not accounting for the\ninductive biases leads to decreased performance on downstream tasks, and vice\nversa, how accounting for inductive biases can be done effectively by using an\ninvariant projection of the latent representations. We propose principles for\nhow to choose such a projection, and show the impact of using these principles\nin two common examples: First, we study a permutation equivariant variational\nauto-encoder trained for molecule graph generation; here we show that invariant\nprojections can be designed that incur no loss of information in the resulting\ninvariant representation. Next, we study a rotation-equivariant representation\nused for image classification. Here, we illustrate how random invariant\nprojections can be used to obtain an invariant representation with a high\ndegree of retained information. In both cases, the analysis of invariant latent\nrepresentations proves superior to their equivariant counterparts. Finally, we\nillustrate that the phenomena documented here for equivariant neural networks\nhave counterparts in standard neural networks where invariance is encouraged\nvia augmentation. Thus, while these ambiguities may be known by experienced\ndevelopers of equivariant models, we make both the knowledge as well as\neffective tools to handle the ambiguities available to the broader community.\n","authors":["Andreas Abildtrup Hansen","Anna Calissano","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2401.12588v2.pdf","comment":"This paper was updated to reflect the version accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2407.13977v3","updated":"2025-01-15T09:25:02Z","published":"2024-07-19T02:06:08Z","title":"A Unified Confidence Sequence for Generalized Linear Models, with\n  Applications to Bandits","summary":"  We present a unified likelihood ratio-based confidence sequence (CS) for any\n(self-concordant) generalized linear model (GLM) that is guaranteed to be\nconvex and numerically tight. We show that this is on par or improves upon\nknown CSs for various GLMs, including Gaussian, Bernoulli, and Poisson. In\nparticular, for the first time, our CS for Bernoulli has a\n$\\mathrm{poly}(S)$-free radius where $S$ is the norm of the unknown parameter.\nOur first technical novelty is its derivation, which utilizes a time-uniform\nPAC-Bayesian bound with a uniform prior/posterior, despite the latter being a\nrather unpopular choice for deriving CSs. As a direct application of our new\nCS, we propose a simple and natural optimistic algorithm called OFUGLB,\napplicable to any generalized linear bandits (GLB; Filippi et al. (2010)). Our\nanalysis shows that the celebrated optimistic approach simultaneously attains\nstate-of-the-art regrets for various self-concordant (not necessarily bounded)\nGLBs, and even $\\mathrm{poly}(S)$-free for bounded GLBs, including logistic\nbandits. The regret analysis, our second technical novelty, follows from\ncombining our new CS with a new proof technique that completely avoids the\npreviously widely used self-concordant control lemma (Faury et al., 2020, Lemma\n9). Numerically, OFUGLB outperforms or is at par with prior algorithms for\nlogistic bandits.\n","authors":["Junghyun Lee","Se-Young Yun","Kwang-Sung Jun"],"pdf_url":"https://arxiv.org/pdf/2407.13977v3.pdf","comment":"39 pages, 2 figures, 2 tables; Accepted to the 38th Conference on\n  Neural Information Processing Systems (NeurIPS 2024) (ver3: minor revisions,\n  code refactoring; ver2: major revision, including new experiments,\n  reorganization, fixing typos in the proofs of ver1, etc)"},{"id":"http://arxiv.org/abs/2401.15299v3","updated":"2025-01-15T09:23:55Z","published":"2024-01-27T05:14:17Z","title":"SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph\n  Neural Networks","summary":"  Graph Neural Networks (GNNs) have gained traction across different domains\nsuch as transportation, bio-informatics, language processing, and computer\nvision. However, there is a noticeable absence of research on applying GNNs to\nsupply chain networks. Supply chain networks are inherently graph-like in\nstructure, making them prime candidates for applying GNN methodologies. This\nopens up a world of possibilities for optimizing, predicting, and solving even\nthe most complex supply chain problems. A major setback in this approach lies\nin the absence of real-world benchmark datasets to facilitate the research and\nresolution of supply chain problems using GNNs. To address the issue, we\npresent a real-world benchmark dataset for temporal tasks, obtained from one of\nthe leading FMCG companies in Bangladesh, focusing on supply chain planning for\nproduction purposes. The dataset includes temporal data as node features to\nenable sales predictions, production planning, and the identification of\nfactory issues. By utilizing this dataset, researchers can employ GNNs to\naddress numerous supply chain problems, thereby advancing the field of supply\nchain analytics and planning. Source: https://github.com/CIOL-SUST/SupplyGraph\n","authors":["Azmine Toushik Wasi","MD Shafikul Islam","Adipto Raihan Akib"],"pdf_url":"https://arxiv.org/pdf/2401.15299v3.pdf","comment":"Accepted to 4th workshop on Graphs and more Complex structures for\n  Learning and Reasoning, colocated with AAAI 2024"},{"id":"http://arxiv.org/abs/2501.08679v1","updated":"2025-01-15T09:20:02Z","published":"2025-01-15T09:20:02Z","title":"Diagonal Over-parameterization in Reproducing Kernel Hilbert Spaces as\n  an Adaptive Feature Model: Generalization and Adaptivity","summary":"  This paper introduces a diagonal adaptive kernel model that dynamically\nlearns kernel eigenvalues and output coefficients simultaneously during\ntraining. Unlike fixed-kernel methods tied to the neural tangent kernel theory,\nthe diagonal adaptive kernel model adapts to the structure of the truth\nfunction, significantly improving generalization over fixed-kernel methods,\nespecially when the initial kernel is misaligned with the target. Moreover, we\nshow that the adaptivity comes from learning the right eigenvalues during\ntraining, showing a feature learning behavior. By extending to deeper\nparameterization, we further show how extra depth enhances adaptability and\ngeneralization. This study combines the insights from feature learning and\nimplicit regularization and provides new perspective into the adaptivity and\ngeneralization potential of neural networks beyond the kernel regime.\n","authors":["Yicheng Li","Qian Lin"],"pdf_url":"https://arxiv.org/pdf/2501.08679v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2409.00894"},{"id":"http://arxiv.org/abs/2410.10524v2","updated":"2025-01-15T09:17:01Z","published":"2024-10-14T14:04:36Z","title":"Get Rid of Isolation: A Continuous Multi-task Spatio-Temporal Learning\n  Framework","summary":"  Spatiotemporal learning has become a pivotal technique to enable urban\nintelligence. Traditional spatiotemporal models mostly focus on a specific task\nby assuming a same distribution between training and testing sets. However,\ngiven that urban systems are usually dynamic, multi-sourced with imbalanced\ndata distributions, current specific task-specific models fail to generalize to\nnew urban conditions and adapt to new domains without explicitly modeling\ninterdependencies across various dimensions and types of urban data. To this\nend, we argue that there is an essential to propose a Continuous Multi-task\nSpatio-Temporal learning framework (CMuST) to empower collective urban\nintelligence, which reforms the urban spatiotemporal learning from\nsingle-domain to cooperatively multi-dimensional and multi-task learning.\nSpecifically, CMuST proposes a new multi-dimensional spatiotemporal interaction\nnetwork (MSTI) to allow cross-interactions between context and main\nobservations as well as self-interactions within spatial and temporal aspects\nto be exposed, which is also the core for capturing task-level commonality and\npersonalization. To ensure continuous task learning, a novel Rolling Adaptation\ntraining scheme (RoAda) is devised, which not only preserves task uniqueness by\nconstructing data summarization-driven task prompts, but also harnesses\ncorrelated patterns among tasks by iterative model behavior modeling. We\nfurther establish a benchmark of three cities for multi-task spatiotemporal\nlearning, and empirically demonstrate the superiority of CMuST via extensive\nevaluations on these datasets. The impressive improvements on both few-shot\nstreaming data and new domain tasks against existing SOAT methods are achieved.\nCode is available at https://github.com/DILab-USTCSZ/CMuST.\n","authors":["Zhongchao Yi","Zhengyang Zhou","Qihe Huang","Yanjiang Chen","Liheng Yu","Xu Wang","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.10524v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2501.08678v1","updated":"2025-01-15T09:08:05Z","published":"2025-01-15T09:08:05Z","title":"Investigating Parameter-Efficiency of Hybrid QuGANs Based on Geometric\n  Properties of Generated Sea Route Graphs","summary":"  The demand for artificially generated data for the development, training and\ntesting of new algorithms is omnipresent. Quantum computing (QC), does offer\nthe hope that its inherent probabilistic functionality can be utilised in this\nfield of generative artificial intelligence. In this study, we use\nquantum-classical hybrid generative adversarial networks (QuGANs) to\nartificially generate graphs of shipping routes. We create a training dataset\nbased on real shipping data and investigate to what extent QuGANs are able to\nlearn and reproduce inherent distributions and geometric features of this data.\nWe compare hybrid QuGANs with classical Generative Adversarial Networks (GANs),\nwith a special focus on their parameter efficiency. Our results indicate that\nQuGANs are indeed able to quickly learn and represent underlying geometric\nproperties and distributions, although they seem to have difficulties in\nintroducing variance into the sampled data. Compared to classical GANs of\ngreater size, measured in the number of parameters used, some QuGANs show\nsimilar result quality. Our reference to concrete use cases, such as the\ngeneration of shipping data, provides an illustrative example and demonstrate\nthe potential and diversity in which QC can be used.\n","authors":["Tobias Rohe","Florian Burger","Michael Kölle","Sebastian Wölckert","Maximilian Zorn","Claudia Linnhoff-Popien"],"pdf_url":"https://arxiv.org/pdf/2501.08678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08669v1","updated":"2025-01-15T09:04:19Z","published":"2025-01-15T09:04:19Z","title":"SPEQ: Stabilization Phases for Efficient Q-Learning in High\n  Update-To-Data Ratio Reinforcement Learning","summary":"  A key challenge in Deep Reinforcement Learning is sample efficiency,\nespecially in real-world applications where collecting environment interactions\nis expensive or risky. Recent off-policy algorithms improve sample efficiency\nby increasing the Update-To-Data (UTD) ratio and performing more gradient\nupdates per environment interaction. While this improves sample efficiency, it\nsignificantly increases computational cost due to the higher number of gradient\nupdates required. In this paper we propose a sample-efficient method to improve\ncomputational efficiency by separating training into distinct learning phases\nin order to exploit gradient updates more effectively. Our approach builds on\ntop of the Dropout Q-Functions (DroQ) algorithm and alternates between an\nonline, low UTD ratio training phase, and an offline stabilization phase.\nDuring the stabilization phase, we fine-tune the Q-functions without collecting\nnew environment interactions. This process improves the effectiveness of the\nreplay buffer and reduces computational overhead. Our experimental results on\ncontinuous control problems show that our method achieves results comparable to\nstate-of-the-art, high UTD ratio algorithms while requiring 56\\% fewer gradient\nupdates and 50\\% less training time than DroQ. Our approach offers an effective\nand computationally economical solution while maintaining the same sample\nefficiency as the more costly, high UTD ratio state-of-the-art.\n","authors":["Carlo Romeo","Girolamo Macaluso","Alessandro Sestini","Andrew D. Bagdanov"],"pdf_url":"https://arxiv.org/pdf/2501.08669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04671v2","updated":"2025-01-15T09:01:09Z","published":"2024-12-05T23:47:58Z","title":"Fully Distributed, Flexible Compositional Visual Representations via\n  Soft Tensor Products","summary":"  Since the inception of the classicalist vs. connectionist debate, it has been\nargued that the ability to systematically combine symbol-like entities into\ncompositional representations is crucial for human intelligence. In\nconnectionist systems, the field of disentanglement has gained prominence for\nits ability to produce explicitly compositional representations; however, it\nrelies on a fundamentally symbolic, concatenative representation of\ncompositional structure that clashes with the continuous, distributed\nfoundations of deep learning. To resolve this tension, we extend Smolensky's\nTensor Product Representation (TPR) and introduce Soft TPR, a representational\nform that encodes compositional structure in an inherently distributed,\nflexible manner, along with Soft TPR Autoencoder, a theoretically-principled\narchitecture designed specifically to learn Soft TPRs. Comprehensive\nevaluations in the visual representation learning domain demonstrate that the\nSoft TPR framework consistently outperforms conventional disentanglement\nalternatives -- achieving state-of-the-art disentanglement, boosting\nrepresentation learner convergence, and delivering superior sample efficiency\nand low-sample regime performance in downstream tasks. These findings highlight\nthe promise of a distributed and flexible approach to representing\ncompositional structure by potentially enhancing alignment with the core\nprinciples of deep learning over the conventional symbolic approach.\n","authors":["Bethia Sun","Maurice Pagnucco","Yang Song"],"pdf_url":"https://arxiv.org/pdf/2412.04671v2.pdf","comment":"Accepted to Neurips 2024. 10 pages + supplementary"},{"id":"http://arxiv.org/abs/2501.08662v1","updated":"2025-01-15T08:57:41Z","published":"2025-01-15T08:57:41Z","title":"Product of Gaussian Mixture Diffusion Model for non-linear MRI Inversion","summary":"  Diffusion models have recently shown remarkable results in magnetic resonance\nimaging reconstruction. However, the employed networks typically are black-box\nestimators of the (smoothed) prior score with tens of millions of parameters,\nrestricting interpretability and increasing reconstruction time. Furthermore,\nparallel imaging reconstruction algorithms either rely on off-line coil\nsensitivity estimation, which is prone to misalignment and restricting sampling\ntrajectories, or perform per-coil reconstruction, making the computational cost\nproportional to the number of coils. To overcome this, we jointly reconstruct\nthe image and the coil sensitivities using the lightweight,\nparameter-efficient, and interpretable product of Gaussian mixture diffusion\nmodel as an image prior and a classical smoothness priors on the coil\nsensitivities. The proposed method delivers promising results while allowing\nfor fast inference and demonstrating robustness to contrast out-of-distribution\ndata and sampling trajectories, comparable to classical variational penalties\nsuch as total variation. Finally, the probabilistic formulation allows the\ncalculation of the posterior expectation and pixel-wise variance.\n","authors":["Laurenz Nagler","Martin Zach","Thomas Pock"],"pdf_url":"https://arxiv.org/pdf/2501.08662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08653v1","updated":"2025-01-15T08:38:07Z","published":"2025-01-15T08:38:07Z","title":"Fine-grained Spatio-temporal Event Prediction with Self-adaptive Anchor\n  Graph","summary":"  Event prediction tasks often handle spatio-temporal data distributed in a\nlarge spatial area. Different regions in the area exhibit different\ncharacteristics while having latent correlations. This spatial heterogeneity\nand correlations greatly affect the spatio-temporal distributions of event\noccurrences, which has not been addressed by state-of-the-art models. Learning\nspatial dependencies of events in a continuous space is challenging due to its\nfine granularity and a lack of prior knowledge. In this work, we propose a\nnovel Graph Spatio-Temporal Point Process (GSTPP) model for fine-grained event\nprediction. It adopts an encoder-decoder architecture that jointly models the\nstate dynamics of spatially localized regions using neural Ordinary\nDifferential Equations (ODEs). The state evolution is built on the foundation\nof a novel Self-Adaptive Anchor Graph (SAAG) that captures spatial\ndependencies. By adaptively localizing the anchor nodes in the space and\njointly constructing the correlation edges between them, the SAAG enhances the\nmodel's ability of learning complex spatial event patterns. The proposed GSTPP\nmodel greatly improves the accuracy of fine-grained event prediction. Extensive\nexperimental results show that our method greatly improves the prediction\naccuracy over existing spatio-temporal event prediction approaches.\n","authors":["Wang-Tao Zhou","Zhao Kang","Sicong Liu","Lizong Zhang","Ling Tian"],"pdf_url":"https://arxiv.org/pdf/2501.08653v1.pdf","comment":"Accepted to SIAM International Conference on Data Mining 2025\n  (SDM'25)"},{"id":"http://arxiv.org/abs/2501.08649v1","updated":"2025-01-15T08:24:35Z","published":"2025-01-15T08:24:35Z","title":"Joint Learning of Depth and Appearance for Portrait Image Animation","summary":"  2D portrait animation has experienced significant advancements in recent\nyears. Much research has utilized the prior knowledge embedded in large\ngenerative diffusion models to enhance high-quality image manipulation.\nHowever, most methods only focus on generating RGB images as output, and the\nco-generation of consistent visual plus 3D output remains largely\nunder-explored. In our work, we propose to jointly learn the visual appearance\nand depth simultaneously in a diffusion-based portrait image generator. Our\nmethod embraces the end-to-end diffusion paradigm and introduces a new\narchitecture suitable for learning this conditional joint distribution,\nconsisting of a reference network and a channel-expanded diffusion backbone.\nOnce trained, our framework can be efficiently adapted to various downstream\napplications, such as facial depth-to-image and image-to-depth generation,\nportrait relighting, and audio-driven talking head animation with consistent 3D\noutput.\n","authors":["Xinya Ji","Gaspard Zoss","Prashanth Chandran","Lingchen Yang","Xun Cao","Barbara Solenthaler","Derek Bradley"],"pdf_url":"https://arxiv.org/pdf/2501.08649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16705v2","updated":"2025-01-15T08:20:19Z","published":"2024-02-26T16:21:53Z","title":"SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware\n  Self-Reflection","summary":"  Instruction tuning (IT) is crucial to tailoring large language models (LLMs)\ntowards human-centric interactions. Recent advancements have shown that the\ncareful selection of a small, high-quality subset of IT data can significantly\nenhance the performance of LLMs. Despite this, common approaches often rely on\nadditional models or data, which increases costs and limits widespread\nadoption. In this work, we propose a novel approach, termed SelectIT, that\ncapitalizes on the foundational capabilities of the LLM itself. Specifically,\nwe exploit the intrinsic uncertainty present in LLMs to more effectively select\nhigh-quality IT data, without the need for extra resources. Furthermore, we\nintroduce a curated IT dataset, the Selective Alpaca, created by applying\nSelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT\nusing Selective Alpaca leads to substantial model ability enhancement. The\nrobustness of SelectIT has also been corroborated in various foundation models\nand domain-specific tasks. Our findings suggest that longer and more\ncomputationally intensive IT data may serve as superior sources of IT, offering\nvaluable insights for future research in this area. Data, code, and scripts are\nfreely available at https://github.com/Blue-Raincoat/SelectIT.\n","authors":["Liangxin Liu","Xuebo Liu","Derek F. Wong","Dongfang Li","Ziyi Wang","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.16705v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.19212v5","updated":"2025-01-15T08:18:27Z","published":"2024-09-28T02:30:44Z","title":"An Accelerated Algorithm for Stochastic Bilevel Optimization under\n  Unbounded Smoothness","summary":"  This paper investigates a class of stochastic bilevel optimization problems\nwhere the upper-level function is nonconvex with potentially unbounded\nsmoothness and the lower-level problem is strongly convex. These problems have\nsignificant applications in sequential data learning, such as text\nclassification using recurrent neural networks. The unbounded smoothness is\ncharacterized by the smoothness constant of the upper-level function scaling\nlinearly with the gradient norm, lacking a uniform upper bound. Existing\nstate-of-the-art algorithms require $\\widetilde{O}(1/\\epsilon^4)$ oracle calls\nof stochastic gradient or Hessian/Jacobian-vector product to find an\n$\\epsilon$-stationary point. However, it remains unclear if we can further\nimprove the convergence rate when the assumptions for the function in the\npopulation level also hold for each random realization almost surely. To\naddress this issue, we propose a new Accelerated Bilevel Optimization algorithm\nnamed AccBO. The algorithm updates the upper-level variable by normalized\nstochastic gradient descent with recursive momentum and the lower-level\nvariable by the stochastic Nesterov accelerated gradient descent algorithm with\naveraging. We prove that our algorithm achieves an oracle complexity of\n$\\widetilde{O}(1/\\epsilon^3)$ to find an $\\epsilon$-stationary point, when the\nlower-level stochastic gradient's variance is $O(\\epsilon)$. Our proof relies\non a novel lemma characterizing the dynamics of stochastic Nesterov accelerated\ngradient descent algorithm under distribution drift with high probability for\nthe lower-level variable, which is of independent interest and also plays a\ncrucial role in analyzing the hypergradient estimation error over time.\nExperimental results on various tasks confirm that our proposed algorithm\nachieves the predicted theoretical acceleration and significantly outperforms\nbaselines in bilevel optimization.\n","authors":["Xiaochuan Gong","Jie Hao","Mingrui Liu"],"pdf_url":"https://arxiv.org/pdf/2409.19212v5.pdf","comment":"Accepted by NeurIPS 2024. The code is available at\n  https://github.com/MingruiLiu-ML-Lab/Accelerated-Bilevel-Optimization-Unbounded-Smoothness"},{"id":"http://arxiv.org/abs/2304.03271v4","updated":"2025-01-15T08:07:44Z","published":"2023-04-06T17:55:27Z","title":"Making AI Less \"Thirsty\": Uncovering and Addressing the Secret Water\n  Footprint of AI Models","summary":"  The growing carbon footprint of artificial intelligence (AI) has been\nundergoing public scrutiny. Nonetheless, the equally important water\n(withdrawal and consumption) footprint of AI has largely remained under the\nradar. For example, training the GPT-3 language model in Microsoft's\nstate-of-the-art U.S. data centers can directly evaporate 700,000 liters of\nclean freshwater, but such information has been kept a secret. More critically,\nthe global AI demand is projected to account for 4.2-6.6 billion cubic meters\nof water withdrawal in 2027, which is more than the total annual water\nwithdrawal of 4-6 Denmark or half of the United Kingdom. This is concerning, as\nfreshwater scarcity has become one of the most pressing challenges. To respond\nto the global water challenges, AI can, and also must, take social\nresponsibility and lead by example by addressing its own water footprint. In\nthis paper, we provide a principled methodology to estimate the water footprint\nof AI, and also discuss the unique spatial-temporal diversities of AI's runtime\nwater efficiency. Finally, we highlight the necessity of holistically\naddressing water footprint along with carbon footprint to enable truly\nsustainable AI.\n","authors":["Pengfei Li","Jianyi Yang","Mohammad A. Islam","Shaolei Ren"],"pdf_url":"https://arxiv.org/pdf/2304.03271v4.pdf","comment":"Accepted by Communications of the ACM. Source codes available at:\n  https://github.com/Ren-Research/Making-AI-Less-Thirsty"},{"id":"http://arxiv.org/abs/2501.08640v1","updated":"2025-01-15T08:06:03Z","published":"2025-01-15T08:06:03Z","title":"Quantum Reservoir Computing and Risk Bounds","summary":"  We propose a way to bound the generalisation errors of several classes of\nquantum reservoirs using the Rademacher complexity. We give specific,\nparameter-dependent bounds for two particular quantum reservoir classes. We\nanalyse how the generalisation bounds scale with growing numbers of qubits.\nApplying our results to classes with polynomial readout functions, we find that\nthe risk bounds converge in the number of training samples. The explicit\ndependence on the quantum reservoir and readout parameters in our bounds can be\nused to control the generalisation error to a certain extent. It should be\nnoted that the bounds scale exponentially with the number of qubits $n$. The\nupper bounds on the Rademacher complexity can be applied to other reservoir\nclasses that fulfill a few hypotheses on the quantum dynamics and the readout\nfunction.\n","authors":["Naomi Mona Chmielewski","Nina Amini","Joseph Mikael"],"pdf_url":"https://arxiv.org/pdf/2501.08640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12117v3","updated":"2025-01-15T08:03:55Z","published":"2024-07-16T18:59:49Z","title":"MEMO: Fine-grained Tensor Management For Ultra-long Context LLM Training","summary":"  Nowadays, Large Language Models (LLMs) have been trained using extended\ncontext lengths to foster more creative applications. However, long context\ntraining poses great challenges considering the constraint of GPU memory. It\nnot only leads to substantial activation memory consumption during training,\nbut also incurs considerable memory fragmentation. To facilitate long context\ntraining, existing frameworks have adopted strategies such as recomputation and\nvarious forms of parallelisms. Nevertheless, these techniques rely on redundant\ncomputation or extensive communication, resulting in low Model FLOPS\nUtilization (MFU). In this paper, we propose MEMO, a novel LLM training\nframework designed for fine-grained activation memory management. Given the\nquadratic scaling of computation and linear scaling of memory with sequence\nlengths when using FlashAttention, we offload memory-consuming activations to\nCPU memory after each layer's forward pass and fetch them during the backward\npass. To maximize the swapping of activations without hindering computation,\nand to avoid exhausting limited CPU memory, we implement a token-wise\nactivation recomputation and swapping mechanism. Furthermore, we tackle the\nmemory fragmentation issue by employing a bi-level Mixed Integer Programming\n(MIP) approach, optimizing memory reuse across transformer layers. Empirical\nresults demonstrate that MEMO achieves an average of 1.97x and 1.80x MFU\ncompared to Megatron-LM and DeepSpeed, respectively. This improvement is\nattributed to MEMO's ability to minimize memory fragmentation, reduce\nrecomputation and intensive communication, and circumvent the delays associated\nwith the memory reorganization process due to fragmentation. By leveraging\nfine-grained activation memory management, MEMO facilitates efficient training\nof 7B LLM with 1 million sequence length on just 8 A800 GPUs, achieving an MFU\nof 52.30%.\n","authors":["Pinxue Zhao","Hailin Zhang","Fangcheng Fu","Xiaonan Nie","Qibin Liu","Fang Yang","Yuanbo Peng","Dian Jiao","Shuaipeng Li","Jinbao Xue","Yangyu Tao","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2407.12117v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04239v3","updated":"2025-01-15T07:59:39Z","published":"2025-01-08T02:32:48Z","title":"Dynamic Localisation of Spatial-Temporal Graph Neural Network","summary":"  Spatial-temporal data, fundamental to many intelligent applications, reveals\ndependencies indicating causal links between present measurements at specific\nlocations and historical data at the same or other locations. Within this\ncontext, adaptive spatial-temporal graph neural networks (ASTGNNs) have emerged\nas valuable tools for modelling these dependencies, especially through a\ndata-driven approach rather than pre-defined spatial graphs. While this\napproach offers higher accuracy, it presents increased computational demands.\nAddressing this challenge, this paper delves into the concept of localisation\nwithin ASTGNNs, introducing an innovative perspective that spatial dependencies\nshould be dynamically evolving over time. We introduce \\textit{DynAGS}, a\nlocalised ASTGNN framework aimed at maximising efficiency and accuracy in\ndistributed deployment. This framework integrates dynamic localisation,\ntime-evolving spatial graphs, and personalised localisation, all orchestrated\naround the Dynamic Graph Generator, a light-weighted central module leveraging\ncross attention. The central module can integrate historical information in a\nnode-independent manner to enhance the feature representation of nodes at the\ncurrent moment. This improved feature representation is then used to generate a\ndynamic sparse graph without the need for costly data exchanges, and it\nsupports personalised localisation. Performance assessments across two core\nASTGNN architectures and nine real-world datasets from various applications\nreveal that \\textit{DynAGS} outshines current benchmarks, underscoring that the\ndynamic modelling of spatial dependencies can drastically improve model\nexpressibility, flexibility, and system efficiency, especially in distributed\nsettings.\n","authors":["Wenying Duan","Shujun Guo","Wei huang","Hong Rao","Xiaoxi He"],"pdf_url":"https://arxiv.org/pdf/2501.04239v3.pdf","comment":"This paper was accepted by KDD'25"},{"id":"http://arxiv.org/abs/2501.08631v1","updated":"2025-01-15T07:36:19Z","published":"2025-01-15T07:36:19Z","title":"SWSC: Shared Weight for Similar Channel in LLM","summary":"  Large language models (LLMs) have spurred development in multiple industries.\nHowever, the growing number of their parameters brings substantial storage and\ncomputing burdens, making it essential to explore model compression techniques\nfor parameter reduction and easier deployment. We propose SWSC, an LLM\ncompression method based on the concept of Shared Weight for Similar Channel.\nIt uses the K-Means clustering algorithm to cluster model weights\nchannel-by-channel, generating clusters with highly similar vectors within\neach. A representative vector from each cluster is selected to approximately\nreplace all vectors in the cluster, significantly reducing the number of model\nweight parameters. However, approximate restoration will inevitably cause\ndamage to the performance of the model. To tackle this issue, we perform\nsingular value decomposition on the weight error values before and after\ncompression and retain the larger singular values and their corresponding\nsingular vectors to compensate for the accuracy. The experimental results show\nthat our method can effectively ensure the performance of the compressed LLM\neven under low-precision conditions.\n","authors":["Binrui Zeng","Yongtao Tang","Xiaodong Liu","Xiaopeng Li"],"pdf_url":"https://arxiv.org/pdf/2501.08631v1.pdf","comment":"5pages, 3 figures, work in progress"},{"id":"http://arxiv.org/abs/2411.15098v4","updated":"2025-01-15T07:30:29Z","published":"2024-11-22T17:55:15Z","title":"OminiControl: Minimal and Universal Control for Diffusion Transformer","summary":"  In this paper, we introduce OminiControl, a highly versatile and\nparameter-efficient framework that integrates image conditions into pre-trained\nDiffusion Transformer (DiT) models. At its core, OminiControl leverages a\nparameter reuse mechanism, enabling the DiT to encode image conditions using\nitself as a powerful backbone and process them with its flexible multi-modal\nattention processors. Unlike existing methods, which rely heavily on additional\nencoder modules with complex architectures, OminiControl (1) effectively and\nefficiently incorporates injected image conditions with only ~0.1% additional\nparameters, and (2) addresses a wide range of image conditioning tasks in a\nunified manner, including subject-driven generation and spatially-aligned\nconditions such as edges, depth, and more. Remarkably, these capabilities are\nachieved by training on images generated by the DiT itself, which is\nparticularly beneficial for subject-driven generation. Extensive evaluations\ndemonstrate that OminiControl outperforms existing UNet-based and DiT-adapted\nmodels in both subject-driven and spatially-aligned conditional generation.\nAdditionally, we release our training dataset, Subjects200K, a diverse\ncollection of over 200,000 identity-consistent images, along with an efficient\ndata synthesis pipeline to advance research in subject-consistent generation.\n","authors":["Zhenxiong Tan","Songhua Liu","Xingyi Yang","Qiaochu Xue","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15098v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08628v1","updated":"2025-01-15T07:18:51Z","published":"2025-01-15T07:18:51Z","title":"Transformer-based Multivariate Time Series Anomaly Localization","summary":"  With the growing complexity of Cyber-Physical Systems (CPS) and the\nintegration of Internet of Things (IoT), the use of sensors for online\nmonitoring generates large volume of multivariate time series (MTS) data.\nConsequently, the need for robust anomaly diagnosis in MTS is paramount to\nmaintaining system reliability and safety. While significant advancements have\nbeen made in anomaly detection, localization remains a largely underexplored\narea, though crucial for intelligent decision-making. This paper introduces a\nnovel transformer-based model for unsupervised anomaly diagnosis in MTS, with a\nfocus on improving localization performance, through an in-depth analysis of\nthe self-attention mechanism's learning behavior under both normal and\nanomalous conditions. We formulate the anomaly localization problem as a\nthree-stage process: time-step, window, and segment-based. This leads to the\ndevelopment of the Space-Time Anomaly Score (STAS), a new metric inspired by\nthe connection between transformer latent representations and space-time\nstatistical models. STAS is designed to capture individual anomaly behaviors\nand inter-series dependencies, delivering enhanced localization performance.\nAdditionally, the Statistical Feature Anomaly Score (SFAS) complements STAS by\nanalyzing statistical features around anomalies, with their combination helping\nto reduce false alarms. Experiments on real world and synthetic datasets\nillustrate the model's superiority over state-of-the-art methods in both\ndetection and localization tasks.\n","authors":["Charalampos Shimillas","Kleanthis Malialis","Konstantinos Fokianos","Marios M. Polycarpou"],"pdf_url":"https://arxiv.org/pdf/2501.08628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00453v4","updated":"2025-01-15T07:18:43Z","published":"2024-11-01T09:05:47Z","title":"Diffusion Models as Network Optimizers: Explorations and Analysis","summary":"  Network optimization is a fundamental challenge in the Internet of Things\n(IoT) network, often characterized by complex features that make it difficult\nto solve these problems. Recently, generative diffusion models (GDMs) have\nemerged as a promising new approach to network optimization, with the potential\nto directly address these optimization problems. However, the application of\nGDMs in this field is still in its early stages, and there is a noticeable lack\nof theoretical research and empirical findings. In this study, we first explore\nthe intrinsic characteristics of generative models. Next, we provide a concise\ntheoretical proof and intuitive demonstration of the advantages of generative\nmodels over discriminative models in network optimization. Based on this\nexploration, we implement GDMs as optimizers aimed at learning high-quality\nsolution distributions for given inputs, sampling from these distributions\nduring inference to approximate or achieve optimal solutions. Specifically, we\nutilize denoising diffusion probabilistic models (DDPMs) and employ a\nclassifier-free guidance mechanism to manage conditional guidance based on\ninput parameters. We conduct extensive experiments across three challenging\nnetwork optimization problems. By investigating various model configurations\nand the principles of GDMs as optimizers, we demonstrate the ability to\novercome prediction errors and validate the convergence of generated solutions\nto optimal solutions. We provide code and data at\nhttps://github.com/qiyu3816/DiffSG.\n","authors":["Ruihuai Liang","Bo Yang","Pengyu Chen","Xianjin Li","Yifan Xue","Zhiwen Yu","Xuelin Cao","Yan Zhang","Mérouane Debbah","H. Vincent Poor","Chau Yuen"],"pdf_url":"https://arxiv.org/pdf/2411.00453v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10919v3","updated":"2025-01-15T07:17:58Z","published":"2024-08-20T15:04:14Z","title":"CrossFi: A Cross Domain Wi-Fi Sensing Framework Based on Siamese Network","summary":"  In recent years, Wi-Fi sensing has garnered significant attention due to its\nnumerous benefits, such as privacy protection, low cost, and penetration\nability. Extensive research has been conducted in this field, focusing on areas\nsuch as gesture recognition, people identification, and fall detection.\nHowever, many data-driven methods encounter challenges related to domain shift,\nwhere the model fails to perform well in environments different from the\ntraining data. One major factor contributing to this issue is the limited\navailability of Wi-Fi sensing datasets, which makes models learn excessive\nirrelevant information and over-fit to the training set. Unfortunately,\ncollecting large-scale Wi-Fi sensing datasets across diverse scenarios is a\nchallenging task. To address this problem, we propose CrossFi, a siamese\nnetwork-based approach that excels in both in-domain scenario and cross-domain\nscenario, including few-shot, zero-shot scenarios, and even works in few-shot\nnew-class scenario where testing set contains new categories. The core\ncomponent of CrossFi is a sample-similarity calculation network called CSi-Net,\nwhich improves the structure of the siamese network by using an attention\nmechanism to capture similarity information, instead of simply calculating the\ndistance or cosine similarity. Based on it, we develop an extra Weight-Net that\ncan generate a template for each class, so that our CrossFi can work in\ndifferent scenarios. Experimental results demonstrate that our CrossFi achieves\nstate-of-the-art performance across various scenarios. In gesture recognition\ntask, our CrossFi achieves an accuracy of 98.17% in in-domain scenario, 91.72%\nin one-shot cross-domain scenario, 64.81% in zero-shot cross-domain scenario,\nand 84.75% in one-shot new-class scenario. The code for our model is publicly\navailable at https://github.com/RS2002/CrossFi.\n","authors":["Zijian Zhao","Tingwei Chen","Zhijie Cai","Xiaoyang Li","Hang Li","Qimei Chen","Guangxu Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.10919v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08626v1","updated":"2025-01-15T07:07:48Z","published":"2025-01-15T07:07:48Z","title":"A Learning Algorithm That Attains the Human Optimum in a Repeated\n  Human-Machine Interaction Game","summary":"  When humans interact with learning-based control systems, a common goal is to\nminimize a cost function known only to the human. For instance, an exoskeleton\nmay adapt its assistance in an effort to minimize the human's metabolic\ncost-of-transport. Conventional approaches to synthesizing the learning\nalgorithm solve an inverse problem to infer the human's cost. However, these\nproblems can be ill-posed, hard to solve, or sensitive to problem data. Here we\nshow a game-theoretic learning algorithm that works solely by observing human\nactions to find the cost minimum, avoiding the need to solve an inverse\nproblem. We evaluate the performance of our algorithm in an extensive set of\nhuman subjects experiments, demonstrating consistent convergence to the minimum\nof a prescribed human cost function in scalar and multidimensional\ninstantiations of the game. We conclude by outlining future directions for\ntheoretical and empirical extensions of our results.\n","authors":["Jason T. Isa","Lillian J. Ratliff","Samuel A. Burden"],"pdf_url":"https://arxiv.org/pdf/2501.08626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00961v2","updated":"2025-01-15T06:46:51Z","published":"2025-01-01T21:45:00Z","title":"The Silent Majority: Demystifying Memorization Effect in the Presence of\n  Spurious Correlations","summary":"  Machine learning models often rely on simple spurious features -- patterns in\ntraining data that correlate with targets but are not causally related to them,\nlike image backgrounds in foreground classification. This reliance typically\nleads to imbalanced test performance across minority and majority groups. In\nthis work, we take a closer look at the fundamental cause of such imbalanced\nperformance through the lens of memorization, which refers to the ability to\npredict accurately on \\textit{atypical} examples (minority groups) in the\ntraining set but failing in achieving the same accuracy in the testing set.\nThis paper systematically shows the ubiquitous existence of spurious features\nin a small set of neurons within the network, providing the first-ever evidence\nthat memorization may contribute to imbalanced group performance. Through three\nexperimental sources of converging empirical evidence, we find the property of\na small subset of neurons or channels in memorizing minority group information.\nInspired by these findings, we articulate the hypothesis: the imbalanced group\nperformance is a byproduct of ``noisy'' spurious memorization confined to a\nsmall set of neurons. To further substantiate this hypothesis, we show that\neliminating these unnecessary spurious memorization patterns via a novel\nframework during training can significantly affect the model performance on\nminority groups. Our experimental results across various architectures and\nbenchmarks offer new insights on how neural networks encode core and spurious\nknowledge, laying the groundwork for future research in demystifying robustness\nto spurious correlation.\n","authors":["Chenyu You","Haocheng Dai","Yifei Min","Jasjeet S. Sekhon","Sarang Joshi","James S. Duncan"],"pdf_url":"https://arxiv.org/pdf/2501.00961v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08620v1","updated":"2025-01-15T06:35:39Z","published":"2025-01-15T06:35:39Z","title":"CT-PatchTST: Channel-Time Patch Time-Series Transformer for Long-Term\n  Renewable Energy Forecasting","summary":"  Accurately predicting renewable energy output is crucial for the efficient\nintegration of solar and wind power into modern energy systems. This study\ndevelops and evaluates an advanced deep learning model, Channel-Time Patch\nTime-Series Transformer (CT-PatchTST), to forecast the power output of\nphotovoltaic and wind energy systems using annual offshore wind power, onshore\nwind power, and solar power generation data from Denmark. While the original\nPatch Time-Series Transformer(PatchTST) model employs a channel-independent\n(CI) approach, it tends to overlook inter-channel relationships during\ntraining, potentially leading to a loss of critical information. To address\nthis limitation and further leverage the benefits of increased data granularity\nbrought by CI, we propose CT-PatchTST. This enhanced model improves the\nprocessing of inter-channel information while maintaining the advantages of the\nchannel-independent approach. The predictive performance of CT-PatchTST is\nrigorously analyzed, demonstrating its ability to provide precise and reliable\nenergy forecasts. This work contributes to improving the predictability of\nrenewable energy systems, supporting their broader adoption and integration\ninto energy grids.\n","authors":["Menghao Huo","Kuan Lu","Yuxiao Li","Qiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2501.08620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08617v1","updated":"2025-01-15T06:33:15Z","published":"2025-01-15T06:33:15Z","title":"RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation","summary":"  Generative AI systems like foundation models (FMs) must align well with human\nvalues to ensure their behavior is helpful and trustworthy. While Reinforcement\nLearning from Human Feedback (RLHF) has shown promise for optimizing model\nperformance using human judgments, existing RLHF pipelines predominantly rely\non immediate feedback, which can fail to accurately reflect the downstream\nimpact of an interaction on users' utility. We demonstrate that feedback based\non evaluators' foresight estimates of downstream consequences systematically\ninduces Goodhart's Law dynamics, incentivizing misaligned behaviors like\nsycophancy and deception and ultimately degrading user outcomes. To alleviate\nthis, we propose decoupling evaluation from prediction by refocusing RLHF on\nhindsight feedback. Our theoretical analysis reveals that conditioning\nevaluator feedback on downstream observations mitigates misalignment and\nimproves expected human utility, even when these observations are simulated by\nthe AI system itself. To leverage this insight in a practical alignment\nalgorithm, we introduce Reinforcement Learning from Hindsight Simulation\n(RLHS), which first simulates plausible consequences and then elicits feedback\nto assess what behaviors were genuinely beneficial in hindsight. We apply RLHS\nto two widely-employed online and offline preference optimization methods --\nProximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) --\nand show empirically that misalignment is significantly reduced with both\nmethods. Through an online human user study, we show that RLHS consistently\noutperforms RLHF in helping users achieve their goals and earns higher\nsatisfaction ratings, despite being trained solely with simulated hindsight\nfeedback. These results underscore the importance of focusing on long-term\nconsequences, even simulated ones, to mitigate misalignment in RLHF.\n","authors":["Kaiqu Liang","Haimin Hu","Ryan Liu","Thomas L. Griffiths","Jaime Fernández Fisac"],"pdf_url":"https://arxiv.org/pdf/2501.08617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08615v1","updated":"2025-01-15T06:30:26Z","published":"2025-01-15T06:30:26Z","title":"Towards Aligned Data Forgetting via Twin Machine Unlearning","summary":"  Modern privacy regulations have spurred the evolution of machine unlearning,\na technique enabling a trained model to efficiently forget specific training\ndata. In prior unlearning methods, the concept of \"data forgetting\" is often\ninterpreted and implemented as achieving zero classification accuracy on such\ndata. Nevertheless, the authentic aim of machine unlearning is to achieve\nalignment between the unlearned model and the gold model, i.e., encouraging\nthem to have identical classification accuracy. On the other hand, the gold\nmodel often exhibits non-zero classification accuracy due to its generalization\nability. To achieve aligned data forgetting, we propose a Twin Machine\nUnlearning (TMU) approach, where a twin unlearning problem is defined\ncorresponding to the original unlearning problem. Consequently, the\ngeneralization-label predictor trained on the twin problem can be transferred\nto the original problem, facilitating aligned data forgetting. Comprehensive\nempirical experiments illustrate that our approach significantly enhances the\nalignment between the unlearned model and the gold model.\n","authors":["Zhenxing Niu","Haoxuan Ji","Yuyao Sun","Zheng Lin","Fei Gao","Yuhang Wang","Haichao Gao"],"pdf_url":"https://arxiv.org/pdf/2501.08615v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2408.11433"},{"id":"http://arxiv.org/abs/2501.08612v1","updated":"2025-01-15T06:20:25Z","published":"2025-01-15T06:20:25Z","title":"Neural Risk-sensitive Satisficing in Contextual Bandits","summary":"  The contextual bandit problem, which is a type of reinforcement learning\ntasks, provides an effective framework for solving challenges in recommendation\nsystems, such as satisfying real-time requirements, enabling personalization,\naddressing cold-start problems. However, contextual bandit algorithms face\nchallenges since they need to handle large state-action spaces sequentially.\nThese challenges include the high costs for learning and balancing exploration\nand exploitation, as well as large variations in performance that depend on the\ndomain of application. To address these challenges, Tsuboya et~al. proposed the\nRegional Linear Risk-sensitive Satisficing (RegLinRS) algorithm. RegLinRS\nswitches between exploration and exploitation based on how well the agent has\nachieved the target. However, the reward expectations in RegLinRS are linearly\napproximated based on features, which limits its applicability when the\nrelationship between features and reward expectations is non-linear. To handle\nmore complex environments, we proposed Neural Risk-sensitive Satisficing\n(NeuralRS), which incorporates neural networks into RegLinRS, and demonstrated\nits utility.\n","authors":["Shogo Ito","Tatsuji Takahashi","Yu Kono"],"pdf_url":"https://arxiv.org/pdf/2501.08612v1.pdf","comment":"Accepted by AROB-ISBC 2025"},{"id":"http://arxiv.org/abs/2407.01960v2","updated":"2025-01-15T06:06:31Z","published":"2024-07-02T05:31:59Z","title":"Zero-shot Video Restoration and Enhancement Using Pre-Trained Image\n  Diffusion Model","summary":"  Diffusion-based zero-shot image restoration and enhancement models have\nachieved great success in various tasks of image restoration and enhancement.\nHowever, directly applying them to video restoration and enhancement results in\nsevere temporal flickering artifacts. In this paper, we propose the first\nframework for zero-shot video restoration and enhancement based on the\npre-trained image diffusion model. By replacing the spatial self-attention\nlayer with the proposed short-long-range (SLR) temporal attention layer, the\npre-trained image diffusion model can take advantage of the temporal\ncorrelation between frames. We further propose temporal consistency guidance,\nspatial-temporal noise sharing, and an early stopping sampling strategy to\nimprove temporally consistent sampling. Our method is a plug-and-play module\nthat can be inserted into any diffusion-based image restoration or enhancement\nmethods to further improve their performance. Experimental results demonstrate\nthe superiority of our proposed method. Our code is available at\nhttps://github.com/cao-cong/ZVRD.\n","authors":["Cong Cao","Huanjing Yue","Xin Liu","Jingyu Yang"],"pdf_url":"https://arxiv.org/pdf/2407.01960v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2401.04385v4","updated":"2025-01-15T06:00:17Z","published":"2024-01-09T07:14:45Z","title":"Machine unlearning through fine-grained model parameters perturbation","summary":"  Machine unlearning techniques, which involve retracting data records and\nreducing influence of said data on trained models, help with the user privacy\nprotection objective but incur significant computational costs. Weight\nperturbation-based unlearning is a general approach, but it typically involves\nglobally modifying the parameters. We propose fine-grained Top-K and Random-k\nparameters perturbed inexact machine unlearning strategies that address the\nprivacy needs while keeping the computational costs tractable.\n  In order to demonstrate the efficacy of our strategies we also tackle the\nchallenge of evaluating the effectiveness of machine unlearning by considering\nthe model's generalization performance across both unlearning and remaining\ndata. To better assess the unlearning effect and model generalization, we\npropose novel metrics, namely, the forgetting rate and memory retention rate.\nHowever, for inexact machine unlearning, current metrics are inadequate in\nquantifying the degree of forgetting that occurs after unlearning strategies\nare applied. To address this, we introduce SPD-GAN, which subtly perturbs the\ndistribution of data targeted for unlearning. Then, we evaluate the degree of\nunlearning by measuring the performance difference of the models on the\nperturbed unlearning data before and after the unlearning process. By\nimplementing these innovative techniques and metrics, we achieve\ncomputationally efficacious privacy protection in machine learning applications\nwithout significant sacrifice of model performance. Furthermore, this approach\nprovides a novel method for evaluating the degree of unlearning.\n","authors":["Zhiwei Zuo","Zhuo Tang","Kenli Li","Anwitaman Datta"],"pdf_url":"https://arxiv.org/pdf/2401.04385v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.02285v5","updated":"2025-01-15T05:53:54Z","published":"2023-06-04T07:26:20Z","title":"Clarify Confused Nodes via Separated Learning","summary":"  Graph neural networks (GNNs) have achieved remarkable advances in\ngraph-oriented tasks. However, real-world graphs invariably contain a certain\nproportion of heterophilous nodes, challenging the homophily assumption of\ntraditional GNNs and hindering their performance. Most existing studies\ncontinue to design generic models with shared weights between heterophilous and\nhomophilous nodes. Despite the incorporation of high-order messages or\nmulti-channel architectures, these efforts often fall short. A minority of\nstudies attempt to train different node groups separately but suffer from\ninappropriate separation metrics and low efficiency. In this paper, we first\npropose a new metric, termed Neighborhood Confusion (NC), to facilitate a more\nreliable separation of nodes. We observe that node groups with different levels\nof NC values exhibit certain differences in intra-group accuracy and visualized\nembeddings. These pave the way for Neighborhood Confusion-guided Graph\nConvolutional Network (NCGCN), in which nodes are grouped by their NC values\nand accept intra-group weight sharing and message passing. Extensive\nexperiments on both homophilous and heterophilous benchmarks demonstrate that\nour framework can effectively separate nodes and yield significant performance\nimprovement compared to the latest methods. The source code will be available\nin https://github.com/GISec-Team/NCGNN.\n","authors":["Jiajun Zhou","Shengbo Gong","Xuanze Chen","Chenxuan Xie","Shanqing Yu","Qi Xuan","Xiaoniu Yang"],"pdf_url":"https://arxiv.org/pdf/2306.02285v5.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence"},{"id":"http://arxiv.org/abs/2412.09468v2","updated":"2025-01-15T05:25:35Z","published":"2024-12-12T17:15:49Z","title":"STORM: A Spatio-Temporal Factor Model Based on Dual Vector Quantized\n  Variational Autoencoders for Financial Trading","summary":"  In financial trading, factor models are widely used to price assets and\ncapture excess returns from mispricing. Recently, we have witnessed the rise of\nvariational autoencoder-based latent factor models, which learn latent factors\nself-adaptively. While these models focus on modeling overall market\nconditions, they often fail to effectively capture the temporal patterns of\nindividual stocks. Additionally, representing multiple factors as single values\nsimplifies the model but limits its ability to capture complex relationships\nand dependencies. As a result, the learned factors are of low quality and lack\ndiversity, reducing their effectiveness and robustness across different trading\nperiods. To address these issues, we propose a Spatio-Temporal factOR Model\nbased on dual vector quantized variational autoencoders, named STORM, which\nextracts features of stocks from temporal and spatial perspectives, then fuses\nand aligns these features at the fine-grained and semantic level, and\nrepresents the factors as multi-dimensional embeddings. The discrete codebooks\ncluster similar factor embeddings, ensuring orthogonality and diversity, which\nhelps distinguish between different factors and enables factor selection in\nfinancial trading. To show the performance of the proposed factor model, we\napply it to two downstream experiments: portfolio management on two stock\ndatasets and individual trading tasks on six specific stocks. The extensive\nexperiments demonstrate STORM's flexibility in adapting to downstream tasks and\nsuperior performance over baseline models.\n","authors":["Yilei Zhao","Wentao Zhang","Tingran Yang","Yong Jiang","Fei Huang","Wei Yang Bryan Lim"],"pdf_url":"https://arxiv.org/pdf/2412.09468v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08591v1","updated":"2025-01-15T05:20:01Z","published":"2025-01-15T05:20:01Z","title":"OpenMLDB: A Real-Time Relational Data Feature Computation System for\n  Online ML","summary":"  Efficient and consistent feature computation is crucial for a wide range of\nonline ML applications. Typically, feature computation is divided into two\ndistinct phases, i.e., offline stage for model training and online stage for\nmodel serving. These phases often rely on execution engines with different\ninterface languages and function implementations, causing significant\ninconsistencies. Moreover, many online ML features involve complex time-series\ncomputations (e.g., functions over varied-length table windows) that differ\nfrom standard streaming and analytical queries. Existing data processing\nsystems (e.g., Spark, Flink, DuckDB) often incur multi-second latencies for\nthese computations, making them unsuitable for real-time online ML applications\nthat demand timely feature updates.\n  This paper presents OpenMLDB, a feature computation system deployed in\n4Paradigm's SageOne platform and over 100 real scenarios. Technically, OpenMLDB\nfirst employs a unified query plan generator for consistent computation results\nacross the offline and online stages, significantly reducing feature deployment\noverhead. Second, OpenMLDB provides an online execution engine that resolves\nperformance bottlenecks caused by long window computations (via\npre-aggregation) and multi-table window unions (via data self-adjusting). It\nalso provides a high-performance offline execution engine with window parallel\noptimization and time-aware data skew resolving. Third, OpenMLDB features a\ncompact data format and stream-focused indexing to maximize memory usage and\naccelerate data access. Evaluations in testing and real workloads reveal\nsignificant performance improvements and resource savings compared to the\nbaseline systems. The open community of OpenMLDB now has over 150 contributors\nand gained 1.6k stars on GitHub.\n","authors":["Xuanhe Zhou","Wei Zhou","Liguo Qi","Hao Zhang","Dihao Chen","Bingsheng He","Mian Lu","Guoliang Li","Fan Wu","Yuqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2501.08591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08589v1","updated":"2025-01-15T05:17:38Z","published":"2025-01-15T05:17:38Z","title":"Molecular Graph Contrastive Learning with Line Graph","summary":"  Trapped by the label scarcity in molecular property prediction and drug\ndesign, graph contrastive learning (GCL) came forward. Leading contrastive\nlearning works show two kinds of view generators, that is, random or learnable\ndata corruption and domain knowledge incorporation. While effective, the two\nways also lead to molecular semantics altering and limited generalization\ncapability, respectively. To this end, we relate the \\textbf{L}in\\textbf{E}\ngraph with \\textbf{MO}lecular graph co\\textbf{N}trastive learning and propose a\nnovel method termed \\textit{LEMON}. Specifically, by contrasting the given\ngraph with the corresponding line graph, the graph encoder can freely encode\nthe molecular semantics without omission. Furthermore, we present a new patch\nwith edge attribute fusion and two local contrastive losses enhance information\ntransmission and tackle hard negative samples. Compared with state-of-the-art\n(SOTA) methods for view generation, superior performance on molecular property\nprediction suggests the effectiveness of our proposed framework.\n","authors":["Xueyuan Chen","Shangzhe Li","Ruomei Liu","Bowen Shi","Jiaheng Liu","Junran Wu","Ke Xu"],"pdf_url":"https://arxiv.org/pdf/2501.08589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08581v1","updated":"2025-01-15T05:01:14Z","published":"2025-01-15T05:01:14Z","title":"Normalize Then Propagate: Efficient Homophilous Regularization for\n  Few-shot Semi-Supervised Node Classification","summary":"  Graph Neural Networks (GNNs) have demonstrated remarkable ability in\nsemi-supervised node classification. However, most existing GNNs rely heavily\non a large amount of labeled data for training, which is labor-intensive and\nrequires extensive domain knowledge. In this paper, we first analyze the\nrestrictions of GNNs generalization from the perspective of supervision signals\nin the context of few-shot semi-supervised node classification. To address\nthese challenges, we propose a novel algorithm named NormProp, which utilizes\nthe homophily assumption of unlabeled nodes to generate additional supervision\nsignals, thereby enhancing the generalization against label scarcity. The key\nidea is to efficiently capture both the class information and the consistency\nof aggregation during message passing, via decoupling the direction and\nEuclidean norm of node representations. Moreover, we conduct a theoretical\nanalysis to determine the upper bound of Euclidean norm, and then propose\nhomophilous regularization to constraint the consistency of unlabeled nodes.\nExtensive experiments demonstrate that NormProp achieve state-of-the-art\nperformance under low-label rate scenarios with low computational complexity.\n","authors":["Baoming Zhang","MingCai Chen","Jianqing Song","Shuangjie Li","Jie Zhang","Chongjun Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08581v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2409.18426v2","updated":"2025-01-15T04:59:43Z","published":"2024-09-27T03:27:46Z","title":"Dual Cone Gradient Descent for Training Physics-Informed Neural Networks","summary":"  Physics-informed neural networks (PINNs) have emerged as a prominent approach\nfor solving partial differential equations (PDEs) by minimizing a combined loss\nfunction that incorporates both boundary loss and PDE residual loss. Despite\ntheir remarkable empirical performance in various scientific computing tasks,\nPINNs often fail to generate reasonable solutions, and such pathological\nbehaviors remain difficult to explain and resolve. In this paper, we identify\nthat PINNs can be adversely trained when gradients of each loss function\nexhibit a significant imbalance in their magnitudes and present a negative\ninner product value. To address these issues, we propose a novel optimization\nframework, Dual Cone Gradient Descent (DCGD), which adjusts the direction of\nthe updated gradient to ensure it falls within a dual cone region. This region\nis defined as a set of vectors where the inner products with both the gradients\nof the PDE residual loss and the boundary loss are non-negative. Theoretically,\nwe analyze the convergence properties of DCGD algorithms in a non-convex\nsetting. On a variety of benchmark equations, we demonstrate that DCGD\noutperforms other optimization algorithms in terms of various evaluation\nmetrics. In particular, DCGD achieves superior predictive accuracy and enhances\nthe stability of training for failure modes of PINNs and complex PDEs, compared\nto existing optimally tuned models. Moreover, DCGD can be further improved by\ncombining it with popular strategies for PINNs, including learning rate\nannealing and the Neural Tangent Kernel (NTK).\n","authors":["Youngsik Hwang","Dong-Young Lim"],"pdf_url":"https://arxiv.org/pdf/2409.18426v2.pdf","comment":"The Thirty-eighth Annual Conference on Neural Information Processing\n  Systems, 2024"},{"id":"http://arxiv.org/abs/2411.02281v2","updated":"2025-01-15T04:51:48Z","published":"2024-11-04T17:09:58Z","title":"Conformal-in-the-Loop for Learning with Imbalanced Noisy Data","summary":"  Class imbalance and label noise are pervasive in large-scale datasets, yet\nmuch of machine learning research assumes well-labeled, balanced data, which\nrarely reflects real world conditions. Existing approaches typically address\neither label noise or class imbalance in isolation, leading to suboptimal\nresults when both issues coexist. In this work, we propose\nConformal-in-the-Loop (CitL), a novel training framework that addresses both\nchallenges with a conformal prediction-based approach. CitL evaluates sample\nuncertainty to adjust weights and prune unreliable examples, enhancing model\nresilience and accuracy with minimal computational cost. Our extensive\nexperiments include a detailed analysis showing how CitL effectively emphasizes\nimpactful data in noisy, imbalanced datasets. Our results show that CitL\nconsistently boosts model performance, achieving up to a 6.1% increase in\nclassification accuracy and a 5.0 mIoU improvement in segmentation. Our code is\npublicly available: CitL.\n","authors":["John Brandon Graham-Knight","Jamil Fayyad","Nourhan Bayasi","Patricia Lasserre","Homayoun Najjaran"],"pdf_url":"https://arxiv.org/pdf/2411.02281v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2501.08572v1","updated":"2025-01-15T04:36:55Z","published":"2025-01-15T04:36:55Z","title":"DNMDR: Dynamic Networks and Multi-view Drug Representations for Safe\n  Medication Recommendation","summary":"  Medication Recommendation (MR) is a promising research topic which booms\ndiverse applications in the healthcare and clinical domains. However, existing\nmethods mainly rely on sequential modeling and static graphs for representation\nlearning, which ignore the dynamic correlations in diverse medical events of a\npatient's temporal visits, leading to insufficient global structural\nexploration on nodes. Additionally, mitigating drug-drug interactions (DDIs) is\nanother issue determining the utility of the MR systems. To address the\nchallenges mentioned above, this paper proposes a novel MR method with the\nintegration of dynamic networks and multi-view drug representations (DNMDR).\nSpecifically, weighted snapshot sequences for dynamic heterogeneous networks\nare constructed based on discrete visits in temporal EHRs, and all the dynamic\nnetworks are jointly trained to gain both structural correlations in diverse\nmedical events and temporal dependency in historical health conditions, for\nachieving comprehensive patient representations with both semantic features and\nstructural relationships. Moreover, combining the drug co-occurrences and\nadverse drug-drug interactions (DDIs) in internal view of drug molecule\nstructure and interactive view of drug pairs, the safe drug representations are\navailable to obtain high-quality medication combination recommendation.\nFinally, extensive experiments on real world datasets are conducted for\nperformance evaluation, and the experimental results demonstrate that the\nproposed DNMDR method outperforms the state-of-the-art baseline models with a\nlarge margin on various metrics such as PRAUC, Jaccard, DDI rates and so on.\n","authors":["Guanlin Liu","Xiaomei Yu","Zihao Liu","Xue Li","Xingxu Fan","Xiangwei Zheng"],"pdf_url":"https://arxiv.org/pdf/2501.08572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19213v2","updated":"2025-01-15T04:17:38Z","published":"2024-05-29T15:56:33Z","title":"EdgeSight: Enabling Modeless and Cost-Efficient Inference at the Edge","summary":"  Traditional ML inference is evolving toward modeless inference, which\nabstracts the complexity of model selection from users, allowing the system to\nautomatically choose the most appropriate model for each request based on\naccuracy and resource requirements. While prior studies have focused on\nmodeless inference within data centers, this paper tackles the pressing need\nfor cost-efficient modeless inference at the edge -- particularly within its\nunique constraints of limited device memory, volatile network conditions, and\nrestricted power consumption.\n  To overcome these challenges, we propose EdgeSight, a system that provides\ncost-efficient EdgeSight serving for diverse DNNs at the edge. EdgeSight\nemploys an edge-data center (edge-DC) architecture, utilizing confidence\nscaling to reduce the number of model options while meeting diverse accuracy\nrequirements. Additionally, it supports lossy inference in volatile network\nenvironments. Our experimental results show that EdgeSight outperforms existing\nsystems by up to 1.6x in P99 latency for modeless services. Furthermore, our\nFPGA prototype demonstrates similar performance at certain accuracy levels,\nwith a power consumption reduction of up to 3.34x.\n","authors":["ChonLam Lao","Jiaqi Gao","Ganesh Ananthanarayanan","Aditya Akella","Minlan Yu"],"pdf_url":"https://arxiv.org/pdf/2405.19213v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2501.08563v1","updated":"2025-01-15T04:09:21Z","published":"2025-01-15T04:09:21Z","title":"Adaptive Sampled Softmax with Inverted Multi-Index: Methods, Theory and\n  Applications","summary":"  The softmax function is a cornerstone of multi-class classification, integral\nto a wide range of machine learning applications, from large-scale retrieval\nand ranking models to advanced large language models. However, its\ncomputational cost grows linearly with the number of classes, which becomes\nprohibitively expensive in scenarios with millions or even billions of classes.\nThe sampled softmax, which relies on self-normalized importance sampling, has\nemerged as a powerful alternative, significantly reducing computational\ncomplexity. Yet, its estimator remains unbiased only when the sampling\ndistribution matches the true softmax distribution. To improve both\napproximation accuracy and sampling efficiency, we propose the MIDX Sampler, a\nnovel adaptive sampling strategy based on an inverted multi-index approach.\nConcretely, we decompose the softmax probability into several multinomial\nprobabilities, each associated with a specific set of codewords and the last\nassociated with the residual score of queries, thus reducing time complexity to\nthe number of codewords instead of the number of classes. To further boost\nefficiency, we replace the query-specific residual probability with a simple\nuniform distribution, simplifying the computation while retaining high\nperformance. Our method is backed by rigorous theoretical analysis, addressing\nkey concerns such as sampling bias, gradient bias, convergence rates, and\ngeneralization error bounds. The results demonstrate that a smaller divergence\nfrom the ideal softmax distribution leads to faster convergence and improved\ngeneralization. Extensive experiments on large-scale language models,\nsequential recommenders, and extreme multi-class classification tasks confirm\nthat the MIDX-Sampler delivers superior effectiveness and efficiency compared\nto existing approaches.\n","authors":["Jin Chen","Jin Zhang","Xu huang","Yi Yang","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2501.08563v1.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2501.08562v1","updated":"2025-01-15T04:07:06Z","published":"2025-01-15T04:07:06Z","title":"MIAFEx: An Attention-based Feature Extraction Method for Medical Image\n  Classification","summary":"  Feature extraction techniques are crucial in medical image classification;\nhowever, classical feature extractors in addition to traditional machine\nlearning classifiers often exhibit significant limitations in providing\nsufficient discriminative information for complex image sets. While\nConvolutional Neural Networks (CNNs) and Vision Transformer (ViT) have shown\npromise in feature extraction, they are prone to overfitting due to the\ninherent characteristics of medical imaging data, including small sample sizes\nor high intra-class variance. In this work, the Medical Image Attention-based\nFeature Extractor (MIAFEx) is proposed, a novel method that employs a learnable\nrefinement mechanism to enhance the classification token within the Transformer\nencoder architecture. This mechanism adjusts the token based on learned\nweights, improving the extraction of salient features and enhancing the model's\nadaptability to the challenges presented by medical imaging data. The MIAFEx\noutput features quality is compared against classical feature extractors using\ntraditional and hybrid classifiers. Also, the performance of these features is\ncompared against modern CNN and ViT models in classification tasks,\ndemonstrating its superiority in accuracy and robustness across multiple\ncomplex classification medical imaging datasets. This advantage is particularly\npronounced in scenarios with limited training data, where traditional and\nmodern models often struggle to generalize effectively. The source code of this\nproposal can be found at\nhttps://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx\n","authors":["Oscar Ramos-Soto","Jorge Ramos-Frutos","Ezequiel Perez-Zarate","Diego Oliva","Sandra E. Balderas-Mata"],"pdf_url":"https://arxiv.org/pdf/2501.08562v1.pdf","comment":"In preparation for Journal Submission"},{"id":"http://arxiv.org/abs/2501.08561v1","updated":"2025-01-15T04:04:57Z","published":"2025-01-15T04:04:57Z","title":"ANSR-DT: An Adaptive Neuro-Symbolic Learning and Reasoning Framework for\n  Digital Twins","summary":"  In this paper, we propose an Adaptive Neuro-Symbolic Learning Framework for\ndigital twin technology called ``ANSR-DT.\" Our approach combines pattern\nrecognition algorithms with reinforcement learning and symbolic reasoning to\nenable real-time learning and adaptive intelligence. This integration enhances\nthe understanding of the environment and promotes continuous learning, leading\nto better and more effective decision-making in real-time for applications that\nrequire human-machine collaboration. We evaluated the \\textit{ANSR-DT}\nframework for its ability to learn and adapt to dynamic patterns, observing\nsignificant improvements in decision accuracy, reliability, and\ninterpretability when compared to existing state-of-the-art methods. However,\nchallenges still exist in extracting and integrating symbolic rules in complex\nenvironments, which limits the full potential of our framework in heterogeneous\nsettings. Moreover, our ongoing research aims to address this issue in the\nfuture by ensuring seamless integration of neural models at large. In addition,\nour open-source implementation promotes reproducibility and encourages future\nresearch to build on our foundational work.\n","authors":["Safayat Bin Hakim","Muhammad Adil","Alvaro Velasquez","Houbing Herbert Song"],"pdf_url":"https://arxiv.org/pdf/2501.08561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08558v1","updated":"2025-01-15T03:49:08Z","published":"2025-01-15T03:49:08Z","title":"LAMS: LLM-Driven Automatic Mode Switching for Assistive Teleoperation","summary":"  Teleoperating high degrees-of-freedom (DoF) robotic manipulators via low-DoF\ncontrollers like joysticks often requires frequent switching between control\nmodes, where each mode maps controller movements to specific robot actions.\nManually performing this frequent switching can make teleoperation cumbersome\nand inefficient. On the other hand, existing automatic mode-switching\nsolutions, such as heuristic-based or learning-based methods, are often\ntask-specific and lack generalizability. In this paper, we introduce LLM-Driven\nAutomatic Mode Switching (LAMS), a novel approach that leverages Large Language\nModels (LLMs) to automatically switch control modes based on task context.\nUnlike existing methods, LAMS requires no prior task demonstrations and\nincrementally improves by integrating user-generated mode-switching examples.\nWe validate LAMS through an ablation study and a user study with 10\nparticipants on complex, long-horizon tasks, demonstrating that LAMS\neffectively reduces manual mode switches, is preferred over alternative\nmethods, and improves performance over time. The project website with\nsupplementary materials is at https://lams-assistance.github.io/.\n","authors":["Yiran Tao","Jehan Yang","Dan Ding","Zackory Erickson"],"pdf_url":"https://arxiv.org/pdf/2501.08558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04820v2","updated":"2025-01-15T03:43:22Z","published":"2024-08-09T02:22:51Z","title":"Natural Language Outlines for Code: Literate Programming in the LLM Era","summary":"  We propose using natural language outlines as a novel modality and\ninteraction surface for providing AI assistance to developers throughout the\nsoftware development process. An NL outline for a code function comprises\nmultiple statements written in concise prose, which partition the code and\nsummarize its main ideas in the style of literate programming. Crucially, we\nfind that modern LLMs can generate accurate and high-quality NL outlines in\npractice. Moreover, NL outlines enable a bidirectional sync between code and\nNL, allowing changes in one to be automatically reflected in the other. We\ndiscuss many use cases for NL outlines: they can accelerate understanding and\nnavigation of code and diffs, simplify code maintenance, augment code search,\nsteer code generation, and more. We then propose and compare multiple LLM\nprompting techniques for generating outlines and ask professional developers to\njudge outline quality. Finally, we present two case studies applying NL\noutlines toward code review and malware detection.\n","authors":["Kensen Shi","Deniz Altınbüken","Saswat Anand","Mihai Christodorescu","Katja Grünwedel","Alexa Koenings","Sai Naidu","Anurag Pathak","Marc Rasi","Fredde Ribeiro","Brandon Ruffin","Siddhant Sanyam","Maxim Tabachnyk","Sara Toth","Roy Tu","Tobias Welp","Pengcheng Yin","Manzil Zaheer","Satish Chandra","Charles Sutton"],"pdf_url":"https://arxiv.org/pdf/2408.04820v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02512v2","updated":"2025-01-15T03:23:39Z","published":"2024-09-04T08:21:47Z","title":"Continual Diffuser (CoD): Mastering Continual Offline Reinforcement\n  Learning with Experience Rehearsal","summary":"  Artificial neural networks, especially recent diffusion-based models, have\nshown remarkable superiority in gaming, control, and QA systems, where the\ntraining tasks' datasets are usually static. However, in real-world\napplications, such as robotic control of reinforcement learning (RL), the tasks\nare changing, and new tasks arise in a sequential order. This situation poses\nthe new challenge of plasticity-stability trade-off for training an agent who\ncan adapt to task changes and retain acquired knowledge. In view of this, we\npropose a rehearsal-based continual diffusion model, called Continual Diffuser\n(CoD), to endow the diffuser with the capabilities of quick adaptation\n(plasticity) and lasting retention (stability). Specifically, we first\nconstruct an offline benchmark that contains 90 tasks from multiple domains.\nThen, we train the CoD on each task with sequential modeling and conditional\ngeneration for making decisions. Next, we preserve a small portion of previous\ndatasets as the rehearsal buffer and replay it to retain the acquired\nknowledge. Extensive experiments on a series of tasks show CoD can achieve a\npromising plasticity-stability trade-off and outperform existing\ndiffusion-based methods and other representative baselines on most tasks.\n","authors":["Jifeng Hu","Li Shen","Sili Huang","Zhejian Yang","Hechang Chen","Lichao Sun","Yi Chang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2409.02512v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2501.08552v1","updated":"2025-01-15T03:23:06Z","published":"2025-01-15T03:23:06Z","title":"Reinforcement Learning-Enhanced Procedural Generation for Dynamic\n  Narrative-Driven AR Experiences","summary":"  Procedural Content Generation (PCG) is widely used to create scalable and\ndiverse environments in games. However, existing methods, such as the Wave\nFunction Collapse (WFC) algorithm, are often limited to static scenarios and\nlack the adaptability required for dynamic, narrative-driven applications,\nparticularly in augmented reality (AR) games. This paper presents a\nreinforcement learning-enhanced WFC framework designed for mobile AR\nenvironments. By integrating environment-specific rules and dynamic tile weight\nadjustments informed by reinforcement learning (RL), the proposed method\ngenerates maps that are both contextually coherent and responsive to gameplay\nneeds. Comparative evaluations and user studies demonstrate that the framework\nachieves superior map quality and delivers immersive experiences, making it\nwell-suited for narrative-driven AR games. Additionally, the method holds\npromise for broader applications in education, simulation training, and\nimmersive extended reality (XR) experiences, where dynamic and adaptive\nenvironments are critical.\n","authors":["Aniruddha Srinivas Joshi"],"pdf_url":"https://arxiv.org/pdf/2501.08552v1.pdf","comment":"Number of pages: 13, Number of figures: 4. Accepted for presentation\n  at GRAPP 2025 - 20th International Conference on Computer Graphics Theory and\n  Applications (for additional details on the conference visit\n  https://grapp.scitevents.org). Disclaimer: This preprint may differ from the\n  final version published in the conference proceedings"},{"id":"http://arxiv.org/abs/2501.08551v1","updated":"2025-01-15T03:20:16Z","published":"2025-01-15T03:20:16Z","title":"A Theory of Optimistically Universal Online Learnability for General\n  Concept Classes","summary":"  We provide a full characterization of the concept classes that are\noptimistically universally online learnable with $\\{0, 1\\}$ labels. The notion\nof optimistically universal online learning was defined in [Hanneke, 2021] in\norder to understand learnability under minimal assumptions. In this paper,\nfollowing the philosophy behind that work, we investigate two questions,\nnamely, for every concept class: (1) What are the minimal assumptions on the\ndata process admitting online learnability? (2) Is there a learning algorithm\nwhich succeeds under every data process satisfying the minimal assumptions?\nSuch an algorithm is said to be optimistically universal for the given concept\nclass. We resolve both of these questions for all concept classes, and\nmoreover, as part of our solution, we design general learning algorithms for\neach case. Finally, we extend these algorithms and results to the agnostic\ncase, showing an equivalence between the minimal assumptions on the data\nprocess for learnability in the agnostic and realizable cases, for every\nconcept class, as well as the equivalence of optimistically universal\nlearnability.\n","authors":["Steve Hanneke","Hongao Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08551v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2501.08547v1","updated":"2025-01-15T03:14:18Z","published":"2025-01-15T03:14:18Z","title":"OMEGA: A Low-Latency GNN Serving System for Large Graphs","summary":"  Graph Neural Networks (GNNs) have been widely adopted for their ability to\ncompute expressive node representations in graph datasets. However, serving\nGNNs on large graphs is challenging due to the high communication, computation,\nand memory overheads of constructing and executing computation graphs, which\nrepresent information flow across large neighborhoods. Existing approximation\ntechniques in training can mitigate the overheads but, in serving, still lead\nto high latency and/or accuracy loss. To this end, we propose OMEGA, a system\nthat enables low-latency GNN serving for large graphs with minimal accuracy\nloss through two key ideas. First, OMEGA employs selective recomputation of\nprecomputed embeddings, which allows for reusing precomputed computation\nsubgraphs while selectively recomputing a small fraction to minimize accuracy\nloss. Second, we develop computation graph parallelism, which reduces\ncommunication overhead by parallelizing the creation and execution of\ncomputation graphs across machines. Our evaluation with large graph datasets\nand GNN models shows that OMEGA significantly outperforms state-of-the-art\ntechniques.\n","authors":["Geon-Woo Kim","Donghyun Kim","Jeongyoon Moon","Henry Liu","Tarannum Khan","Anand Iyer","Daehyeok Kim","Aditya Akella"],"pdf_url":"https://arxiv.org/pdf/2501.08547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08187v2","updated":"2025-01-15T02:59:32Z","published":"2025-01-14T15:12:19Z","title":"A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction\n  Following","summary":"  Large language models excel at interpreting complex natural language\ninstructions, enabling them to perform a wide range of tasks. In the life\nsciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language\nof cellular biology\", capturing intricate gene expression patterns at the\nsingle-cell level. However, interacting with this \"language\" through\nconventional tools is often inefficient and unintuitive, posing challenges for\nresearchers. To address these limitations, we present InstructCell, a\nmulti-modal AI copilot that leverages natural language as a medium for more\ndirect and flexible single-cell analysis. We construct a comprehensive\nmulti-modal instruction dataset that pairs text-based instructions with\nscRNA-seq profiles from diverse tissues and species. Building on this, we\ndevelop a multi-modal cell language architecture capable of simultaneously\ninterpreting and processing both modalities. InstructCell empowers researchers\nto accomplish critical tasks-such as cell type annotation, conditional\npseudo-cell generation, and drug sensitivity prediction-using straightforward\nnatural language commands. Extensive evaluations demonstrate that InstructCell\nconsistently meets or exceeds the performance of existing single-cell\nfoundation models, while adapting to diverse experimental conditions. More\nimportantly, InstructCell provides an accessible and intuitive tool for\nexploring complex single-cell data, lowering technical barriers and enabling\ndeeper biological insights.\n","authors":["Yin Fang","Xinle Deng","Kangwei Liu","Ningyu Zhang","Jingyang Qian","Penghui Yang","Xiaohui Fan","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2501.08187v2.pdf","comment":"37 pages; 13 figures; Code: https://github.com/zjunlp/Instructcell,\n  Models: https://huggingface.co/zjunlp/Instructcell-chat,\n  https://huggingface.co/zjunlp/InstructCell-instruct"},{"id":"http://arxiv.org/abs/2501.08538v1","updated":"2025-01-15T02:56:50Z","published":"2025-01-15T02:56:50Z","title":"Homophily-aware Heterogeneous Graph Contrastive Learning","summary":"  Heterogeneous graph pre-training (HGP) has demonstrated remarkable\nperformance across various domains. However, the issue of heterophily in\nreal-world heterogeneous graphs (HGs) has been largely overlooked. To bridge\nthis research gap, we proposed a novel heterogeneous graph contrastive learning\nframework, termed HGMS, which leverages connection strength and multi-view\nself-expression to learn homophilous node representations. Specifically, we\ndesign a heterogeneous edge dropping augmentation strategy that enhances the\nhomophily of augmented views. Moreover, we introduce a multi-view\nself-expressive learning method to infer the homophily between nodes. In\npractice, we develop two approaches to solve the self-expressive matrix. The\nsolved self-expressive matrix serves as an additional augmented view to provide\nhomophilous information and is used to identify false negatives in contrastive\nloss. Extensive experimental results demonstrate the superiority of HGMS across\ndifferent downstream tasks.\n","authors":["Haosen Wang","Chenglong Shi","Can Xu","Surong Yan","Pan Tang"],"pdf_url":"https://arxiv.org/pdf/2501.08538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08537v1","updated":"2025-01-15T02:54:52Z","published":"2025-01-15T02:54:52Z","title":"Complexity Control Facilitates Reasoning-Based Compositional\n  Generalization in Transformers","summary":"  Transformers have demonstrated impressive capabilities across various tasks,\nyet their performance on compositional problems remains a subject of debate. In\nthis study, we investigate the internal mechanisms underlying Transformers'\nbehavior in compositional tasks. We find that complexity control strategies\nsignificantly influence whether the model learns primitive-level rules that\ngeneralize out-of-distribution (reasoning-based solutions) or relies solely on\nmemorized mappings (memory-based solutions). By applying masking strategies to\nthe model's information circuits and employing multiple complexity metrics, we\nreveal distinct internal working mechanisms associated with different solution\ntypes. Further analysis reveals that reasoning-based solutions exhibit a lower\ncomplexity bias, which aligns with the well-studied neuron condensation\nphenomenon. This lower complexity bias is hypothesized to be the key factor\nenabling these solutions to learn reasoning rules. We validate these\nconclusions across multiple real-world datasets, including image generation and\nnatural language processing tasks, confirming the broad applicability of our\nfindings.\n","authors":["Zhongwang Zhang","Pengxiao Lin","Zhiwei Wang","Yaoyu Zhang","Zhi-Qin John Xu"],"pdf_url":"https://arxiv.org/pdf/2501.08537v1.pdf","comment":"Mistakenly submitted as a replacement to 2405.05409v4"},{"id":"http://arxiv.org/abs/2403.15796v3","updated":"2025-01-15T02:48:59Z","published":"2024-03-23T11:03:31Z","title":"Understanding Emergent Abilities of Language Models from the Loss\n  Perspective","summary":"  Recent studies have put into question the belief that emergent abilities in\nlanguage models are exclusive to large models. This skepticism arises from two\nobservations: 1) smaller models can also exhibit high performance on emergent\nabilities and 2) there is doubt on the discontinuous metrics used to measure\nthese abilities. In this paper, we propose to study emergent abilities in the\nlens of pre-training loss, instead of model size or training compute. We\ndemonstrate that the Transformer models with the same pre-training loss, but\ndifferent model and data sizes, generate the same performance on various\ndownstream tasks, with a fixed data corpus, tokenization, and model\narchitecture. We also discover that a model exhibits emergent abilities on\ncertain tasks -- regardless of the continuity of metrics -- when its\npre-training loss falls below a specific threshold. Before reaching this\nthreshold, its performance remains at the level of random guessing. This\ninspires us to redefine emergent abilities as those that manifest in models\nwith lower pre-training losses, highlighting that these abilities cannot be\npredicted by merely extrapolating the performance trends of models with higher\npre-training losses.\n","authors":["Zhengxiao Du","Aohan Zeng","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2403.15796v3.pdf","comment":"23 pages, 8 figures. Accepted in NeurIPS 2024"},{"id":"http://arxiv.org/abs/2501.08109v2","updated":"2025-01-15T02:48:33Z","published":"2025-01-14T13:40:08Z","title":"Data-driven inventory management for new products: A warm-start and\n  adjusted Dyna-$Q$ approach","summary":"  In this paper, we propose a novel reinforcement learning algorithm for\ninventory management of newly launched products with no or limited historical\ndemand information. The algorithm follows the classic Dyna-$Q$ structure,\nbalancing the model-based and model-free approaches, while accelerating the\ntraining process of Dyna-$Q$ and mitigating the model discrepancy generated by\nthe model-based feedback. Warm-start information from the demand data of\nexisting similar products can be incorporated into the algorithm to further\nstabilize the early-stage training and reduce the variance of the estimated\noptimal policy. Our approach is validated through a case study of bakery\ninventory management with real data. The adjusted Dyna-$Q$ shows up to a 23.7%\nreduction in average daily cost compared with $Q$-learning, and up to a 77.5%\nreduction in training time within the same horizon compared with classic\nDyna-$Q$. By incorporating the warm-start information, it can be found that the\nadjusted Dyna-$Q$ has the lowest total cost, lowest variance in total cost, and\nrelatively low shortage percentages among all the algorithms under a 30-day\ntesting.\n","authors":["Xinye Qu","Longxiao Liu","Wenjie Huang"],"pdf_url":"https://arxiv.org/pdf/2501.08109v2.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2409.18946v3","updated":"2025-01-15T02:42:42Z","published":"2024-09-27T17:46:05Z","title":"Unconditional stability of a recurrent neural circuit implementing\n  divisive normalization","summary":"  Stability in recurrent neural models poses a significant challenge,\nparticularly in developing biologically plausible neurodynamical models that\ncan be seamlessly trained. Traditional cortical circuit models are notoriously\ndifficult to train due to expansive nonlinearities in the dynamical system,\nleading to an optimization problem with nonlinear stability constraints that\nare difficult to impose. Conversely, recurrent neural networks (RNNs) excel in\ntasks involving sequential data but lack biological plausibility and\ninterpretability. In this work, we address these challenges by linking dynamic\ndivisive normalization (DN) to the stability of ORGaNICs, a biologically\nplausible recurrent cortical circuit model that dynamically achieves DN and\nthat has been shown to simulate a wide range of neurophysiological phenomena.\nBy using the indirect method of Lyapunov, we prove the remarkable property of\nunconditional local stability for an arbitrary-dimensional ORGaNICs circuit\nwhen the recurrent weight matrix is the identity. We thus connect ORGaNICs to a\nsystem of coupled damped harmonic oscillators, which enables us to derive the\ncircuit's energy function, providing a normative principle of what the circuit,\nand individual neurons, aim to accomplish. Further, for a generic recurrent\nweight matrix, we prove the stability of the 2D model and demonstrate\nempirically that stability holds in higher dimensions. Finally, we show that\nORGaNICs can be trained by backpropagation through time without gradient\nclipping/scaling, thanks to its intrinsic stability property and adaptive time\nconstants, which address the problems of exploding, vanishing, and oscillating\ngradients. By evaluating the model's performance on RNN benchmarks, we find\nthat ORGaNICs outperform alternative neurodynamical models on static image\nclassification tasks and perform comparably to LSTMs on sequential tasks.\n","authors":["Shivang Rawat","David J. Heeger","Stefano Martiniani"],"pdf_url":"https://arxiv.org/pdf/2409.18946v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19727v2","updated":"2025-01-15T02:29:14Z","published":"2024-09-29T14:57:45Z","title":"Investigating the Effect of Network Pruning on Performance and\n  Interpretability","summary":"  Deep Neural Networks (DNNs) are often over-parameterized for their tasks and\ncan be compressed quite drastically by removing weights, a process called\npruning. We investigate the impact of different pruning techniques on the\nclassification performance and interpretability of GoogLeNet. We systematically\napply unstructured and structured pruning, as well as connection sparsity\n(pruning of input weights) methods to the network and analyze the outcomes\nregarding the network's performance on the validation set of ImageNet. We also\ncompare different retraining strategies, such as iterative pruning and one-shot\npruning. We find that with sufficient retraining epochs, the performance of the\nnetworks can approximate the performance of the default GoogLeNet - and even\nsurpass it in some cases. To assess interpretability, we employ the Mechanistic\nInterpretability Score (MIS) developed by Zimmermann et al. . Our experiments\nreveal that there is no significant relationship between interpretability and\npruning rate when using MIS as a measure. Additionally, we observe that\nnetworks with extremely low accuracy can still achieve high MIS scores,\nsuggesting that the MIS may not always align with intuitive notions of\ninterpretability, such as understanding the basis of correct decisions.\n","authors":["Jonathan von Rad","Florian Seuffert"],"pdf_url":"https://arxiv.org/pdf/2409.19727v2.pdf","comment":"4 pages, 6 figures"},{"id":"http://arxiv.org/abs/2304.09123v3","updated":"2025-01-15T02:19:34Z","published":"2023-04-18T16:39:51Z","title":"Finite-Sample Bounds for Adaptive Inverse Reinforcement Learning using\n  Passive Langevin Dynamics","summary":"  This paper provides a finite-sample analysis of a passive stochastic gradient\nLangevin dynamics (PSGLD) algorithm. This algorithm is designed to achieve\nadaptive inverse reinforcement learning (IRL). Adaptive IRL aims to estimate\nthe cost function of a forward learner performing a stochastic gradient\nalgorithm (e.g., policy gradient reinforcement learning) by observing their\nestimates in real-time. The PSGLD algorithm is considered passive because it\nincorporates noisy gradients provided by an external stochastic gradient\nalgorithm (forward learner), of which it has no control. The PSGLD algorithm\nacts as a randomized sampler to achieve adaptive IRL by reconstructing the\nforward learner's cost function nonparametrically from the stationary measure\nof a Langevin diffusion. This paper analyzes the non-asymptotic (finite-sample)\nperformance; we provide explicit bounds on the 2-Wasserstein distance between\nPSGLD algorithm sample measure and the stationary measure encoding the cost\nfunction, and provide guarantees for a kernel density estimation scheme which\nreconstructs the cost function from empirical samples. Our analysis uses tools\nfrom the study of Markov diffusion operators. The derived bounds have both\npractical and theoretical significance. They provide finite-time guarantees for\nan adaptive IRL mechanism, and substantially generalize the analytical\nframework of a line of research in passive stochastic gradient algorithms.\n","authors":["Luke Snow","Vikram Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2304.09123v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08521v1","updated":"2025-01-15T02:17:38Z","published":"2025-01-15T02:17:38Z","title":"Mitigating Domain Shift in Federated Learning via Intra- and\n  Inter-Domain Prototypes","summary":"  Federated Learning (FL) has emerged as a decentralized machine learning\ntechnique, allowing clients to train a global model collaboratively without\nsharing private data. However, most FL studies ignore the crucial challenge of\nheterogeneous domains where each client has a distinct feature distribution,\nwhich is common in real-world scenarios. Prototype learning, which leverages\nthe mean feature vectors within the same classes, has become a prominent\nsolution for federated learning under domain skew. However, existing federated\nprototype learning methods only consider inter-domain prototypes on the server\nand overlook intra-domain characteristics. In this work, we introduce a novel\nfederated prototype learning method, namely I$^2$PFL, which incorporates\n$\\textbf{I}$ntra-domain and $\\textbf{I}$nter-domain $\\textbf{P}$rototypes, to\nmitigate domain shifts and learn a generalized global model across multiple\ndomains in federated learning. To construct intra-domain prototypes, we propose\nfeature alignment with MixUp-based augmented prototypes to capture the\ndiversity of local domains and enhance the generalization of local features.\nAdditionally, we introduce a reweighting mechanism for inter-domain prototypes\nto generate generalized prototypes to provide inter-domain knowledge and reduce\ndomain skew across multiple clients. Extensive experiments on the Digits,\nOffice-10, and PACS datasets illustrate the superior performance of our method\ncompared to other baselines.\n","authors":["Huy Q. Le","Ye Lin Tun","Yu Qiao","Minh N. H. Nguyen","Keon Oh Kim","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2501.08521v1.pdf","comment":"13 pages, 9 figures, 10 tables"},{"id":"http://arxiv.org/abs/2501.08515v1","updated":"2025-01-15T01:59:24Z","published":"2025-01-15T01:59:24Z","title":"Learning Hyperplane Tree: A Piecewise Linear and Fully Interpretable\n  Decision-making Framework","summary":"  This paper introduces a novel tree-based model, Learning Hyperplane Tree\n(LHT), which outperforms state-of-the-art (SOTA) tree models for classification\ntasks on several public datasets. The structure of LHT is simple and efficient:\nit partitions the data using several hyperplanes to progressively distinguish\nbetween target and non-target class samples. Although the separation is not\nperfect at each stage, LHT effectively improves the distinction through\nsuccessive partitions. During testing, a sample is classified by evaluating the\nhyperplanes defined in the branching blocks and traversing down the tree until\nit reaches the corresponding leaf block. The class of the test sample is then\ndetermined using the piecewise linear membership function defined in the leaf\nblocks, which is derived through least-squares fitting and fuzzy logic. LHT is\nhighly transparent and interpretable--at each branching block, the contribution\nof each feature to the classification can be clearly observed.\n","authors":["Hongyi Li","Jun Xu","William Ward Armstrong"],"pdf_url":"https://arxiv.org/pdf/2501.08515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00205v2","updated":"2025-01-15T01:46:25Z","published":"2024-10-31T20:56:07Z","title":"Compositional Automata Embeddings for Goal-Conditioned Reinforcement\n  Learning","summary":"  Goal-conditioned reinforcement learning is a powerful way to control an AI\nagent's behavior at runtime. That said, popular goal representations, e.g.,\ntarget states or natural language, are either limited to Markovian tasks or\nrely on ambiguous task semantics. We propose representing temporal goals using\ncompositions of deterministic finite automata (cDFAs) and use cDFAs to guide RL\nagents. cDFAs balance the need for formal temporal semantics with ease of\ninterpretation: if one can understand a flow chart, one can understand a cDFA.\nOn the other hand, cDFAs form a countably infinite concept class with Boolean\nsemantics, and subtle changes to the automaton can result in very different\ntasks, making them difficult to condition agent behavior on. To address this,\nwe observe that all paths through a DFA correspond to a series of reach-avoid\ntasks and propose pre-training graph neural network embeddings on \"reach-avoid\nderived\" DFAs. Through empirical evaluation, we demonstrate that the proposed\npre-training method enables zero-shot generalization to various cDFA task\nclasses and accelerated policy specialization without the myopic suboptimality\nof hierarchical methods.\n","authors":["Beyazit Yalcinkaya","Niklas Lauffer","Marcell Vazquez-Chanlatte","Sanjit A. Seshia"],"pdf_url":"https://arxiv.org/pdf/2411.00205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00656v2","updated":"2025-01-15T01:44:16Z","published":"2024-12-31T21:55:10Z","title":"2 OLMo 2 Furious","summary":"  We present OLMo 2, the next generation of our fully open language models.\nOLMo 2 includes dense autoregressive models with improved architecture and\ntraining recipe, pretraining data mixtures, and instruction tuning recipes. Our\nmodified model architecture and training recipe achieve both better training\nstability and improved per-token efficiency. Our updated pretraining data\nmixture introduces a new, specialized data mix called Dolmino Mix 1124, which\nsignificantly improves model capabilities across many downstream task\nbenchmarks when introduced via late-stage curriculum training (i.e. specialized\ndata during the annealing phase of pretraining). Finally, we incorporate best\npractices from T\\\"ulu 3 to develop OLMo 2-Instruct, focusing on permissive data\nand extending our final-stage reinforcement learning with verifiable rewards\n(RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to\ncompute, often matching or outperforming open-weight only models like Llama 3.1\nand Qwen 2.5 while using fewer FLOPs and with fully transparent training data,\ncode, and recipe. Our fully open OLMo 2-Instruct models are competitive with or\nsurpassing open-weight only models of comparable size, including Qwen 2.5,\nLlama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B\nand 13B scales, both pretrained and post-trained, including their full training\ndata, training code and recipes, training logs and thousands of intermediate\ncheckpoints. The final instruction model is available on the Ai2 Playground as\na free research demo.\n","authors":["Team OLMo","Pete Walsh","Luca Soldaini","Dirk Groeneveld","Kyle Lo","Shane Arora","Akshita Bhagia","Yuling Gu","Shengyi Huang","Matt Jordan","Nathan Lambert","Dustin Schwenk","Oyvind Tafjord","Taira Anderson","David Atkinson","Faeze Brahman","Christopher Clark","Pradeep Dasigi","Nouha Dziri","Michal Guerquin","Hamish Ivison","Pang Wei Koh","Jiacheng Liu","Saumya Malik","William Merrill","Lester James V. Miranda","Jacob Morrison","Tyler Murray","Crystal Nam","Valentina Pyatkin","Aman Rangapur","Michael Schmitz","Sam Skjonsberg","David Wadden","Christopher Wilhelm","Michael Wilson","Luke Zettlemoyer","Ali Farhadi","Noah A. Smith","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2501.00656v2.pdf","comment":"Model demo available at playground.allenai.org"},{"id":"http://arxiv.org/abs/2412.19228v2","updated":"2025-01-15T01:16:30Z","published":"2024-12-26T14:09:16Z","title":"Learning Cross-Domain Representations for Transferable Drug\n  Perturbations on Single-Cell Transcriptional Responses","summary":"  Phenotypic drug discovery has attracted widespread attention because of its\npotential to identify bioactive molecules. Transcriptomic profiling provides a\ncomprehensive reflection of phenotypic changes in cellular responses to\nexternal perturbations. In this paper, we propose XTransferCDR, a novel\ngenerative framework designed for feature decoupling and transferable\nrepresentation learning across domains. Given a pair of perturbed expression\nprofiles, our approach decouples the perturbation representations from basal\nstates through domain separation encoders and then cross-transfers them in the\nlatent space. The transferred representations are then used to reconstruct the\ncorresponding perturbed expression profiles via a shared decoder. This\ncross-transfer constraint effectively promotes the learning of transferable\ndrug perturbation representations. We conducted extensive evaluations of our\nmodel on multiple datasets, including single-cell transcriptional responses to\ndrugs and single- and combinatorial genetic perturbations. The experimental\nresults show that XTransferCDR achieved better performance than current\nstate-of-the-art methods, showcasing its potential to advance phenotypic drug\ndiscovery.\n","authors":["Hui Liu","Shikai Jin"],"pdf_url":"https://arxiv.org/pdf/2412.19228v2.pdf","comment":"Accepted by The 39th Annual AAAI Conference on Artificial Intelligenc\n  (AAAI 2025)"},{"id":"http://arxiv.org/abs/2501.08508v1","updated":"2025-01-15T01:10:59Z","published":"2025-01-15T01:10:59Z","title":"Score-based 3D molecule generation with neural fields","summary":"  We introduce a new representation for 3D molecules based on their continuous\natomic density fields. Using this representation, we propose a new model based\non walk-jump sampling for unconditional 3D molecule generation in the\ncontinuous space using neural fields. Our model, FuncMol, encodes molecular\nfields into latent codes using a conditional neural field, samples noisy codes\nfrom a Gaussian-smoothed distribution with Langevin MCMC (walk), denoises these\nsamples in a single step (jump), and finally decodes them into molecular\nfields. FuncMol performs all-atom generation of 3D molecules without\nassumptions on the molecular structure and scales well with the size of\nmolecules, unlike most approaches. Our method achieves competitive results on\ndrug-like molecules and easily scales to macro-cyclic peptides, with at least\none order of magnitude faster sampling. The code is available at\nhttps://github.com/prescient-design/funcmol.\n","authors":["Matthieu Kirchmeyer","Pedro O. Pinheiro","Saeed Saremi"],"pdf_url":"https://arxiv.org/pdf/2501.08508v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2501.08506v1","updated":"2025-01-15T00:56:59Z","published":"2025-01-15T00:56:59Z","title":"Exploring the Efficacy of Meta-Learning: Unveiling Superior Data\n  Diversity Utilization of MAML Over Pre-training","summary":"  Currently, data and model size dominate the narrative in the training of\nsuper-large, powerful models. However, there has been a lack of exploration on\nthe effect of other attributes of the training dataset on model performance. We\nhypothesize that dataset diversity can impact the performance of vision models.\nOur study shows positive correlations between test set accuracy and data\ndiversity, providing an argument for furthering the research of dataset\nattributes beyond size. We analyzed pre-training and model-agnostic\nmeta-learning methods on twelve popular visual datasets (e.g., Omniglot,\nCIFAR-FS, Aircraft) and five model configurations, including MAML variants with\ndifferent numbers of inner gradient steps and supervised learning. We show\nmoderate to strong positive correlations (R-squared: 0.15-0.42) between\naccuracy and data diversity and weaker but significant correlations (R-squared:\n~0.2) between loss and diversity. These findings support our hypothesis and\ndemonstrate a promising way for a deeper exploration of how formal data\ndiversity influences model performance. This initial study highlights the\npotential of (Task2Vec) data diversity as a valuable measure in the rapidly\nevolving field of large-scale learning and emphasizes that understanding the\ndataset is key to building more powerful and generalizable models.\n","authors":["Kavita Selva","Satita Vittayaareekul","Brando Miranda"],"pdf_url":"https://arxiv.org/pdf/2501.08506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08504v1","updated":"2025-01-15T00:54:12Z","published":"2025-01-15T00:54:12Z","title":"SuperSAM: Crafting a SAM Supernetwork via Structured Pruning and\n  Unstructured Parameter Prioritization","summary":"  Neural Architecture Search (NAS) is a powerful approach of automating the\ndesign of efficient neural architectures. In contrast to traditional NAS\nmethods, recently proposed one-shot NAS methods prove to be more efficient in\nperforming NAS. One-shot NAS works by generating a singular weight-sharing\nsupernetwork that acts as a search space (container) of subnetworks. Despite\nits achievements, designing the one-shot search space remains a major\nchallenge. In this work we propose a search space design strategy for Vision\nTransformer (ViT)-based architectures. In particular, we convert the Segment\nAnything Model (SAM) into a weight-sharing supernetwork called SuperSAM. Our\napproach involves automating the search space design via layer-wise structured\npruning and parameter prioritization. While the structured pruning applies\nprobabilistic removal of certain transformer layers, parameter prioritization\nperforms weight reordering and slicing of MLP-blocks in the remaining layers.\nWe train supernetworks on several datasets using the sandwich rule. For\ndeployment, we enhance subnetwork discovery by utilizing a program autotuner to\nidentify efficient subnetworks within the search space. The resulting\nsubnetworks are 30-70% smaller in size compared to the original pre-trained SAM\nViT-B, yet outperform the pretrained model. Our work introduces a new and\neffective method for ViT NAS search-space design.\n","authors":["Waqwoya Abebe","Sadegh Jafari","Sixing Yu","Akash Dutta","Jan Strube","Nathan R. Tallent","Luanzheng Guo","Pablo Munoz","Ali Jannesari"],"pdf_url":"https://arxiv.org/pdf/2501.08504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14762v3","updated":"2025-01-15T00:53:38Z","published":"2024-11-22T06:50:44Z","title":"Efficient Long Video Tokenization via Coordinate-based Patch\n  Reconstruction","summary":"  Efficient tokenization of videos remains a challenge in training vision\nmodels that can process long videos. One promising direction is to develop a\ntokenizer that can encode long video clips, as it would enable the tokenizer to\nleverage the temporal coherence of videos better for tokenization. However,\ntraining existing tokenizers on long videos often incurs a huge training cost\nas they are trained to reconstruct all the frames at once. In this paper, we\nintroduce CoordTok, a video tokenizer that learns a mapping from\ncoordinate-based representations to the corresponding patches of input videos,\ninspired by recent advances in 3D generative models. In particular, CoordTok\nencodes a video into factorized triplane representations and reconstructs\npatches that correspond to randomly sampled $(x,y,t)$ coordinates. This allows\nfor training large tokenizer models directly on long videos without requiring\nexcessive training resources. Our experiments show that CoordTok can\ndrastically reduce the number of tokens for encoding long video clips. For\ninstance, CoordTok can encode a 128-frame video with 128$\\times$128 resolution\ninto 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar\nreconstruction quality. We further show that this efficient video tokenization\nenables memory-efficient training of a diffusion transformer that can generate\n128 frames at once.\n","authors":["Huiwon Jang","Sihyun Yu","Jinwoo Shin","Pieter Abbeel","Younggyo Seo"],"pdf_url":"https://arxiv.org/pdf/2411.14762v3.pdf","comment":"Code is available on the project webpage:\n  https://huiwon-jang.github.io/coordtok/"},{"id":"http://arxiv.org/abs/2501.08501v1","updated":"2025-01-15T00:38:13Z","published":"2025-01-15T00:38:13Z","title":"Scalable Bayesian Physics-Informed Kolmogorov-Arnold Networks","summary":"  Uncertainty quantification (UQ) plays a pivotal role in scientific machine\nlearning, especially when surrogate models are used to approximate complex\nsystems. Although multilayer perceptions (MLPs) are commonly employed as\nsurrogates, they often suffer from overfitting due to their large number of\nparameters. Kolmogorov-Arnold networks (KANs) offer an alternative solution\nwith fewer parameters. However, gradient-based inference methods, such as\nHamiltonian Monte Carlo (HMC), may result in computational inefficiency when\napplied to KANs, especially for large-scale datasets, due to the high cost of\nback-propagation.To address these challenges, we propose a novel approach,\ncombining the dropout Tikhonov ensemble Kalman inversion (DTEKI) with Chebyshev\nKANs. This gradient-free method effectively mitigates overfitting and enhances\nnumerical stability. Additionally, we incorporate the active subspace method to\nreduce the parameter-space dimensionality, allowing us to improve the accuracy\nof predictions and obtain more reliable uncertainty estimates.Extensive\nexperiments demonstrate the efficacy of our approach in various test cases,\nincluding scenarios with large datasets and high noise levels. Our results show\nthat the new method achieves comparable or better accuracy, much higher\nefficiency as well as stability compared to HMC, in addition to scalability.\nMoreover, by leveraging the low-dimensional parameter subspace, our method\npreserves prediction accuracy while substantially reducing further the\ncomputational cost.\n","authors":["Zhiwei Gao","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2501.08501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14340v2","updated":"2025-01-15T00:02:00Z","published":"2024-12-18T21:17:02Z","title":"A Unifying Information-theoretic Perspective on Evaluating Generative\n  Models","summary":"  Considering the difficulty of interpreting generative model output, there is\nsignificant current research focused on determining meaningful evaluation\nmetrics. Several recent approaches utilize \"precision\" and \"recall,\" borrowed\nfrom the classification domain, to individually quantify the output fidelity\n(realism) and output diversity (representation of the real data variation),\nrespectively. With the increase in metric proposals, there is a need for a\nunifying perspective, allowing for easier comparison and clearer explanation of\ntheir benefits and drawbacks. To this end, we unify a class of\nkth-nearest-neighbors (kNN)-based metrics under an information-theoretic lens\nusing approaches from kNN density estimation. Additionally, we propose a\ntri-dimensional metric composed of Precision Cross-Entropy (PCE), Recall\nCross-Entropy (RCE), and Recall Entropy (RE), which separately measure fidelity\nand two distinct aspects of diversity, inter- and intra-class. Our\ndomain-agnostic metric, derived from the information-theoretic concepts of\nentropy and cross-entropy, can be dissected for both sample- and mode-level\nanalysis. Our detailed experimental results demonstrate the sensitivity of our\nmetric components to their respective qualities and reveal undesirable\nbehaviors of other metrics.\n","authors":["Alexis Fox","Samarth Swarup","Abhijin Adiga"],"pdf_url":"https://arxiv.org/pdf/2412.14340v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15473v2","updated":"2025-01-15T23:11:07Z","published":"2024-12-20T01:05:23Z","title":"Predicting Long-Term Student Outcomes from Short-Term EdTech Log Data","summary":"  Educational stakeholders are often particularly interested in sparse, delayed\nstudent outcomes, like end-of-year statewide exams. The rare occurrence of such\nassessments makes it harder to identify students likely to fail such\nassessments, as well as making it slow for researchers and educators to be able\nto assess the effectiveness of particular educational tools. Prior work has\nprimarily focused on using logs from students full usage (e.g. year-long) of an\neducational product to predict outcomes, or considered predictive accuracy\nusing a few minutes to predict outcomes after a short (e.g. 1 hour) session. In\ncontrast, we investigate machine learning predictors using students' logs\nduring their first few hours of usage can provide useful predictive insight\ninto those students' end-of-school year external assessment. We do this on\nthree diverse datasets: from students in Uganda using a literacy game product,\nand from students in the US using two mathematics intelligent tutoring systems.\nWe consider various measures of the accuracy of the resulting predictors,\nincluding its ability to identify students at different parts along the\nassessment performance distribution. Our findings suggest that short-term log\nusage data, from 2-5 hours, can be used to provide valuable signal about\nstudents' long-term external performance.\n","authors":["Ge Gao","Amelia Leon","Andrea Jetten","Jasmine Turner","Husni Almoubayyed","Stephen Fancsali","Emma Brunskill"],"pdf_url":"https://arxiv.org/pdf/2412.15473v2.pdf","comment":"Accepted to the 15th International Learning Analytics and Knowledge\n  Conference (LAK2025)"},{"id":"http://arxiv.org/abs/2412.07051v3","updated":"2025-01-15T22:50:44Z","published":"2024-12-09T23:22:15Z","title":"A Misclassification Network-Based Method for Comparative Genomic\n  Analysis","summary":"  Classifying genome sequences based on metadata has been an active area of\nresearch in comparative genomics for decades with many important applications\nacross the life sciences. Established methods for classifying genomes can be\nbroadly grouped into sequence alignment-based and alignment-free models.\nConventional alignment-based models rely on genome similarity measures\ncalculated based on local sequence alignments or consistent ordering among\nsequences. However, such methods are computationally expensive when dealing\nwith large ensembles of even moderately sized genomes. In contrast,\nalignment-free (AF) approaches measure genome similarity based on summary\nstatistics in an unsupervised setting and are efficient enough to analyze large\ndatasets. However, both alignment-based and AF methods typically assume fixed\nscoring rubrics that lack the flexibility to assign varying importance to\ndifferent parts of the sequences based on prior knowledge. In this study, we\nintegrate AI and network science approaches to develop a comparative genomic\nanalysis framework that addresses these limitations. Our approach, termed the\nGenome Misclassification Network Analysis (GMNA), simultaneously leverages\nmisclassified instances, a learned scoring rubric, and label information to\nclassify genomes based on associated metadata and better understand potential\ndrivers of misclassification. We evaluate the utility of the GMNA using Naive\nBayes and convolutional neural network models, supplemented by additional\nexperiments with transformer-based models, to construct SARS-CoV-2 sampling\nlocation classifiers using over 500,000 viral genome sequences and study the\nresulting network of misclassifications. We demonstrate the global health\npotential of the GMNA by leveraging the SARS-CoV-2 genome misclassification\nnetworks to investigate the role human mobility played in structuring\ngeographic clustering of SARS-CoV-2.\n","authors":["Wan He","Tina Eliassi-Rad","Samuel V. Scarpino"],"pdf_url":"https://arxiv.org/pdf/2412.07051v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09189v1","updated":"2025-01-15T22:33:55Z","published":"2025-01-15T22:33:55Z","title":"Testing Noise Assumptions of Learning Algorithms","summary":"  We pose a fundamental question in computational learning theory: can we\nefficiently test whether a training set satisfies the assumptions of a given\nnoise model? This question has remained unaddressed despite decades of research\non learning in the presence of noise. In this work, we show that this task is\ntractable and present the first efficient algorithm to test various noise\nassumptions on the training data.\n  To model this question, we extend the recently proposed testable learning\nframework of Rubinfeld and Vasilyan (2023) and require a learner to run an\nassociated test that satisfies the following two conditions: (1) whenever the\ntest accepts, the learner outputs a classifier along with a certificate of\noptimality, and (2) the test must pass for any dataset drawn according to a\nspecified modeling assumption on both the marginal distribution and the noise\nmodel. We then consider the problem of learning halfspaces over Gaussian\nmarginals with Massart noise (where each label can be flipped with probability\nless than $1/2$ depending on the input features), and give a fully-polynomial\ntime testable learning algorithm.\n  We also show a separation between the classical setting of learning in the\npresence of structured noise and testable learning. In fact, for the simple\ncase of random classification noise (where each label is flipped with fixed\nprobability $\\eta = 1/2$), we show that testable learning requires\nsuper-polynomial time while classical learning is trivial.\n","authors":["Surbhi Goel","Adam R. Klivans","Konstantinos Stavropoulos","Arsen Vasilyan"],"pdf_url":"https://arxiv.org/pdf/2501.09189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09187v1","updated":"2025-01-15T22:26:26Z","published":"2025-01-15T22:26:26Z","title":"Patch-aware Vector Quantized Codebook Learning for Unsupervised Visual\n  Defect Detection","summary":"  Unsupervised visual defect detection is critical in industrial applications,\nrequiring a representation space that captures normal data features while\ndetecting deviations. Achieving a balance between expressiveness and\ncompactness is challenging; an overly expressive space risks inefficiency and\nmode collapse, impairing detection accuracy. We propose a novel approach using\nan enhanced VQ-VAE framework optimized for unsupervised defect detection. Our\nmodel introduces a patch-aware dynamic code assignment scheme, enabling\ncontext-sensitive code allocation to optimize spatial representation. This\nstrategy enhances normal-defect distinction and improves detection accuracy\nduring inference. Experiments on MVTecAD, BTAD, and MTSD datasets show our\nmethod achieves state-of-the-art performance.\n","authors":["Qisen Cheng","Shuhui Qu","Janghwan Lee"],"pdf_url":"https://arxiv.org/pdf/2501.09187v1.pdf","comment":"7 pages, Accepted to 36th IEEE ICTAI 2024"},{"id":"http://arxiv.org/abs/2501.09178v1","updated":"2025-01-15T22:12:27Z","published":"2025-01-15T22:12:27Z","title":"Enhancing Graph Representation Learning with Localized Topological\n  Features","summary":"  Representation learning on graphs is a fundamental problem that can be\ncrucial in various tasks. Graph neural networks, the dominant approach for\ngraph representation learning, are limited in their representation power.\nTherefore, it can be beneficial to explicitly extract and incorporate\nhigh-order topological and geometric information into these models. In this\npaper, we propose a principled approach to extract the rich connectivity\ninformation of graphs based on the theory of persistent homology. Our method\nutilizes the topological features to enhance the representation learning of\ngraph neural networks and achieve state-of-the-art performance on various node\nclassification and link prediction benchmarks. We also explore the option of\nend-to-end learning of the topological features, i.e., treating topological\ncomputation as a differentiable operator during learning. Our theoretical\nanalysis and empirical study provide insights and potential guidelines for\nemploying topological features in graph learning tasks.\n","authors":["Zuoyu Yan","Qi Zhao","Ze Ye","Tengfei Ma","Liangcai Gao","Zhi Tang","Yusu Wang","Chao Chen"],"pdf_url":"https://arxiv.org/pdf/2501.09178v1.pdf","comment":"Accepted in JMLR 2025"},{"id":"http://arxiv.org/abs/2501.09171v1","updated":"2025-01-15T21:46:01Z","published":"2025-01-15T21:46:01Z","title":"Generative AI Takes a Statistics Exam: A Comparison of Performance\n  between ChatGPT3.5, ChatGPT4, and ChatGPT4o-mini","summary":"  Many believe that use of generative AI as a private tutor has the potential\nto shrink access and achievement gaps between students and schools with\nabundant resources versus those with fewer resources. Shrinking the gap is\npossible only if paid and free versions of the platforms perform with the same\naccuracy. In this experiment, we investigate the performance of GPT versions\n3.5, 4.0, and 4o-mini on the same 16-question statistics exam given to a class\nof first-year graduate students. While we do not advocate using any generative\nAI platform to complete an exam, the use of exam questions allows us to explore\naspects of ChatGPT's responses to typical questions that students might\nencounter in a statistics course. Results on accuracy indicate that GPT 3.5\nwould fail the exam, GPT4 would perform well, and GPT4o-mini would perform\nsomewhere in between. While we acknowledge the existence of other Generative\nAI/LLMs, our discussion concerns only ChatGPT because it is the most widely\nused platform on college campuses at this time. We further investigate\ndifferences among the AI platforms in the answers for each problem using\nmethods developed for text analytics, such as reading level evaluation and\ntopic modeling. Results indicate that GPT3.5 and 4o-mini have characteristics\nthat are more similar than either of them have with GPT4.\n","authors":["Monnie McGee","Bivin Sadler"],"pdf_url":"https://arxiv.org/pdf/2501.09171v1.pdf","comment":"24 pages, 2 figures, 3 tables. Submitted for publication August,\n  2024; revision submitted January 2025"},{"id":"http://arxiv.org/abs/2501.09166v1","updated":"2025-01-15T21:33:53Z","published":"2025-01-15T21:33:53Z","title":"Attention is All You Need Until You Need Retention","summary":"  This work introduces a novel Retention Layer mechanism for Transformer based\narchitectures, addressing their inherent lack of intrinsic retention\ncapabilities. Unlike human cognition, which can encode and dynamically recall\nsymbolic templates, Generative Pretrained Transformers rely solely on fixed\npretrained weights and ephemeral context windows, limiting their adaptability.\nThe proposed Retention Layer incorporates a persistent memory module capable of\nreal time data population, dynamic recall, and guided output generation. This\nenhancement allows models to store, update, and reuse observed patterns across\nsessions, enabling incremental learning and bridging the gap between static\npretraining and dynamic, context sensitive adaptation. The Retention Layer\ndesign parallels social learning processes, encompassing attention, retention,\nreproduction, and motivation stages. Technically, it integrates a memory\nattention mechanism and episodic buffers to manage memory scalability, mitigate\noverfitting, and ensure efficient recall. Applications span adaptive personal\nassistants, real time fraud detection, autonomous robotics, content moderation,\nand healthcare diagnostics. In each domain, the retention mechanism enables\nsystems to learn incrementally, personalize outputs, and respond to evolving\nreal world challenges effectively. By emulating key aspects of human learning,\nthis retention enhanced architecture fosters a more fluid and responsive AI\nparadigm, paving the way for dynamic, session aware models that extend the\ncapabilities of traditional Transformers into domains requiring continual\nadaptation.\n","authors":["M. Murat Yaslioglu"],"pdf_url":"https://arxiv.org/pdf/2501.09166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09163v1","updated":"2025-01-15T21:29:29Z","published":"2025-01-15T21:29:29Z","title":"Towards Understanding Extrapolation: a Causal Lens","summary":"  Canonical work handling distribution shifts typically necessitates an entire\ntarget distribution that lands inside the training distribution. However,\npractical scenarios often involve only a handful of target samples, potentially\nlying outside the training support, which requires the capability of\nextrapolation. In this work, we aim to provide a theoretical understanding of\nwhen extrapolation is possible and offer principled methods to achieve it\nwithout requiring an on-support target distribution. To this end, we formulate\nthe extrapolation problem with a latent-variable model that embodies the\nminimal change principle in causal mechanisms. Under this formulation, we cast\nthe extrapolation problem into a latent-variable identification problem. We\nprovide realistic conditions on shift properties and the estimation objectives\nthat lead to identification even when only one off-support target sample is\navailable, tackling the most challenging scenarios. Our theory reveals the\nintricate interplay between the underlying manifold's smoothness and the shift\nproperties. We showcase how our theoretical results inform the design of\npractical adaptation algorithms. Through experiments on both synthetic and\nreal-world data, we validate our theoretical findings and their practical\nimplications.\n","authors":["Lingjing Kong","Guangyi Chen","Petar Stojanov","Haoxuan Li","Eric P. Xing","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.09163v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2501.09160v1","updated":"2025-01-15T21:22:09Z","published":"2025-01-15T21:22:09Z","title":"AutoLoop: Fast Visual SLAM Fine-tuning through Agentic Curriculum\n  Learning","summary":"  Current visual SLAM systems face significant challenges in balancing\ncomputational efficiency with robust loop closure handling. Traditional\napproaches require careful manual tuning and incur substantial computational\noverhead, while learning-based methods either lack explicit loop closure\ncapabilities or implement them through computationally expensive methods. We\npresent AutoLoop, a novel approach that combines automated curriculum learning\nwith efficient fine-tuning for visual SLAM systems. Our method employs a DDPG\n(Deep Deterministic Policy Gradient) agent to dynamically adjust loop closure\nweights during training, eliminating the need for manual hyperparameter search\nwhile significantly reducing the required training steps. The approach\npre-computes potential loop closure pairs offline and leverages them through an\nagent-guided curriculum, allowing the model to adapt efficiently to new\nscenarios. Experiments conducted on TartanAir for training and validated across\nmultiple benchmarks including KITTI, EuRoC, ICL-NUIM and TUM RGB-D demonstrate\nthat AutoLoop achieves comparable or superior performance while reducing\ntraining time by an order of magnitude compared to traditional approaches.\nAutoLoop provides a practical solution for rapid adaptation of visual SLAM\nsystems, automating the weight tuning process that traditionally requires\nmultiple manual iterations. Our results show that this automated curriculum\nstrategy not only accelerates training but also maintains or improves the\nmodel's performance across diverse environmental conditions.\n","authors":["Assaf Lahiany","Oren Gal"],"pdf_url":"https://arxiv.org/pdf/2501.09160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10966v2","updated":"2025-01-15T21:20:03Z","published":"2024-12-14T20:54:37Z","title":"FlowDock: Geometric Flow Matching for Generative Protein-Ligand Docking\n  and Affinity Prediction","summary":"  Powerful generative AI models of protein-ligand structure have recently been\nproposed, but few of these methods support both flexible protein-ligand docking\nand affinity estimation. Of those that do, none can directly model multiple\nbinding ligands concurrently or have been rigorously benchmarked on\npharmacologically relevant drug targets, hindering their widespread adoption in\ndrug discovery efforts. In this work, we propose FlowDock, the first deep\ngeometric generative model based on conditional flow matching that learns to\ndirectly map unbound (apo) structures to their bound (holo) counterparts for an\narbitrary number of binding ligands. Furthermore, FlowDock provides predicted\nstructural confidence scores and binding affinity values with each of its\ngenerated protein-ligand complex structures, enabling fast virtual screening of\nnew (multi-ligand) drug targets. For the well-known PoseBusters Benchmark\ndataset, FlowDock outperforms single-sequence AlphaFold 3 with a 51% blind\ndocking success rate using unbound (apo) protein input structures and without\nany information derived from multiple sequence alignments, and for the\nchallenging new DockGen-E dataset, FlowDock outperforms single-sequence\nAlphaFold 3 and matches single-sequence Chai-1 for binding pocket\ngeneralization. Additionally, in the ligand category of the 16th community-wide\nCritical Assessment of Techniques for Structure Prediction (CASP16), FlowDock\nranked among the top-5 methods for pharmacological binding affinity estimation\nacross 140 protein-ligand complexes, demonstrating the efficacy of its learned\nrepresentations in virtual screening. Source code, data, and pre-trained models\nare available at https://github.com/BioinfoMachineLearning/FlowDock.\n","authors":["Alex Morehead","Jianlin Cheng"],"pdf_url":"https://arxiv.org/pdf/2412.10966v2.pdf","comment":"10 pages, 2 tables, 2 algorithms, 7 figures. Code, data, pre-trained\n  models, and baseline method predictions are available at\n  https://github.com/BioinfoMachineLearning/FlowDock"},{"id":"http://arxiv.org/abs/2404.10845v2","updated":"2025-01-15T21:09:22Z","published":"2024-04-16T18:47:07Z","title":"Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs","summary":"  This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions.\n","authors":["Amit Kumar Bhuyan","Hrishikesh Dutta","Subir Biswas"],"pdf_url":"https://arxiv.org/pdf/2404.10845v2.pdf","comment":"16 pages, 8 figures, 2 algorithms, 2 tables, journal"},{"id":"http://arxiv.org/abs/2501.09146v1","updated":"2025-01-15T20:55:13Z","published":"2025-01-15T20:55:13Z","title":"Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs","summary":"  This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.\n","authors":["Amit Kumar Bhuyan","Hrishikesh Dutta","Subir Biswas"],"pdf_url":"https://arxiv.org/pdf/2501.09146v1.pdf","comment":"25 pages, 11 figures, 1 table, 4 algorithms, journal"},{"id":"http://arxiv.org/abs/2302.13336v2","updated":"2025-01-15T20:50:17Z","published":"2023-02-26T15:45:19Z","title":"Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early\n  Knee Osteoarthritis Detection","summary":"  Knee Osteoarthritis (KOA) is a common musculoskeletal condition that\nsignificantly affects mobility and quality of life, particularly in elderly\npopulations. However, training deep learning models for early KOA\nclassification is often hampered by the limited availability of annotated\nmedical datasets, owing to the high costs and labour-intensive nature of data\nlabelling. Traditional data augmentation techniques, while useful, rely on\nsimple transformations and fail to introduce sufficient diversity into the\ndataset. To address these challenges, we propose the Key-Exchange Convolutional\nAuto-Encoder (KECAE) as an innovative Artificial Intelligence (AI)-based data\naugmentation strategy for early KOA classification. Our model employs a\nconvolutional autoencoder with a novel key-exchange mechanism that generates\nsynthetic images by selectively exchanging key pathological features between\nX-ray images, which not only diversifies the dataset but also ensures the\nclinical validity of the augmented data. A hybrid loss function is introduced\nto supervise feature learning and reconstruction, integrating multiple\ncomponents, including reconstruction, supervision, and feature separation\nlosses. Experimental results demonstrate that the KECAE-generated data\nsignificantly improve the performance of KOA classification models, with\naccuracy gains of up to 1.98% across various standard and state-of-the-art\narchitectures. Furthermore, a clinical validation study involving expert\nradiologists confirms the anatomical plausibility and diagnostic realism of the\nsynthetic outputs. These findings highlight the potential of KECAE as a robust\ntool for augmenting medical datasets in early KOA detection.\n","authors":["Zhe Wang","Aladine Chetouani","Mohamed Jarraya","Yung Hsin Chen","Yuhua Ru","Fang Chen","Fabian Bauer","Liping Zhang","Didier Hans","Rachid Jennane"],"pdf_url":"https://arxiv.org/pdf/2302.13336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09137v1","updated":"2025-01-15T20:43:36Z","published":"2025-01-15T20:43:36Z","title":"Gradient Descent Converges Linearly to Flatter Minima than Gradient Flow\n  in Shallow Linear Networks","summary":"  We study the gradient descent (GD) dynamics of a depth-2 linear neural\nnetwork with a single input and output. We show that GD converges at an\nexplicit linear rate to a global minimum of the training loss, even with a\nlarge stepsize -- about $2/\\textrm{sharpness}$. It still converges for even\nlarger stepsizes, but may do so very slowly. We also characterize the solution\nto which GD converges, which has lower norm and sharpness than the gradient\nflow solution. Our analysis reveals a trade off between the speed of\nconvergence and the magnitude of implicit regularization. This sheds light on\nthe benefits of training at the ``Edge of Stability'', which induces additional\nregularization by delaying convergence and may have implications for training\nmore complex models.\n","authors":["Pierfrancesco Beneventano","Blake Woodworth"],"pdf_url":"https://arxiv.org/pdf/2501.09137v1.pdf","comment":"23 pages, 3 figures"},{"id":"http://arxiv.org/abs/2209.10825v4","updated":"2025-01-15T20:43:18Z","published":"2022-09-22T07:12:48Z","title":"Nonsmooth Nonconvex-Nonconcave Minimax Optimization: Primal-Dual\n  Balancing and Iteration Complexity Analysis","summary":"  Nonconvex-nonconcave minimax optimization has gained widespread interest over\nthe last decade. However, most existing works focus on variants of gradient\ndescent-ascent (GDA) algorithms, which are only applicable to smooth\nnonconvex-concave settings. To address this limitation, we propose a novel\nalgorithm named smoothed proximal linear descent-ascent (smoothed PLDA), which\ncan effectively handle a broad range of structured nonsmooth\nnonconvex-nonconcave minimax problems. Specifically, we consider the setting\nwhere the primal function has a nonsmooth composite structure and the dual\nfunction possesses the Kurdyka-Lojasiewicz (KL) property with exponent $\\theta\n\\in [0,1)$. We introduce a novel convergence analysis framework for smoothed\nPLDA, the key components of which are our newly developed nonsmooth primal\nerror bound and dual error bound. Using this framework, we show that smoothed\nPLDA can find both $\\epsilon$-game-stationary points and\n$\\epsilon$-optimization-stationary points of the problems of interest in\n$\\mathcal{O}(\\epsilon^{-2\\max\\{2\\theta,1\\}})$ iterations. Furthermore, when\n$\\theta \\in [0,\\frac{1}{2}]$, smoothed PLDA achieves the optimal iteration\ncomplexity of $\\mathcal{O}(\\epsilon^{-2})$. To further demonstrate the\neffectiveness and wide applicability of our analysis framework, we show that\ncertain max-structured problem possesses the KL property with exponent\n$\\theta=0$ under mild assumptions. As a by-product, we establish\nalgorithm-independent quantitative relationships among various stationarity\nconcepts, which may be of independent interest.\n","authors":["Jiajin Li","Linglingzhi Zhu","Anthony Man-Cho So"],"pdf_url":"https://arxiv.org/pdf/2209.10825v4.pdf","comment":"Accepted for publication in Mathematical Programming"}],"Multimedia":[{"id":"http://arxiv.org/abs/2501.09012v1","updated":"2025-01-15T18:56:22Z","published":"2025-01-15T18:56:22Z","title":"Multimodal LLMs Can Reason about Aesthetics in Zero-Shot","summary":"  We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability\nshall be elicited to evaluate the aesthetics of artworks. To facilitate this\ninvestigation, we construct MM-StyleBench, a novel high-quality dataset for\nbenchmarking artistic stylization. We then develop a principled method for\nhuman preference modeling and perform a systematic correlation analysis between\nMLLMs' responses and human preference. Our experiments reveal an inherent\nhallucination issue of MLLMs in art evaluation, associated with response\nsubjectivity. ArtCoT is proposed, demonstrating that art-specific task\ndecomposition and the use of concrete language boost MLLMs' reasoning ability\nfor aesthetics. Our findings offer valuable insights into MLLMs for art and can\nbenefit a wide range of downstream applications, such as style transfer and\nartistic image generation. Code available at\nhttps://github.com/songrise/MLLM4Art.\n","authors":["Ruixiang Jiang","Changwen Chen"],"pdf_url":"https://arxiv.org/pdf/2501.09012v1.pdf","comment":"WIP, Homepage https://github.com/songrise/MLLM4Art"},{"id":"http://arxiv.org/abs/2412.11409v3","updated":"2025-01-15T01:59:02Z","published":"2024-12-16T03:25:23Z","title":"Multi-modal and Multi-scale Spatial Environment Understanding for\n  Immersive Visual Text-to-Speech","summary":"  Visual Text-to-Speech (VTTS) aims to take the environmental image as the\nprompt to synthesize the reverberant speech for the spoken content. The\nchallenge of this task lies in understanding the spatial environment from the\nimage. Many attempts have been made to extract global spatial visual\ninformation from the RGB space of an spatial image. However, local and depth\nimage information are crucial for understanding the spatial environment, which\nprevious works have ignored. To address the issues, we propose a novel\nmulti-modal and multi-scale spatial environment understanding scheme to achieve\nimmersive VTTS, termed M2SE-VTTS. The multi-modal aims to take both the RGB and\nDepth spaces of the spatial image to learn more comprehensive spatial\ninformation, and the multi-scale seeks to model the local and global spatial\nknowledge simultaneously. Specifically, we first split the RGB and Depth images\ninto patches and adopt the Gemini-generated environment captions to guide the\nlocal spatial understanding. After that, the multi-modal and multi-scale\nfeatures are integrated by the local-aware global spatial understanding. In\nthis way, M2SE-VTTS effectively models the interactions between local and\nglobal spatial contexts in the multi-modal spatial environment. Objective and\nsubjective evaluations suggest that our model outperforms the advanced\nbaselines in environmental speech generation. The code and audio samples are\navailable at: https://github.com/AI-S2-Lab/M2SE-VTTS.\n","authors":["Rui Liu","Shuwei He","Yifan Hu","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2412.11409v3.pdf","comment":"9 pages,2 figures, Accepted by AAAI'2025"},{"id":"http://arxiv.org/abs/2501.08514v1","updated":"2025-01-15T01:52:54Z","published":"2025-01-15T01:52:54Z","title":"Multimodal Fake News Video Explanation Generation","summary":"  Multi-modal explanation involves the assessment of the veracity of a variety\nof different content, and relies on multiple information modalities to\ncomprehensively consider the relevance and consistency between modalities. Most\nexisting fake news video detection methods focus on improving accuracy while\nignoring the importance of providing explanations. In this paper, we propose a\nnovel problem - Fake News Video Explanation (FNVE) - Given a multimodal news\ncontaining both video and caption text, we aim to generate natural language\nexplanations to reveal the truth of predictions. To this end, we develop\nFakeNVE, a new dataset of explanations for truthfully multimodal posts, where\neach explanation is a natural language (English) sentence describing the\nattribution of a news thread. We benchmark FakeNVE by using a multimodal\ntransformer-based architecture. Subsequently, a BART-based autoregressive\ndecoder is used as the generator. Empirical results show compelling results for\nvarious baselines (applicable to FNVE) across multiple evaluation metrics. We\nalso perform human evaluation on explanation generation, achieving high scores\nfor both adequacy and fluency.\n","authors":["Lizhi Chen","Zhong Qian","Peifeng Li","Qiaoming Zhu"],"pdf_url":"https://arxiv.org/pdf/2501.08514v1.pdf","comment":null}]},"2025-01-16T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2501.09751v1","updated":"2025-01-16T18:58:06Z","published":"2025-01-16T18:58:06Z","title":"OmniThink: Expanding Knowledge Boundaries in Machine Writing through\n  Thinking","summary":"  Machine writing with large language models often relies on\nretrieval-augmented generation. However, these approaches remain confined\nwithin the boundaries of the model's predefined scope, limiting the generation\nof content with rich information. Specifically, vanilla-retrieved information\ntends to lack depth, utility, and suffers from redundancy, which negatively\nimpacts the quality of generated articles, leading to shallow, repetitive, and\nunoriginal outputs. To address these issues, we propose OmniThink, a machine\nwriting framework that emulates the human-like process of iterative expansion\nand reflection. The core idea behind OmniThink is to simulate the cognitive\nbehavior of learners as they progressively deepen their knowledge of the\ntopics. Experimental results demonstrate that OmniThink improves the knowledge\ndensity of generated articles without compromising metrics such as coherence\nand depth. Human evaluations and expert feedback further highlight the\npotential of OmniThink to address real-world challenges in the generation of\nlong-form articles.\n","authors":["Zekun Xi","Wenbiao Yin","Jizhan Fang","Jialong Wu","Runnan Fang","Ningyu Zhang","Jiang Yong","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2501.09751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09749v1","updated":"2025-01-16T18:57:20Z","published":"2025-01-16T18:57:20Z","title":"Enhancing Lexicon-Based Text Embeddings with Large Language Models","summary":"  Recent large language models (LLMs) have demonstrated exceptional performance\non general-purpose text embedding tasks. While dense embeddings have dominated\nrelated research, we introduce the first Lexicon-based EmbeddiNgS (LENS)\nleveraging LLMs that achieve competitive performance on these tasks. Regarding\nthe inherent tokenization redundancy issue and unidirectional attention\nlimitations in traditional causal LLMs, LENS consolidates the vocabulary space\nthrough token embedding clustering, and investigates bidirectional attention\nand various pooling strategies. Specifically, LENS simplifies lexicon matching\nby assigning each dimension to a specific token cluster, where semantically\nsimilar tokens are grouped together, and unlocking the full potential of LLMs\nthrough bidirectional attention. Extensive experiments demonstrate that LENS\noutperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB),\ndelivering compact feature representations that match the sizes of dense\ncounterparts. Notably, combining LENSE with dense embeddings achieves\nstate-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).\n","authors":["Yibin Lei","Tao Shen","Yu Cao","Andrew Yates"],"pdf_url":"https://arxiv.org/pdf/2501.09749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09745v1","updated":"2025-01-16T18:55:38Z","published":"2025-01-16T18:55:38Z","title":"Suggesting Code Edits in Interactive Machine Learning Notebooks Using\n  Large Language Models","summary":"  Machine learning developers frequently use interactive computational\nnotebooks, such as Jupyter notebooks, to host code for data processing and\nmodel training. Jupyter notebooks provide a convenient tool for writing machine\nlearning pipelines and interactively observing outputs, however, maintaining\nJupyter notebooks, e.g., to add new features or fix bugs, can be challenging\ndue to the length and complexity of the notebooks. Moreover, there is no\nexisting benchmark related to developer edits on Jupyter notebooks. To address\nthis, we present the first dataset of 48,398 Jupyter notebook edits derived\nfrom 20,095 revisions of 792 machine learning repositories on GitHub, and\nperform the first study of the using LLMs to predict code edits in Jupyter\nnotebooks. Our dataset captures granular details of cell-level and line-level\nmodifications, offering a foundation for understanding real-world maintenance\npatterns in machine learning workflows. We observed that the edits on Jupyter\nnotebooks are highly localized, with changes averaging only 166 lines of code\nin repositories. While larger models outperform smaller counterparts in code\nediting, all models have low accuracy on our dataset even after finetuning,\ndemonstrating the complexity of real-world machine learning maintenance tasks.\nOur findings emphasize the critical role of contextual information in improving\nmodel performance and point toward promising avenues for advancing large\nlanguage models' capabilities in engineering machine learning code.\n","authors":["Bihui Jin","Jiayue Wang","Pengyu Nie"],"pdf_url":"https://arxiv.org/pdf/2501.09745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09722v1","updated":"2025-01-16T18:10:37Z","published":"2025-01-16T18:10:37Z","title":"Attention based Bidirectional GRU hybrid model for inappropriate content\n  detection in Urdu language","summary":"  With the increased use of the internet and social networks for online\ndiscussions, the spread of toxic and inappropriate content on social networking\nsites has also increased. Several studies have been conducted in different\nlanguages. However, there is less work done for South Asian languages for\ninappropriate content identification using deep learning techniques. In Urdu\nlanguage, the spellings are not unique, and people write different common\nspellings for the same word, while mixing it other languages, like English in\nthe text makes it more challenging, and limited research work is available to\nprocess such language with the finest algorithms. The use of attention layer\nwith a deep learning model can help handling the long-term dependencies and\nincrease its efficiency . To explore the effects of the attention layer, this\nstudy proposes attention-based Bidirectional GRU hybrid model for identifying\ninappropriate content in Urdu Unicode text language. Four different baseline\ndeep learning models; LSTM, Bi-LSTM, GRU, and TCN, are used to compare the\nperformance of the proposed model. The results of these models were compared\nbased on evaluation metrics, dataset size, and impact of the word embedding\nlayer. The pre-trained Urdu word2Vec embeddings were utilized for our case. Our\nproposed model BiGRU-A outperformed all other baseline models by yielding 84\\%\naccuracy without using pre-trained word2Vec layer. From our experiments, we\nhave established that the attention layer improves the model's efficiency, and\npre-trained word2Vec embedding does not work well with an inappropriate content\ndataset.\n","authors":["Ezzah Shoukat","Rabia Irfan","Iqra Basharat","Muhammad Ali Tahir","Sameen Shaukat"],"pdf_url":"https://arxiv.org/pdf/2501.09722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02933v3","updated":"2025-01-16T18:08:22Z","published":"2024-04-03T01:09:41Z","title":"NL2KQL: From Natural Language to Kusto Query","summary":"  Data is growing rapidly in volume and complexity. Proficiency in database\nquery languages is pivotal for crafting effective queries. As coding assistants\nbecome more prevalent, there is significant opportunity to enhance database\nquery languages. The Kusto Query Language (KQL) is a widely used query language\nfor large semi-structured data such as logs, telemetries, and time-series for\nbig data analytics platforms. This paper introduces NL2KQL an innovative\nframework that uses large language models (LLMs) to convert natural language\nqueries (NLQs) to KQL queries. The proposed NL2KQL framework includes several\nkey components: Schema Refiner which narrows down the schema to its most\npertinent elements; the Few-shot Selector which dynamically selects relevant\nexamples from a few-shot dataset; and the Query Refiner which repairs syntactic\nand semantic errors in KQL queries. Additionally, this study outlines a method\nfor generating large datasets of synthetic NLQ-KQL pairs which are valid within\na specific database contexts. To validate NL2KQL's performance, we utilize an\narray of online (based on query execution) and offline (based on query parsing)\nmetrics. Through ablation studies, the significance of each framework component\nis examined, and the datasets used for benchmarking are made publicly\navailable. This work is the first of its kind and is compared with available\nbaselines to demonstrate its effectiveness.\n","authors":["Xinye Tang","Amir H. Abdi","Jeremias Eichelbaum","Mahan Das","Alex Klein","Nihal Irmak Pakis","William Blum","Daniel L Mace","Tanvi Raja","Namrata Padmanabhan","Ye Xing"],"pdf_url":"https://arxiv.org/pdf/2404.02933v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09719v1","updated":"2025-01-16T18:06:22Z","published":"2025-01-16T18:06:22Z","title":"Comparative Insights from 12 Machine Learning Models in Extracting\n  Economic Ideology from Political Text","summary":"  This study conducts a systematic assessment of the capabilities of 12 machine\nlearning models and model variations in detecting economic ideology. As an\nevaluation benchmark, I use manifesto data spanning six elections in the United\nKingdom and pre-annotated by expert and crowd coders. The analysis assesses the\nperformance of several generative, fine-tuned, and zero-shot models at the\ngranular and aggregate levels. The results show that generative models such as\nGPT-4o and Gemini 1.5 Flash consistently outperform other models against all\nbenchmarks. However, they pose issues of accessibility and resource\navailability. Fine-tuning yielded competitive performance and offers a reliable\nalternative through domain-specific optimization. But its dependency on\ntraining data severely limits scalability. Zero-shot models consistently face\ndifficulties with identifying signals of economic ideology, often resulting in\nnegative associations with human coding. Using general knowledge for the\ndomain-specific task of ideology scaling proved to be unreliable. Other key\nfindings include considerable within-party variation, fine-tuning benefiting\nfrom larger training data, and zero-shot's sensitivity to prompt content. The\nassessments include the strengths and limitations of each model and derive\nbest-practices for automated analyses of political content.\n","authors":["Jihed Ncib"],"pdf_url":"https://arxiv.org/pdf/2501.09719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09706v1","updated":"2025-01-16T17:58:32Z","published":"2025-01-16T17:58:32Z","title":"Domain Adaptation of Foundation LLMs for e-Commerce","summary":"  We present the e-Llama models: 8 billion and 70 billion parameter large\nlanguage models that are adapted towards the e-commerce domain. These models\nare meant as foundation models with deep knowledge about e-commerce, that form\na base for instruction- and fine-tuning. The e-Llama models are obtained by\ncontinuously pretraining the Llama 3.1 base models on 1 trillion tokens of\ndomain-specific data.\n  We discuss our approach and motivate our choice of hyperparameters with a\nseries of ablation studies. To quantify how well the models have been adapted\nto the e-commerce domain, we define and implement a set of multilingual,\ne-commerce specific evaluation tasks.\n  We show that, when carefully choosing the training setup, the Llama 3.1\nmodels can be adapted towards the new domain without sacrificing significant\nperformance on general domain tasks. We also explore the possibility of merging\nthe adapted model and the base model for a better control of the performance\ntrade-off between domains.\n","authors":["Christian Herold","Michael Kozielski","Tala Bazazo","Pavel Petrushkov","Hadi Hashemi","Patrycja Cieplicka","Dominika Basaj","Shahram Khadivi"],"pdf_url":"https://arxiv.org/pdf/2501.09706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09686v1","updated":"2025-01-16T17:37:58Z","published":"2025-01-16T17:37:58Z","title":"Towards Large Reasoning Models: A Survey of Reinforced Reasoning with\n  Large Language Models","summary":"  Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions.\n","authors":["Fengli Xu","Qianyue Hao","Zefang Zong","Jingwei Wang","Yunke Zhang","Jingyi Wang","Xiaochong Lan","Jiahui Gong","Tianjian Ouyang","Fanjin Meng","Chenyang Shao","Yuwei Yan","Qinglong Yang","Yiwen Song","Sijian Ren","Xinyuan Hu","Yu Li","Jie Feng","Chen Gao","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2501.09686v1.pdf","comment":"36 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.09653v1","updated":"2025-01-16T16:48:41Z","published":"2025-01-16T16:48:41Z","title":"The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating\n  Large Language Models","summary":"  The recent rise in the popularity of large language models has spurred the\ndevelopment of extensive code datasets needed to train them. This has left\nlimited code available for collection and use in the downstream investigation\nof specific behaviors, or evaluation of large language models without suffering\nfrom data contamination. To address this problem, we release The Heap, a large\nmultilingual dataset covering 57 programming languages that has been\ndeduplicated with respect to other open datasets of code, enabling researchers\nto conduct fair evaluations of large language models without significant data\ncleaning overhead.\n","authors":["Jonathan Katzy","Razvan Mihai Popescu","Arie van Deursen","Maliheh Izadi"],"pdf_url":"https://arxiv.org/pdf/2501.09653v1.pdf","comment":"Pre-Print. Accepted to FORGE 2025 Dataset Track"},{"id":"http://arxiv.org/abs/2501.09645v1","updated":"2025-01-16T16:37:33Z","published":"2025-01-16T16:37:33Z","title":"CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through\n  Category-Bounding","summary":"  In today's assistant landscape, personalisation enhances interactions,\nfosters long-term relationships, and deepens engagement. However, many systems\nstruggle with retaining user preferences, leading to repetitive user requests\nand disengagement. Furthermore, the unregulated and opaque extraction of user\npreferences in industry applications raises significant concerns about privacy\nand trust, especially in regions with stringent regulations like Europe. In\nresponse to these challenges, we propose a long-term memory system for voice\nassistants, structured around predefined categories. This approach leverages\nLarge Language Models to efficiently extract, store, and retrieve preferences\nwithin these categories, ensuring both personalisation and transparency. We\nalso introduce a synthetic multi-turn, multi-session conversation dataset\n(CarMem), grounded in real industry data, tailored to an in-car voice assistant\nsetting. Benchmarked on the dataset, our system achieves an F1-score of .78 to\n.95 in preference extraction, depending on category granularity. Our\nmaintenance strategy reduces redundant preferences by 95% and contradictory\nones by 92%, while the accuracy of optimal retrieval is at .87. Collectively,\nthe results demonstrate the system's suitability for industrial applications.\n","authors":["Johannes Kirmayr","Lukas Stappen","Phillip Schneider","Florian Matthes","Elisabeth André"],"pdf_url":"https://arxiv.org/pdf/2501.09645v1.pdf","comment":"Accepted for presentation at the International Conference on\n  Computational Linguistics (COLING 2025)"},{"id":"http://arxiv.org/abs/2501.06278v2","updated":"2025-01-16T16:19:24Z","published":"2025-01-10T13:07:11Z","title":"Aligning Brain Activity with Advanced Transformer Models: Exploring the\n  Role of Punctuation in Semantic Processing","summary":"  This research examines the congruence between neural activity and advanced\ntransformer models, emphasizing the semantic significance of punctuation in\ntext understanding. Utilizing an innovative approach originally proposed by\nToneva and Wehbe, we evaluate four advanced transformer models RoBERTa,\nDistiliBERT, ALBERT, and ELECTRA against neural activity data. Our findings\nindicate that RoBERTa exhibits the closest alignment with neural activity,\nsurpassing BERT in accuracy. Furthermore, we investigate the impact of\npunctuation removal on model performance and neural alignment, revealing that\nBERT's accuracy enhances in the absence of punctuation. This study contributes\nto the comprehension of how neural networks represent language and the\ninfluence of punctuation on semantic processing within the human brain.\n","authors":["Zenon Lamprou","Frank Polick","Yashar Moshfeghi"],"pdf_url":"https://arxiv.org/pdf/2501.06278v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09980v4","updated":"2025-01-16T15:56:56Z","published":"2022-07-20T15:39:30Z","title":"ReFactor GNNs: Revisiting Factorisation-based Models from a\n  Message-Passing Perspective","summary":"  Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring\nsuccess for Knowledge Graph Completion (KGC) tasks, often outperforming Graph\nNeural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node\nfeatures and generalise to unseen nodes in inductive settings. Our work bridges\nthe gap between FMs and GNNs by proposing ReFactor GNNs. This new architecture\ndraws upon both modelling paradigms, which previously were largely thought of\nas disjoint. Concretely, using a message-passing formalism, we show how FMs can\nbe cast as GNNs by reformulating the gradient descent procedure as\nmessage-passing operations, which forms the basis of our ReFactor GNNs. Across\na multitude of well-established KGC benchmarks, our ReFactor GNNs achieve\ncomparable transductive performance to FMs, and state-of-the-art inductive\nperformance while using an order of magnitude fewer parameters.\n","authors":["Yihong Chen","Pushkar Mishra","Luca Franceschi","Pasquale Minervini","Pontus Stenetorp","Sebastian Riedel"],"pdf_url":"https://arxiv.org/pdf/2207.09980v4.pdf","comment":"36th Conference on Neural Information Processing Systems (NeurIPS\n  2022)"},{"id":"http://arxiv.org/abs/2501.04484v2","updated":"2025-01-16T15:49:07Z","published":"2025-01-08T13:09:45Z","title":"PolInterviews -- A Dataset of German Politician Public Broadcast\n  Interviews","summary":"  This paper presents a novel dataset of public broadcast interviews featuring\nhigh-ranking German politicians. The interviews were sourced from YouTube,\ntranscribed, processed for speaker identification, and stored in a tidy and\nopen format. The dataset comprises 99 interviews with 33 different German\npoliticians across five major interview formats, containing a total of 28,146\nsentences. As the first of its kind, this dataset offers valuable opportunities\nfor research on various aspects of political communication in the (German)\npolitical contexts, such as agenda-setting, interviewer dynamics, or\npoliticians' self-presentation.\n","authors":["Lukas Birkenmaier","Laureen Sieber","Felix Bergstein"],"pdf_url":"https://arxiv.org/pdf/2501.04484v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17962v4","updated":"2025-01-16T15:47:58Z","published":"2024-06-25T22:44:17Z","title":"Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework","summary":"  Large Language Models (LLMs) demonstrate remarkable ability to comprehend\ninstructions and generate human-like text, enabling sophisticated agent\nsimulation beyond basic behavior replication. However, the potential for\ncreating freely customisable characters remains underexplored. We introduce the\nCustomisable Conversation Agent Framework, which employs LLMs to simulate\nreal-world characters through personalised characteristic feature injection,\nenabling diverse character creation according to user preferences. We propose\nthe SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn\nrole-playing dialogues across 1,360 real-world scenes. Characters are initially\ncustomised using pre-defined elements (career, aspiration, traits, skills),\nthen expanded through personal and social profiles. Building on this, we\npresent SimsChat, a freely customisable role-playing agent incorporating\nvarious realistic settings and topic-specified character interactions.\nExperimental results on both SimsConv and WikiRoleEval datasets demonstrate\nSimsChat's superior performance in maintaining character consistency, knowledge\naccuracy, and appropriate question rejection compared to existing models. Our\nframework provides valuable insights for developing more accurate and\ncustomisable human simulacra. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat.\n","authors":["Bohao Yang","Dong Liu","Chenghao Xiao","Kun Zhao","Chen Tang","Chao Li","Lin Yuan","Guang Yang","Lanxiao Huang","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2406.17962v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09604v1","updated":"2025-01-16T15:24:41Z","published":"2025-01-16T15:24:41Z","title":"From Scarcity to Capability: Empowering Fake News Detection in\n  Low-Resource Languages with LLMs","summary":"  The rapid spread of fake news presents a significant global challenge,\nparticularly in low-resource languages like Bangla, which lack adequate\ndatasets and detection tools. Although manual fact-checking is accurate, it is\nexpensive and slow to prevent the dissemination of fake news. Addressing this\ngap, we introduce BanFakeNews-2.0, a robust dataset to enhance Bangla fake news\ndetection. This version includes 11,700 additional, meticulously curated fake\nnews articles validated from credible sources, creating a proportional dataset\nof 47,000 authentic and 13,000 fake news items across 13 categories. In\naddition, we created a manually curated independent test set of 460 fake and\n540 authentic news items for rigorous evaluation. We invest efforts in\ncollecting fake news from credible sources and manually verified while\npreserving the linguistic richness. We develop a benchmark system utilizing\ntransformer-based architectures, including fine-tuned Bidirectional Encoder\nRepresentations from Transformers variants (F1-87\\%) and Large Language Models\nwith Quantized Low-Rank Approximation (F1-89\\%), that significantly outperforms\ntraditional methods. BanFakeNews-2.0 offers a valuable resource to advance\nresearch and application in fake news detection for low-resourced languages. We\npublicly release our dataset and model on Github to foster research in this\ndirection.\n","authors":["Hrithik Majumdar Shibu","Shrestha Datta","Md. Sumon Miah","Nasrullah Sami","Mahruba Sharmin Chowdhury","Md. Saiful Islam"],"pdf_url":"https://arxiv.org/pdf/2501.09604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07678v2","updated":"2025-01-16T14:33:40Z","published":"2024-12-10T17:06:33Z","title":"Can linguists better understand DNA?","summary":"  Multilingual transfer ability, which reflects how well models fine-tuned on\none source language can be applied to other languages, has been well studied in\nmultilingual pre-trained models. However, the existence of such capability\ntransfer between natural language and gene sequences/languages remains under\nexplored.This study addresses this gap by drawing inspiration from the\nsentence-pair classification task used for evaluating sentence similarity in\nnatural language. We constructed two analogous tasks: DNA-pair\nclassification(DNA sequence similarity) and DNA-protein-pair\nclassification(gene coding determination). These tasks were designed to\nvalidate the transferability of capabilities from natural language to gene\nsequences. Even a small-scale pre-trained model like GPT-2-small, which was\npre-trained on English, achieved an accuracy of 78% on the DNA-pair\nclassification task after being fine-tuned on English sentence-pair\nclassification data(XTREME PAWS-X). While training a BERT model on multilingual\ntext, the precision reached 89%. On the more complex DNA-protein-pair\nclassification task, however, the model's output was barely distinguishable\nfrom random output.Experimental validation has confirmed that the transfer of\ncapabilities from natural language to biological language is unequivocally\npresent. Building on this foundation, we have also investigated the impact of\nmodel parameter scale and pre-training on this capability transfer. We provide\nrecommendations for facilitating the transfer of capabilities from natural\nlanguage to genetic language,as well as new approaches for conducting\nbiological research based on this capability.This study offers an intriguing\nnew perspective on exploring the relationship between natural language and\ngenetic language.\n","authors":["Wang Liang"],"pdf_url":"https://arxiv.org/pdf/2412.07678v2.pdf","comment":"20 pages,7 figures"},{"id":"http://arxiv.org/abs/2501.09561v1","updated":"2025-01-16T14:26:48Z","published":"2025-01-16T14:26:48Z","title":"Stylomech: Unveiling Authorship via Computational Stylometry in English\n  and Romanized Sinhala","summary":"  With the advent of Web 2.0, the development in social technology coupled with\nglobal communication systematically brought positive and negative impacts to\nsociety. Copyright claims and Author identification are deemed crucial as there\nhas been a considerable amount of increase in content violation owing to the\nlack of proper ethics in society. The Author's attribution in both English and\nRomanized Sinhala became a major requirement in the last few decades. As an\narea largely unexplored, particularly within the context of Romanized Sinhala,\nthe research contributes significantly to the field of computational\nlinguistics. The proposed author attribution system offers a unique approach,\nallowing for the comparison of only two sets of text: suspect author and\nanonymous text, a departure from traditional methodologies which often rely on\nlarger corpora. This work focuses on using the numerical representation of\nvarious pairs of the same and different authors allowing for, the model to\ntrain on these representations as opposed to text, this allows for it to apply\nto a multitude of authors and contexts, given that the suspected author text,\nand the anonymous text are of reasonable quality. By expanding the scope of\nauthorship attribution to encompass diverse linguistic contexts, the work\ncontributes to fostering trust and accountability in digital communication,\nespecially in Sri Lanka. This research presents a pioneering approach to author\nattribution in both English and Romanized Sinhala, addressing a critical need\nfor content verification and intellectual property rights enforcement in the\ndigital age.\n","authors":["Nabeelah Faumi","Adeepa Gunathilake","Benura Wickramanayake","Deelaka Dias","TGDK Sumanathilaka"],"pdf_url":"https://arxiv.org/pdf/2501.09561v1.pdf","comment":"3 figure, 1 image"},{"id":"http://arxiv.org/abs/2501.09538v1","updated":"2025-01-16T13:42:09Z","published":"2025-01-16T13:42:09Z","title":"Analyzing Continuous Semantic Shifts with Diachronic Word Similarity\n  Matrices","summary":"  The meanings and relationships of words shift over time. This phenomenon is\nreferred to as semantic shift.Research focused on understanding how semantic\nshifts occur over multiple time periods is essential for gaining a detailed\nunderstanding of semantic shifts.However, detecting change points only between\nadjacent time periods is insufficient for analyzing detailed semantic shifts,\nand using BERT-based methods to examine word sense proportions incurs a high\ncomputational cost.To address those issues, we propose a simple yet intuitive\nframework for how semantic shifts occur over multiple time periods by\nleveraging a similarity matrix between the embeddings of the same word through\ntime.We compute a diachronic word similarity matrix using fast and lightweight\nword embeddings across arbitrary time periods, making it deeper to analyze\ncontinuous semantic shifts.Additionally, by clustering the similarity matrices\nfor different words, we can categorize words that exhibit similar behavior of\nsemantic shift in an unsupervised manner.\n","authors":["Hajime Kiyama","Taichi Aida","Mamoru Komachi","Toshinobu Ogiso","Hiroya Takamura","Daichi Mochihashi"],"pdf_url":"https://arxiv.org/pdf/2501.09538v1.pdf","comment":"COLING2025"},{"id":"http://arxiv.org/abs/2501.09527v1","updated":"2025-01-16T13:23:07Z","published":"2025-01-16T13:23:07Z","title":"Confidence Estimation for Error Detection in Text-to-SQL Systems","summary":"  Text-to-SQL enables users to interact with databases through natural\nlanguage, simplifying the retrieval and synthesis of information. Despite the\nsuccess of large language models (LLMs) in converting natural language\nquestions into SQL queries, their broader adoption is limited by two main\nchallenges: achieving robust generalization across diverse queries and ensuring\ninterpretative confidence in their predictions. To tackle these issues, our\nresearch investigates the integration of selective classifiers into Text-to-SQL\nsystems. We analyse the trade-off between coverage and risk using entropy based\nconfidence estimation with selective classifiers and assess its impact on the\noverall performance of Text-to-SQL models. Additionally, we explore the models'\ninitial calibration and improve it with calibration techniques for better model\nalignment between confidence and accuracy. Our experimental results show that\nencoder-decoder T5 is better calibrated than in-context-learning GPT 4 and\ndecoder-only Llama 3, thus the designated external entropy-based selective\nclassifier has better performance. The study also reveal that, in terms of\nerror detection, selective classifier with a higher probability detects errors\nassociated with irrelevant questions rather than incorrect query generations.\n","authors":["Oleg Somov","Elena Tutubalina"],"pdf_url":"https://arxiv.org/pdf/2501.09527v1.pdf","comment":"15 pages, 11 figures, to be published in AAAI 2025 Proceedings"},{"id":"http://arxiv.org/abs/2501.09521v1","updated":"2025-01-16T13:16:37Z","published":"2025-01-16T13:16:37Z","title":"Augmenting a Large Language Model with a Combination of Text and Visual\n  Data for Conversational Visualization of Global Geospatial Data","summary":"  We present a method for augmenting a Large Language Model (LLM) with a\ncombination of text and visual data to enable accurate question answering in\nvisualization of scientific data, making conversational visualization possible.\nLLMs struggle with tasks like visual data interaction, as they lack contextual\nvisual information. We address this problem by merging a text description of a\nvisualization and dataset with snapshots of the visualization. We extract their\nessential features into a structured text file, highly compact, yet descriptive\nenough to appropriately augment the LLM with contextual information, without\nany fine-tuning. This approach can be applied to any visualization that is\nalready finally rendered, as long as it is associated with some textual\ndescription.\n","authors":["Omar Mena","Alexandre Kouyoumdjian","Lonni Besançon","Michael Gleicher","Ivan Viola","Anders Ynnerman"],"pdf_url":"https://arxiv.org/pdf/2501.09521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09512v1","updated":"2025-01-16T12:57:33Z","published":"2025-01-16T12:57:33Z","title":"PIER: A Novel Metric for Evaluating What Matters in Code-Switching","summary":"  Code-switching, the alternation of languages within a single discourse,\npresents a significant challenge for Automatic Speech Recognition. Despite the\nunique nature of the task, performance is commonly measured with established\nmetrics such as Word-Error-Rate (WER). However, in this paper, we question\nwhether these general metrics accurately assess performance on code-switching.\nSpecifically, using both Connectionist-Temporal-Classification and\nEncoder-Decoder models, we show fine-tuning on non-code-switched data from both\nmatrix and embedded language improves classical metrics on code-switching test\nsets, although actual code-switched words worsen (as expected). Therefore, we\npropose Point-of-Interest Error Rate (PIER), a variant of WER that focuses only\non specific words of interest. We instantiate PIER on code-switched utterances\nand show that this more accurately describes the code-switching performance,\nshowing huge room for improvement in future work. This focused evaluation\nallows for a more precise assessment of model performance, particularly in\nchallenging aspects such as inter-word and intra-word code-switching.\n","authors":["Enes Yavuz Ugan","Ngoc-Quan Pham","Leonard Bärmann","Alex Waibel"],"pdf_url":"https://arxiv.org/pdf/2501.09512v1.pdf","comment":"Accepted at ICASSP 2025"},{"id":"http://arxiv.org/abs/2410.13187v3","updated":"2025-01-16T12:46:53Z","published":"2024-10-17T03:32:02Z","title":"aiXcoder-7B: A Lightweight and Effective Large Language Model for Code\n  Processing","summary":"  Large Language Models (LLMs) have been widely used in code completion, and\nresearchers are focusing on scaling up LLMs to improve their accuracy. However,\nlarger LLMs have lower inference efficiency, affecting developers' experience\nand productivity. In this paper, we propose a lightweight and effective LLM for\ncode completion named aiXcoder-7B. Compared to existing LLMs, aiXcoder-7B\nachieves higher code completion accuracy while having smaller scales (i.e., 7\nbillion parameters). We attribute the superiority of aiXcoder-7B to three key\nfactors: (1) Multi-objective training. We employ three training objectives, one\nof which is our proposed Structured Fill-In-the-Middle (SFIM). SFIM considers\nthe syntax structures in code and effectively improves the performance of LLMs\nfor code. (2) Diverse data sampling strategies. They consider inter-file\nrelationships and enhance the capability of LLMs in understanding cross-file\ncontexts. (3) Extensive high-quality data. We establish a rigorous data\ncollection pipeline and consume a total of 1.2 trillion unique tokens for\ntraining aiXcoder-7B. This vast volume of data enables aiXcoder-7B to learn a\nbroad distribution of code. We evaluate aiXcoder-7B in five popular code\ncompletion benchmarks and a new benchmark collected by this paper. The results\nshow that aiXcoder-7B outperforms the latest six LLMs with similar sizes and\neven surpasses four larger LLMs (e.g., StarCoder2-15B and CodeLlama-34B),\npositioning aiXcoder-7B as a lightweight and effective LLM for academia and\nindustry. Finally, we summarize three valuable insights for helping\npractitioners train the next generations of LLMs for code. aiXcoder-7B has been\nopen-souced and gained significant attention. Until January 2025, aiXcoder-7B\nhas received 2,226 GitHub Stars.\n","authors":["Siyuan Jiang","Jia Li","He Zong","Huanyu Liu","Hao Zhu","Shukai Hu","Erlu Li","Jiazheng Ding","Yu Han","Wei Ning","Gen Wang","Yihong Dong","Kechi Zhang","Ge Li"],"pdf_url":"https://arxiv.org/pdf/2410.13187v3.pdf","comment":"(1) Accepted by the 47th International Conference on Software\n  Engineering (ICSE 2025). (2) aiXcoder-7B is available at\n  https://github.com/aixcoder-plugin/aiXcoder-7B"},{"id":"http://arxiv.org/abs/2409.08199v2","updated":"2025-01-16T12:17:18Z","published":"2024-09-12T16:36:39Z","title":"AudioBERT: Audio Knowledge Augmented Language Model","summary":"  Recent studies have identified that language models, pretrained on text-only\ndatasets, often lack elementary visual knowledge, \\textit{e.g.,} colors of\neveryday objects. Motivated by this observation, we ask whether a similar\nshortcoming exists in terms of the \\textit{auditory} knowledge. To answer this\nquestion, we construct a new dataset called AuditoryBench, which consists of\ntwo novel tasks for evaluating auditory knowledge. Based on our analysis using\nthe benchmark, we find that language models also suffer from a severe lack of\nauditory knowledge. To address this limitation, we propose AudioBERT, a novel\nmethod to augment the auditory knowledge of BERT through a retrieval-based\napproach. First, we detect auditory knowledge spans in prompts to query our\nretrieval model efficiently. Then, we inject audio knowledge into BERT and\nswitch on low-rank adaptation for effective adaptation when audio knowledge is\nrequired. Our experiments demonstrate that AudioBERT is quite effective,\nachieving superior performance on the AuditoryBench. The dataset and code are\navailable at \\bulurl{https://github.com/HJ-Ok/AudioBERT}.\n","authors":["Hyunjong Ok","Suho Yoo","Jaeho Lee"],"pdf_url":"https://arxiv.org/pdf/2409.08199v2.pdf","comment":"5 pages, 3 figures, ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.09484v1","updated":"2025-01-16T11:41:14Z","published":"2025-01-16T11:41:14Z","title":"Exploring the Inquiry-Diagnosis Relationship with Advanced Patient\n  Simulators","summary":"  Online medical consultation (OMC) restricts doctors to gathering patient\ninformation solely through inquiries, making the already complex sequential\ndecision-making process of diagnosis even more challenging. Recently, the rapid\nadvancement of large language models has demonstrated a significant potential\nto transform OMC. However, most studies have primarily focused on improving\ndiagnostic accuracy under conditions of relatively sufficient information,\nwhile paying limited attention to the \"inquiry\" phase of the consultation\nprocess. This lack of focus has left the relationship between \"inquiry\" and\n\"diagnosis\" insufficiently explored. In this paper, we first extract real\npatient interaction strategies from authentic doctor-patient conversations and\nuse these strategies to guide the training of a patient simulator that closely\nmirrors real-world behavior. By inputting medical records into our patient\nsimulator to simulate patient responses, we conduct extensive experiments to\nexplore the relationship between \"inquiry\" and \"diagnosis\" in the consultation\nprocess. Experimental results demonstrate that inquiry and diagnosis adhere to\nthe Liebig's law: poor inquiry quality limits the effectiveness of diagnosis,\nregardless of diagnostic capability, and vice versa. Furthermore, the\nexperiments reveal significant differences in the inquiry performance of\nvarious models. To investigate this phenomenon, we categorize the inquiry\nprocess into four types: (1) chief complaint inquiry; (2) specification of\nknown symptoms; (3) inquiry about accompanying symptoms; and (4) gathering\nfamily or medical history. We analyze the distribution of inquiries across the\nfour types for different models to explore the reasons behind their significant\nperformance differences. We plan to open-source the weights and related code of\nour patient simulator at https://github.com/LIO-H-ZEN/PatientSimulator.\n","authors":["Zhaocheng Liu","Quan Tu","Wen Ye","Yu Xiao","Zhishou Zhang","Hengfu Cui","Yalun Zhu","Qiang Ju","Shizheng Li","Jian Xie"],"pdf_url":"https://arxiv.org/pdf/2501.09484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22944v2","updated":"2025-01-16T11:26:02Z","published":"2024-10-30T12:01:48Z","title":"Focus On This, Not That! Steering LLMs With Adaptive Feature\n  Specification","summary":"  Despite the success of Instruction Tuning (IT) in training large language\nmodels (LLMs) to perform arbitrary user-specified tasks, these models often\nstill leverage spurious or biased features learned from their training data,\nleading to undesired behaviours when deploying them in new contexts. In this\nwork, we introduce Focus Instruction Tuning (FIT), which trains LLMs to\ncondition their responses by focusing on specific features whilst ignoring\nothers, leading to different behaviours based on what features are specified.\nAcross several experimental settings, we show that focus-tuned models can be\nadaptively steered by focusing on different features at inference-time: for\ninstance, robustness can be improved by focusing on task-causal features and\nignoring spurious features, and social bias can be mitigated by ignoring\ndemographic categories. Furthermore, FIT can steer behaviour in new contexts,\ngeneralising under distribution shift and to new unseen features at inference\ntime, and thereby facilitating more robust, fair, and controllable LLM\napplications in real-world environments.\n","authors":["Tom A. Lamb","Adam Davies","Alasdair Paren","Philip H. S. Torr","Francesco Pinto"],"pdf_url":"https://arxiv.org/pdf/2410.22944v2.pdf","comment":"28pages, 14 figures"},{"id":"http://arxiv.org/abs/2501.09451v1","updated":"2025-01-16T10:26:17Z","published":"2025-01-16T10:26:17Z","title":"Scaling Graph-Based Dependency Parsing with Arc Vectorization and\n  Attention-Based Refinement","summary":"  We propose a novel architecture for graph-based dependency parsing that\nexplicitly constructs vectors, from which both arcs and labels are scored. Our\nmethod addresses key limitations of the standard two-pipeline approach by\nunifying arc scoring and labeling into a single network, reducing scalability\nissues caused by the information bottleneck and lack of parameter sharing.\nAdditionally, our architecture overcomes limited arc interactions with\ntransformer layers to efficiently simulate higher-order dependencies.\nExperiments on PTB and UD show that our model outperforms state-of-the-art\nparsers in both accuracy and efficiency.\n","authors":["Nicolas Floquet","Joseph Le Roux","Nadi Tomeh","Thierry Charnois"],"pdf_url":"https://arxiv.org/pdf/2501.09451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11775v2","updated":"2025-01-16T10:20:03Z","published":"2024-08-21T17:00:05Z","title":"Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context\n  Support: For 3GPP Standards","summary":"  Recent studies show that large language models (LLMs) struggle with technical\nstandards in telecommunications. We propose a fine-tuned retrieval-augmented\ngeneration (RAG) system based on the Phi-2 small language model (SLM) to serve\nas an oracle for communication networks. Our developed system leverages\nforward-looking semantic chunking to adaptively determine parsing breakpoints\nbased on embedding similarity, enabling effective processing of diverse\ndocument formats. To handle the challenge of multiple similar contexts in\ntechnical standards, we employ a re-ranking algorithm to prioritize the most\nrelevant retrieved chunks. Recognizing the limitations of Phi-2's small context\nwindow, we implement a recent technique, namely SelfExtend, to expand the\ncontext window during inference, which not only boosts the performance but also\ncan accommodate a wider range of user queries and design requirements from\ncustomers to specialized technicians. For fine-tuning, we utilize the low-rank\nadaptation (LoRA) technique to enhance computational efficiency during training\nand enable effective fine-tuning on small datasets. Our comprehensive\nexperiments demonstrate substantial improvements over existing\nquestion-answering approaches in the telecom domain, achieving performance that\nexceeds larger language models such as GPT-4 (which is about 880 times larger\nin size). This work presents a novel approach to leveraging SLMs for\ncommunication networks, offering a balance of efficiency and performance. This\nwork can serve as a foundation towards agentic language models for networks.\n","authors":["Omar Erak","Nouf Alabbasi","Omar Alhussein","Ismail Lotfi","Amr Hussein","Sami Muhaidat","Merouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2408.11775v2.pdf","comment":"submitted to Proc. IEEE Globecom"},{"id":"http://arxiv.org/abs/2501.09444v1","updated":"2025-01-16T10:17:58Z","published":"2025-01-16T10:17:58Z","title":"Solving the unsolvable: Translating case law in Hong Kong","summary":"  This paper addresses the challenges translating case law under Hong Kong's\nbilingual legal system. It highlights the initial success of translating all\nwritten statutes into Chinese before the 1997 handover, a task mandated by the\nBasic Law. The effort involved significant collaboration among legal,\nlinguistic, and translation experts, resulting in a comprehensive and\nculturally appropriate bilingual legal system. However, translating case law\nremains a significant challenge due to the sheer volume and continuous growth\nof judicial decisions. The paper critiques the governments and judiciarys\nsporadic and uncoordinated efforts to translate case law, contrasting it with\nthe thorough approach previously taken for statute translation. Although the\ngovernment acknowledges the importance of legal bilingualism, it lacks a\nsustainable strategy for translating case law. The Judiciarys position that\ntranslating all judgments is unnecessary, unrealistic, and not cost-effectiveis\nanalyzed and critiqued for its impact on legal transparency and public trust. A\nproposed solution involves leveraging machine translation technology through a\nhuman-machine interactive translation platform, which undergoes two major\ntransitions. Initially based on a neural model, the platform transitions to\nusing a large language model for improved translation accuracy. Furthermore, it\nevolves from a single-agent system to a multi-agent system, incorporating\nTranslator, Annotator, and Proofreader agents. This multi-agent approach,\nsupported by a grant, aims to facilitate efficient, high-quality translation of\njudicial judgments by integrating advanced artificial intelligence and\ncontinuous feedback mechanisms, thus better meeting the needs of a bilingual\nlegal system.\n","authors":["King-kui Sin","Xi Xuan","Chunyu Kit","Clara Ho-yan Chan","Honic Ho-kin Ip"],"pdf_url":"https://arxiv.org/pdf/2501.09444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11005v2","updated":"2025-01-16T10:05:17Z","published":"2024-06-25T20:23:15Z","title":"RAGBench: Explainable Benchmark for Retrieval-Augmented Generation\n  Systems","summary":"  Retrieval-Augmented Generation (RAG) has become a standard architectural\npattern for incorporating domain-specific knowledge into user-facing chat\napplications powered by Large Language Models (LLMs). RAG systems are\ncharacterized by (1) a document retriever that queries a domain-specific corpus\nfor context information relevant to an input query, and (2) an LLM that\ngenerates a response based on the provided query and context. However,\ncomprehensive evaluation of RAG systems remains a challenge due to the lack of\nunified evaluation criteria and annotated datasets. In response, we introduce\nRAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k\nexamples. It covers five unique industry-specific domains and various RAG task\ntypes. RAGBench examples are sourced from industry corpora such as user\nmanuals, making it particularly relevant for industry applications. Further, we\nformalize the TRACe evaluation framework: a set of explainable and actionable\nRAG evaluation metrics applicable across all RAG domains. We release the\nlabeled dataset at https://huggingface.co/datasets/rungalileo/ragbench.\nRAGBench explainable labels facilitate holistic evaluation of RAG systems,\nenabling actionable feedback for continuous improvement of production\napplications. Thorough extensive benchmarking, we find that LLM-based RAG\nevaluation methods struggle to compete with a finetuned RoBERTa model on the\nRAG evaluation task. We identify areas where existing approaches fall short and\npropose the adoption of RAGBench with TRACe towards advancing the state of RAG\nevaluation systems.\n","authors":["Robert Friel","Masha Belyi","Atindriyo Sanyal"],"pdf_url":"https://arxiv.org/pdf/2407.11005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09431v1","updated":"2025-01-16T09:59:45Z","published":"2025-01-16T09:59:45Z","title":"A Survey on Responsible LLMs: Inherent Risk, Malicious Use, and\n  Mitigation Strategy","summary":"  While large language models (LLMs) present significant potential for\nsupporting numerous real-world applications and delivering positive social\nimpacts, they still face significant challenges in terms of the inherent risk\nof privacy leakage, hallucinated outputs, and value misalignment, and can be\nmaliciously used for generating toxic content and unethical purposes after been\njailbroken. Therefore, in this survey, we present a comprehensive review of\nrecent advancements aimed at mitigating these issues, organized across the four\nphases of LLM development and usage: data collecting and pre-training,\nfine-tuning and alignment, prompting and reasoning, and post-processing and\nauditing. We elaborate on the recent advances for enhancing the performance of\nLLMs in terms of privacy protection, hallucination reduction, value alignment,\ntoxicity elimination, and jailbreak defenses. In contrast to previous surveys\nthat focus on a single dimension of responsible LLMs, this survey presents a\nunified framework that encompasses these diverse dimensions, providing a\ncomprehensive view of enhancing LLMs to better serve real-world applications.\n","authors":["Huandong Wang","Wenjie Fu","Yingzhou Tang","Zhilong Chen","Yuxi Huang","Jinghua Piao","Chen Gao","Fengli Xu","Tao Jiang","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2501.09431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09426v1","updated":"2025-01-16T09:57:12Z","published":"2025-01-16T09:57:12Z","title":"AutoCBT: An Autonomous Multi-agent Framework for Cognitive Behavioral\n  Therapy in Psychological Counseling","summary":"  Traditional in-person psychological counseling remains primarily niche, often\nchosen by individuals with psychological issues, while online automated\ncounseling offers a potential solution for those hesitant to seek help due to\nfeelings of shame. Cognitive Behavioral Therapy (CBT) is an essential and\nwidely used approach in psychological counseling. The advent of large language\nmodels (LLMs) and agent technology enables automatic CBT diagnosis and\ntreatment. However, current LLM-based CBT systems use agents with a fixed\nstructure, limiting their self-optimization capabilities, or providing hollow,\nunhelpful suggestions due to redundant response patterns. In this work, we\nutilize Quora-like and YiXinLi single-round consultation models to build a\ngeneral agent framework that generates high-quality responses for single-turn\npsychological consultation scenarios. We use a bilingual dataset to evaluate\nthe quality of single-response consultations generated by each framework. Then,\nwe incorporate dynamic routing and supervisory mechanisms inspired by real\npsychological counseling to construct a CBT-oriented autonomous multi-agent\nframework, demonstrating its general applicability. Experimental results\nindicate that AutoCBT can provide higher-quality automated psychological\ncounseling services.\n","authors":["Ancheng Xu","Di Yang","Renhao Li","Jingwei Zhu","Minghuan Tan","Min Yang","Wanxin Qiu","Mingchen Ma","Haihong Wu","Bingyu Li","Feng Sha","Chengming Li","Xiping Hu","Qiang Qu","Derek F. Wong","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2501.09426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09425v1","updated":"2025-01-16T09:55:42Z","published":"2025-01-16T09:55:42Z","title":"Vision-Language Models Do Not Understand Negation","summary":"  Many practical vision-language applications require models that understand\nnegation, e.g., when using natural language to retrieve images which contain\ncertain objects but not others. Despite advancements in vision-language models\n(VLMs) through large-scale training, their ability to comprehend negation\nremains underexplored. This study addresses the question: how well do current\nVLMs understand negation? We introduce NegBench, a new benchmark designed to\nevaluate negation understanding across 18 task variations and 79k examples\nspanning image, video, and medical datasets. The benchmark consists of two core\ntasks designed to evaluate negation understanding in diverse multimodal\nsettings: Retrieval with Negation and Multiple Choice Questions with Negated\nCaptions. Our evaluation reveals that modern VLMs struggle significantly with\nnegation, often performing at chance level. To address these shortcomings, we\nexplore a data-centric approach wherein we finetune CLIP models on large-scale\nsynthetic datasets containing millions of negated captions. We show that this\napproach can result in a 10% increase in recall on negated queries and a 40%\nboost in accuracy on multiple-choice questions with negated captions.\n","authors":["Kumail Alhamoud","Shaden Alshammari","Yonglong Tian","Guohao Li","Philip Torr","Yoon Kim","Marzyeh Ghassemi"],"pdf_url":"https://arxiv.org/pdf/2501.09425v1.pdf","comment":"Project page: https://negbench.github.io"},{"id":"http://arxiv.org/abs/2501.09409v1","updated":"2025-01-16T09:35:15Z","published":"2025-01-16T09:35:15Z","title":"mGeNTE: A Multilingual Resource for Gender-Neutral Language and\n  Translation","summary":"  Gender-neutral language reflects societal and linguistic shifts towards\ngreater inclusivity by avoiding the implication that one gender is the norm\nover others. This is particularly relevant for grammatical gender languages,\nwhich heavily encode the gender of terms for human referents and over-relies on\nmasculine forms, even when gender is unspecified or irrelevant. Language\ntechnologies are known to mirror these inequalities, being affected by a male\nbias and perpetuating stereotypical associations when translating into\nlanguages with extensive gendered morphology. In such cases, gender-neutral\nlanguage can help avoid undue binary assumptions. However, despite its\nimportance for creating fairer multi- and cross-lingual technologies, inclusive\nlanguage research remains scarce and insufficiently supported in current\nresources. To address this gap, we present the multilingual mGeNTe dataset.\nDerived from the bilingual GeNTE (Piergentili et al., 2023), mGeNTE extends the\noriginal corpus to include the English-Italian/German/Spanish language pairs.\nSince each language pair is English-aligned with gendered and neutral sentences\nin the target languages, mGeNTE enables research in both automatic\nGender-Neutral Translation (GNT) and language modelling for three grammatical\ngender languages.\n","authors":["Beatrice Savoldi","Eleonora Cupin","Manjinder Thind","Anne Lauscher","Luisa Bentivogli"],"pdf_url":"https://arxiv.org/pdf/2501.09409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09384v1","updated":"2025-01-16T08:52:50Z","published":"2025-01-16T08:52:50Z","title":"Evaluating LLM Abilities to Understand Tabular Electronic Health\n  Records: A Comprehensive Study of Patient Data Extraction and Retrieval","summary":"  Electronic Health Record (EHR) tables pose unique challenges among which is\nthe presence of hidden contextual dependencies between medical features with a\nhigh level of data dimensionality and sparsity. This study presents the first\ninvestigation into the abilities of LLMs to comprehend EHRs for patient data\nextraction and retrieval. We conduct extensive experiments using the MIMICSQL\ndataset to explore the impact of the prompt structure, instruction, context,\nand demonstration, of two backbone LLMs, Llama2 and Meditron, based on task\nperformance. Through quantitative and qualitative analyses, our findings show\nthat optimal feature selection and serialization methods can enhance task\nperformance by up to 26.79% compared to naive approaches. Similarly, in-context\nlearning setups with relevant example selection improve data extraction\nperformance by 5.95%. Based on our study findings, we propose guidelines that\nwe believe would help the design of LLM-based models to support health search.\n","authors":["Jesus Lovon","Martin Mouysset","Jo Oleiwan","Jose G. Moreno","Christine Damase-Michel","Lynda Tamine"],"pdf_url":"https://arxiv.org/pdf/2501.09384v1.pdf","comment":"To be published as full paper in the Proceedings of the European\n  Conference on Information Retrieval (ECIR) 2025. Preprint"},{"id":"http://arxiv.org/abs/2406.13340v2","updated":"2025-01-16T08:34:36Z","published":"2024-06-19T08:46:29Z","title":"SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond\n  Words","summary":"  Speech encompasses a wealth of information, including but not limited to\ncontent, paralinguistic, and environmental information. This comprehensive\nnature of speech significantly impacts communication and is crucial for\nhuman-computer interaction. Chat-Oriented Large Language Models (LLMs), known\nfor their general-purpose assistance capabilities, have evolved to handle\nmulti-modal inputs, including speech. Although these models can be adept at\nrecognizing and analyzing speech, they often fall short of generating\nappropriate responses. We argue that this is due to the lack of principles on\ntask definition and model development, which requires open-source datasets and\nmetrics suitable for model evaluation. To bridge the gap, we present SD-Eval, a\nbenchmark dataset aimed at multidimensional evaluation of spoken dialogue\nunderstanding and generation. SD-Eval focuses on paralinguistic and\nenvironmental information and includes 7,303 utterances, amounting to 8.76\nhours of speech data. The data is aggregated from eight public datasets,\nrepresenting four perspectives: emotion, accent, age, and background sound. To\nassess the SD-Eval benchmark dataset, we implement three different models and\nconstruct a training set following a process similar to that of SD-Eval. The\ntraining set contains 1,052.72 hours of speech data and 724.4k utterances. We\nalso conduct a comprehensive evaluation using objective evaluation methods\n(e.g. BLEU and ROUGE), subjective evaluations and LLM-based metrics for the\ngenerated responses. Models conditioned with paralinguistic and environmental\ninformation outperform their counterparts in both objective and subjective\nmeasures. Moreover, experiments demonstrate that LLM-based metrics show a\nhigher correlation with human evaluation compared to traditional metrics. We\nopen-source SD-Eval at https://github.com/amphionspace/SD-Eval.\n","authors":["Junyi Ao","Yuancheng Wang","Xiaohai Tian","Dekun Chen","Jun Zhang","Lu Lu","Yuxuan Wang","Haizhou Li","Zhizheng Wu"],"pdf_url":"https://arxiv.org/pdf/2406.13340v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2501.09349v1","updated":"2025-01-16T08:03:32Z","published":"2025-01-16T08:03:32Z","title":"ChartInsighter: An Approach for Mitigating Hallucination in Time-series\n  Chart Summary Generation with A Benchmark Dataset","summary":"  Effective chart summary can significantly reduce the time and effort decision\nmakers spend interpreting charts, enabling precise and efficient communication\nof data insights. Previous studies have faced challenges in generating accurate\nand semantically rich summaries of time-series data charts. In this paper, we\nidentify summary elements and common hallucination types in the generation of\ntime-series chart summaries, which serve as our guidelines for automatic\ngeneration. We introduce ChartInsighter, which automatically generates chart\nsummaries of time-series data, effectively reducing hallucinations in chart\nsummary generation. Specifically, we assign multiple agents to generate the\ninitial chart summary and collaborate iteratively, during which they invoke\nexternal data analysis modules to extract insights and compile them into a\ncoherent summary. Additionally, we implement a self-consistency test method to\nvalidate and correct our summary. We create a high-quality benchmark of charts\nand summaries, with hallucination types annotated on a sentence-by-sentence\nbasis, facilitating the evaluation of the effectiveness of reducing\nhallucinations. Our evaluations using our benchmark show that our method\nsurpasses state-of-the-art models, and that our summary hallucination rate is\nthe lowest, which effectively reduces various hallucinations and improves\nsummary quality. The benchmark is available at\nhttps://github.com/wangfen01/ChartInsighter.\n","authors":["Fen Wang","Bomiao Wang","Xueli Shu","Zhen Liu","Zekai Shao","Chao Liu","Siming Chen"],"pdf_url":"https://arxiv.org/pdf/2501.09349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03584v2","updated":"2025-01-16T07:56:42Z","published":"2025-01-07T07:17:04Z","title":"Discriminative Representation learning via Attention-Enhanced\n  Contrastive Learning for Short Text Clustering","summary":"  Contrastive learning has gained significant attention in short text\nclustering, yet it has an inherent drawback of mistakenly identifying samples\nfrom the same category as negatives and then separating them in the feature\nspace (false negative separation), which hinders the generation of superior\nrepresentations. To generate more discriminative representations for efficient\nclustering, we propose a novel short text clustering method, called\nDiscriminative Representation learning via \\textbf{A}ttention-\\textbf{E}nhanced\n\\textbf{C}ontrastive \\textbf{L}earning for Short Text Clustering\n(\\textbf{AECL}). The \\textbf{AECL} consists of two modules which are the\npseudo-label generation module and the contrastive learning module. Both\nmodules build a sample-level attention mechanism to capture similarity\nrelationships between samples and aggregate cross-sample features to generate\nconsistent representations. Then, the former module uses the more\ndiscriminative consistent representation to produce reliable supervision\ninformation for assist clustering, while the latter module explores similarity\nrelationships and consistent representations optimize the construction of\npositive samples to perform similarity-guided contrastive learning, effectively\naddressing the false negative separation issue. Experimental results\ndemonstrate that the proposed \\textbf{AECL} outperforms state-of-the-art\nmethods. If the paper is accepted, we will open-source the code.\n","authors":["Zhihao Yao"],"pdf_url":"https://arxiv.org/pdf/2501.03584v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15701v2","updated":"2025-01-16T07:01:37Z","published":"2024-12-20T09:21:15Z","title":"Collaborative Gym: A Framework for Enabling and Evaluating Human-Agent\n  Collaboration","summary":"  Recent advancements in language models (LMs) have sparked growing interest in\ndeveloping LM agents. While fully autonomous agents could excel in many\nscenarios, numerous use cases inherently require them to collaborate with\nhumans due to humans' latent preferences, domain expertise, or need for\ncontrol. To facilitate the study of human-agent collaboration, we present\nCollaborative Gym (Co-Gym), a general framework enabling asynchronous,\ntripartite interaction among agents, humans, and task environments. We\ninstantiate Co-Gym with three representative tasks in both simulated and\nreal-world conditions, and propose an evaluation framework that assesses both\nthe collaboration outcomes and processes. Our findings reveal that\ncollaborative agents consistently outperform their fully autonomous\ncounterparts in task performance within those delivered cases, achieving win\nrates of 86% in Travel Planning, 74% in Tabular Analysis, and 66% in Related\nWork when evaluated by real users. However, our study also highlights\nsignificant challenges in developing collaborative agents, requiring\nadvancements in core aspects of intelligence -- communication capabilities,\nsituational awareness, and balancing autonomy and human control.\n","authors":["Yijia Shao","Vinay Samuel","Yucheng Jiang","John Yang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2412.15701v2.pdf","comment":"Preprint. Work in progress"},{"id":"http://arxiv.org/abs/2501.09326v1","updated":"2025-01-16T06:51:32Z","published":"2025-01-16T06:51:32Z","title":"Algorithm for Semantic Network Generation from Texts of Low Resource\n  Languages Such as Kiswahili","summary":"  Processing low-resource languages, such as Kiswahili, using machine learning\nis difficult due to lack of adequate training data. However, such low-resource\nlanguages are still important for human communication and are already in daily\nuse and users need practical machine processing tasks such as summarization,\ndisambiguation and even question answering (QA). One method of processing such\nlanguages, while bypassing the need for training data, is the use semantic\nnetworks. Some low resource languages, such as Kiswahili, are of the\nsubject-verb-object (SVO) structure, and similarly semantic networks are a\ntriple of subject-predicate-object, hence SVO parts of speech tags can map into\na semantic network triple. An algorithm to process raw natural language text\nand map it into a semantic network is therefore necessary and desirable in\nstructuring low resource languages texts. This algorithm tested on the\nKiswahili QA task with upto 78.6% exact match.\n","authors":["Barack Wamkaya Wanjawa","Lawrence Muchemi","Evans Miriti"],"pdf_url":"https://arxiv.org/pdf/2501.09326v1.pdf","comment":"18 pages, 3 figures, published in Open Journal for Information\n  Technology"},{"id":"http://arxiv.org/abs/2501.08335v2","updated":"2025-01-16T06:16:43Z","published":"2024-12-21T05:50:48Z","title":"MERaLiON-TextLLM: Cross-Lingual Understanding of Large Language Models\n  in Chinese, Indonesian, Malay, and Singlish","summary":"  Multilingual large language models (MLLMs) have shown impressive capabilities\nacross a variety of languages. However, efficacy can differ greatly between\ndifferent language families, especially for those with limited linguistic\nresources. This report presents MERaLiON-TextLLM, a series of open-source\nlanguage models specifically tailored to improve understanding and generation\nin Chinese, Indonesian, Malay, and Singlish. The initial released model is\nbuilt on Llama-3-8B-Base and refined through a meticulously crafted process of\ncontinued pre-training and weight merging. Our approach achieves performance\nimprovements across benchmarks in these languages, exceeding the capabilities\nof the official Llama-3 models. We provide the model checkpoints as a resource\nto support further research and development in cross-lingual language\nunderstanding.\n","authors":["Xin Huang","Tarun Kumar Vangani","Minh Duc Pham","Xunlong Zou","Bin Wang","Zhengyuan Liu","Ai Ti Aw"],"pdf_url":"https://arxiv.org/pdf/2501.08335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15862v4","updated":"2025-01-16T06:07:12Z","published":"2024-11-24T14:38:59Z","title":"Do LLMs Really Think Step-by-step In Implicit Reasoning?","summary":"  It has been well-known that Chain-of-Thought can remarkably enhance LLMs'\nperformance on complex tasks. However, because it also introduces slower\ninference speeds and higher computational costs, many researches have attempted\nto use implicit CoT, which does not need LLMs to explicitly generate the\nintermediate steps. However, the invisible reasoning process leaves us a doubt\nthat, can implicit CoT really be equal to explicit CoT? Therefore, in this\nstudy, we address this question through experiments. We probe the information\nof intermediate steps from the model's hidden states when it is either trained\nor prompted to perform implicit CoT. The results surprisingly indicate that\nwhen prompted, LLMs hardly think about intermediate steps, suggesting they may\njust rely on experience rather than strict step-by-step reasoning. But when\ntrained, they indeed calculate intermediate steps. Moreover, in both\nsituations, we find the effect of using implicit CoT is susceptible to the\nformat of the problem, reaffirming the current deficiency of implicit CoT.\n","authors":["Yijiong Yu"],"pdf_url":"https://arxiv.org/pdf/2411.15862v4.pdf","comment":"The code is in\n  https://github.com/yuyijiong/if_step_by_step_implicit_CoT"},{"id":"http://arxiv.org/abs/2501.09311v1","updated":"2025-01-16T05:58:32Z","published":"2025-01-16T05:58:32Z","title":"Shape-Based Single Object Classification Using Ensemble Method\n  Classifiers","summary":"  Nowadays, more and more images are available. Annotation and retrieval of the\nimages pose classification problems, where each class is defined as the group\nof database images labelled with a common semantic label. Various systems have\nbeen proposed for content-based retrieval, as well as for image classification\nand indexing. In this paper, a hierarchical classification framework has been\nproposed for bridging the semantic gap effectively and achieving multi-category\nimage classification. A well known pre-processing and post-processing method\nwas used and applied to three problems; image segmentation, object\nidentification and image classification. The method was applied to classify\nsingle object images from Amazon and Google datasets. The classification was\ntested for four different classifiers; BayesNetwork (BN), Random Forest (RF),\nBagging and Vote. The estimated classification accuracies ranged from 20% to\n99% (using 10-fold cross validation). The Bagging classifier presents the best\nperformance, followed by the Random Forest classifier.\n","authors":["Nur Shazwani Kamarudin","Mokhairi Makhtar","Syadiah Nor Wan Shamsuddin","Syed Abdullah Fadzli"],"pdf_url":"https://arxiv.org/pdf/2501.09311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09310v1","updated":"2025-01-16T05:54:59Z","published":"2025-01-16T05:54:59Z","title":"A Study of In-Context-Learning-Based Text-to-SQL Errors","summary":"  Large language models (LLMs) have been adopted to perform text-to-SQL tasks,\nutilizing their in-context learning (ICL) capability to translate natural\nlanguage questions into structured query language (SQL). However, such a\ntechnique faces correctness problems and requires efficient repairing\nsolutions. In this paper, we conduct the first comprehensive study of\ntext-to-SQL errors. Our study covers four representative ICL-based techniques,\nfive basic repairing methods, two benchmarks, and two LLM settings. We find\nthat text-to-SQL errors are widespread and summarize 29 error types of 7\ncategories. We also find that existing repairing attempts have limited\ncorrectness improvement at the cost of high computational overhead with many\nmis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL\nerror detection and repairing framework. The evaluation demonstrates that\nMapleRepair outperforms existing solutions by repairing 13.8% more queries with\nneglectable mis-repairs and 67.4% less overhead.\n","authors":["Jiawei Shen","Chengcheng Wan","Ruoyi Qiao","Jiazhen Zou","Hang Xu","Yuchen Shao","Yueling Zhang","Weikai Miao","Geguang Pu"],"pdf_url":"https://arxiv.org/pdf/2501.09310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09309v1","updated":"2025-01-16T05:46:27Z","published":"2025-01-16T05:46:27Z","title":"Understanding Mental Health Content on Social Media and Its Effect\n  Towards Suicidal Ideation","summary":"  This review underscores the critical need for effective strategies to\nidentify and support individuals with suicidal ideation, exploiting\ntechnological innovations in ML and DL to further suicide prevention efforts.\nThe study details the application of these technologies in analyzing vast\namounts of unstructured social media data to detect linguistic patterns,\nkeywords, phrases, tones, and contextual cues associated with suicidal\nthoughts. It explores various ML and DL models like SVMs, CNNs, LSTM, neural\nnetworks, and their effectiveness in interpreting complex data patterns and\nemotional nuances within text data. The review discusses the potential of these\ntechnologies to serve as a life-saving tool by identifying at-risk individuals\nthrough their digital traces. Furthermore, it evaluates the real-world\neffectiveness, limitations, and ethical considerations of employing these\ntechnologies for suicide prevention, stressing the importance of responsible\ndevelopment and usage. The study aims to fill critical knowledge gaps by\nanalyzing recent studies, methodologies, tools, and techniques in this field.\nIt highlights the importance of synthesizing current literature to inform\npractical tools and suicide prevention efforts, guiding innovation in reliable,\nethical systems for early intervention. This research synthesis evaluates the\nintersection of technology and mental health, advocating for the ethical and\nresponsible application of ML, DL, and NLP to offer life-saving potential\nworldwide while addressing challenges like generalizability, biases, privacy,\nand the need for further research to ensure these technologies do not\nexacerbate existing inequities and harms.\n","authors":["Mohaiminul Islam Bhuiyan","Nur Shazwani Kamarudin","Nur Hafieza Ismail"],"pdf_url":"https://arxiv.org/pdf/2501.09309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09294v1","updated":"2025-01-16T05:01:30Z","published":"2025-01-16T05:01:30Z","title":"Efficient Few-Shot Medical Image Analysis via Hierarchical Contrastive\n  Vision-Language Learning","summary":"  Few-shot learning in medical image classification presents a significant\nchallenge due to the limited availability of annotated data and the complex\nnature of medical imagery. In this work, we propose Adaptive Vision-Language\nFine-tuning with Hierarchical Contrastive Alignment (HiCA), a novel framework\nthat leverages the capabilities of Large Vision-Language Models (LVLMs) for\nmedical image analysis. HiCA introduces a two-stage fine-tuning strategy,\ncombining domain-specific pretraining and hierarchical contrastive learning to\nalign visual and textual representations at multiple levels. We evaluate our\napproach on two benchmark datasets, Chest X-ray and Breast Ultrasound,\nachieving state-of-the-art performance in both few-shot and zero-shot settings.\nFurther analyses demonstrate the robustness, generalizability, and\ninterpretability of our method, with substantial improvements in performance\ncompared to existing baselines. Our work highlights the potential of\nhierarchical contrastive strategies in adapting LVLMs to the unique challenges\nof medical imaging tasks.\n","authors":["Harrison Fuller","Fernando Gabriela Garcia","Victor Flores"],"pdf_url":"https://arxiv.org/pdf/2501.09294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09292v1","updated":"2025-01-16T04:56:33Z","published":"2025-01-16T04:56:33Z","title":"To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic\n  Retrieval Augmented Generation","summary":"  Retrieval-Augmented Generation equips large language models with the\ncapability to retrieve external knowledge, thereby mitigating hallucinations by\nincorporating information beyond the model's intrinsic abilities. However, most\nprior works have focused on invoking retrieval deterministically, which makes\nit unsuitable for tasks such as long-form question answering. Instead,\ndynamically performing retrieval by invoking it only when the underlying LLM\nlacks the required knowledge can be more efficient. In this context, we delve\ndeeper into the question, \"To Retrieve or Not to Retrieve?\" by exploring\nmultiple uncertainty detection methods. We evaluate these methods for the task\nof long-form question answering, employing dynamic retrieval, and present our\ncomparisons. Our findings suggest that uncertainty detection metrics, such as\nDegree Matrix Jaccard and Eccentricity, can reduce the number of retrieval\ncalls by almost half, with only a slight reduction in question-answering\naccuracy.\n","authors":["Kaustubh D. Dhole"],"pdf_url":"https://arxiv.org/pdf/2501.09292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09265v1","updated":"2025-01-16T03:30:47Z","published":"2025-01-16T03:30:47Z","title":"Perspective Transition of Large Language Models for Solving Subjective\n  Tasks","summary":"  Large language models (LLMs) have revolutionized the field of natural\nlanguage processing, enabling remarkable progress in various tasks. Different\nfrom objective tasks such as commonsense reasoning and arithmetic\nquestion-answering, the performance of LLMs on subjective tasks is still\nlimited, where the perspective on the specific problem plays crucial roles for\nbetter interpreting the context and giving proper response. For example, in\ncertain scenarios, LLMs may perform better when answering from an expert role\nperspective, potentially eliciting their relevant domain knowledge. In\ncontrast, in some scenarios, LLMs may provide more accurate responses when\nanswering from a third-person standpoint, enabling a more comprehensive\nunderstanding of the problem and potentially mitigating inherent biases. In\nthis paper, we propose Reasoning through Perspective Transition (RPT), a method\nbased on in-context learning that enables LLMs to dynamically select among\ndirect, role, and third-person perspectives for the best way to solve\ncorresponding subjective problem. Through extensive experiments on totally 12\nsubjective tasks by using both closed-source and open-source LLMs including\nGPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single\nfixed perspective based methods such as chain-of-thought prompting and expert\nprompting, highlights the intricate ways that LLMs can adapt their perspectives\nto provide nuanced and contextually appropriate responses for different\nproblems.\n","authors":["Xiaolong Wang","Yuanchi Zhang","Ziyue Wang","Yuzhuang Xu","Fuwen Luo","Yile Wang","Peng Li","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2501.09265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09818v3","updated":"2025-01-16T03:29:23Z","published":"2024-12-13T03:15:05Z","title":"MERaLiON-AudioLLM: Bridging Audio and Language with Large Language\n  Models","summary":"  We introduce MERaLiON-AudioLLM (Multimodal Empathetic Reasoning and Learning\nin One Network), the first speech-text model tailored for Singapore's\nmultilingual and multicultural landscape. Developed under the National Large\nLanguage Models Funding Initiative, Singapore, MERaLiON-AudioLLM integrates\nadvanced speech and text processing to address the diverse linguistic nuances\nof local accents and dialects, enhancing accessibility and usability in\ncomplex, multilingual environments. Our results demonstrate improvements in\nboth speech recognition and task-specific understanding, positioning\nMERaLiON-AudioLLM as a pioneering solution for region specific AI applications.\nWe envision this release to set a precedent for future models designed to\naddress localised linguistic and cultural contexts in a global framework.\n","authors":["Yingxu He","Zhuohan Liu","Shuo Sun","Bin Wang","Wenyu Zhang","Xunlong Zou","Nancy F. Chen","Ai Ti Aw"],"pdf_url":"https://arxiv.org/pdf/2412.09818v3.pdf","comment":"https://huggingface.co/MERaLiON/MERaLiON-AudioLLM-Whisper-SEA-LION"},{"id":"http://arxiv.org/abs/2406.15477v2","updated":"2025-01-16T03:26:36Z","published":"2024-06-16T23:01:10Z","title":"CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for\n  Multi-label Social Media Text Classification in Disaster Informatics","summary":"  In the field of crisis/disaster informatics, social media is increasingly\nbeing used for improving situational awareness to inform response and relief\nefforts. Efficient and accurate text classification tools have been a focal\narea of investigation in crisis informatics. However, current methods mostly\nrely on single-label text classification models, which fails to capture\ndifferent insights embedded in dynamic and multifaceted disaster-related social\nmedia data. This study introduces a novel approach to disaster text\nclassification by enhancing a pre-trained Large Language Model (LLM) through\ninstruction fine-tuning targeted for multi-label classification of\ndisaster-related tweets. Our methodology involves creating a comprehensive\ninstruction dataset from disaster-related tweets, which is then used to\nfine-tune an open-source LLM, thereby embedding it with disaster-specific\nknowledge. This fine-tuned model can classify multiple aspects of\ndisaster-related information simultaneously, such as the type of event,\ninformativeness, and involvement of human aid, significantly improving the\nutility of social media data for situational awareness in disasters. The\nresults demonstrate that this approach enhances the categorization of critical\ninformation from social media posts, thereby facilitating a more effective\ndeployment for situational awareness during emergencies. This research paves\nthe way for more advanced, adaptable, and robust disaster management tools,\nleveraging the capabilities of LLMs to improve real-time situational awareness\nand response strategies in disaster scenarios.\n","authors":["Kai Yin","Chengkai Liu","Ali Mostafavi","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2406.15477v2.pdf","comment":"Relevant source code and data is available:\n  https://github.com/KaiYin97/CrsisLLM"},{"id":"http://arxiv.org/abs/2501.06848v3","updated":"2025-01-16T03:18:14Z","published":"2025-01-12T15:34:24Z","title":"A General Framework for Inference-time Scaling and Steering of Diffusion\n  Models","summary":"  Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we propose Feynman Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models, even\nwith off-the-shelf rewards, can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering .\n","authors":["Raghav Singhal","Zachary Horvitz","Ryan Teehan","Mengye Ren","Zhou Yu","Kathleen McKeown","Rajesh Ranganath"],"pdf_url":"https://arxiv.org/pdf/2501.06848v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09258v1","updated":"2025-01-16T03:01:50Z","published":"2025-01-16T03:01:50Z","title":"Delayed Fusion: Integrating Large Language Models into First-Pass\n  Decoding in End-to-end Speech Recognition","summary":"  This paper presents an efficient decoding approach for end-to-end automatic\nspeech recognition (E2E-ASR) with large language models (LLMs). Although\nshallow fusion is the most common approach to incorporate language models into\nE2E-ASR decoding, we face two practical problems with LLMs. (1) LLM inference\nis computationally costly. (2) There may be a vocabulary mismatch between the\nASR model and the LLM. To resolve this mismatch, we need to retrain the ASR\nmodel and/or the LLM, which is at best time-consuming and in many cases not\nfeasible. We propose \"delayed fusion,\" which applies LLM scores to ASR\nhypotheses with a delay during decoding and enables easier use of pre-trained\nLLMs in ASR tasks. This method can reduce not only the number of hypotheses\nscored by the LLM but also the number of LLM inference calls. It also allows\nre-tokenizion of ASR hypotheses during decoding if ASR and LLM employ different\ntokenizations. We demonstrate that delayed fusion provides improved decoding\nspeed and accuracy compared to shallow fusion and N-best rescoring using the\nLibriHeavy ASR corpus and three public LLMs, OpenLLaMA 3B & 7B and Mistral 7B.\n","authors":["Takaaki Hori","Martin Kocour","Adnan Haider","Erik McDermott","Xiaodan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2501.09258v1.pdf","comment":"Accepted to ICASSP2025"},{"id":"http://arxiv.org/abs/2404.13885v2","updated":"2025-01-16T02:45:07Z","published":"2024-04-22T05:12:52Z","title":"Surveying Attitudinal Alignment Between Large Language Models Vs. Humans\n  Towards 17 Sustainable Development Goals","summary":"  Large Language Models (LLMs) have emerged as potent tools for advancing the\nUnited Nations' Sustainable Development Goals (SDGs). However, the attitudinal\ndisparities between LLMs and humans towards these goals can pose significant\nchallenges. This study conducts a comprehensive review and analysis of the\nexisting literature on the attitudes of LLMs towards the 17 SDGs, emphasizing\nthe comparison between their attitudes and support for each goal and those of\nhumans. We examine the potential disparities, primarily focusing on aspects\nsuch as understanding and emotions, cultural and regional differences, task\nobjective variations, and factors considered in the decision-making process.\nThese disparities arise from the underrepresentation and imbalance in LLM\ntraining data, historical biases, quality issues, lack of contextual\nunderstanding, and skewed ethical values reflected. The study also investigates\nthe risks and harms that may arise from neglecting the attitudes of LLMs\ntowards the SDGs, including the exacerbation of social inequalities, racial\ndiscrimination, environmental destruction, and resource wastage. To address\nthese challenges, we propose strategies and recommendations to guide and\nregulate the application of LLMs, ensuring their alignment with the principles\nand goals of the SDGs, and therefore creating a more just, inclusive, and\nsustainable future.\n","authors":["Qingyang Wu","Ying Xu","Tingsong Xiao","Yunze Xiao","Yitong Li","Tianyang Wang","Yichi Zhang","Shanghai Zhong","Yuwei Zhang","Wei Lu","Yifan Yang"],"pdf_url":"https://arxiv.org/pdf/2404.13885v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02797v3","updated":"2025-01-16T02:31:20Z","published":"2024-01-05T13:22:12Z","title":"PeFoMed: Parameter Efficient Fine-tuning of Multimodal Large Language\n  Models for Medical Imaging","summary":"  Multimodal large language models (MLLMs) represent an evolutionary expansion\nin the capabilities of traditional large language models, enabling them to\ntackle challenges that surpass the scope of purely text-based applications. It\nleverages the knowledge previously encoded within these language models,\nthereby enhancing their applicability and functionality in the reign of\nmultimodal contexts. Recent works investigate the adaptation of MLLMs as a\nuniversal solution to address medical multi-modal problems as a generative\ntask. In this paper, we propose a parameter efficient framework for fine-tuning\nMLLMs, specifically validated on medical visual question answering (Med-VQA)\nand medical report generation (MRG) tasks, using public benchmark datasets. We\nalso introduce an evaluation metric using the 5-point Likert scale and its\nweighted average value to measure the quality of the generated reports for MRG\ntasks, where the scale ratings are labelled by both humans manually and the\nGPT-4 model. We further assess the consistency of performance metrics across\ntraditional measures, GPT-4, and human ratings for both VQA and MRG tasks. The\nresults indicate that semantic similarity assessments using GPT-4 align closely\nwith human annotators and provide greater stability, yet they reveal a\ndiscrepancy when compared to conventional lexical similarity measurements. This\nquestions the reliability of lexical similarity metrics for evaluating the\nperformance of generative models in Med-VQA and report generation tasks.\nBesides, our fine-tuned model significantly outperforms GPT-4v. This indicates\nthat without additional fine-tuning, multi-modal models like GPT-4v do not\nperform effectively on medical imaging tasks. The code will be available here:\nhttps://github.com/jinlHe/PeFoMed.\n","authors":["Jinlong He","Pengfei Li","Gang Liu","Genrong He","Zhaolin Chen","Shenjun Zhong"],"pdf_url":"https://arxiv.org/pdf/2401.02797v3.pdf","comment":"12 pages, 8 figures, 12 tables"},{"id":"http://arxiv.org/abs/2406.09948v2","updated":"2025-01-16T01:41:48Z","published":"2024-06-14T11:48:54Z","title":"BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures\n  and Languages","summary":"  Large language models (LLMs) often lack culture-specific knowledge of daily\nlife, especially across diverse regions and non-English languages. Existing\nbenchmarks for evaluating LLMs' cultural sensitivities are limited to a single\nlanguage or collected from online sources such as Wikipedia, which do not\nreflect the mundane everyday lifestyles of diverse regions. That is,\ninformation about the food people eat for their birthday celebrations, spices\nthey typically use, musical instruments youngsters play, or the sports they\npractice in school is common cultural knowledge but uncommon in easily\ncollected online sources, especially for underrepresented cultures. To address\nthis issue, we introduce BLEnD, a hand-crafted benchmark designed to evaluate\nLLMs' everyday knowledge across diverse cultures and languages. BLEnD comprises\n52.6k question-answer pairs from 16 countries/regions, in 13 different\nlanguages, including low-resource ones such as Amharic, Assamese, Azerbaijani,\nHausa, and Sundanese. We construct the benchmark to include two formats of\nquestions: short-answer and multiple-choice. We show that LLMs perform better\nfor cultures that are highly represented online, with a maximum 57.34%\ndifference in GPT-4, the best-performing model, in the short-answer format. For\ncultures represented by mid-to-high-resource languages, LLMs perform better in\ntheir local languages, but for cultures represented by low-resource languages,\nLLMs perform better in English than the local languages. We make our dataset\npublicly available at: https://github.com/nlee0212/BLEnD.\n","authors":["Junho Myung","Nayeon Lee","Yi Zhou","Jiho Jin","Rifki Afina Putri","Dimosthenis Antypas","Hsuvas Borkakoty","Eunsu Kim","Carla Perez-Almendros","Abinew Ali Ayele","Víctor Gutiérrez-Basulto","Yazmín Ibáñez-García","Hwaran Lee","Shamsuddeen Hassan Muhammad","Kiwoong Park","Anar Sabuhi Rzayev","Nina White","Seid Muhie Yimam","Mohammad Taher Pilehvar","Nedjma Ousidhoum","Jose Camacho-Collados","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2406.09948v2.pdf","comment":"Accepted to NeurIPS 2024 Datasets & Benchmark Track"},{"id":"http://arxiv.org/abs/2501.09223v1","updated":"2025-01-16T01:03:56Z","published":"2025-01-16T01:03:56Z","title":"Foundations of Large Language Models","summary":"  This is a book about large language models. As indicated by the title, it\nprimarily focuses on foundational concepts rather than comprehensive coverage\nof all cutting-edge technologies. The book is structured into four main\nchapters, each exploring a key area: pre-training, generative models, prompting\ntechniques, and alignment methods. It is intended for college students,\nprofessionals, and practitioners in natural language processing and related\nfields, and can serve as a reference for anyone interested in large language\nmodels.\n","authors":["Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2501.09223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09219v1","updated":"2025-01-16T00:35:56Z","published":"2025-01-16T00:35:56Z","title":"A Simple Graph Contrastive Learning Framework for Short Text\n  Classification","summary":"  Short text classification has gained significant attention in the information\nage due to its prevalence and real-world applications. Recent advancements in\ngraph learning combined with contrastive learning have shown promising results\nin addressing the challenges of semantic sparsity and limited labeled data in\nshort text classification. However, existing models have certain limitations.\nThey rely on explicit data augmentation techniques to generate contrastive\nviews, resulting in semantic corruption and noise. Additionally, these models\nonly focus on learning the intrinsic consistency between the generated views,\nneglecting valuable discriminative information from other potential views. To\naddress these issues, we propose a Simple graph contrastive learning framework\nfor Short Text Classification (SimSTC). Our approach involves performing graph\nlearning on multiple text-related component graphs to obtain multi-view text\nembeddings. Subsequently, we directly apply contrastive learning on these\nembeddings. Notably, our method eliminates the need for data augmentation\noperations to generate contrastive views while still leveraging the benefits of\nmulti-view contrastive learning. Despite its simplicity, our model achieves\noutstanding performance, surpassing large language models on various datasets.\n","authors":["Yonghao Liu","Fausto Giunchiglia","Lan Huang","Ximing Li","Xiaoyue Feng","Renchu Guan"],"pdf_url":"https://arxiv.org/pdf/2501.09219v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2501.09214v1","updated":"2025-01-16T00:26:15Z","published":"2025-01-16T00:26:15Z","title":"Boosting Short Text Classification with Multi-Source Information\n  Exploration and Dual-Level Contrastive Learning","summary":"  Short text classification, as a research subtopic in natural language\nprocessing, is more challenging due to its semantic sparsity and insufficient\nlabeled samples in practical scenarios. We propose a novel model named\nMI-DELIGHT for short text classification in this work. Specifically, it first\nperforms multi-source information (i.e., statistical information, linguistic\ninformation, and factual information) exploration to alleviate the sparsity\nissues. Then, the graph learning approach is adopted to learn the\nrepresentation of short texts, which are presented in graph forms. Moreover, we\nintroduce a dual-level (i.e., instance-level and cluster-level) contrastive\nlearning auxiliary task to effectively capture different-grained contrastive\ninformation within massive unlabeled data. Meanwhile, previous models merely\nperform the main task and auxiliary tasks in parallel, without considering the\nrelationship among tasks. Therefore, we introduce a hierarchical architecture\nto explicitly model the correlations between tasks. We conduct extensive\nexperiments across various benchmark datasets, demonstrating that MI-DELIGHT\nsignificantly surpasses previous competitive models. It even outperforms\npopular large language models on several datasets.\n","authors":["Yonghao Liu","Mengyu Li","Wei Pang","Fausto Giunchiglia","Lan Huang","Xiaoyue Feng","Renchu Guan"],"pdf_url":"https://arxiv.org/pdf/2501.09214v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2501.09213v1","updated":"2025-01-16T00:19:19Z","published":"2025-01-16T00:19:19Z","title":"FineMedLM-o1: Enhancing the Medical Reasoning Ability of LLM from\n  Supervised Fine-Tuning to Test-Time Training","summary":"  Recent advancements in large language models (LLMs) have shown promise in\nmedical applications such as disease diagnosis and treatment planning. However,\nmost existing medical LLMs struggle with the advanced reasoning required for\ncomplex clinical scenarios, such as differential diagnosis or personalized\ntreatment suggestions. We proposed FineMedLM-o1, which leverages high-quality\nsynthetic medical data and long-form reasoning data for Supervised Fine-Tuning\n(SFT) and Direct Preference Optimization (DPO), enabling advanced dialogue and\ndeep reasoning capabilities. Additionally, we introduced Test-Time Training\n(TTT) in the medical domain for the first time, facilitating domain adaptation\nand ensuring reliable, accurate reasoning. Experimental results demonstrate\nthat FineMedLM-o1 achieves a 23% average performance improvement over prior\nmodels on key medical benchmarks. Furthermore, the introduction of TTT provides\nan additional 14% performance boost, highlighting its effectiveness in\nenhancing medical reasoning capabilities. To support this process, we also\nproposed a novel method for synthesizing medical dialogue. Compared to other\nopen-source datasets, our dataset stands out as superior in both quality and\ncomplexity. The project and data will be released on GitHub.\n","authors":["Hongzhou Yu","Tianhao Cheng","Ying Cheng","Rui Feng"],"pdf_url":"https://arxiv.org/pdf/2501.09213v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2501.09757v1","updated":"2025-01-16T18:59:53Z","published":"2025-01-16T18:59:53Z","title":"Distilling Multi-modal Large Language Models for Autonomous Driving","summary":"  Autonomous driving demands safe motion planning, especially in critical\n\"long-tail\" scenarios. Recent end-to-end autonomous driving systems leverage\nlarge language models (LLMs) as planners to improve generalizability to rare\nevents. However, using LLMs at test time introduces high computational costs.\nTo address this, we propose DiMA, an end-to-end autonomous driving system that\nmaintains the efficiency of an LLM-free (or vision-based) planner while\nleveraging the world knowledge of an LLM. DiMA distills the information from a\nmulti-modal LLM to a vision-based end-to-end planner through a set of specially\ndesigned surrogate tasks. Under a joint training strategy, a scene encoder\ncommon to both networks produces structured representations that are\nsemantically grounded as well as aligned to the final planning objective.\nNotably, the LLM is optional at inference, enabling robust planning without\ncompromising on efficiency. Training with DiMA results in a 37% reduction in\nthe L2 trajectory error and an 80% reduction in the collision rate of the\nvision-based planner, as well as a 44% trajectory error reduction in longtail\nscenarios. DiMA also achieves state-of-the-art performance on the nuScenes\nplanning benchmark.\n","authors":["Deepti Hegde","Rajeev Yasarla","Hong Cai","Shizhong Han","Apratim Bhattacharyya","Shweta Mahajan","Litian Liu","Risheek Garrepalli","Vishal M. Patel","Fatih Porikli"],"pdf_url":"https://arxiv.org/pdf/2501.09757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09756v1","updated":"2025-01-16T18:59:48Z","published":"2025-01-16T18:59:48Z","title":"SynthLight: Portrait Relighting with Diffusion Model by Learning to\n  Re-render Synthetic Faces","summary":"  We introduce SynthLight, a diffusion model for portrait relighting. Our\napproach frames image relighting as a re-rendering problem, where pixels are\ntransformed in response to changes in environmental lighting conditions. Using\na physically-based rendering engine, we synthesize a dataset to simulate this\nlighting-conditioned transformation with 3D head assets under varying lighting.\nWe propose two training and inference strategies to bridge the gap between the\nsynthetic and real image domains: (1) multi-task training that takes advantage\nof real human portraits without lighting labels; (2) an inference time\ndiffusion sampling procedure based on classifier-free guidance that leverages\nthe input portrait to better preserve details. Our method generalizes to\ndiverse real photographs and produces realistic illumination effects, including\nspecular highlights and cast shadows, while preserving the subject's identity.\nOur quantitative experiments on Light Stage data demonstrate results comparable\nto state-of-the-art relighting methods. Our qualitative results on in-the-wild\nimages showcase rich and unprecedented illumination effects. Project Page:\n\\url{https://vrroom.github.io/synthlight/}\n","authors":["Sumit Chaturvedi","Mengwei Ren","Yannick Hold-Geoffroy","Jingyuan Liu","Julie Dorsey","Zhixin Shu"],"pdf_url":"https://arxiv.org/pdf/2501.09756v1.pdf","comment":"27 pages, 25 figures, Project Page\n  https://vrroom.github.io/synthlight/"},{"id":"http://arxiv.org/abs/2501.09755v1","updated":"2025-01-16T18:59:04Z","published":"2025-01-16T18:59:04Z","title":"Learnings from Scaling Visual Tokenizers for Reconstruction and\n  Generation","summary":"  Visual tokenization via auto-encoding empowers state-of-the-art image and\nvideo generative models by compressing pixels into a latent space. Although\nscaling Transformer-based generators has been central to recent advances, the\ntokenizer component itself is rarely scaled, leaving open questions about how\nauto-encoder design choices influence both its objective of reconstruction and\ndownstream generative performance. Our work aims to conduct an exploration of\nscaling in auto-encoders to fill in this blank. To facilitate this exploration,\nwe replace the typical convolutional backbone with an enhanced Vision\nTransformer architecture for Tokenization (ViTok). We train ViTok on\nlarge-scale image and video datasets far exceeding ImageNet-1K, removing data\nconstraints on tokenizer scaling. We first study how scaling the auto-encoder\nbottleneck affects both reconstruction and generation -- and find that while it\nis highly correlated with reconstruction, its relationship with generation is\nmore complex. We next explored the effect of separately scaling the\nauto-encoders' encoder and decoder on reconstruction and generation\nperformance. Crucially, we find that scaling the encoder yields minimal gains\nfor either reconstruction or generation, while scaling the decoder boosts\nreconstruction but the benefits for generation are mixed. Building on our\nexploration, we design ViTok as a lightweight auto-encoder that achieves\ncompetitive performance with state-of-the-art auto-encoders on ImageNet-1K and\nCOCO reconstruction tasks (256p and 512p) while outperforming existing\nauto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x\nfewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates\ncompetitive performance on image generation for ImageNet-1K and sets new\nstate-of-the-art benchmarks for class-conditional video generation on UCF-101.\n","authors":["Philippe Hansen-Estruch","David Yan","Ching-Yao Chung","Orr Zohar","Jialiang Wang","Tingbo Hou","Tao Xu","Sriram Vishwanath","Peter Vajda","Xinlei Chen"],"pdf_url":"https://arxiv.org/pdf/2501.09755v1.pdf","comment":"28 pages, 25 figures, 7 Tables"},{"id":"http://arxiv.org/abs/2501.09754v1","updated":"2025-01-16T18:59:03Z","published":"2025-01-16T18:59:03Z","title":"Lost in Translation, Found in Context: Sign Language Translation with\n  Contextual Cues","summary":"  Our objective is to translate continuous sign language into spoken language\ntext. Inspired by the way human interpreters rely on context for accurate\ntranslation, we incorporate additional contextual cues together with the\nsigning video, into a new translation framework. Specifically, besides visual\nsign recognition features that encode the input video, we integrate\ncomplementary textual information from (i) captions describing the background\nshow, (ii) translation of previous sentences, as well as (iii) pseudo-glosses\ntranscribing the signing. These are automatically extracted and inputted along\nwith the visual features to a pre-trained large language model (LLM), which we\nfine-tune to generate spoken language translations in text form. Through\nextensive ablation studies, we show the positive contribution of each input cue\nto the translation performance. We train and evaluate our approach on BOBSL --\nthe largest British Sign Language dataset currently available. We show that our\ncontextual approach significantly enhances the quality of the translations\ncompared to previously reported results on BOBSL, and also to state-of-the-art\nmethods that we implement as baselines. Furthermore, we demonstrate the\ngenerality of our approach by applying it also to How2Sign, an American Sign\nLanguage dataset, and achieve competitive results.\n","authors":["Youngjoon Jang","Haran Raajesh","Liliane Momeni","Gül Varol","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2501.09754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09753v1","updated":"2025-01-16T18:59:02Z","published":"2025-01-16T18:59:02Z","title":"SRE-Conv: Symmetric Rotation Equivariant Convolution for Biomedical\n  Image Classification","summary":"  Convolutional neural networks (CNNs) are essential tools for computer vision\ntasks, but they lack traditionally desired properties of extracted features\nthat could further improve model performance, e.g., rotational equivariance.\nSuch properties are ubiquitous in biomedical images, which often lack explicit\norientation. While current work largely relies on data augmentation or explicit\nmodules to capture orientation information, this comes at the expense of\nincreased training costs or ineffective approximations of the desired\nequivariance. To overcome these challenges, we propose a novel and efficient\nimplementation of the Symmetric Rotation-Equivariant (SRE) Convolution\n(SRE-Conv) kernel, designed to learn rotation-invariant features while\nsimultaneously compressing the model size. The SRE-Conv kernel can easily be\nincorporated into any CNN backbone. We validate the ability of a deep SRE-CNN\nto capture equivariance to rotation using the public MedMNISTv2 dataset (16\ntotal tasks). SRE-Conv-CNN demonstrated improved rotated image classification\nperformance accuracy on all 16 test datasets in both 2D and 3D images, all\nwhile increasing efficiency with fewer parameters and reduced memory footprint.\nThe code is available at https://github.com/XYPB/SRE-Conv.\n","authors":["Yuexi Du","Jiazhen Zhang","Tal Zeevi","Nicha C. Dvornek","John A. Onofrey"],"pdf_url":"https://arxiv.org/pdf/2501.09753v1.pdf","comment":"Accepted by IEEE ISBI 2025 4-page paper"},{"id":"http://arxiv.org/abs/2403.12953v2","updated":"2025-01-16T18:58:31Z","published":"2024-03-19T17:55:22Z","title":"FutureDepth: Learning to Predict the Future Improves Video Depth\n  Estimation","summary":"  In this paper, we propose a novel video depth estimation approach,\nFutureDepth, which enables the model to implicitly leverage multi-frame and\nmotion cues to improve depth estimation by making it learn to predict the\nfuture at training. More specifically, we propose a future prediction network,\nF-Net, which takes the features of multiple consecutive frames and is trained\nto predict multi-frame features one time step ahead iteratively. In this way,\nF-Net learns the underlying motion and correspondence information, and we\nincorporate its features into the depth decoding process. Additionally, to\nenrich the learning of multiframe correspondence cues, we further leverage a\nreconstruction network, R-Net, which is trained via adaptively masked\nauto-encoding of multiframe feature volumes. At inference time, both F-Net and\nR-Net are used to produce queries to work with the depth decoder, as well as a\nfinal refinement network. Through extensive experiments on several benchmarks,\ni.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and\nopen-domain scenarios, we show that FutureDepth significantly improves upon\nbaseline models, outperforms existing video depth estimation methods, and sets\nnew state-of-the-art (SOTA) accuracy. Furthermore, FutureDepth is more\nefficient than existing SOTA video depth estimation models and has similar\nlatencies when comparing to monocular models\n","authors":["Rajeev Yasarla","Manish Kumar Singh","Hong Cai","Yunxiao Shi","Jisoo Jeong","Yinhao Zhu","Shizhong Han","Risheek Garrepalli","Fatih Porikli"],"pdf_url":"https://arxiv.org/pdf/2403.12953v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2307.14336v3","updated":"2025-01-16T18:55:36Z","published":"2023-07-26T17:55:32Z","title":"MAMo: Leveraging Memory and Attention for Monocular Video Depth\n  Estimation","summary":"  We propose MAMo, a novel memory and attention frame-work for monocular video\ndepth estimation. MAMo can augment and improve any single-image depth\nestimation networks into video depth estimation models, enabling them to take\nadvantage of the temporal information to predict more accurate depth. In MAMo,\nwe augment model with memory which aids the depth prediction as the model\nstreams through the video. Specifically, the memory stores learned visual and\ndisplacement tokens of the previous time instances. This allows the depth\nnetwork to cross-reference relevant features from the past when predicting\ndepth on the current frame. We introduce a novel scheme to continuously update\nthe memory, optimizing it to keep tokens that correspond with both the past and\nthe present visual information. We adopt attention-based approach to process\nmemory features where we first learn the spatio-temporal relation among the\nresultant visual and displacement memory tokens using self-attention module.\nFurther, the output features of self-attention are aggregated with the current\nvisual features through cross-attention. The cross-attended features are\nfinally given to a decoder to predict depth on the current frame. Through\nextensive experiments on several benchmarks, including KITTI, NYU-Depth V2, and\nDDAD, we show that MAMo consistently improves monocular depth estimation\nnetworks and sets new state-of-the-art (SOTA) accuracy. Notably, our MAMo video\ndepth estimation provides higher accuracy with lower latency, when omparing to\nSOTA cost-volume-based video depth models.\n","authors":["Rajeev Yasarla","Hong Cai","Jisoo Jeong","Yunxiao Shi","Risheek Garrepalli","Fatih Porikli"],"pdf_url":"https://arxiv.org/pdf/2307.14336v3.pdf","comment":"Accepted at ICCV 2023"},{"id":"http://arxiv.org/abs/2501.09733v1","updated":"2025-01-16T18:35:45Z","published":"2025-01-16T18:35:45Z","title":"ComplexVAD: Detecting Interaction Anomalies in Video","summary":"  Existing video anomaly detection datasets are inadequate for representing\ncomplex anomalies that occur due to the interactions between objects. The\nabsence of complex anomalies in previous video anomaly detection datasets\naffects research by shifting the focus onto simple anomalies. To address this\nproblem, we introduce a new large-scale dataset: ComplexVAD. In addition, we\npropose a novel method to detect complex anomalies via modeling the\ninteractions between objects using a scene graph with spatio-temporal\nattributes. With our proposed method and two other state-of-the-art video\nanomaly detection methods, we obtain baseline scores on ComplexVAD and\ndemonstrate that our new method outperforms existing works.\n","authors":["Furkan Mumcu","Michael J. Jones","Yasin Yilmaz","Anoop Cherian"],"pdf_url":"https://arxiv.org/pdf/2501.09733v1.pdf","comment":"16 pages, 11 figures, to appear in WACV Workshop ASTAD 2025"},{"id":"http://arxiv.org/abs/2501.09732v1","updated":"2025-01-16T18:30:37Z","published":"2025-01-16T18:30:37Z","title":"Inference-Time Scaling for Diffusion Models beyond Scaling Denoising\n  Steps","summary":"  Generative models have made significant impacts across various domains,\nlargely due to their ability to scale during training by increasing data,\ncomputational resources, and model size, a phenomenon characterized by the\nscaling laws. Recent research has begun to explore inference-time scaling\nbehavior in Large Language Models (LLMs), revealing how performance can further\nimprove with additional computation during inference. Unlike LLMs, diffusion\nmodels inherently possess the flexibility to adjust inference-time computation\nvia the number of denoising steps, although the performance gains typically\nflatten after a few dozen. In this work, we explore the inference-time scaling\nbehavior of diffusion models beyond increasing denoising steps and investigate\nhow the generation performance can further improve with increased computation.\nSpecifically, we consider a search problem aimed at identifying better noises\nfor the diffusion sampling process. We structure the design space along two\naxes: the verifiers used to provide feedback, and the algorithms used to find\nbetter noise candidates. Through extensive experiments on class-conditioned and\ntext-conditioned image generation benchmarks, our findings reveal that\nincreasing inference-time compute leads to substantial improvements in the\nquality of samples generated by diffusion models, and with the complicated\nnature of images, combinations of the components in the framework can be\nspecifically chosen to conform with different application scenario.\n","authors":["Nanye Ma","Shangyuan Tong","Haolin Jia","Hexiang Hu","Yu-Chuan Su","Mingda Zhang","Xuan Yang","Yandong Li","Tommi Jaakkola","Xuhui Jia","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2501.09732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09720v1","updated":"2025-01-16T18:09:22Z","published":"2025-01-16T18:09:22Z","title":"A Simple Aerial Detection Baseline of Multimodal Language Models","summary":"  The multimodal language models (MLMs) based on generative pre-trained\nTransformer are considered powerful candidates for unifying various domains and\ntasks. MLMs developed for remote sensing (RS) have demonstrated outstanding\nperformance in multiple tasks, such as visual question answering and visual\ngrounding. In addition to visual grounding that detects specific objects\ncorresponded to given instruction, aerial detection, which detects all objects\nof multiple categories, is also a valuable and challenging task for RS\nfoundation models. However, aerial detection has not been explored by existing\nRS MLMs because the autoregressive prediction mechanism of MLMs differs\nsignificantly from the detection outputs. In this paper, we present a simple\nbaseline for applying MLMs to aerial detection for the first time, named\nLMMRotate. Specifically, we first introduce a normalization method to transform\ndetection outputs into textual outputs to be compatible with the MLM framework.\nThen, we propose a evaluation method, which ensures a fair comparison between\nMLMs and conventional object detection models. We construct the baseline by\nfine-tuning open-source general-purpose MLMs and achieve impressive detection\nperformance comparable to conventional detector. We hope that this baseline\nwill serve as a reference for future MLM development, enabling more\ncomprehensive capabilities for understanding RS images. Code is available at\nhttps://github.com/Li-Qingyun/mllm-mmrotate.\n","authors":["Qingyun Li","Yushi Chen","Xinya Shu","Dong Chen","Xin He","Yi Yu","Xue Yang"],"pdf_url":"https://arxiv.org/pdf/2501.09720v1.pdf","comment":"4 pages, 1 table, 4 figures"},{"id":"http://arxiv.org/abs/2501.09718v1","updated":"2025-01-16T18:06:09Z","published":"2025-01-16T18:06:09Z","title":"FLOL: Fast Baselines for Real-World Low-Light Enhancement","summary":"  Low-Light Image Enhancement (LLIE) is a key task in computational photography\nand imaging. The problem of enhancing images captured during night or in dark\nenvironments has been well-studied in the image signal processing literature.\nHowever, current deep learning-based solutions struggle with efficiency and\nrobustness in real-world scenarios (e.g. scenes with noise, saturated pixels,\nbad illumination). We propose a lightweight neural network that combines image\nprocessing in the frequency and spatial domains. Our method, FLOL+, is one of\nthe fastest models for this task, achieving state-of-the-art results on popular\nreal scenes datasets such as LOL and LSRW. Moreover, we are able to process\n1080p images under 12ms. Code and models at https://github.com/cidautai/FLOL\n","authors":["Juan C. Benito","Daniel Feijoo","Alvaro Garcia","Marcos V. Conde"],"pdf_url":"https://arxiv.org/pdf/2501.09718v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2501.09705v1","updated":"2025-01-16T17:57:53Z","published":"2025-01-16T17:57:53Z","title":"Practical Continual Forgetting for Pre-trained Vision Models","summary":"  For privacy and security concerns, the need to erase unwanted information\nfrom pre-trained vision models is becoming evident nowadays. In real-world\nscenarios, erasure requests originate at any time from both users and model\nowners, and these requests usually form a sequence. Therefore, under such a\nsetting, selective information is expected to be continuously removed from a\npre-trained model while maintaining the rest. We define this problem as\ncontinual forgetting and identify three key challenges. (i) For unwanted\nknowledge, efficient and effective deleting is crucial. (ii) For remaining\nknowledge, the impact brought by the forgetting procedure should be minimal.\n(iii) In real-world scenarios, the training samples may be scarce or partially\nmissing during the process of forgetting. To address them, we first propose\nGroup Sparse LoRA (GS-LoRA). Specifically, towards (i), we introduce LoRA\nmodules to fine-tune the FFN layers in Transformer blocks for each forgetting\ntask independently, and towards (ii), a simple group sparse regularization is\nadopted, enabling automatic selection of specific LoRA groups and zeroing out\nthe others. To further extend GS-LoRA to more practical scenarios, we\nincorporate prototype information as additional supervision and introduce a\nmore practical approach, GS-LoRA++. For each forgotten class, we move the\nlogits away from its original prototype. For the remaining classes, we pull the\nlogits closer to their respective prototypes. We conduct extensive experiments\non face recognition, object detection and image classification and demonstrate\nthat our method manages to forget specific classes with minimal impact on other\nclasses. Codes have been released on https://github.com/bjzhb666/GS-LoRA.\n","authors":["Hongbo Zhao","Fei Zhu","Bolin Ni","Feng Zhu","Gaofeng Meng","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.09705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09695v1","updated":"2025-01-16T17:48:03Z","published":"2025-01-16T17:48:03Z","title":"Mitigating Hallucinations in Large Vision-Language Models via DPO:\n  On-Policy Data Hold the Key","summary":"  Hallucination remains a major challenge for Large Vision-Language Models\n(LVLMs). Direct Preference Optimization (DPO) has gained increasing attention\nas a simple solution to hallucination issues. It directly learns from\nconstructed preference pairs that reflect the severity of hallucinations in\nresponses to the same prompt and image. Nonetheless, different data\nconstruction methods in existing works bring notable performance variations. We\nidentify a crucial factor here: outcomes are largely contingent on whether the\nconstructed data aligns on-policy w.r.t the initial (reference) policy of DPO.\nTheoretical analysis suggests that learning from off-policy data is impeded by\nthe presence of KL-divergence between the updated policy and the reference\npolicy. From the perspective of dataset distribution, we systematically\nsummarize the inherent flaws in existing algorithms that employ DPO to address\nhallucination issues. To alleviate the problems, we propose On-Policy Alignment\n(OPA)-DPO framework, which uniquely leverages expert feedback to correct\nhallucinated responses and aligns both the original and expert-revised\nresponses in an on-policy manner. Notably, with only 4.8k data, OPA-DPO\nachieves an additional reduction in the hallucination rate of LLaVA-1.5-7B:\n13.26% on the AMBER benchmark and 5.39% on the Object-Hal benchmark, compared\nto the previous SOTA algorithm trained with 16k samples.\n","authors":["Zhihe Yang","Xufang Luo","Dongqi Han","Yunjian Xu","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2501.09695v1.pdf","comment":"18 pages, 15 figures"},{"id":"http://arxiv.org/abs/2501.09688v1","updated":"2025-01-16T17:40:19Z","published":"2025-01-16T17:40:19Z","title":"Fine-Grained Image-Text Correspondence with Cost Aggregation for\n  Open-Vocabulary Part Segmentation","summary":"  Open-Vocabulary Part Segmentation (OVPS) is an emerging field for recognizing\nfine-grained parts in unseen categories. We identify two primary challenges in\nOVPS: (1) the difficulty in aligning part-level image-text correspondence, and\n(2) the lack of structural understanding in segmenting object parts. To address\nthese issues, we propose PartCATSeg, a novel framework that integrates\nobject-aware part-level cost aggregation, compositional loss, and structural\nguidance from DINO. Our approach employs a disentangled cost aggregation\nstrategy that handles object and part-level costs separately, enhancing the\nprecision of part-level segmentation. We also introduce a compositional loss to\nbetter capture part-object relationships, compensating for the limited part\nannotations. Additionally, structural guidance from DINO features improves\nboundary delineation and inter-part understanding. Extensive experiments on\nPascal-Part-116, ADE20K-Part-234, and PartImageNet datasets demonstrate that\nour method significantly outperforms state-of-the-art approaches, setting a new\nbaseline for robust generalization to unseen part categories.\n","authors":["Jiho Choi","Seonho Lee","Minhyun Lee","Seungho Lee","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2501.09688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01184v2","updated":"2025-01-16T17:11:06Z","published":"2025-01-02T10:21:34Z","title":"Vulnerability-Aware Spatio-Temporal Learning for Generalizable and\n  Interpretable Deepfake Video Detection","summary":"  Detecting deepfake videos is highly challenging due to the complex\nintertwined spatial and temporal artifacts in forged sequences. Most recent\napproaches rely on binary classifiers trained on both real and fake data.\nHowever, such methods may struggle to focus on important artifacts, which can\nhinder their generalization capability. Additionally, these models often lack\ninterpretability, making it difficult to understand how predictions are made.\nTo address these issues, we propose FakeSTormer, offering two key\ncontributions. First, we introduce a multi-task learning framework with\nadditional spatial and temporal branches that enable the model to focus on\nsubtle spatio-temporal artifacts. These branches also provide interpretability\nby highlighting video regions that may contain artifacts. Second, we propose a\nvideo-level data synthesis algorithm that generates pseudo-fake videos with\nsubtle artifacts, providing the model with high-quality samples and ground\ntruth data for our spatial and temporal branches. Extensive experiments on\nseveral challenging benchmarks demonstrate the competitiveness of our approach\ncompared to recent state-of-the-art methods. The code is available at\nhttps://github.com/10Ring/FakeSTormer.\n","authors":["Dat Nguyen","Marcella Astrid","Anis Kacem","Enjie Ghorbel","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2501.01184v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05728v2","updated":"2025-01-16T17:09:57Z","published":"2025-01-10T05:53:32Z","title":"Super-class guided Transformer for Zero-Shot Attribute Classification","summary":"  Attribute classification is crucial for identifying specific characteristics\nwithin image regions. Vision-Language Models (VLMs) have been effective in\nzero-shot tasks by leveraging their general knowledge from large-scale\ndatasets. Recent studies demonstrate that transformer-based models with\nclass-wise queries can effectively address zero-shot multi-label\nclassification. However, poor utilization of the relationship between seen and\nunseen attributes makes the model lack generalizability. Additionally,\nattribute classification generally involves many attributes, making maintaining\nthe model's scalability difficult. To address these issues, we propose\nSuper-class guided transFormer (SugaFormer), a novel framework that leverages\nsuper-classes to enhance scalability and generalizability for zero-shot\nattribute classification. SugaFormer employs Super-class Query Initialization\n(SQI) to reduce the number of queries, utilizing common semantic information\nfrom super-classes, and incorporates Multi-context Decoding (MD) to handle\ndiverse visual cues. To strengthen generalizability, we introduce two knowledge\ntransfer strategies that utilize VLMs. During training, Super-class guided\nConsistency Regularization (SCR) aligns model's features with VLMs using\nsuper-class guided prompts, and during inference, Zero-shot Retrieval-based\nScore Enhancement (ZRSE) refines predictions for unseen attributes. Extensive\nexperiments demonstrate that SugaFormer achieves state-of-the-art performance\nacross three widely-used attribute classification benchmarks under zero-shot,\nand cross-dataset transfer settings. Our code is available at\nhttps://github.com/mlvlab/SugaFormer.\n","authors":["Sehyung Kim","Chanhyeong Yang","Jihwan Park","Taehoon Song","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2501.05728v2.pdf","comment":"AAAI25"},{"id":"http://arxiv.org/abs/2501.09672v1","updated":"2025-01-16T17:08:12Z","published":"2025-01-16T17:08:12Z","title":"Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP\n  Evaluation Benchmark","summary":"  The proliferation of Vision-Language Models (VLMs) in the past several years\ncalls for rigorous and comprehensive evaluation methods and benchmarks. This\nwork analyzes existing VLM evaluation techniques, including automated metrics,\nAI-based assessments, and human evaluations across diverse tasks. We first\nintroduce Robin - a novel suite of VLMs that we built by combining Large\nLanguage Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use\nRobin to identify shortcomings of current evaluation approaches across scales.\nNext, to overcome the identified limitations, we introduce CHIRP - a new long\nform response benchmark we developed for more robust and complete VLM\nevaluation. We provide open access to the Robin training code, model suite, and\nCHIRP benchmark to promote reproducibility and advance VLM research.\n","authors":["Alexis Roger","Prateek Humane","Daniel Z. Kaplan","Kshitij Gupta","Qi Sun","George Adamopoulos","Jonathan Siu Chi Lim","Quentin Anthony","Edwin Fennell","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2501.09672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01034v2","updated":"2025-01-16T16:45:29Z","published":"2024-02-01T21:45:12Z","title":"VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image\n  Segmentation and Classification","summary":"  Artificial Intelligence (AI) has the potential to revolutionize diagnosis and\nsegmentation in medical imaging. However, development and clinical\nimplementation face multiple challenges including limited data availability,\nlack of generalizability, and the necessity to incorporate multi-modal data\neffectively. A foundation model, which is a large-scale pre-trained AI model,\noffers a versatile base that can be adapted to a variety of specific tasks and\ncontexts. Here, we present VIsualization and Segmentation Masked AutoEncoder\n(VIS-MAE), novel model weights specifically designed for medical imaging.\nSpecifically, VIS-MAE is trained on a dataset of 2.5 million unlabeled images\nfrom various modalities (CT, MR, PET,X-rays, and ultrasound), using\nself-supervised learning techniques. It is then adapted to classification and\nsegmentation tasks using explicit labels. VIS-MAE has high label efficiency,\noutperforming several benchmark models in both in-domain and out-of-domain\napplications. In addition, VIS-MAE has improved label efficiency as it can\nachieve similar performance to other models with a reduced amount of labeled\ntraining data (50% or 80%) compared to other pre-trained weights. VIS-MAE\nrepresents a significant advancement in medical imaging AI, offering a\ngeneralizable and robust solution for improving segmentation and classification\ntasks while reducing the data annotation workload. The source code of this work\nis available at https://github.com/lzl199704/VIS-MAE.\n","authors":["Zelong Liu","Andrew Tieu","Nikhil Patel","Georgios Soultanidis","Louisa Deyer","Ying Wang","Sean Huver","Alexander Zhou","Yunhao Mei","Zahi A. Fayad","Timothy Deyer","Xueyan Mei"],"pdf_url":"https://arxiv.org/pdf/2402.01034v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17097v2","updated":"2025-01-16T16:27:33Z","published":"2024-05-27T12:12:26Z","title":"A Comparative Study on Multi-task Uncertainty Quantification in Semantic\n  Segmentation and Monocular Depth Estimation","summary":"  Deep neural networks excel in perception tasks such as semantic segmentation\nand monocular depth estimation, making them indispensable in safety-critical\napplications like autonomous driving and industrial inspection. However, they\noften suffer from overconfidence and poor explainability, especially for\nout-of-domain data. While uncertainty quantification has emerged as a promising\nsolution to these challenges, multi-task settings have yet to be explored. In\nan effort to shed light on this, we evaluate Monte Carlo Dropout, Deep\nSub-Ensembles, and Deep Ensembles for joint semantic segmentation and monocular\ndepth estimation. Thereby, we reveal that Deep Ensembles stand out as the\npreferred choice, particularly in out-of-domain scenarios, and show the\npotential benefit of multi-task learning with regard to the uncertainty quality\nin comparison to solving both tasks separately. Additionally, we highlight the\nimpact of employing different uncertainty thresholds to classify pixels as\ncertain or uncertain, with the median uncertainty emerging as a robust default.\n","authors":["Steven Landgraf","Markus Hillemann","Theodor Kapler","Markus Ulrich"],"pdf_url":"https://arxiv.org/pdf/2405.17097v2.pdf","comment":"This manuscript is an extended version of a previously published\n  conference paper and is currently in review for a journal"},{"id":"http://arxiv.org/abs/2501.09635v1","updated":"2025-01-16T16:24:21Z","published":"2025-01-16T16:24:21Z","title":"Unified Face Matching and Physical-Digital Spoofing Attack Detection","summary":"  Face recognition technology has dramatically transformed the landscape of\nsecurity, surveillance, and authentication systems, offering a user-friendly\nand non-invasive biometric solution. However, despite its significant\nadvantages, face recognition systems face increasing threats from physical and\ndigital spoofing attacks. Current research typically treats face recognition\nand attack detection as distinct classification challenges. This approach\nnecessitates the implementation of separate models for each task, leading to\nconsiderable computational complexity, particularly on devices with limited\nresources. Such inefficiencies can stifle scalability and hinder performance.\nIn response to these challenges, this paper introduces an innovative unified\nmodel designed for face recognition and detection of physical and digital\nattacks. By leveraging the advanced Swin Transformer backbone and incorporating\nHiLo attention in a convolutional neural network framework, we address unified\nface recognition and spoof attack detection more effectively. Moreover, we\nintroduce augmentation techniques that replicate the traits of physical and\ndigital spoofing cues, significantly enhancing our model robustness. Through\ncomprehensive experimental evaluation across various datasets, we showcase the\neffectiveness of our model in unified face recognition and spoof detection.\nAdditionally, we confirm its resilience against unseen physical and digital\nspoofing attacks, underscoring its potential for real-world applications.\n","authors":["Arun Kunwar","Ajita Rattani"],"pdf_url":"https://arxiv.org/pdf/2501.09635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10729v3","updated":"2025-01-16T16:04:07Z","published":"2024-06-15T20:04:06Z","title":"A Comprehensive Survey of Foundation Models in Medicine","summary":"  Foundation models (FMs) are large-scale deep learning models trained on\nmassive datasets, often using self-supervised learning techniques. These models\nserve as a versatile base for a wide range of downstream tasks, including those\nin medicine and healthcare. FMs have demonstrated remarkable success across\nmultiple healthcare domains. However, existing surveys in this field do not\ncomprehensively cover all areas where FMs have made significant strides. In\nthis survey, we present a comprehensive review of FMs in medicine, focusing on\ntheir evolution, learning strategies, flagship models, applications, and\nassociated challenges. We examine how prominent FMs, such as the BERT and GPT\nfamilies, are transforming various aspects of healthcare, including clinical\nlarge language models, medical image analysis, and omics research.\nAdditionally, we provide a detailed taxonomy of FM-enabled healthcare\napplications, spanning clinical natural language processing, medical computer\nvision, graph learning, and other biology- and omics- related tasks. Despite\nthe transformative potentials of FMs, they also pose unique challenges. This\nsurvey delves into these challenges and highlights open research questions and\nlessons learned to guide researchers and practitioners. Our goal is to provide\nvaluable insights into the capabilities of FMs in health, facilitating\nresponsible deployment and mitigating associated risks.\n","authors":["Wasif Khan","Seowung Leem","Kyle B. See","Joshua K. Wong","Shaoting Zhang","Ruogu Fang"],"pdf_url":"https://arxiv.org/pdf/2406.10729v3.pdf","comment":"Currently under review in IEEE REVIEWS IN BIOMEDICAL ENGINEERING"},{"id":"http://arxiv.org/abs/2501.05555v2","updated":"2025-01-16T16:00:37Z","published":"2025-01-09T20:02:10Z","title":"Improving Zero-Shot Object-Level Change Detection by Incorporating\n  Visual Correspondence","summary":"  Detecting object-level changes between two images across possibly different\nviews is a core task in many applications that involve visual inspection or\ncamera surveillance. Existing change-detection approaches suffer from three\nmajor limitations: (1) lack of evaluation on image pairs that contain no\nchanges, leading to unreported false positive rates; (2) lack of\ncorrespondences (i.e., localizing the regions before and after a change); and\n(3) poor zero-shot generalization across different domains. To address these\nissues, we introduce a novel method that leverages change correspondences (a)\nduring training to improve change detection accuracy, and (b) at test time, to\nminimize false positives. That is, we harness the supervision labels of where\nan object is added or removed to supervise change detectors, improving their\naccuracy over previous work by a large margin. Our work is also the first to\npredict correspondences between pairs of detected changes using estimated\nhomography and the Hungarian algorithm. Our model demonstrates superior\nperformance over existing methods, achieving state-of-the-art results in change\ndetection and change correspondence accuracy across both in-distribution and\nzero-shot benchmarks.\n","authors":["Hung Huy Nguyen","Pooyan Rahmanzadehgervi","Long Mai","Anh Totti Nguyen"],"pdf_url":"https://arxiv.org/pdf/2501.05555v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09617v1","updated":"2025-01-16T15:44:24Z","published":"2025-01-16T15:44:24Z","title":"WMamba: Wavelet-based Mamba for Face Forgery Detection","summary":"  With the rapid advancement of deepfake generation technologies, the demand\nfor robust and accurate face forgery detection algorithms has become\nincreasingly critical. Recent studies have demonstrated that wavelet analysis\ncan uncover subtle forgery artifacts that remain imperceptible in the spatial\ndomain. Wavelets effectively capture important facial contours, which are often\nslender, fine-grained, and global in nature. However, existing wavelet-based\napproaches fail to fully leverage these unique characteristics, resulting in\nsub-optimal feature extraction and limited generalizability. To address this\nchallenge, we introduce WMamba, a novel wavelet-based feature extractor built\nupon the Mamba architecture. WMamba maximizes the utility of wavelet\ninformation through two key innovations. First, we propose Dynamic Contour\nConvolution (DCConv), which employs specially crafted deformable kernels to\nadaptively model slender facial contours. Second, by leveraging the Mamba\narchitecture, our method captures long-range spatial relationships with linear\ncomputational complexity. This efficiency allows for the extraction of\nfine-grained, global forgery artifacts from small image patches. Extensive\nexperimental results show that WMamba achieves state-of-the-art (SOTA)\nperformance, highlighting its effectiveness and superiority in face forgery\ndetection.\n","authors":["Siran Peng","Tianshuo Zhang","Li Gao","Xiangyu Zhu","Haoyuan Zhang","Kai Pang","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2501.09617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09608v1","updated":"2025-01-16T15:32:41Z","published":"2025-01-16T15:32:41Z","title":"Metric Learning with Progressive Self-Distillation for Audio-Visual\n  Embedding Learning","summary":"  Metric learning projects samples into an embedded space, where similarities\nand dissimilarities are quantified based on their learned representations.\nHowever, existing methods often rely on label-guided representation learning,\nwhere representations of different modalities, such as audio and visual data,\nare aligned based on annotated labels. This approach tends to underutilize\nlatent complex features and potential relationships inherent in the\ndistributions of audio and visual data that are not directly tied to the\nlabels, resulting in suboptimal performance in audio-visual embedding learning.\nTo address this issue, we propose a novel architecture that integrates\ncross-modal triplet loss with progressive self-distillation. Our method\nenhances representation learning by leveraging inherent distributions and\ndynamically refining soft audio-visual alignments -- probabilistic alignments\nbetween audio and visual data that capture the inherent relationships beyond\nexplicit labels. Specifically, the model distills audio-visual\ndistribution-based knowledge from annotated labels in a subset of each batch.\nThis self-distilled knowledge is used t\n","authors":["Donghuo Zeng","Kazushi Ikeda"],"pdf_url":"https://arxiv.org/pdf/2501.09608v1.pdf","comment":"5 pages, 3 figures, 2 tables. Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.07227v2","updated":"2025-01-16T15:30:54Z","published":"2025-01-13T11:28:49Z","title":"MECD+: Unlocking Event-Level Causal Graph Discovery for Video Reasoning","summary":"  Video causal reasoning aims to achieve a high-level understanding of videos\nfrom a causal perspective. However, it exhibits limitations in its scope,\nprimarily executed in a question-answering paradigm and focusing on brief video\nsegments containing isolated events and basic causal relations, lacking\ncomprehensive and structured causality analysis for videos with multiple\ninterconnected events. To fill this gap, we introduce a new task and dataset,\nMulti-Event Causal Discovery (MECD). It aims to uncover the causal relations\nbetween events distributed chronologically across long videos. Given visual\nsegments and textual descriptions of events, MECD identifies the causal\nassociations between these events to derive a comprehensive and structured\nevent-level video causal graph explaining why and how the result event\noccurred. To address the challenges of MECD, we devise a novel framework\ninspired by the Granger Causality method, incorporating an efficient mask-based\nevent prediction model to perform an Event Granger Test. It estimates causality\nby comparing the predicted result event when premise events are masked versus\nunmasked. Furthermore, we integrate causal inference techniques such as\nfront-door adjustment and counterfactual inference to mitigate challenges in\nMECD like causality confounding and illusory causality. Additionally, context\nchain reasoning is introduced to conduct more robust and generalized reasoning.\nExperiments validate the effectiveness of our framework in reasoning complete\ncausal relations, outperforming GPT-4o and VideoChat2 by 5.77% and 2.70%,\nrespectively. Further experiments demonstrate that causal relation graphs can\nalso contribute to downstream video understanding tasks such as video question\nanswering and video event prediction.\n","authors":["Tieyuan Chen","Huabin Liu","Yi Wang","Yihang Chen","Tianyao He","Chaofan Gan","Huanyu He","Weiyao Lin"],"pdf_url":"https://arxiv.org/pdf/2501.07227v2.pdf","comment":"IEEE TPAMI Submission. continuous work of arXiv:2409.17647 (NeurIPS\n  2024)"},{"id":"http://arxiv.org/abs/2501.09600v1","updated":"2025-01-16T15:22:06Z","published":"2025-01-16T15:22:06Z","title":"Mesh2SLAM in VR: A Fast Geometry-Based SLAM Framework for Rapid\n  Prototyping in Virtual Reality Applications","summary":"  SLAM is a foundational technique with broad applications in robotics and\nAR/VR. SLAM simulations evaluate new concepts, but testing on\nresource-constrained devices, such as VR HMDs, faces challenges: high\ncomputational cost and restricted sensor data access. This work proposes a\nsparse framework using mesh geometry projections as features, which improves\nefficiency and circumvents direct sensor data access, advancing SLAM research\nas we demonstrate in VR and through numerical evaluation.\n","authors":["Carlos Augusto Pinheiro de Sousa","Heiko Hamann","Oliver Deussen"],"pdf_url":"https://arxiv.org/pdf/2501.09600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01957v2","updated":"2025-01-16T15:00:16Z","published":"2025-01-03T18:59:52Z","title":"VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction","summary":"  Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction.\n","authors":["Chaoyou Fu","Haojia Lin","Xiong Wang","Yi-Fan Zhang","Yunhang Shen","Xiaoyu Liu","Yangze Li","Zuwei Long","Heting Gao","Ke Li","Long Ma","Xiawu Zheng","Rongrong Ji","Xing Sun","Caifeng Shan","Ran He"],"pdf_url":"https://arxiv.org/pdf/2501.01957v2.pdf","comment":"https://github.com/VITA-MLLM/VITA"},{"id":"http://arxiv.org/abs/2501.09579v1","updated":"2025-01-16T14:56:41Z","published":"2025-01-16T14:56:41Z","title":"Sequential PatchCore: Anomaly Detection for Surface Inspection using\n  Synthetic Impurities","summary":"  The appearance of surface impurities (e.g., water stains, fingerprints,\nstickers) is an often-mentioned issue that causes degradation of automated\nvisual inspection systems. At the same time, synthetic data generation\ntechniques for visual surface inspection have focused primarily on generating\nperfect examples and defects, disregarding impurities. This study highlights\nthe importance of considering impurities when generating synthetic data. We\nintroduce a procedural method to include photorealistic water stains in\nsynthetic data. The synthetic datasets are generated to correspond to real\ndatasets and are further used to train an anomaly detection model and\ninvestigate the influence of water stains. The high-resolution images used for\nsurface inspection lead to memory bottlenecks during anomaly detection\ntraining. To address this, we introduce Sequential PatchCore - a method to\nbuild coresets sequentially and make training on large images using\nconsumer-grade hardware tractable. This allows us to perform transfer learning\nusing coresets pre-trained on different dataset versions. Our results show the\nbenefits of using synthetic data for pre-training an explicit coreset anomaly\nmodel and the extended performance benefits of finetuning the coreset using\nreal data. We observed how the impurities and labelling ambiguity lower the\nmodel performance and have additionally reported the defect-wise recall to\nprovide an industrially relevant perspective on model performance.\n","authors":["Runzhou Mao","Juraj Fulir","Christoph Garth","Petra Gospodnetić"],"pdf_url":"https://arxiv.org/pdf/2501.09579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20891v4","updated":"2025-01-16T14:45:36Z","published":"2024-07-30T15:07:13Z","title":"Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian\n  Neural Networks","summary":"  Computational complexity of Bayesian learning is impeding its adoption in\npractical, large-scale tasks. Despite demonstrations of significant merits such\nas improved robustness and resilience to unseen or out-of-distribution inputs\nover their non- Bayesian counterparts, their practical use has faded to near\ninsignificance. In this study, we introduce an innovative framework to mitigate\nthe computational burden of Bayesian neural networks (BNNs). Our approach\nfollows the principle of Bayesian techniques based on deep ensembles, but\nsignificantly reduces their cost via multiple low-rank perturbations of\nparameters arising from a pre-trained neural network. Both vanilla version of\nensembles as well as more sophisticated schemes such as Bayesian learning with\nStein Variational Gradient Descent (SVGD), previously deemed impractical for\nlarge models, can be seamlessly implemented within the proposed framework,\ncalled Bayesian Low-Rank LeArning (Bella). In a nutshell, i) Bella achieves a\ndramatic reduction in the number of trainable parameters required to\napproximate a Bayesian posterior; and ii) it not only maintains, but in some\ninstances, surpasses the performance of conventional Bayesian learning methods\nand non-Bayesian baselines. Our results with large-scale tasks such as\nImageNet, CAMELYON17, DomainNet, VQA with CLIP, LLaVA demonstrate the\neffectiveness and versatility of Bella in building highly scalable and\npractical Bayesian deep models for real-world applications.\n","authors":["Bao Gia Doan","Afshar Shamsi","Xiao-Yu Guo","Arash Mohammadi","Hamid Alinejad-Rokny","Dino Sejdinovic","Damien Teney","Damith C. Ranasinghe","Ehsan Abbasnejad"],"pdf_url":"https://arxiv.org/pdf/2407.20891v4.pdf","comment":"This paper is accepted in AAAI'2025"},{"id":"http://arxiv.org/abs/2412.04755v2","updated":"2025-01-16T14:44:39Z","published":"2024-12-06T03:40:21Z","title":"Latent Space Characterization of Autoencoder Variants","summary":"  Understanding the latent spaces learned by deep learning models is crucial in\nexploring how they represent and generate complex data. Autoencoders (AEs) have\nplayed a key role in the area of representation learning, with numerous\nregularization techniques and training principles developed not only to enhance\ntheir ability to learn compact and robust representations, but also to reveal\nhow different architectures influence the structure and smoothness of the\nlower-dimensional non-linear manifold. We strive to characterize the structure\nof the latent spaces learned by different autoencoders including convolutional\nautoencoders (CAEs), denoising autoencoders (DAEs), and variational\nautoencoders (VAEs) and how they change with the perturbations in the input. By\ncharacterizing the matrix manifolds corresponding to the latent spaces, we\nprovide an explanation for the well-known observation that the latent spaces of\nCAE and DAE form non-smooth manifolds, while that of VAE forms a smooth\nmanifold. We also map the points of the matrix manifold to a Hilbert space\nusing distance preserving transforms and provide an alternate view in terms of\nthe subspaces generated in the Hilbert space as a function of the distortion in\nthe input. The results show that the latent manifolds of CAE and DAE are\nstratified with each stratum being a smooth product manifold, while the\nmanifold of VAE is a smooth product manifold of two symmetric positive definite\nmatrices and a symmetric positive semi-definite matrix.\n","authors":["Anika Shrivastava","Renu Rameshan","Samar Agnihotri"],"pdf_url":"https://arxiv.org/pdf/2412.04755v2.pdf","comment":"9 pages, 6 figures, and 1 table"},{"id":"http://arxiv.org/abs/2501.09565v1","updated":"2025-01-16T14:40:02Z","published":"2025-01-16T14:40:02Z","title":"A New Teacher-Reviewer-Student Framework for Semi-supervised 2D Human\n  Pose Estimation","summary":"  Conventional 2D human pose estimation methods typically require extensive\nlabeled annotations, which are both labor-intensive and expensive. In contrast,\nsemi-supervised 2D human pose estimation can alleviate the above problems by\nleveraging a large amount of unlabeled data along with a small portion of\nlabeled data. Existing semi-supervised 2D human pose estimation methods update\nthe network through backpropagation, ignoring crucial historical information\nfrom the previous training process. Therefore, we propose a novel\nsemi-supervised 2D human pose estimation method by utilizing a newly designed\nTeacher-Reviewer-Student framework. Specifically, we first mimic the phenomenon\nthat human beings constantly review previous knowledge for consolidation to\ndesign our framework, in which the teacher predicts results to guide the\nstudent's learning and the reviewer stores important historical parameters to\nprovide additional supervision signals. Secondly, we introduce a Multi-level\nFeature Learning strategy, which utilizes the outputs from different stages of\nthe backbone to estimate the heatmap to guide network training, enriching the\nsupervisory information while effectively capturing keypoint relationships.\nFinally, we design a data augmentation strategy, i.e., Keypoint-Mix, to perturb\npose information by mixing different keypoints, thus enhancing the network's\nability to discern keypoints. Extensive experiments on publicly available\ndatasets, demonstrate our method achieves significant improvements compared to\nthe existing methods.\n","authors":["Wulian Yun","Mengshi Qi","Fei Peng","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2501.09565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09555v1","updated":"2025-01-16T14:18:06Z","published":"2025-01-16T14:18:06Z","title":"Text-driven Adaptation of Foundation Models for Few-shot Surgical\n  Workflow Analysis","summary":"  Purpose: Surgical workflow analysis is crucial for improving surgical\nefficiency and safety. However, previous studies rely heavily on large-scale\nannotated datasets, posing challenges in cost, scalability, and reliance on\nexpert annotations. To address this, we propose Surg-FTDA (Few-shot Text-driven\nAdaptation), designed to handle various surgical workflow analysis tasks with\nminimal paired image-label data.\n  Methods: Our approach has two key components. First, Few-shot selection-based\nmodality alignment selects a small subset of images and aligns their embeddings\nwith text embeddings from the downstream task, bridging the modality gap.\nSecond, Text-driven adaptation leverages only text data to train a decoder,\neliminating the need for paired image-text data. This decoder is then applied\nto aligned image embeddings, enabling image-related tasks without explicit\nimage-text pairs.\n  Results: We evaluate our approach to generative tasks (image captioning) and\ndiscriminative tasks (triplet recognition and phase recognition). Results show\nthat Surg-FTDA outperforms baselines and generalizes well across downstream\ntasks.\n  Conclusion: We propose a text-driven adaptation approach that mitigates the\nmodality gap and handles multiple downstream tasks in surgical workflow\nanalysis, with minimal reliance on large annotated datasets. The code and\ndataset will be released in https://github.com/TingxuanSix/Surg-FTDA.\n","authors":["Tingxuan Chen","Kun Yuan","Vinkle Srivastav","Nassir Navab","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2501.09555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09552v1","updated":"2025-01-16T14:12:33Z","published":"2025-01-16T14:12:33Z","title":"Exploring AI-based System Design for Pixel-level Protected Health\n  Information Detection in Medical Images","summary":"  De-identification of medical images is a critical step to ensure privacy\nduring data sharing in research and clinical settings. The initial step in this\nprocess involves detecting Protected Health Information (PHI), which can be\nfound in image metadata or imprinted within image pixels. Despite the\nimportance of such systems, there has been limited evaluation of existing\nAI-based solutions, creating barriers to the development of reliable and robust\ntools. In this study, we present an AI-based pipeline for PHI detection,\ncomprising three key components: text detection, text extraction, and analysis\nof PHI content in medical images. By experimenting with exchanging roles of\nvision and language models within the pipeline, we evaluate the performance and\nrecommend the best setup for the PHI detection task.\n","authors":["Tuan Truong","Ivo M. Baltruschat","Mark Klemens","Grit Werner","Matthias Lenga"],"pdf_url":"https://arxiv.org/pdf/2501.09552v1.pdf","comment":"In progress"},{"id":"http://arxiv.org/abs/2404.14388v3","updated":"2025-01-16T14:02:26Z","published":"2024-04-22T17:46:29Z","title":"STROOBnet Optimization via GPU-Accelerated Proximal Recurrence\n  Strategies","summary":"  Spatiotemporal networks' observational capabilities are crucial for accurate\ndata gathering and informed decisions across multiple sectors. This study\nfocuses on the Spatiotemporal Ranged Observer-Observable Bipartite Network\n(STROOBnet), linking observational nodes (e.g., surveillance cameras) to events\nwithin defined geographical regions, enabling efficient monitoring. Using data\nfrom Real-Time Crime Camera (RTCC) systems and Calls for Service (CFS) in New\nOrleans, where RTCC combats rising crime amidst reduced police presence, we\naddress the network's initial observational imbalances. Aiming for uniform\nobservational efficacy, we propose the Proximal Recurrence approach. It\noutperformed traditional clustering methods like k-means and DBSCAN by offering\nholistic event frequency and spatial consideration, enhancing observational\ncoverage.\n","authors":["Ted Edward Holmberg","Mahdi Abdelguerfi","Elias Ioup"],"pdf_url":"https://arxiv.org/pdf/2404.14388v3.pdf","comment":"10 pages, 17 figures, 2023 IEEE International Conference on Big Data\n  (BigData)"},{"id":"http://arxiv.org/abs/2409.07989v2","updated":"2025-01-16T14:01:58Z","published":"2024-09-12T12:34:29Z","title":"Enhancing Few-Shot Image Classification through Learnable Multi-Scale\n  Embedding and Attention Mechanisms","summary":"  In the context of few-shot classification, the goal is to train a classifier\nusing a limited number of samples while maintaining satisfactory performance.\nHowever, traditional metric-based methods exhibit certain limitations in\nachieving this objective. These methods typically rely on a single distance\nvalue between the query feature and support feature, thereby overlooking the\ncontribution of shallow features. To overcome this challenge, we propose a\nnovel approach in this paper. Our approach involves utilizing a multi-output\nembedding network that maps samples into distinct feature spaces. The proposed\nmethod extracts feature vectors at different stages, enabling the model to\ncapture both global and abstract features. By utilizing these diverse feature\nspaces, our model enhances its performance. Moreover, employing a\nself-attention mechanism improves the refinement of features at each stage,\nleading to even more robust representations and improved overall performance.\nFurthermore, assigning learnable weights to each stage significantly improved\nperformance and results. We conducted comprehensive evaluations on the\nMiniImageNet and FC100 datasets, specifically in the 5-way 1-shot and 5-way\n5-shot scenarios. Additionally, we performed cross-domain tasks across eight\nbenchmark datasets, achieving high accuracy in the testing domains. These\nevaluations demonstrate the efficacy of our proposed method in comparison to\nstate-of-the-art approaches. https://github.com/FatemehAskari/MSENet\n","authors":["Fatemeh Askari","Amirreza Fateh","Mohammad Reza Mohammadi"],"pdf_url":"https://arxiv.org/pdf/2409.07989v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09532v1","updated":"2025-01-16T13:34:33Z","published":"2025-01-16T13:34:33Z","title":"AdaFV: Accelerating VLMs with Self-Adaptive Cross-Modality Attention\n  Mixture","summary":"  The success of VLMs often relies on the dynamic high-resolution schema that\nadaptively augments the input images to multiple crops, so that the details of\nthe images can be retained. However, such approaches result in a large number\nof redundant visual tokens, thus significantly reducing the efficiency of the\nVLMs. To improve the VLMs' efficiency without introducing extra training costs,\nmany research works are proposed to reduce the visual tokens by filtering the\nuninformative visual tokens or aggregating their information. Some approaches\npropose to reduce the visual tokens according to the self-attention of VLMs,\nwhich are biased, to result in inaccurate responses. The token reduction\napproaches solely rely on visual cues are text-agnostic, and fail to focus on\nthe areas that are most relevant to the question, especially when the queried\nobjects are non-salient to the image. In this work, we first conduct\nexperiments to show that the original text embeddings are aligned with the\nvisual tokens, without bias on the tailed visual tokens. We then propose a\nself-adaptive cross-modality attention mixture mechanism that dynamically\nleverages the effectiveness of visual saliency and text-to-image similarity in\nthe pre-LLM layers to select the visual tokens that are informative. Extensive\nexperiments demonstrate that the proposed approach achieves state-of-the-art\ntraining-free VLM acceleration performance, especially when the reduction rate\nis sufficiently large.\n","authors":["Jiayi Han","Liang Du","Yiwen Wu","Xiangguo Zhou","Hongwei Du","Weibo Zheng"],"pdf_url":"https://arxiv.org/pdf/2501.09532v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.24031v3","updated":"2025-01-16T13:20:56Z","published":"2024-10-31T15:29:51Z","title":"A Multi-Modal Approach for Face Anti-Spoofing in Non-Calibrated Systems\n  using Disparity Maps","summary":"  Face recognition technologies are increasingly used in various applications,\nyet they are vulnerable to face spoofing attacks. These spoofing attacks often\ninvolve unique 3D structures, such as printed papers or mobile device screens.\nAlthough stereo-depth cameras can detect such attacks effectively, their\nhigh-cost limits their widespread adoption. Conversely, two-sensor systems\nwithout extrinsic calibration offer a cost-effective alternative but are unable\nto calculate depth using stereo techniques. In this work, we propose a method\nto overcome this challenge by leveraging facial attributes to derive disparity\ninformation and estimate relative depth for anti-spoofing purposes, using\nnon-calibrated systems. We introduce a multi-modal anti-spoofing model, coined\nDisparity Model, that incorporates created disparity maps as a third modality\nalongside the two original sensor modalities. We demonstrate the effectiveness\nof the Disparity Model in countering various spoof attacks using a\ncomprehensive dataset collected from the Intel RealSense ID Solution F455. Our\nmethod outperformed existing methods in the literature, achieving an Equal\nError Rate (EER) of 1.71% and a False Negative Rate (FNR) of 2.77% at a False\nPositive Rate (FPR) of 1%. These errors are lower by 2.45% and 7.94% than the\nerrors of the best comparison method, respectively. Additionally, we introduce\na model ensemble that addresses 3D spoof attacks as well, achieving an EER of\n2.04% and an FNR of 3.83% at an FPR of 1%. Overall, our work provides a\nstate-of-the-art solution for the challenging task of anti-spoofing in\nnon-calibrated systems that lack depth information.\n","authors":["Ariel Larey","Eyal Rond","Omer Achrack"],"pdf_url":"https://arxiv.org/pdf/2410.24031v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09504v1","updated":"2025-01-16T12:33:48Z","published":"2025-01-16T12:33:48Z","title":"HydraMix: Multi-Image Feature Mixing for Small Data Image Classification","summary":"  Training deep neural networks requires datasets with a large number of\nannotated examples. The collection and annotation of these datasets is not only\nextremely expensive but also faces legal and privacy problems. These factors\nare a significant limitation for many real-world applications. To address this,\nwe introduce HydraMix, a novel architecture that generates new image\ncompositions by mixing multiple different images from the same class. HydraMix\nlearns the fusion of the content of various images guided by a\nsegmentation-based mixing mask in feature space and is optimized via a\ncombination of unsupervised and adversarial training. Our data augmentation\nscheme allows the creation of models trained from scratch on very small\ndatasets. We conduct extensive experiments on ciFAIR-10, STL-10, and\nciFAIR-100. Additionally, we introduce a novel text-image metric to assess the\ngenerality of the augmented datasets. Our results show that HydraMix\noutperforms existing state-of-the-art methods for image classification on small\ndatasets.\n","authors":["Christoph Reinders","Frederik Schubert","Bodo Rosenhahn"],"pdf_url":"https://arxiv.org/pdf/2501.09504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09503v1","updated":"2025-01-16T12:28:39Z","published":"2025-01-16T12:28:39Z","title":"AnyStory: Towards Unified Single and Multiple Subject Personalization in\n  Text-to-Image Generation","summary":"  Recently, large-scale generative models have demonstrated outstanding\ntext-to-image generation capabilities. However, generating high-fidelity\npersonalized images with specific subjects still presents challenges,\nespecially in cases involving multiple subjects. In this paper, we propose\nAnyStory, a unified approach for personalized subject generation. AnyStory not\nonly achieves high-fidelity personalization for single subjects, but also for\nmultiple subjects, without sacrificing subject fidelity. Specifically, AnyStory\nmodels the subject personalization problem in an \"encode-then-route\" manner. In\nthe encoding step, AnyStory utilizes a universal and powerful image encoder,\ni.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve\nhigh-fidelity encoding of subject features. In the routing step, AnyStory\nutilizes a decoupled instance-aware subject router to accurately perceive and\npredict the potential location of the corresponding subject in the latent\nspace, and guide the injection of subject conditions. Detailed experimental\nresults demonstrate the excellent performance of our method in retaining\nsubject details, aligning text descriptions, and personalizing for multiple\nsubjects. The project page is at https://aigcdesigngroup.github.io/AnyStory/ .\n","authors":["Junjie He","Yuxiang Tuo","Binghui Chen","Chongyang Zhong","Yifeng Geng","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2501.09503v1.pdf","comment":"Tech report; Project page:\n  https://aigcdesigngroup.github.io/AnyStory/"},{"id":"http://arxiv.org/abs/2501.09502v1","updated":"2025-01-16T12:27:05Z","published":"2025-01-16T12:27:05Z","title":"Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling\n  for Multimodal Emotion Analysis","summary":"  Understanding emotions accurately is essential for fields like human-computer\ninteraction. Due to the complexity of emotions and their multi-modal nature\n(e.g., emotions are influenced by facial expressions and audio), researchers\nhave turned to using multi-modal models to understand human emotions rather\nthan single-modality. However, current video multi-modal large language models\n(MLLMs) encounter difficulties in effectively integrating audio and identifying\nsubtle facial micro-expressions. Furthermore, the lack of detailed emotion\nanalysis datasets also limits the development of multimodal emotion analysis.\nTo address these issues, we introduce a self-reviewed dataset and a\nhuman-reviewed dataset, comprising 24,137 coarse-grained samples and 3,500\nmanually annotated samples with detailed emotion annotations, respectively.\nThese datasets allow models to learn from diverse scenarios and better\ngeneralize to real-world applications. Moreover, in addition to the audio\nmodeling, we propose to explicitly integrate facial encoding models into the\nexisting advanced Video MLLM, enabling the MLLM to effectively unify audio and\nthe subtle facial cues for emotion understanding. By aligning these features\nwithin a unified space and employing instruction tuning in our proposed\ndatasets, our Omni-Emotion achieves state-of-the-art performance in both\nemotion recognition and reasoning tasks.\n","authors":["Qize Yang","Detao Bai","Yi-Xing Peng","Xihan Wei"],"pdf_url":"https://arxiv.org/pdf/2501.09502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09499v1","updated":"2025-01-16T12:20:40Z","published":"2025-01-16T12:20:40Z","title":"VanGogh: A Unified Multimodal Diffusion-based Framework for Video\n  Colorization","summary":"  Video colorization aims to transform grayscale videos into vivid color\nrepresentations while maintaining temporal consistency and structural\nintegrity. Existing video colorization methods often suffer from color bleeding\nand lack comprehensive control, particularly under complex motion or diverse\nsemantic cues. To this end, we introduce VanGogh, a unified multimodal\ndiffusion-based framework for video colorization. VanGogh tackles these\nchallenges using a Dual Qformer to align and fuse features from multiple\nmodalities, complemented by a depth-guided generation process and an optical\nflow loss, which help reduce color overflow. Additionally, a color injection\nstrategy and luma channel replacement are implemented to improve generalization\nand mitigate flickering artifacts. Thanks to this design, users can exercise\nboth global and local control over the generation process, resulting in\nhigher-quality colorized videos. Extensive qualitative and quantitative\nevaluations, and user studies, demonstrate that VanGogh achieves superior\ntemporal consistency and color fidelity.Project page:\nhttps://becauseimbatman0.github.io/VanGogh.\n","authors":["Zixun Fang","Zhiheng Liu","Kai Zhu","Yu Liu","Ka Leong Cheng","Wei Zhai","Yang Cao","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2501.09499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.09377v3","updated":"2025-01-16T12:12:24Z","published":"2023-06-15T08:18:29Z","title":"Evaluating alignment between humans and neural network representations\n  in image-based learning tasks","summary":"  Humans represent scenes and objects in rich feature spaces, carrying\ninformation that allows us to generalise about category memberships and\nabstract functions with few examples. What determines whether a neural network\nmodel generalises like a human? We tested how well the representations of $86$\npretrained neural network models mapped to human learning trajectories across\ntwo tasks where humans had to learn continuous relationships and categories of\nnatural images. In these tasks, both human participants and neural networks\nsuccessfully identified the relevant stimulus features within a few trials,\ndemonstrating effective generalisation. We found that while training dataset\nsize was a core determinant of alignment with human choices, contrastive\ntraining with multi-modal data (text and imagery) was a common feature of\ncurrently publicly available models that predicted human generalisation.\nIntrinsic dimensionality of representations had different effects on alignment\nfor different model types. Lastly, we tested three sets of human-aligned\nrepresentations and found no consistent improvements in predictive accuracy\ncompared to the baselines. In conclusion, pretrained neural networks can serve\nto extract representations for cognitive models, as they appear to capture some\nfundamental aspects of cognition that are transferable across tasks. Both our\nparadigms and modelling approach offer a novel way to quantify alignment\nbetween neural networks and humans and extend cognitive science into more\nnaturalistic domains.\n","authors":["Can Demircan","Tankred Saanum","Leonardo Pettini","Marcel Binz","Blazej M Baczkowski","Christian F Doeller","Mona M Garvert","Eric Schulz"],"pdf_url":"https://arxiv.org/pdf/2306.09377v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08443v2","updated":"2025-01-16T12:06:35Z","published":"2024-12-26T05:41:31Z","title":"Instruction-Guided Fusion of Multi-Layer Visual Features in Large\n  Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) have achieved significant success in\nmultimodal tasks by combining pre-trained vision encoders and large language\nmodels. However, current LVLMs mainly rely on features from the final layers of\nthe vision encoder, neglecting complementary information in shallower layers.\nWhile recent methods have explored multi-layer features, they are often\ntask-agnostic. We investigate the contributions of visual features from\ndifferent encoder layers across 18 benchmarks and 6 task categories. Our\nresults show that multi-layer features provide complementary strengths with\nvarying task dependencies, and uniform fusion performs suboptimally. Based on\nthese findings, we propose an instruction-guided vision aggregator that\ndynamically integrates multi-layer features based on textual instructions,\nwithout increasing the number of visual tokens. Extensive evaluations show\nsuperior performance, and analysis reveals the dominance of mid-to-high-level\nfeatures in semantic tasks and the critical role of low-level features in\nfine-grained perception. This work provides valuable insights into the adaptive\nuse of hierarchical visual features in LVLMs, advancing more flexible\nmultimodal systems.\n","authors":["Xu Li","Yi Zheng","Haotian Chen","Xiaolei Chen","Yuxuan Liang","Chenghang Lai","Bin Li","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2501.08443v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09490v1","updated":"2025-01-16T12:01:44Z","published":"2025-01-16T12:01:44Z","title":"Comparison of Various SLAM Systems for Mobile Robot in an Indoor\n  Environment","summary":"  This article presents a comparative analysis of a mobile robot trajectories\ncomputed by various ROS-based SLAM systems. For this reason we developed a\nprototype of a mobile robot with common sensors: 2D lidar, a monocular and ZED\nstereo cameras. Then we conducted experiments in a typical office environment\nand collected data from all sensors, running all tested SLAM systems based on\nthe acquired dataset. We studied the following SLAM systems: (a) 2D\nlidar-based: GMapping, Hector SLAM, Cartographer; (b) monocular camera-based:\nLarge Scale Direct monocular SLAM (LSD SLAM), ORB SLAM, Direct Sparse Odometry\n(DSO); and (c) stereo camera-based: ZEDfu, Real-Time Appearance-Based Mapping\n(RTAB map), ORB SLAM, Stereo Parallel Tracking and Mapping (S-PTAM). Since all\nSLAM methods were tested on the same dataset we compared results for different\nSLAM systems with appropriate metrics, demonstrating encouraging results for\nlidar-based Cartographer SLAM, Monocular ORB SLAM and Stereo RTAB Map methods.\n","authors":["Maksim Filipenko","Ilya Afanasyev"],"pdf_url":"https://arxiv.org/pdf/2501.09490v1.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.09485v1","updated":"2025-01-16T11:44:29Z","published":"2025-01-16T11:44:29Z","title":"The Devil is in the Details: Simple Remedies for Image-to-LiDAR\n  Representation Learning","summary":"  LiDAR is a crucial sensor in autonomous driving, commonly used alongside\ncameras. By exploiting this camera-LiDAR setup and recent advances in image\nrepresentation learning, prior studies have shown the promising potential of\nimage-to-LiDAR distillation. These prior arts focus on the designs of their own\nlosses to effectively distill the pre-trained 2D image representations into a\n3D model. However, the other parts of the designs have been surprisingly\nunexplored. We find that fundamental design elements, e.g., the LiDAR\ncoordinate system, quantization according to the existing input interface, and\ndata utilization, are more critical than developing loss functions, which have\nbeen overlooked in prior works. In this work, we show that simple fixes to\nthese designs notably outperform existing methods by 16% in 3D semantic\nsegmentation on the nuScenes dataset and 13% in 3D object detection on the\nKITTI dataset in downstream task performance. We focus on overlooked design\nchoices along the spatial and temporal axes. Spatially, prior work has used\ncylindrical coordinate and voxel sizes without considering their side effects\nyielded with a commonly deployed sparse convolution layer input interface,\nleading to spatial quantization errors in 3D models. Temporally, existing work\nhas avoided cumbersome data curation by discarding unsynced data, limiting the\nuse to only the small portion of data that is temporally synced across sensors.\nWe analyze these effects and propose simple solutions for each overlooked\naspect.\n","authors":["Wonjun Jo","Kwon Byung-Ki","Kim Ji-Yeon","Hawook Jeong","Kyungdon Joo","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2501.09485v1.pdf","comment":"Accepted to ACCV2024"},{"id":"http://arxiv.org/abs/2501.09481v1","updated":"2025-01-16T11:35:22Z","published":"2025-01-16T11:35:22Z","title":"MonoSOWA: Scalable monocular 3D Object detector Without human\n  Annotations","summary":"  Detecting the three-dimensional position and orientation of objects using a\nsingle RGB camera is a foundational task in computer vision with many important\napplications. Traditionally, 3D object detection methods are trained in a\nfully-supervised setup, requiring vast amounts of human annotations, which are\nlaborious, costly, and do not scale well with the ever-increasing amounts of\ndata being captured.\n  In this paper, we present the first method to train 3D object detectors for\nmonocular RGB cameras without domain-specific human annotations, thus making\norders of magnitude more data available for training. Thanks to newly proposed\nCanonical Object Space, the method can not only exploit data across a variety\nof datasets and camera setups to train a single 3D detector, but unlike\nprevious work it also works out of the box in previously unseen camera setups.\nAll this is crucial for practical applications, where the data and cameras are\nextremely heterogeneous.\n  The method is evaluated on two standard autonomous driving datasets, where it\noutperforms previous works, which, unlike our method, still rely on 2D human\nannotations.\n","authors":["Jan Skvrna","Lukas Neumann"],"pdf_url":"https://arxiv.org/pdf/2501.09481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.04747v6","updated":"2025-01-16T11:17:04Z","published":"2022-09-10T22:00:30Z","title":"Diffusion Models in Vision: A Survey","summary":"  Denoising diffusion models represent a recent emerging topic in computer\nvision, demonstrating remarkable results in the area of generative modeling. A\ndiffusion model is a deep generative model that is based on two stages, a\nforward diffusion stage and a reverse diffusion stage. In the forward diffusion\nstage, the input data is gradually perturbed over several steps by adding\nGaussian noise. In the reverse stage, a model is tasked at recovering the\noriginal input data by learning to gradually reverse the diffusion process,\nstep by step. Diffusion models are widely appreciated for the quality and\ndiversity of the generated samples, despite their known computational burdens,\ni.e. low speeds due to the high number of steps involved during sampling. In\nthis survey, we provide a comprehensive review of articles on denoising\ndiffusion models applied in vision, comprising both theoretical and practical\ncontributions in the field. First, we identify and present three generic\ndiffusion modeling frameworks, which are based on denoising diffusion\nprobabilistic models, noise conditioned score networks, and stochastic\ndifferential equations. We further discuss the relations between diffusion\nmodels and other deep generative models, including variational auto-encoders,\ngenerative adversarial networks, energy-based models, autoregressive models and\nnormalizing flows. Then, we introduce a multi-perspective categorization of\ndiffusion models applied in computer vision. Finally, we illustrate the current\nlimitations of diffusion models and envision some interesting directions for\nfuture research.\n","authors":["Florinel-Alin Croitoru","Vlad Hondru","Radu Tudor Ionescu","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2209.04747v6.pdf","comment":"Accepted in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence. 25 pages, 3 figures"},{"id":"http://arxiv.org/abs/2501.09466v1","updated":"2025-01-16T10:59:29Z","published":"2025-01-16T10:59:29Z","title":"DEFOM-Stereo: Depth Foundation Model Based Stereo Matching","summary":"  Stereo matching is a key technique for metric depth estimation in computer\nvision and robotics. Real-world challenges like occlusion and non-texture\nhinder accurate disparity estimation from binocular matching cues. Recently,\nmonocular relative depth estimation has shown remarkable generalization using\nvision foundation models. Thus, to facilitate robust stereo matching with\nmonocular depth cues, we incorporate a robust monocular relative depth model\ninto the recurrent stereo-matching framework, building a new framework for\ndepth foundation model-based stereo-matching, DEFOM-Stereo. In the feature\nextraction stage, we construct the combined context and matching feature\nencoder by integrating features from conventional CNNs and DEFOM. In the update\nstage, we use the depth predicted by DEFOM to initialize the recurrent\ndisparity and introduce a scale update module to refine the disparity at the\ncorrect scale. DEFOM-Stereo is verified to have comparable performance on the\nScene Flow dataset with state-of-the-art (SOTA) methods and notably shows much\nstronger zero-shot generalization. Moreover, DEFOM-Stereo achieves SOTA\nperformance on the KITTI 2012, KITTI 2015, Middlebury, and ETH3D benchmarks,\nranking 1st on many metrics. In the joint evaluation under the robust vision\nchallenge, our model simultaneously outperforms previous models on the\nindividual benchmarks. Both results demonstrate the outstanding capabilities of\nthe proposed model.\n","authors":["Hualie Jiang","Zhiqiang Lou","Laiyan Ding","Rui Xu","Minglang Tan","Wenjie Jiang","Rui Huang"],"pdf_url":"https://arxiv.org/pdf/2501.09466v1.pdf","comment":"Code: https://github.com/Insta360-Research-Team/DEFOM-Stereo"},{"id":"http://arxiv.org/abs/2312.14150v3","updated":"2025-01-16T10:57:44Z","published":"2023-12-21T18:59:12Z","title":"DriveLM: Driving with Graph Visual Question Answering","summary":"  We study how vision-language models (VLMs) trained on web-scale data can be\nintegrated into end-to-end driving systems to boost generalization and enable\ninteractivity with human users. While recent approaches adapt VLMs to driving\nvia single-round visual question answering (VQA), human drivers reason about\ndecisions in multiple steps. Starting from the localization of key objects,\nhumans estimate object interactions before taking actions. The key insight is\nthat with our proposed task, Graph VQA, where we model graph-structured\nreasoning through perception, prediction and planning question-answer pairs, we\nobtain a suitable proxy task to mimic the human reasoning process. We\ninstantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and propose\na VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQA\nand end-to-end driving. The experiments demonstrate that Graph VQA provides a\nsimple, principled framework for reasoning about a driving scene, and\nDriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agent\nbaseline performs end-to-end autonomous driving competitively in comparison to\nstate-of-the-art driving-specific architectures. Notably, its benefits are\npronounced when it is evaluated zero-shot on unseen objects or sensor\nconfigurations. We hope this work can be the starting point to shed new light\non how to apply VLMs for autonomous driving. To facilitate future research, all\ncode, data, and models are available to the public.\n","authors":["Chonghao Sima","Katrin Renz","Kashyap Chitta","Li Chen","Hanxue Zhang","Chengen Xie","Jens Beißwenger","Ping Luo","Andreas Geiger","Hongyang Li"],"pdf_url":"https://arxiv.org/pdf/2312.14150v3.pdf","comment":"Accepted to ECCV 2024 as Oral paper"},{"id":"http://arxiv.org/abs/2501.09465v1","updated":"2025-01-16T10:56:45Z","published":"2025-01-16T10:56:45Z","title":"RE-POSE: Synergizing Reinforcement Learning-Based Partitioning and\n  Offloading for Edge Object Detection","summary":"  Object detection plays a crucial role in smart video analysis, with\napplications ranging from autonomous driving and security to smart cities.\nHowever, achieving real-time object detection on edge devices presents\nsignificant challenges due to their limited computational resources and the\nhigh demands of deep neural network (DNN)-based detection models, particularly\nwhen processing high-resolution video. Conventional strategies, such as input\ndown-sampling and network up-scaling, often compromise detection accuracy for\nfaster performance or lead to higher inference latency. To address these\nissues, this paper introduces RE-POSE, a Reinforcement Learning (RL)-Driven\nPartitioning and Edge Offloading framework designed to optimize the\naccuracy-latency trade-off in resource-constrained edge environments. Our\napproach features an RL-Based Dynamic Clustering Algorithm (RL-DCA) that\npartitions video frames into non-uniform blocks based on object distribution\nand the computational characteristics of DNNs. Furthermore, a parallel edge\noffloading scheme is implemented to distribute these blocks across multiple\nedge servers for concurrent processing. Experimental evaluations show that\nRE-POSE significantly enhances detection accuracy and reduces inference\nlatency, surpassing existing methods.\n","authors":["Jianrui Shi","Yong Zhao","Zeyang Cui","Xiaoming Shen","Minhang Zeng","Xiaojie Liu"],"pdf_url":"https://arxiv.org/pdf/2501.09465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08258v2","updated":"2025-01-16T10:55:41Z","published":"2025-01-14T17:10:02Z","title":"Towards an End-to-End (E2E) Adversarial Learning and Application in the\n  Physical World","summary":"  The traditional learning process of patch-based adversarial attacks,\nconducted in the digital domain and then applied in the physical domain (e.g.,\nvia printed stickers), may suffer from reduced performance due to adversarial\npatches' limited transferability from the digital domain to the physical\ndomain. Given that previous studies have considered using projectors to apply\nadversarial attacks, we raise the following question: can adversarial learning\n(i.e., patch generation) be performed entirely in the physical domain with a\nprojector? In this work, we propose the Physical-domain Adversarial Patch\nLearning Augmentation (PAPLA) framework, a novel end-to-end (E2E) framework\nthat converts adversarial learning from the digital domain to the physical\ndomain using a projector. We evaluate PAPLA across multiple scenarios,\nincluding controlled laboratory settings and realistic outdoor environments,\ndemonstrating its ability to ensure attack success compared to conventional\ndigital learning-physical application (DL-PA) methods. We also analyze the\nimpact of environmental factors, such as projection surface color, projector\nstrength, ambient light, distance, and angle of the target object relative to\nthe camera, on the effectiveness of projected patches. Finally, we demonstrate\nthe feasibility of the attack against a parked car and a stop sign in a\nreal-world outdoor environment. Our results show that under specific\nconditions, E2E adversarial learning in the physical domain eliminates the\ntransferability issue and ensures evasion by object detectors. Finally, we\nprovide insights into the challenges and opportunities of applying adversarial\nlearning in the physical domain and explain where such an approach is more\neffective than using a sticker.\n","authors":["Dudi Biton","Jacob Shams","Satoru Koda","Asaf Shabtai","Yuval Elovici","Ben Nassi"],"pdf_url":"https://arxiv.org/pdf/2501.08258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09460v1","updated":"2025-01-16T10:42:29Z","published":"2025-01-16T10:42:29Z","title":"Normal-NeRF: Ambiguity-Robust Normal Estimation for Highly Reflective\n  Scenes","summary":"  Neural Radiance Fields (NeRF) often struggle with reconstructing and\nrendering highly reflective scenes. Recent advancements have developed various\nreflection-aware appearance models to enhance NeRF's capability to render\nspecular reflections. However, the robust reconstruction of highly reflective\nscenes is still hindered by the inherent shape ambiguity on specular surfaces.\nExisting methods typically rely on additional geometry priors to regularize the\nshape prediction, but this can lead to oversmoothed geometry in complex scenes.\nObserving the critical role of surface normals in parameterizing reflections,\nwe introduce a transmittance-gradient-based normal estimation technique that\nremains robust even under ambiguous shape conditions. Furthermore, we propose a\ndual activated densities module that effectively bridges the gap between smooth\nsurface normals and sharp object boundaries. Combined with a reflection-aware\nappearance model, our proposed method achieves robust reconstruction and\nhigh-fidelity rendering of scenes featuring both highly specular reflections\nand intricate geometric structures. Extensive experiments demonstrate that our\nmethod outperforms existing state-of-the-art methods on various datasets.\n","authors":["Ji Shi","Xianghua Ying","Ruohao Guo","Bowei Xing","Wenzhen Yue"],"pdf_url":"https://arxiv.org/pdf/2501.09460v1.pdf","comment":"AAAI 2025, code available at https://github.com/sjj118/Normal-NeRF"},{"id":"http://arxiv.org/abs/2501.09456v1","updated":"2025-01-16T10:31:51Z","published":"2025-01-16T10:31:51Z","title":"On the Relation between Optical Aperture and Automotive Object Detection","summary":"  We explore the impact of aperture size and shape on automotive camera systems\nfor deep-learning-based tasks like traffic sign recognition and light state\ndetection. A method is proposed to simulate optical effects using the point\nspread function (PSF), enhancing realism and reducing the domain gap between\nsynthetic and real-world images. Computer-generated scenes are refined with\nthis technique to model optical distortions and improve simulation accuracy.\n","authors":["Ofer Bar-Shalom","Tzvi Philipp","Eran Kishon"],"pdf_url":"https://arxiv.org/pdf/2501.09456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09446v1","updated":"2025-01-16T10:20:48Z","published":"2025-01-16T10:20:48Z","title":"Double Visual Defense: Adversarial Pre-training and Instruction Tuning\n  for Improving Vision-Language Model Robustness","summary":"  This paper investigates the robustness of vision-language models against\nadversarial visual perturbations and introduces a novel ``double visual\ndefense\" to enhance this robustness. Unlike previous approaches that resort to\nlightweight adversarial fine-tuning of a pre-trained CLIP model, we perform\nlarge-scale adversarial vision-language pre-training from scratch using\nweb-scale data. We then strengthen the defense by incorporating adversarial\nvisual instruction tuning. The resulting models from each stage, $\\Delta$CLIP\nand $\\Delta^2$LLaVA, show substantially enhanced zero-shot robustness and set a\nnew state-of-the-art in adversarial defense for vision-language models. For\nexample, the adversarial robustness of $\\Delta$CLIP surpasses that of the\nprevious best models on ImageNet-1k by ~20%. %For example, $\\Delta$CLIP\nsurpasses the previous best models on ImageNet-1k by ~20% in terms of\nadversarial robustness. Similarly, compared to prior art, $\\Delta^2$LLaVA\nbrings a ~30% robustness improvement to image captioning task and a ~20%\nrobustness improvement to visual question answering task. Furthermore, our\nmodels exhibit stronger zero-shot recognition capability, fewer hallucinations,\nand superior reasoning performance compared to baselines. Our project page is\nhttps://doublevisualdefense.github.io/.\n","authors":["Zeyu Wang","Cihang Xie","Brian Bartoldson","Bhavya Kailkhura"],"pdf_url":"https://arxiv.org/pdf/2501.09446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15500v4","updated":"2025-01-16T10:20:32Z","published":"2024-07-22T09:31:30Z","title":"TextureCrop: Enhancing Synthetic Image Detection through Texture-based\n  Cropping","summary":"  Generative AI technologies produce increasingly realistic imagery, which,\ndespite its potential for creative applications, can also be misused to produce\nmisleading and harmful content. This renders Synthetic Image Detection (SID)\nmethods essential for identifying AI-generated content online. State-of-the-art\nSID methods typically resize or center-crop input images due to architectural\nor computational constraints, which hampers the detection of artifacts that\nappear in high-resolution images. To address this limitation, we propose\nTextureCrop, an image pre-processing component that can be plugged in any\npre-trained SID model to improve its performance. By focusing on high-frequency\nimage parts where generative artifacts are prevalent, TextureCrop enhances SID\nperformance with manageable memory requirements. Experimental results\ndemonstrate a consistent improvement in AUC across various detectors by 6.1%\ncompared to center cropping and by 15% compared to resizing, across\nhigh-resolution images from the Forensynths, Synthbuster and TWIGMA datasets.\nCode available at https : //github.com/mever-team/texture-crop.\n","authors":["Despina Konstantinidou","Christos Koutlis","Symeon Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.15500v4.pdf","comment":"10 pages, 7 images"},{"id":"http://arxiv.org/abs/2501.09436v1","updated":"2025-01-16T10:07:44Z","published":"2025-01-16T10:07:44Z","title":"Scaling up self-supervised learning for improved surgical foundation\n  models","summary":"  Foundation models have revolutionized computer vision by achieving vastly\nsuperior performance across diverse tasks through large-scale pretraining on\nextensive datasets. However, their application in surgical computer vision has\nbeen limited. This study addresses this gap by introducing SurgeNetXL, a novel\nsurgical foundation model that sets a new benchmark in surgical computer\nvision. Trained on the largest reported surgical dataset to date, comprising\nover 4.7 million video frames, SurgeNetXL achieves consistent top-tier\nperformance across six datasets spanning four surgical procedures and three\ntasks, including semantic segmentation, phase recognition, and critical view of\nsafety (CVS) classification. Compared with the best-performing surgical\nfoundation models, SurgeNetXL shows mean improvements of 2.4, 9.0, and 12.6\npercent for semantic segmentation, phase recognition, and CVS classification,\nrespectively. Additionally, SurgeNetXL outperforms the best-performing\nImageNet-based variants by 14.4, 4.0, and 1.6 percent in the respective tasks.\nIn addition to advancing model performance, this study provides key insights\ninto scaling pretraining datasets, extending training durations, and optimizing\nmodel architectures specifically for surgical computer vision. These findings\npave the way for improved generalizability and robustness in data-scarce\nscenarios, offering a comprehensive framework for future research in this\ndomain. All models and a subset of the SurgeNetXL dataset, including over 2\nmillion video frames, are publicly available at:\nhttps://github.com/TimJaspers0801/SurgeNet.\n","authors":["Tim J. M. Jaspers","Ronald L. P. D. de Jong","Yiping Li","Carolus H. J. Kusters","Franciscus H. A. Bakker","Romy C. van Jaarsveld","Gino M. Kuiper","Richard van Hillegersberg","Jelle P. Ruurda","Willem M. Brinkman","Josien P. W. Pluim","Peter H. N. de With","Marcel Breeuwer","Yasmina Al Khalil","Fons van der Sommen"],"pdf_url":"https://arxiv.org/pdf/2501.09436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09433v1","updated":"2025-01-16T10:03:15Z","published":"2025-01-16T10:03:15Z","title":"CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation","summary":"  The synthesis of high-quality 3D assets from textual or visual inputs has\nbecome a central objective in modern generative modeling. Despite the\nproliferation of 3D generation algorithms, they frequently grapple with\nchallenges such as multi-view inconsistency, slow generation times, low\nfidelity, and surface reconstruction problems. While some studies have\naddressed some of these issues, a comprehensive solution remains elusive. In\nthis paper, we introduce \\textbf{CaPa}, a carve-and-paint framework that\ngenerates high-fidelity 3D assets efficiently. CaPa employs a two-stage\nprocess, decoupling geometry generation from texture synthesis. Initially, a 3D\nlatent diffusion model generates geometry guided by multi-view inputs, ensuring\nstructural consistency across perspectives. Subsequently, leveraging a novel,\nmodel-agnostic Spatially Decoupled Attention, the framework synthesizes\nhigh-resolution textures (up to 4K) for a given geometry. Furthermore, we\npropose a 3D-aware occlusion inpainting algorithm that fills untextured\nregions, resulting in cohesive results across the entire model. This pipeline\ngenerates high-quality 3D assets in less than 30 seconds, providing\nready-to-use outputs for commercial applications. Experimental results\ndemonstrate that CaPa excels in both texture fidelity and geometric stability,\nestablishing a new standard for practical, scalable 3D asset generation.\n","authors":["Hwan Heo","Jangyeong Kim","Seongyeong Lee","Jeong A Wi","Junyoung Choi","Sangjun Ahn"],"pdf_url":"https://arxiv.org/pdf/2501.09433v1.pdf","comment":"project page: https://ncsoft.github.io/CaPa/"},{"id":"http://arxiv.org/abs/2501.09428v1","updated":"2025-01-16T09:57:40Z","published":"2025-01-16T09:57:40Z","title":"AugRefer: Advancing 3D Visual Grounding via Cross-Modal Augmentation and\n  Spatial Relation-based Referring","summary":"  3D visual grounding (3DVG), which aims to correlate a natural language\ndescription with the target object within a 3D scene, is a significant yet\nchallenging task. Despite recent advancements in this domain, existing\napproaches commonly encounter a shortage: a limited amount and diversity of\ntext3D pairs available for training. Moreover, they fall short in effectively\nleveraging different contextual clues (e.g., rich spatial relations within the\n3D visual space) for grounding. To address these limitations, we propose\nAugRefer, a novel approach for advancing 3D visual grounding. AugRefer\nintroduces cross-modal augmentation designed to extensively generate diverse\ntext-3D pairs by placing objects into 3D scenes and creating accurate and\nsemantically rich descriptions using foundation models. Notably, the resulting\npairs can be utilized by any existing 3DVG methods for enriching their training\ndata. Additionally, AugRefer presents a language-spatial adaptive decoder that\neffectively adapts the potential referring objects based on the language\ndescription and various 3D spatial relations. Extensive experiments on three\nbenchmark datasets clearly validate the effectiveness of AugRefer.\n","authors":["Xinyi Wang","Na Zhao","Zhiyuan Han","Dan Guo","Xun Yang"],"pdf_url":"https://arxiv.org/pdf/2501.09428v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2501.09425v1","updated":"2025-01-16T09:55:42Z","published":"2025-01-16T09:55:42Z","title":"Vision-Language Models Do Not Understand Negation","summary":"  Many practical vision-language applications require models that understand\nnegation, e.g., when using natural language to retrieve images which contain\ncertain objects but not others. Despite advancements in vision-language models\n(VLMs) through large-scale training, their ability to comprehend negation\nremains underexplored. This study addresses the question: how well do current\nVLMs understand negation? We introduce NegBench, a new benchmark designed to\nevaluate negation understanding across 18 task variations and 79k examples\nspanning image, video, and medical datasets. The benchmark consists of two core\ntasks designed to evaluate negation understanding in diverse multimodal\nsettings: Retrieval with Negation and Multiple Choice Questions with Negated\nCaptions. Our evaluation reveals that modern VLMs struggle significantly with\nnegation, often performing at chance level. To address these shortcomings, we\nexplore a data-centric approach wherein we finetune CLIP models on large-scale\nsynthetic datasets containing millions of negated captions. We show that this\napproach can result in a 10% increase in recall on negated queries and a 40%\nboost in accuracy on multiple-choice questions with negated captions.\n","authors":["Kumail Alhamoud","Shaden Alshammari","Yonglong Tian","Guohao Li","Philip Torr","Yoon Kim","Marzyeh Ghassemi"],"pdf_url":"https://arxiv.org/pdf/2501.09425v1.pdf","comment":"Project page: https://negbench.github.io"},{"id":"http://arxiv.org/abs/2501.09420v1","updated":"2025-01-16T09:47:18Z","published":"2025-01-16T09:47:18Z","title":"Dynamic Neural Style Transfer for Artistic Image Generation using VGG19","summary":"  Throughout history, humans have created remarkable works of art, but\nartificial intelligence has only recently started to make strides in generating\nvisually compelling art. Breakthroughs in the past few years have focused on\nusing convolutional neural networks (CNNs) to separate and manipulate the\ncontent and style of images, applying texture synthesis techniques.\nNevertheless, a number of current techniques continue to encounter obstacles,\nincluding lengthy processing times, restricted choices of style images, and the\ninability to modify the weight ratio of styles. We proposed a neural style\ntransfer system that can add various artistic styles to a desired image to\naddress these constraints allowing flexible adjustments to style weight ratios\nand reducing processing time. The system uses the VGG19 model for feature\nextraction, ensuring high-quality, flexible stylization without compromising\ncontent integrity.\n","authors":["Kapil Kashyap","Mehak Garg","Sean Fargose","Sindhu Nair"],"pdf_url":"https://arxiv.org/pdf/2501.09420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09411v1","updated":"2025-01-16T09:38:22Z","published":"2025-01-16T09:38:22Z","title":"Towards Robust and Realistic Human Pose Estimation via WiFi Signals","summary":"  Robust WiFi-based human pose estimation is a challenging task that bridges\ndiscrete and subtle WiFi signals to human skeletons. This paper revisits this\nproblem and reveals two critical yet overlooked issues: 1) cross-domain gap,\ni.e., due to significant variations between source-target domain pose\ndistributions; and 2) structural fidelity gap, i.e., predicted skeletal poses\nmanifest distorted topology, usually with misplaced joints and disproportionate\nbone lengths. This paper fills these gaps by reformulating the task into a\nnovel two-phase framework dubbed DT-Pose: Domain-consistent representation\nlearning and Topology-constrained Pose decoding. Concretely, we first propose a\ntemporal-consistent contrastive learning strategy with uniformity\nregularization, coupled with self-supervised masking-reconstruction operations,\nto enable robust learning of domain-consistent and motion-discriminative\nWiFi-specific representations. Beyond this, we introduce a simple yet effective\npose decoder with task prompts, which integrates Graph Convolution Network\n(GCN) and Transformer layers to constrain the topology structure of the\ngenerated skeleton by exploring the adjacent-overarching relationships among\nhuman joints. Extensive experiments conducted on various benchmark datasets\nhighlight the superior performance of our method in tackling these fundamental\nchallenges in both 2D/3D human pose estimation tasks.\n","authors":["Yang Chen","Jingcai Guo","Song Guo","Jingren Zhou","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2501.09411v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.04829v3","updated":"2025-01-16T09:31:01Z","published":"2024-06-07T10:54:40Z","title":"IOR: Inversed Objects Replay for Incremental Object Detection","summary":"  Existing Incremental Object Detection (IOD) methods partially alleviate\ncatastrophic forgetting when incrementally detecting new objects in real-world\nscenarios. However, many of these methods rely on the assumption that unlabeled\nold-class objects may co-occur with labeled new-class objects in the\nincremental data. When unlabeled old-class objects are absent, the performance\nof existing methods tends to degrade. The absence can be mitigated by\ngenerating old-class samples, but it incurs high costs. This paper argues that\nprevious generation-based IOD suffers from redundancy, both in the use of\ngenerative models, which require additional training and storage, and in the\noverproduction of generated samples, many of which do not contribute\nsignificantly to performance improvements. To eliminate the redundancy, we\npropose Inversed Objects Replay (IOR). Specifically, we generate old-class\nsamples by inversing the original detectors, thus eliminating the necessity of\ntraining and storing additional generative models. We propose augmented replay\nto reuse the objects in generated samples, reducing redundant generations.\nMoreover, we propose high-value knowledge distillation focusing on the\npositions of old-class objects overwhelmed by the background, which transfers\nthe knowledge to the incremental detector. Extensive experiments conducted on\nMS COCO 2017 demonstrate that our method can efficiently improve detection\nperformance in IOD scenarios with the absence of old-class objects.\n","authors":["Zijia An","Boyu Diao","Libo Huang","Ruiqi Liu","Zhulin An","Yongjun Xu"],"pdf_url":"https://arxiv.org/pdf/2406.04829v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09403v1","updated":"2025-01-16T09:18:59Z","published":"2025-01-16T09:18:59Z","title":"PISCO: Self-Supervised k-Space Regularization for Improved Neural\n  Implicit k-Space Representations of Dynamic MRI","summary":"  Neural implicit k-space representations (NIK) have shown promising results\nfor dynamic magnetic resonance imaging (MRI) at high temporal resolutions. Yet,\nreducing acquisition time, and thereby available training data, results in\nsevere performance drops due to overfitting. To address this, we introduce a\nnovel self-supervised k-space loss function $\\mathcal{L}_\\mathrm{PISCO}$,\napplicable for regularization of NIK-based reconstructions. The proposed loss\nfunction is based on the concept of parallel imaging-inspired self-consistency\n(PISCO), enforcing a consistent global k-space neighborhood relationship\nwithout requiring additional data. Quantitative and qualitative evaluations on\nstatic and dynamic MR reconstructions show that integrating PISCO significantly\nimproves NIK representations. Particularly for high acceleration factors\n(R$\\geq$54), NIK with PISCO achieves superior spatio-temporal reconstruction\nquality compared to state-of-the-art methods. Furthermore, an extensive\nanalysis of the loss assumptions and stability shows PISCO's potential as\nversatile self-supervised k-space loss function for further applications and\narchitectures. Code is available at:\nhttps://github.com/compai-lab/2025-pisco-spieker\n","authors":["Veronika Spieker","Hannah Eichhorn","Wenqi Huang","Jonathan K. Stelter","Tabita Catalan","Rickmer F. Braren","Daniel Rueckert","Francisco Sahli Costabal","Kerstin Hammernik","Dimitrios C. Karampinos","Claudia Prieto","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2501.09403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09396v1","updated":"2025-01-16T09:07:01Z","published":"2025-01-16T09:07:01Z","title":"Joint Transmission and Deblurring: A Semantic Communication Approach\n  Using Events","summary":"  Deep learning-based joint source-channel coding (JSCC) is emerging as a\npromising technology for effective image transmission. However, most existing\napproaches focus on transmitting clear images, overlooking real-world\nchallenges such as motion blur caused by camera shaking or fast-moving objects.\nMotion blur often degrades image quality, making transmission and\nreconstruction more challenging. Event cameras, which asynchronously record\npixel intensity changes with extremely low latency, have shown great potential\nfor motion deblurring tasks. However, the efficient transmission of the\nabundant data generated by event cameras remains a significant challenge. In\nthis work, we propose a novel JSCC framework for the joint transmission of\nblurry images and events, aimed at achieving high-quality reconstructions under\nlimited channel bandwidth. This approach is designed as a deblurring\ntask-oriented JSCC system. Since RGB cameras and event cameras capture the same\nscene through different modalities, their outputs contain both shared and\ndomain-specific information. To avoid repeatedly transmitting the shared\ninformation, we extract and transmit their shared information and\ndomain-specific information, respectively. At the receiver, the received\nsignals are processed by a deblurring decoder to generate clear images.\nAdditionally, we introduce a multi-stage training strategy to train the\nproposed model. Simulation results demonstrate that our method significantly\noutperforms existing JSCC-based image transmission schemes, addressing motion\nblur effectively.\n","authors":["Pujing Yang","Guangyi Zhang","Yunlong Cai","Lei Yu","Guanding Yu"],"pdf_url":"https://arxiv.org/pdf/2501.09396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09393v1","updated":"2025-01-16T09:05:46Z","published":"2025-01-16T09:05:46Z","title":"SVIA: A Street View Image Anonymization Framework for Self-Driving\n  Applications","summary":"  In recent years, there has been an increasing interest in image\nanonymization, particularly focusing on the de-identification of faces and\nindividuals. However, for self-driving applications, merely de-identifying\nfaces and individuals might not provide sufficient privacy protection since\nstreet views like vehicles and buildings can still disclose locations,\ntrajectories, and other sensitive information. Therefore, it remains crucial to\nextend anonymization techniques to street view images to fully preserve the\nprivacy of users, pedestrians, and vehicles. In this paper, we propose a Street\nView Image Anonymization (SVIA) framework for self-driving applications. The\nSVIA framework consists of three integral components: a semantic segmenter to\nsegment an input image into functional regions, an inpainter to generate\nalternatives to privacy-sensitive regions, and a harmonizer to seamlessly\nstitch modified regions to guarantee visual coherence. Compared to existing\nmethods, SVIA achieves a much better trade-off between image generation quality\nand privacy protection, as evidenced by experimental results for five common\nmetrics on two widely used public datasets.\n","authors":["Dongyu Liu","Xuhong Wang","Cen Chen","Yanhao Wang","Shengyue Yao","Yilun Lin"],"pdf_url":"https://arxiv.org/pdf/2501.09393v1.pdf","comment":"8 pages, 6 figures, 3 tables. Accepted by IEEE ITSC 2024"},{"id":"http://arxiv.org/abs/2410.20986v2","updated":"2025-01-16T08:58:44Z","published":"2024-10-28T13:04:44Z","title":"Skinned Motion Retargeting with Dense Geometric Interaction Perception","summary":"  Capturing and maintaining geometric interactions among different body parts\nis crucial for successful motion retargeting in skinned characters. Existing\napproaches often overlook body geometries or add a geometry correction stage\nafter skeletal motion retargeting. This results in conflicts between skeleton\ninteraction and geometry correction, leading to issues such as jittery,\ninterpenetration, and contact mismatches. To address these challenges, we\nintroduce a new retargeting framework, MeshRet, which directly models the dense\ngeometric interactions in motion retargeting. Initially, we establish dense\nmesh correspondences between characters using semantically consistent sensors\n(SCS), effective across diverse mesh topologies. Subsequently, we develop a\nnovel spatio-temporal representation called the dense mesh interaction (DMI)\nfield. This field, a collection of interacting SCS feature vectors, skillfully\ncaptures both contact and non-contact interactions between body geometries. By\naligning the DMI field during retargeting, MeshRet not only preserves motion\nsemantics but also prevents self-interpenetration and ensures contact\npreservation. Extensive experiments on the public Mixamo dataset and our\nnewly-collected ScanRet dataset demonstrate that MeshRet achieves\nstate-of-the-art performance. Code available at\nhttps://github.com/abcyzj/MeshRet.\n","authors":["Zijie Ye","Jia-Wei Liu","Jia Jia","Shikun Sun","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2410.20986v2.pdf","comment":"NeurIPS 2024 Spotlight"},{"id":"http://arxiv.org/abs/2407.03653v3","updated":"2025-01-16T08:55:49Z","published":"2024-07-04T05:48:28Z","title":"reBEN: Refined BigEarthNet Dataset for Remote Sensing Image Analysis","summary":"  This paper presents refined BigEarthNet (reBEN) that is a large-scale,\nmulti-modal remote sensing dataset constructed to support deep learning (DL)\nstudies for remote sensing image analysis. The reBEN dataset consists of\n549,488 pairs of Sentinel-1 and Sentinel-2 image patches. To construct reBEN,\nwe initially consider the Sentinel-1 and Sentinel-2 tiles used to construct the\nBigEarthNet dataset and then divide them into patches of size 1200 m x 1200 m.\nWe apply atmospheric correction to the Sentinel-2 patches using the latest\nversion of the sen2cor tool, resulting in higher-quality patches compared to\nthose present in BigEarthNet. Each patch is then associated with a pixel-level\nreference map and scene-level multi-labels. This makes reBEN suitable for\npixel- and scene-based learning tasks. The labels are derived from the most\nrecent CORINE Land Cover (CLC) map of 2018 by utilizing the 19-class\nnomenclature as in BigEarthNet. The use of the most recent CLC map results in\novercoming the label noise present in BigEarthNet. Furthermore, we introduce a\nnew geographical-based split assignment algorithm that significantly reduces\nthe spatial correlation among the train, validation, and test sets with respect\nto those present in BigEarthNet. This increases the reliability of the\nevaluation of DL models. To minimize the DL model training time, we introduce\nsoftware tools that convert the reBEN dataset into a DL-optimized data format.\nIn our experiments, we show the potential of reBEN for multi-modal multi-label\nimage classification problems by considering several state-of-the-art DL\nmodels. The pre-trained model weights, associated code, and complete dataset\nare available at https://bigearth.net.\n","authors":["Kai Norman Clasen","Leonard Hackel","Tom Burgert","Gencer Sumbul","Begüm Demir","Volker Markl"],"pdf_url":"https://arxiv.org/pdf/2407.03653v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09372v1","updated":"2025-01-16T08:34:39Z","published":"2025-01-16T08:34:39Z","title":"Image Segmentation with transformers: An Overview, Challenges and Future","summary":"  Image segmentation, a key task in computer vision, has traditionally relied\non convolutional neural networks (CNNs), yet these models struggle with\ncapturing complex spatial dependencies, objects with varying scales, need for\nmanually crafted architecture components and contextual information. This paper\nexplores the shortcomings of CNN-based models and the shift towards transformer\narchitectures -to overcome those limitations. This work reviews\nstate-of-the-art transformer-based segmentation models, addressing\nsegmentation-specific challenges and their solutions. The paper discusses\ncurrent challenges in transformer-based segmentation and outlines promising\nfuture trends, such as lightweight architectures and enhanced data efficiency.\nThis survey serves as a guide for understanding the impact of transformers in\nadvancing segmentation capabilities and overcoming the limitations of\ntraditional models.\n","authors":["Deepjyoti Chetia","Debasish Dutta","Sanjib Kr Kalita"],"pdf_url":"https://arxiv.org/pdf/2501.09372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03659v3","updated":"2025-01-16T08:20:15Z","published":"2025-01-07T09:47:46Z","title":"DehazeGS: Seeing Through Fog with 3D Gaussian Splatting","summary":"  Current novel view synthesis tasks primarily rely on high-quality and clear\nimages. However, in foggy scenes, scattering and attenuation can significantly\ndegrade the reconstruction and rendering quality. Although NeRF-based dehazing\nreconstruction algorithms have been developed, their use of deep fully\nconnected neural networks and per-ray sampling strategies leads to high\ncomputational costs. Moreover, NeRF's implicit representation struggles to\nrecover fine details from hazy scenes. In contrast, recent advancements in 3D\nGaussian Splatting achieve high-quality 3D scene reconstruction by explicitly\nmodeling point clouds into 3D Gaussians. In this paper, we propose leveraging\nthe explicit Gaussian representation to explain the foggy image formation\nprocess through a physically accurate forward rendering process. We introduce\nDehazeGS, a method capable of decomposing and rendering a fog-free background\nfrom participating media using only muti-view foggy images as input. We model\nthe transmission within each Gaussian distribution to simulate the formation of\nfog. During this process, we jointly learn the atmospheric light and scattering\ncoefficient while optimizing the Gaussian representation of the hazy scene. In\nthe inference stage, we eliminate the effects of scattering and attenuation on\nthe Gaussians and directly project them onto a 2D plane to obtain a clear view.\nExperiments on both synthetic and real-world foggy datasets demonstrate that\nDehazeGS achieves state-of-the-art performance in terms of both rendering\nquality and computational efficiency. visualizations are available at\nhttps://dehazegs.github.io/\n","authors":["Jinze Yu","Yiqun Wang","Zhengda Lu","Jianwei Guo","Yong Li","Hongxing Qin","Xiaopeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.03659v3.pdf","comment":"9 pages,4 figures"},{"id":"http://arxiv.org/abs/2501.05777v2","updated":"2025-01-16T08:20:11Z","published":"2025-01-10T08:18:37Z","title":"StructSR: Refuse Spurious Details in Real-World Image Super-Resolution","summary":"  Diffusion-based models have shown great promise in real-world image\nsuper-resolution (Real-ISR), but often generate content with structural errors\nand spurious texture details due to the empirical priors and illusions of these\nmodels. To address this issue, we introduce StructSR, a simple, effective, and\nplug-and-play method that enhances structural fidelity and suppresses spurious\ndetails for diffusion-based Real-ISR. StructSR operates without the need for\nadditional fine-tuning, external model priors, or high-level semantic\nknowledge. At its core is the Structure-Aware Screening (SAS) mechanism, which\nidentifies the image with the highest structural similarity to the\nlow-resolution (LR) input in the early inference stage, allowing us to leverage\nit as a historical structure knowledge to suppress the generation of spurious\ndetails. By intervening in the diffusion inference process, StructSR seamlessly\nintegrates with existing diffusion-based Real-ISR models. Our experimental\nresults demonstrate that StructSR significantly improves the fidelity of\nstructure and texture, improving the PSNR and SSIM metrics by an average of\n5.27% and 9.36% on a synthetic dataset (DIV2K-Val) and 4.13% and 8.64% on two\nreal-world datasets (RealSR and DRealSR) when integrated with four\nstate-of-the-art diffusion-based Real-ISR methods.\n","authors":["Yachao Li","Dong Liang","Tianyu Ding","Sheng-Jun Huang"],"pdf_url":"https://arxiv.org/pdf/2501.05777v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09363v1","updated":"2025-01-16T08:18:03Z","published":"2025-01-16T08:18:03Z","title":"Identification of Traditional Medicinal Plant Leaves Using an effective\n  Deep Learning model and Self-Curated Dataset","summary":"  Medicinal plants have been a key component in producing traditional and\nmodern medicines, especially in the field of Ayurveda, an ancient Indian\nmedical system. Producing these medicines and collecting and extracting the\nright plant is a crucial step due to the visually similar nature of some\nplants. The extraction of these plants from nonmedicinal plants requires human\nexpert intervention. To solve the issue of accurate plant identification and\nreduce the need for a human expert in the collection process; employing\ncomputer vision methods will be efficient and beneficial. In this paper, we\nhave proposed a model that solves such issues. The proposed model is a custom\nconvolutional neural network (CNN) architecture with 6 convolution layers,\nmax-pooling layers, and dense layers. The model was tested on three different\ndatasets named Indian Medicinal Leaves Image Dataset,MED117 Medicinal Plant\nLeaf Dataset, and the self-curated dataset by the authors. The proposed model\nachieved respective accuracies of 99.5%, 98.4%, and 99.7% using various\noptimizers including Adam, RMSprop, and SGD with momentum.\n","authors":["Deepjyoti Chetia","Sanjib Kr Kalita","Prof Partha Pratim Baruah","Debasish Dutta","Tanaz Akhter"],"pdf_url":"https://arxiv.org/pdf/2501.09363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09361v1","updated":"2025-01-16T08:17:32Z","published":"2025-01-16T08:17:32Z","title":"Strategic Base Representation Learning via Feature Augmentations for\n  Few-Shot Class Incremental Learning","summary":"  Few-shot class incremental learning implies the model to learn new classes\nwhile retaining knowledge of previously learned classes with a small number of\ntraining instances. Existing frameworks typically freeze the parameters of the\npreviously learned classes during the incorporation of new classes. However,\nthis approach often results in suboptimal class separation of previously\nlearned classes, leading to overlap between old and new classes. Consequently,\nthe performance of old classes degrades on new classes. To address these\nchallenges, we propose a novel feature augmentation driven contrastive learning\nframework designed to enhance the separation of previously learned classes to\naccommodate new classes. Our approach involves augmenting feature vectors and\nassigning proxy labels to these vectors. This strategy expands the feature\nspace, ensuring seamless integration of new classes within the expanded space.\nAdditionally, we employ a self-supervised contrastive loss to improve the\nseparation between previous classes. We validate our framework through\nexperiments on three FSCIL benchmark datasets: CIFAR100, miniImageNet, and\nCUB200. The results demonstrate that our Feature Augmentation driven\nContrastive Learning framework significantly outperforms other approaches,\nachieving state-of-the-art performance.\n","authors":["Parinita Nema","Vinod K Kurmi"],"pdf_url":"https://arxiv.org/pdf/2501.09361v1.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2407.21035v2","updated":"2025-01-16T08:08:57Z","published":"2024-07-17T08:19:11Z","title":"Direct Unlearning Optimization for Robust and Safe Text-to-Image Models","summary":"  Recent advancements in text-to-image (T2I) models have unlocked a wide range\nof applications but also present significant risks, particularly in their\npotential to generate unsafe content. To mitigate this issue, researchers have\ndeveloped unlearning techniques to remove the model's ability to generate\npotentially harmful content. However, these methods are easily bypassed by\nadversarial attacks, making them unreliable for ensuring the safety of\ngenerated images. In this paper, we propose Direct Unlearning Optimization\n(DUO), a novel framework for removing Not Safe For Work (NSFW) content from T2I\nmodels while preserving their performance on unrelated topics. DUO employs a\npreference optimization approach using curated paired image data, ensuring that\nthe model learns to remove unsafe visual concepts while retaining unrelated\nfeatures. Furthermore, we introduce an output-preserving regularization term to\nmaintain the model's generative capabilities on safe content. Extensive\nexperiments demonstrate that DUO can robustly defend against various\nstate-of-the-art red teaming methods without significant performance\ndegradation on unrelated topics, as measured by FID and CLIP scores. Our work\ncontributes to the development of safer and more reliable T2I models, paving\nthe way for their responsible deployment in both closed-source and open-source\nscenarios.\n","authors":["Yong-Hyun Park","Sangdoo Yun","Jin-Hwa Kim","Junho Kim","Geonhui Jang","Yonghyun Jeong","Junghyo Jo","Gayoung Lee"],"pdf_url":"https://arxiv.org/pdf/2407.21035v2.pdf","comment":"This paper has been accepted for NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.10869v2","updated":"2025-01-16T08:06:16Z","published":"2024-06-16T09:38:33Z","title":"Geometric Distortion Guided Transformer for Omnidirectional Image\n  Super-Resolution","summary":"  As virtual and augmented reality applications gain popularity,\nomnidirectional image (ODI) super-resolution has become increasingly important.\nUnlike 2D plain images that are formed on a plane, ODIs are projected onto\nspherical surfaces. Applying established image super-resolution methods to\nODIs, therefore, requires performing equirectangular projection (ERP) to map\nthe ODIs onto a plane. ODI super-resolution needs to take into account\ngeometric distortion resulting from ERP. However, without considering such\ngeometric distortion of ERP images, previous deep-learning-based methods only\nutilize a limited range of pixels and may easily miss self-similar textures for\nreconstruction. In this paper, we introduce a novel Geometric Distortion Guided\nTransformer for Omnidirectional image Super-Resolution (GDGT-OSR).\nSpecifically, a distortion modulated rectangle-window self-attention mechanism,\nintegrated with deformable self-attention, is proposed to better perceive the\ndistortion and thus involve more self-similar textures. Distortion modulation\nis achieved through a newly devised distortion guidance generator that produces\nguidance by exploiting the variability of distortion across latitudes.\nFurthermore, we propose a dynamic feature aggregation scheme to adaptively fuse\nthe features from different self-attention modules. We present extensive\nexperimental results on public datasets and show that the new GDGT-OSR\noutperforms methods in existing literature.\n","authors":["Cuixin Yang","Rongkang Dong","Jun Xiao","Cong Zhang","Kin-Man Lam","Fei Zhou","Guoping Qiu"],"pdf_url":"https://arxiv.org/pdf/2406.10869v2.pdf","comment":"13 pages, 12 figures, journal"},{"id":"http://arxiv.org/abs/2501.09355v1","updated":"2025-01-16T08:06:02Z","published":"2025-01-16T08:06:02Z","title":"YETI (YET to Intervene) Proactive Interventions by Multimodal AI Agents\n  in Augmented Reality Tasks","summary":"  Multimodal AI Agents are AI models that have the capability of interactively\nand cooperatively assisting human users to solve day-to-day tasks. Augmented\nReality (AR) head worn devices can uniquely improve the user experience of\nsolving procedural day-to-day tasks by providing egocentric multimodal (audio\nand video) observational capabilities to AI Agents. Such AR capabilities can\nhelp AI Agents see and listen to actions that users take which can relate to\nmultimodal capabilities of human users. Existing AI Agents, either Large\nLanguage Models (LLMs) or Multimodal Vision-Language Models (VLMs) are reactive\nin nature, which means that models cannot take an action without reading or\nlistening to the human user's prompts. Proactivity of AI Agents on the other\nhand can help the human user detect and correct any mistakes in agent observed\ntasks, encourage users when they do tasks correctly or simply engage in\nconversation with the user - akin to a human teaching or assisting a user. Our\nproposed YET to Intervene (YETI) multimodal agent focuses on the research\nquestion of identifying circumstances that may require the agent to intervene\nproactively. This allows the agent to understand when it can intervene in a\nconversation with human users that can help the user correct mistakes on tasks,\nlike cooking, using AR. Our YETI Agent learns scene understanding signals based\non interpretable notions of Structural Similarity (SSIM) on consecutive video\nframes. We also define the alignment signal which the AI Agent can learn to\nidentify if the video frames corresponding to the user's actions on the task\nare consistent with expected actions. These signals are used by our AI Agent to\ndetermine when it should proactively intervene. We compare our results on the\ninstances of proactive intervention in the HoloAssist multimodal benchmark for\nan expert agent guiding a user to complete procedural tasks.\n","authors":["Saptarashmi Bandyopadhyay","Vikas Bahirwani","Lavisha Aggarwal","Bhanu Guda","Lin Li","Andrea Colaco"],"pdf_url":"https://arxiv.org/pdf/2501.09355v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2501.09350v1","updated":"2025-01-16T08:03:49Z","published":"2025-01-16T08:03:49Z","title":"Making Your Dreams A Reality: Decoding the Dreams into a Coherent Video\n  Story from fMRI Signals","summary":"  This paper studies the brave new idea for Multimedia community, and proposes\na novel framework to convert dreams into coherent video narratives using fMRI\ndata. Essentially, dreams have intrigued humanity for centuries, offering\nglimpses into our subconscious minds. Recent advancements in brain imaging,\nparticularly functional magnetic resonance imaging (fMRI), have provided new\nways to explore the neural basis of dreaming. By combining subjective dream\nexperiences with objective neurophysiological data, we aim to understand the\nvisual aspects of dreams and create complete video narratives. Our process\ninvolves three main steps: reconstructing visual perception, decoding dream\nimagery, and integrating dream stories. Using innovative techniques in fMRI\nanalysis and language modeling, we seek to push the boundaries of dream\nresearch and gain deeper insights into visual experiences during sleep. This\ntechnical report introduces a novel approach to visually decoding dreams using\nfMRI signals and weaving dream visuals into narratives using language models.\nWe gather a dataset of dreams along with descriptions to assess the\neffectiveness of our framework.\n","authors":["Yanwei Fu","Jianxiong Gao","Baofeng Yang","Jianfeng Feng"],"pdf_url":"https://arxiv.org/pdf/2501.09350v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2501.09347v1","updated":"2025-01-16T08:00:17Z","published":"2025-01-16T08:00:17Z","title":"UVRM: A Scalable 3D Reconstruction Model from Unposed Videos","summary":"  Large Reconstruction Models (LRMs) have recently become a popular method for\ncreating 3D foundational models. Training 3D reconstruction models with 2D\nvisual data traditionally requires prior knowledge of camera poses for the\ntraining samples, a process that is both time-consuming and prone to errors.\nConsequently, 3D reconstruction training has been confined to either synthetic\n3D datasets or small-scale datasets with annotated poses. In this study, we\ninvestigate the feasibility of 3D reconstruction using unposed video data of\nvarious objects. We introduce UVRM, a novel 3D reconstruction model capable of\nbeing trained and evaluated on monocular videos without requiring any\ninformation about the pose. UVRM uses a transformer network to implicitly\naggregate video frames into a pose-invariant latent feature space, which is\nthen decoded into a tri-plane 3D representation. To obviate the need for\nground-truth pose annotations during training, UVRM employs a combination of\nthe score distillation sampling (SDS) method and an analysis-by-synthesis\napproach, progressively synthesizing pseudo novel-views using a pre-trained\ndiffusion model. We qualitatively and quantitatively evaluate UVRM's\nperformance on the G-Objaverse and CO3D datasets without relying on pose\ninformation. Extensive experiments show that UVRM is capable of effectively and\nefficiently reconstructing a wide range of 3D objects from unposed videos.\n","authors":["Shiu-hong Kao","Xiao Li","Jinglu Wang","Chi-Keung Tang","Yu-Wing Tai","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2501.09347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04390v2","updated":"2025-01-16T07:58:06Z","published":"2025-01-08T10:08:09Z","title":"iFADIT: Invertible Face Anonymization via Disentangled Identity\n  Transform","summary":"  Face anonymization aims to conceal the visual identity of a face to safeguard\nthe individual's privacy. Traditional methods like blurring and pixelation can\nlargely remove identifying features, but these techniques significantly degrade\nimage quality and are vulnerable to deep reconstruction attacks. Generative\nmodels have emerged as a promising solution for anonymizing faces while\npreserving a natural appearance. However, many still face limitations in visual\nquality and often overlook the potential to recover the original face from the\nanonymized version, which can be valuable in specific contexts such as image\nforensics. This paper proposes a novel framework named iFADIT, an acronym for\nInvertible Face Anonymization via Disentangled Identity Transform. The\nframework features a disentanglement architecture coupled with a secure\nflow-based model: the former decouples identity information from\nnon-identifying attributes, while the latter transforms the decoupled identity\ninto an anonymized version in an invertible manner controlled by a secret key.\nThe anonymized face can then be reconstructed based on a pre-trained StyleGAN\nthat ensures high image quality and realistic facial details. Recovery of the\noriginal face (aka de-anonymization) is possible upon the availability of the\nmatching secret, by inverting the anonymization process based on the same set\nof model parameters. Furthermore, a dedicated secret-key mechanism along with a\ndual-phase training strategy is devised to ensure the desired properties of\nface anonymization. Qualitative and quantitative experiments demonstrate the\nsuperiority of the proposed approach in anonymity, reversibility, security,\ndiversity, and interpretability over competing methods.\n","authors":["Lin Yuan","Kai Liang","Xiong Li","Tao Wu","Nannan Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2501.04390v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09341v1","updated":"2025-01-16T07:50:56Z","published":"2025-01-16T07:50:56Z","title":"SE-BSFV: Online Subspace Learning based Shadow Enhancement and\n  Background Suppression for ViSAR under Complex Background","summary":"  Video synthetic aperture radar (ViSAR) has attracted substantial attention in\nthe moving target detection (MTD) field due to its ability to continuously\nmonitor changes in the target area. In ViSAR, the moving targets' shadows will\nnot offset and defocus, which is widely used as a feature for MTD. However, the\nshadows are difficult to distinguish from the low scattering region in the\nbackground, which will cause more missing and false alarms. Therefore, it is\nworth investigating how to enhance the distinction between the shadows and\nbackground. In this study, we proposed the Shadow Enhancement and Background\nSuppression for ViSAR (SE-BSFV) algorithm. The SE-BSFV algorithm is based on\nthe low-rank representation (LRR) theory and adopts online subspace learning\ntechnique to enhance shadows and suppress background for ViSAR images. Firstly,\nwe use a registration algorithm to register the ViSAR images and utilize\nGaussian mixture distribution (GMD) to model the ViSAR data. Secondly, the\nknowledge learned from the previous frames is leveraged to estimate the GMD\nparameters of the current frame, and the Expectation-maximization (EM)\nalgorithm is used to estimate the subspace parameters. Then, the foreground\nmatrix of the current frame can be obtained. Finally, the alternating direction\nmethod of multipliers (ADMM) is used to eliminate strong scattering objects in\nthe foreground matrix to obtain the final results. The experimental results\nindicate that the SE-BSFV algorithm significantly enhances the shadows'\nsaliency and greatly improves the detection performance while ensuring\nefficiency compared with several other advanced pre-processing algorithms.\n","authors":["Shangqu Yan","Chenyang Luo","Yaowen Fu","Wenpeng Zhang","Wei Yang","Ruofeng Yu"],"pdf_url":"https://arxiv.org/pdf/2501.09341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08331v2","updated":"2025-01-16T07:43:19Z","published":"2025-01-14T18:59:10Z","title":"Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using\n  Real-Time Warped Noise","summary":"  Generative modeling aims to transform random noise into structured outputs.\nIn this work, we enhance video diffusion models by allowing motion control via\nstructured latent noise sampling. This is achieved by just a change in data: we\npre-process training videos to yield structured noise. Consequently, our method\nis agnostic to diffusion model design, requiring no changes to model\narchitectures or training pipelines. Specifically, we propose a novel noise\nwarping algorithm, fast enough to run in real time, that replaces random\ntemporal Gaussianity with correlated warped noise derived from optical flow\nfields, while preserving the spatial Gaussianity. The efficiency of our\nalgorithm enables us to fine-tune modern video diffusion base models using\nwarped noise with minimal overhead, and provide a one-stop solution for a wide\nrange of user-friendly motion control: local object motion control, global\ncamera movement control, and motion transfer. The harmonization between\ntemporal coherence and spatial Gaussianity in our warped noise leads to\neffective motion control while maintaining per-frame pixel quality. Extensive\nexperiments and user studies demonstrate the advantages of our method, making\nit a robust and scalable approach for controlling motion in video diffusion\nmodels. Video results are available on our webpage:\nhttps://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow. Source code\nand model checkpoints are available on GitHub:\nhttps://github.com/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow.\n","authors":["Ryan Burgert","Yuancheng Xu","Wenqi Xian","Oliver Pilarski","Pascal Clausen","Mingming He","Li Ma","Yitong Deng","Lingxiao Li","Mohsen Mousavi","Michael Ryoo","Paul Debevec","Ning Yu"],"pdf_url":"https://arxiv.org/pdf/2501.08331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20406v3","updated":"2025-01-16T07:26:52Z","published":"2024-10-27T10:35:47Z","title":"Point-PRC: A Prompt Learning Based Regulation Framework for\n  Generalizable Point Cloud Analysis","summary":"  This paper investigates the 3D domain generalization (3DDG) ability of large\n3D models based on prevalent prompt learning. Recent works demonstrate the\nperformances of 3D point cloud recognition can be boosted remarkably by\nparameter-efficient prompt tuning. However, we observe that the improvement on\ndownstream tasks comes at the expense of a severe drop in 3D domain\ngeneralization. To resolve this challenge, we present a comprehensive\nregulation framework that allows the learnable prompts to actively interact\nwith the well-learned general knowledge in large 3D models to maintain good\ngeneralization. Specifically, the proposed framework imposes multiple explicit\nconstraints on the prompt learning trajectory by maximizing the mutual\nagreement between task-specific predictions and task-agnostic knowledge. We\ndesign the regulation framework as a plug-and-play module to embed into\nexisting representative large 3D models. Surprisingly, our method not only\nrealizes consistently increasing generalization ability but also enhances\ntask-specific 3D recognition performances across various 3DDG benchmarks by a\nclear margin. Considering the lack of study and evaluation on 3DDG, we also\ncreate three new benchmarks, namely base-to-new, cross-dataset and few-shot\ngeneralization benchmarks, to enrich the field and inspire future research.\nCode and benchmarks are available at\n\\url{https://github.com/auniquesun/Point-PRC}.\n","authors":["Hongyu Sun","Qiuhong Ke","Yongcai Wang","Wang Chen","Kang Yang","Deying Li","Jianfei Cai"],"pdf_url":"https://arxiv.org/pdf/2410.20406v3.pdf","comment":"5 figures, 14 tables; accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2501.09333v1","updated":"2025-01-16T07:07:41Z","published":"2025-01-16T07:07:41Z","title":"Prompt-CAM: A Simpler Interpretable Transformer for Fine-Grained\n  Analysis","summary":"  We present a simple usage of pre-trained Vision Transformers (ViTs) for\nfine-grained analysis, aiming to identify and localize the traits that\ndistinguish visually similar categories, such as different bird species or dog\nbreeds. Pre-trained ViTs such as DINO have shown remarkable capabilities to\nextract localized, informative features. However, using saliency maps like\nGrad-CAM can hardly point out the traits: they often locate the whole object by\na blurred, coarse heatmap, not traits. We propose a novel approach Prompt Class\nAttention Map (Prompt-CAM) to the rescue. Prompt-CAM learns class-specific\nprompts to a pre-trained ViT and uses the corresponding outputs for\nclassification. To classify an image correctly, the true-class prompt must\nattend to the unique image patches not seen in other classes' images, i.e.,\ntraits. As such, the true class's multi-head attention maps reveal traits and\ntheir locations. Implementation-wise, Prompt-CAM is almost a free lunch by\nsimply modifying the prediction head of Visual Prompt Tuning (VPT). This makes\nPrompt-CAM fairly easy to train and apply, sharply contrasting other\ninterpretable methods that design specific models and training processes. It is\neven simpler than the recently published INterpretable TRansformer (INTR),\nwhose encoder-decoder architecture prevents it from leveraging pre-trained\nViTs. Extensive empirical studies on a dozen datasets from various domains\n(e.g., birds, fishes, insects, fungi, flowers, food, and cars) validate\nPrompt-CAM superior interpretation capability.\n","authors":["Arpita Chowdhury","Dipanjyoti Paul","Zheda Mai","Jianyang Gu","Ziheng Zhang","Kazi Sajeed Mehrab","Elizabeth G. Campolongo","Daniel Rubenstein","Charles V. Stewart","Anuj Karpatne","Tanya Berger-Wolf","Yu Su","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2501.09333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19043v2","updated":"2025-01-16T06:46:18Z","published":"2024-06-27T09:50:20Z","title":"CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI","summary":"  Cardiac magnetic resonance imaging (MRI) has emerged as a clinically\ngold-standard technique for diagnosing cardiac diseases, thanks to its ability\nto provide diverse information with multiple modalities and anatomical views.\nAccelerated cardiac MRI is highly expected to achieve time-efficient and\npatient-friendly imaging, and then advanced image reconstruction approaches are\nrequired to recover high-quality, clinically interpretable images from\nundersampled measurements. However, the lack of publicly available cardiac MRI\nk-space dataset in terms of both quantity and diversity has severely hindered\nsubstantial technological progress, particularly for data-driven artificial\nintelligence. Here, we provide a standardized, diverse, and high-quality\nCMRxRecon2024 dataset to facilitate the technical development, fair evaluation,\nand clinical transfer of cardiac MRI reconstruction approaches, towards\npromoting the universal frameworks that enable fast and robust reconstructions\nacross different cardiac MRI protocols in clinical practice. To the best of our\nknowledge, the CMRxRecon2024 dataset is the largest and most protocal-diverse\npublicly available cardiac k-space dataset. It is acquired from 330 healthy\nvolunteers, covering commonly used modalities, anatomical views, and\nacquisition trajectories in clinical cardiac MRI workflows. Besides, an open\nplatform with tutorials, benchmarks, and data processing tools is provided to\nfacilitate data usage, advanced method development, and fair performance\nevaluation.\n","authors":["Zi Wang","Fanwen Wang","Chen Qin","Jun Lyu","Cheng Ouyang","Shuo Wang","Yan Li","Mengyao Yu","Haoyu Zhang","Kunyuan Guo","Zhang Shi","Qirong Li","Ziqiang Xu","Yajing Zhang","Hao Li","Sha Hua","Binghua Chen","Longyu Sun","Mengting Sun","Qin Li","Ying-Hua Chu","Wenjia Bai","Jing Qin","Xiahai Zhuang","Claudia Prieto","Alistair Young","Michael Markl","He Wang","Lianming Wu","Guang Yang","Xiaobo Qu","Chengyan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19043v2.pdf","comment":"23 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2501.09321v1","updated":"2025-01-16T06:25:56Z","published":"2025-01-16T06:25:56Z","title":"Soft Knowledge Distillation with Multi-Dimensional Cross-Net Attention\n  for Image Restoration Models Compression","summary":"  Transformer-based encoder-decoder models have achieved remarkable success in\nimage-to-image transfer tasks, particularly in image restoration. However,\ntheir high computational complexity-manifested in elevated FLOPs and parameter\ncounts-limits their application in real-world scenarios. Existing knowledge\ndistillation methods in image restoration typically employ lightweight student\nmodels that directly mimic the intermediate features and reconstruction results\nof the teacher, overlooking the implicit attention relationships between them.\nTo address this, we propose a Soft Knowledge Distillation (SKD) strategy that\nincorporates a Multi-dimensional Cross-net Attention (MCA) mechanism for\ncompressing image restoration models. This mechanism facilitates interaction\nbetween the student and teacher across both channel and spatial dimensions,\nenabling the student to implicitly learn the attention matrices. Additionally,\nwe employ a Gaussian kernel function to measure the distance between student\nand teacher features in kernel space, ensuring stable and efficient feature\nlearning. To further enhance the quality of reconstructed images, we replace\nthe commonly used L1 or KL divergence loss with a contrastive learning loss at\nthe image level. Experiments on three tasks-image deraining, deblurring, and\ndenoising-demonstrate that our SKD strategy significantly reduces computational\ncomplexity while maintaining strong image restoration capabilities.\n","authors":["Yongheng Zhang","Danfeng Yan"],"pdf_url":"https://arxiv.org/pdf/2501.09321v1.pdf","comment":"Accepted by ICASSP2025"},{"id":"http://arxiv.org/abs/2501.09311v1","updated":"2025-01-16T05:58:32Z","published":"2025-01-16T05:58:32Z","title":"Shape-Based Single Object Classification Using Ensemble Method\n  Classifiers","summary":"  Nowadays, more and more images are available. Annotation and retrieval of the\nimages pose classification problems, where each class is defined as the group\nof database images labelled with a common semantic label. Various systems have\nbeen proposed for content-based retrieval, as well as for image classification\nand indexing. In this paper, a hierarchical classification framework has been\nproposed for bridging the semantic gap effectively and achieving multi-category\nimage classification. A well known pre-processing and post-processing method\nwas used and applied to three problems; image segmentation, object\nidentification and image classification. The method was applied to classify\nsingle object images from Amazon and Google datasets. The classification was\ntested for four different classifiers; BayesNetwork (BN), Random Forest (RF),\nBagging and Vote. The estimated classification accuracies ranged from 20% to\n99% (using 10-fold cross validation). The Bagging classifier presents the best\nperformance, followed by the Random Forest classifier.\n","authors":["Nur Shazwani Kamarudin","Mokhairi Makhtar","Syadiah Nor Wan Shamsuddin","Syed Abdullah Fadzli"],"pdf_url":"https://arxiv.org/pdf/2501.09311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01432v3","updated":"2025-01-16T05:42:28Z","published":"2024-07-18T19:44:44Z","title":"VLG-CBM: Training Concept Bottleneck Models with Vision-Language\n  Guidance","summary":"  Concept Bottleneck Models (CBMs) provide interpretable prediction by\nintroducing an intermediate Concept Bottleneck Layer (CBL), which encodes\nhuman-understandable concepts to explain models' decision. Recent works\nproposed to utilize Large Language Models and pre-trained Vision-Language\nModels to automate the training of CBMs, making it more scalable and automated.\nHowever, existing approaches still fall short in two aspects: First, the\nconcepts predicted by CBL often mismatch the input image, raising doubts about\nthe faithfulness of interpretation. Second, it has been shown that concept\nvalues encode unintended information: even a set of random concepts could\nachieve comparable test accuracy to state-of-the-art CBMs. To address these\ncritical limitations, in this work, we propose a novel framework called\nVision-Language-Guided Concept Bottleneck Model (VLG-CBM) to enable faithful\ninterpretability with the benefits of boosted performance. Our method leverages\noff-the-shelf open-domain grounded object detectors to provide visually\ngrounded concept annotation, which largely enhances the faithfulness of concept\nprediction while further improving the model performance. In addition, we\npropose a new metric called Number of Effective Concepts (NEC) to control the\ninformation leakage and provide better interpretability. Extensive evaluations\nacross five standard benchmarks show that our method, VLG-CBM, outperforms\nexisting methods by at least 4.27% and up to 51.09% on Accuracy at NEC=5\n(denoted as ANEC-5), and by at least 0.45% and up to 29.78% on average accuracy\n(denoted as ANEC-avg), while preserving both faithfulness and interpretability\nof the learned concepts as demonstrated in extensive experiments.\n","authors":["Divyansh Srivastava","Ge Yan","Tsui-Wei Weng"],"pdf_url":"https://arxiv.org/pdf/2408.01432v3.pdf","comment":"Appeared at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2501.09305v1","updated":"2025-01-16T05:39:50Z","published":"2025-01-16T05:39:50Z","title":"Domain-conditioned and Temporal-guided Diffusion Modeling for\n  Accelerated Dynamic MRI Reconstruction","summary":"  Purpose: To propose a domain-conditioned and temporal-guided diffusion\nmodeling method, termed dynamic Diffusion Modeling (dDiMo), for accelerated\ndynamic MRI reconstruction, enabling diffusion process to characterize\nspatiotemporal information for time-resolved multi-coil Cartesian and\nnon-Cartesian data. Methods: The dDiMo framework integrates temporal\ninformation from time-resolved dimensions, allowing for the concurrent capture\nof intra-frame spatial features and inter-frame temporal dynamics in diffusion\nmodeling. It employs additional spatiotemporal ($x$-$t$) and self-consistent\nfrequency-temporal ($k$-$t$) priors to guide the diffusion process. This\napproach ensures precise temporal alignment and enhances the recovery of fine\nimage details. To facilitate a smooth diffusion process, the nonlinear\nconjugate gradient algorithm is utilized during the reverse diffusion steps.\nThe proposed model was tested on two types of MRI data: Cartesian-acquired\nmulti-coil cardiac MRI and Golden-Angle-Radial-acquired multi-coil\nfree-breathing lung MRI, across various undersampling rates. Results: dDiMo\nachieved high-quality reconstructions at various acceleration factors,\ndemonstrating improved temporal alignment and structural recovery compared to\nother competitive reconstruction methods, both qualitatively and\nquantitatively. This proposed diffusion framework exhibited robust performance\nin handling both Cartesian and non-Cartesian acquisitions, effectively\nreconstructing dynamic datasets in cardiac and lung MRI under different imaging\nconditions. Conclusion: This study introduces a novel diffusion modeling method\nfor dynamic MRI reconstruction.\n","authors":["Liping Zhang","Iris Yuwen Zhou","Sydney B. Montesi","Li Feng","Fang Liu"],"pdf_url":"https://arxiv.org/pdf/2501.09305v1.pdf","comment":"21 pages, 15 figures, 2 tables"},{"id":"http://arxiv.org/abs/2501.09304v1","updated":"2025-01-16T05:39:28Z","published":"2025-01-16T05:39:28Z","title":"Finding the Trigger: Causal Abductive Reasoning on Video Events","summary":"  This paper introduces a new problem, Causal Abductive Reasoning on Video\nEvents (CARVE), which involves identifying causal relationships between events\nin a video and generating hypotheses about causal chains that account for the\noccurrence of a target event. To facilitate research in this direction, we\ncreate two new benchmark datasets with both synthetic and realistic videos,\naccompanied by trigger-target labels generated through a novel counterfactual\nsynthesis approach. To explore the challenge of solving CARVE, we present a\nCausal Event Relation Network (CERN) that examines the relationships between\nvideo events in temporal and semantic spaces to efficiently determine the\nroot-cause trigger events. Through extensive experiments, we demonstrate the\ncritical roles of event relational representation learning and interaction\nmodeling in solving video causal reasoning challenges. The introduction of the\nCARVE task, along with the accompanying datasets and the CERN framework, will\nadvance future research on video causal reasoning and significantly facilitate\nvarious applications, including video surveillance, root-cause analysis and\nmovie content management.\n","authors":["Thao Minh Le","Vuong Le","Kien Do","Sunil Gupta","Svetha Venkatesh","Truyen Tran"],"pdf_url":"https://arxiv.org/pdf/2501.09304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09302v1","updated":"2025-01-16T05:37:29Z","published":"2025-01-16T05:37:29Z","title":"Creating Virtual Environments with 3D Gaussian Splatting: A Comparative\n  Study","summary":"  3D Gaussian Splatting (3DGS) has recently emerged as an innovative and\nefficient 3D representation technique. While its potential for extended reality\n(XR) applications is frequently highlighted, its practical effectiveness\nremains underexplored. In this work, we examine three distinct 3DGS-based\napproaches for virtual environment (VE) creation, leveraging their unique\nstrengths for efficient and visually compelling scene representation. By\nconducting a comparable study, we evaluate the feasibility of 3DGS in creating\nimmersive VEs, identify its limitations in XR applications, and discuss future\nresearch and development opportunities.\n","authors":["Shi Qiu","Binzhu Xie","Qixuan Liu","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2501.09302v1.pdf","comment":"IEEE VR 2025 Posters"},{"id":"http://arxiv.org/abs/2501.09294v1","updated":"2025-01-16T05:01:30Z","published":"2025-01-16T05:01:30Z","title":"Efficient Few-Shot Medical Image Analysis via Hierarchical Contrastive\n  Vision-Language Learning","summary":"  Few-shot learning in medical image classification presents a significant\nchallenge due to the limited availability of annotated data and the complex\nnature of medical imagery. In this work, we propose Adaptive Vision-Language\nFine-tuning with Hierarchical Contrastive Alignment (HiCA), a novel framework\nthat leverages the capabilities of Large Vision-Language Models (LVLMs) for\nmedical image analysis. HiCA introduces a two-stage fine-tuning strategy,\ncombining domain-specific pretraining and hierarchical contrastive learning to\nalign visual and textual representations at multiple levels. We evaluate our\napproach on two benchmark datasets, Chest X-ray and Breast Ultrasound,\nachieving state-of-the-art performance in both few-shot and zero-shot settings.\nFurther analyses demonstrate the robustness, generalizability, and\ninterpretability of our method, with substantial improvements in performance\ncompared to existing baselines. Our work highlights the potential of\nhierarchical contrastive strategies in adapting LVLMs to the unique challenges\nof medical imaging tasks.\n","authors":["Harrison Fuller","Fernando Gabriela Garcia","Victor Flores"],"pdf_url":"https://arxiv.org/pdf/2501.09294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03789v3","updated":"2025-01-16T04:13:10Z","published":"2023-07-07T18:28:44Z","title":"Synthesizing Forestry Images Conditioned on Plant Phenotype Using a\n  Generative Adversarial Network","summary":"  Plant phenology and phenotype prediction using remote sensing data are\nincreasingly gaining attention within the plant science community as a\npromising approach to enhance agricultural productivity. This work focuses on\ngenerating synthetic forestry images that satisfy certain phenotypic\nattributes, viz. canopy greenness. We harness a Generative Adversarial Network\n(GAN) to synthesize biologically plausible and phenotypically stable forestry\nimages conditioned on the greenness of vegetation (a continuous attribute) over\na specific region of interest, describing a particular vegetation type in a\nmixed forest. The training data is based on the automated digital camera\nimagery provided by the National Ecological Observatory Network (NEON) and\nprocessed by the PhenoCam Network. Our method helps render the appearance of\nforest sites specific to a greenness value. The synthetic images are\nsubsequently utilized to predict another phenotypic attribute, viz., redness of\nplants. The quality of the synthetic images is assessed using the Structural\nSIMilarity (SSIM) index and Fr\\'echet Inception Distance (FID). Further, the\ngreenness and redness indices of the synthetic images are compared against\nthose of the original images using Root Mean Squared Percentage Error (RMSPE)\nto evaluate their accuracy and integrity. The generalizability and scalability\nof our proposed GAN model are established by effectively transforming it to\ngenerate synthetic images for other forest sites and vegetation types. From a\nbroader perspective, this approach could be leveraged to visualize forestry\nbased on different phenotypic attributes in the context of various\nenvironmental parameters.\n","authors":["Debasmita Pal","Arun Ross"],"pdf_url":"https://arxiv.org/pdf/2307.03789v3.pdf","comment":"Accepted to Pattern Recognition journal"},{"id":"http://arxiv.org/abs/2501.09281v1","updated":"2025-01-16T04:06:59Z","published":"2025-01-16T04:06:59Z","title":"SoccerSynth-Detection: A Synthetic Dataset for Soccer Player Detection","summary":"  In soccer video analysis, player detection is essential for identifying key\nevents and reconstructing tactical positions. The presence of numerous players\nand frequent occlusions, combined with copyright restrictions, severely\nrestricts the availability of datasets, leaving limited options such as\nSoccerNet-Tracking and SportsMOT. These datasets suffer from a lack of\ndiversity, which hinders algorithms from adapting effectively to varied soccer\nvideo contexts. To address these challenges, we developed\nSoccerSynth-Detection, the first synthetic dataset designed for the detection\nof synthetic soccer players. It includes a broad range of random lighting and\ntextures, as well as simulated camera motion blur. We validated its efficacy\nusing the object detection model (Yolov8n) against real-world datasets\n(SoccerNet-Tracking and SportsMoT). In transfer tests, it matched the\nperformance of real datasets and significantly outperformed them in images with\nmotion blur; in pre-training tests, it demonstrated its efficacy as a\npre-training dataset, significantly enhancing the algorithm's overall\nperformance. Our work demonstrates the potential of synthetic datasets to\nreplace real datasets for algorithm training in the field of soccer video\nanalysis.\n","authors":["Haobin Qin","Calvin Yeung","Rikuhei Umemoto","Keisuke Fujii"],"pdf_url":"https://arxiv.org/pdf/2501.09281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09278v1","updated":"2025-01-16T03:54:06Z","published":"2025-01-16T03:54:06Z","title":"Text-guided Synthetic Geometric Augmentation for Zero-shot 3D\n  Understanding","summary":"  Zero-shot recognition models require extensive training data for\ngeneralization. However, in zero-shot 3D classification, collecting 3D data and\ncaptions is costly and laborintensive, posing a significant barrier compared to\n2D vision. Recent advances in generative models have achieved unprecedented\nrealism in synthetic data production, and recent research shows the potential\nfor using generated data as training data. Here, naturally raising the\nquestion: Can synthetic 3D data generated by generative models be used as\nexpanding limited 3D datasets? In response, we present a synthetic 3D dataset\nexpansion method, Textguided Geometric Augmentation (TeGA). TeGA is tailored\nfor language-image-3D pretraining, which achieves SoTA in zero-shot 3D\nclassification, and uses a generative textto-3D model to enhance and extend\nlimited 3D datasets. Specifically, we automatically generate text-guided\nsynthetic 3D data and introduce a consistency filtering strategy to discard\nnoisy samples where semantics and geometric shapes do not match with text. In\nthe experiment to double the original dataset size using TeGA, our approach\ndemonstrates improvements over the baselines, achieving zeroshot performance\ngains of 3.0% on Objaverse-LVIS, 4.6% on ScanObjectNN, and 8.7% on ModelNet40.\nThese results demonstrate that TeGA effectively bridges the 3D data gap,\nenabling robust zero-shot 3D classification even with limited real training\ndata and paving the way for zero-shot 3D vision application.\n","authors":["Kohei Torimi","Ryosuke Yamada","Daichi Otsuka","Kensho Hara","Yuki M. Asano","Hirokatsu Kataoka","Yoshimitsu Aoki"],"pdf_url":"https://arxiv.org/pdf/2501.09278v1.pdf","comment":"14 pages, 8 figures, this paper is submitted to CVPR"},{"id":"http://arxiv.org/abs/2501.08659v2","updated":"2025-01-16T03:51:49Z","published":"2025-01-15T08:50:52Z","title":"BRIGHT-VO: Brightness-Guided Hybrid Transformer for Visual Odometry with\n  Multi-modality Refinement Module","summary":"  Visual odometry (VO) plays a crucial role in autonomous driving, robotic\nnavigation, and other related tasks by estimating the position and orientation\nof a camera based on visual input. Significant progress has been made in\ndata-driven VO methods, particularly those leveraging deep learning techniques\nto extract image features and estimate camera poses. However, these methods\noften struggle in low-light conditions because of the reduced visibility of\nfeatures and the increased difficulty of matching keypoints. To address this\nlimitation, we introduce BrightVO, a novel VO model based on Transformer\narchitecture, which not only performs front-end visual feature extraction, but\nalso incorporates a multi-modality refinement module in the back-end that\nintegrates Inertial Measurement Unit (IMU) data. Using pose graph optimization,\nthis module iteratively refines pose estimates to reduce errors and improve\nboth accuracy and robustness. Furthermore, we create a synthetic low-light\ndataset, KiC4R, which includes a variety of lighting conditions to facilitate\nthe training and evaluation of VO frameworks in challenging environments.\nExperimental results demonstrate that BrightVO achieves state-of-the-art\nperformance on both the KiC4R dataset and the KITTI benchmarks. Specifically,\nit provides an average improvement of 20% in pose estimation accuracy in normal\noutdoor environments and 259% in low-light conditions, outperforming existing\nmethods. For widespread use and further development, the research work is fully\nopen-source at https://github.com/Anastasiawd/BrightVO.\n","authors":["Dongzhihan Wang","Yang Yang","Liang Xu"],"pdf_url":"https://arxiv.org/pdf/2501.08659v2.pdf","comment":"We have identified significant issues in the methodology and data\n  analysis that impact the validity of our conclusions"},{"id":"http://arxiv.org/abs/2501.09277v1","updated":"2025-01-16T03:47:25Z","published":"2025-01-16T03:47:25Z","title":"Bias for Action: Video Implicit Neural Representations with Bias\n  Modulation","summary":"  We propose a new continuous video modeling framework based on implicit neural\nrepresentations (INRs) called ActINR. At the core of our approach is the\nobservation that INRs can be considered as a learnable dictionary, with the\nshapes of the basis functions governed by the weights of the INR, and their\nlocations governed by the biases. Given compact non-linear activation\nfunctions, we hypothesize that an INR's biases are suitable to capture motion\nacross images, and facilitate compact representations for video sequences.\nUsing these observations, we design ActINR to share INR weights across frames\nof a video sequence, while using unique biases for each frame. We further model\nthe biases as the output of a separate INR conditioned on time index to promote\nsmoothness. By training the video INR and this bias INR together, we\ndemonstrate unique capabilities, including $10\\times$ video slow motion,\n$4\\times$ spatial super resolution along with $2\\times$ slow motion, denoising,\nand video inpainting. ActINR performs remarkably well across numerous video\nprocessing tasks (often achieving more than 6dB improvement), setting a new\nstandard for continuous modeling of videos.\n","authors":["Alper Kayabasi","Anil Kumar Vadathya","Guha Balakrishnan","Vishwanath Saragadam"],"pdf_url":"https://arxiv.org/pdf/2501.09277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09268v1","updated":"2025-01-16T03:35:23Z","published":"2025-01-16T03:35:23Z","title":"Knowledge Distillation for Image Restoration : Simultaneous Learning\n  from Degraded and Clean Images","summary":"  Model compression through knowledge distillation has seen extensive\napplication in classification and segmentation tasks. However, its potential in\nimage-to-image translation, particularly in image restoration, remains\nunderexplored. To address this gap, we propose a Simultaneous Learning\nKnowledge Distillation (SLKD) framework tailored for model compression in image\nrestoration tasks. SLKD employs a dual-teacher, single-student architecture\nwith two distinct learning strategies: Degradation Removal Learning (DRL) and\nImage Reconstruction Learning (IRL), simultaneously. In DRL, the student\nencoder learns from Teacher A to focus on removing degradation factors, guided\nby a novel BRISQUE extractor. In IRL, the student decoder learns from Teacher B\nto reconstruct clean images, with the assistance of a proposed PIQE extractor.\nThese strategies enable the student to learn from degraded and clean images\nsimultaneously, ensuring high-quality compression of image restoration models.\nExperimental results across five datasets and three tasks demonstrate that SLKD\nachieves substantial reductions in FLOPs and parameters, exceeding 80\\%, while\nmaintaining strong image restoration performance.\n","authors":["Yongheng Zhang","Danfeng Yan"],"pdf_url":"https://arxiv.org/pdf/2501.09268v1.pdf","comment":"Accepted by ICASSP2025"},{"id":"http://arxiv.org/abs/2501.09267v1","updated":"2025-01-16T03:34:36Z","published":"2025-01-16T03:34:36Z","title":"Are Open-Vocabulary Models Ready for Detection of MEP Elements on\n  Construction Sites","summary":"  The construction industry has long explored robotics and computer vision, yet\ntheir deployment on construction sites remains very limited. These technologies\nhave the potential to revolutionize traditional workflows by enhancing\naccuracy, efficiency, and safety in construction management. Ground robots\nequipped with advanced vision systems could automate tasks such as monitoring\nmechanical, electrical, and plumbing (MEP) systems. The present research\nevaluates the applicability of open-vocabulary vision-language models compared\nto fine-tuned, lightweight, closed-set object detectors for detecting MEP\ncomponents using a mobile ground robotic platform. A dataset collected with\ncameras mounted on a ground robot was manually annotated and analyzed to\ncompare model performance. The results demonstrate that, despite the\nversatility of vision-language models, fine-tuned lightweight models still\nlargely outperform them in specialized environments and for domain-specific\ntasks.\n","authors":["Abdalwhab Abdalwhab","Ali Imran","Sina Heydarian","Ivanka Iordanova","David St-Onge"],"pdf_url":"https://arxiv.org/pdf/2501.09267v1.pdf","comment":"4 pages, 3 figures"},{"id":"http://arxiv.org/abs/2501.06848v3","updated":"2025-01-16T03:18:14Z","published":"2025-01-12T15:34:24Z","title":"A General Framework for Inference-time Scaling and Steering of Diffusion\n  Models","summary":"  Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we propose Feynman Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models, even\nwith off-the-shelf rewards, can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering .\n","authors":["Raghav Singhal","Zachary Horvitz","Ryan Teehan","Mengye Ren","Zhou Yu","Kathleen McKeown","Rajesh Ranganath"],"pdf_url":"https://arxiv.org/pdf/2501.06848v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09259v1","updated":"2025-01-16T03:02:08Z","published":"2025-01-16T03:02:08Z","title":"OpticFusion: Multi-Modal Neural Implicit 3D Reconstruction of\n  Microstructures by Fusing White Light Interferometry and Optical Microscopy","summary":"  White Light Interferometry (WLI) is a precise optical tool for measuring the\n3D topography of microstructures. However, conventional WLI cannot capture the\nnatural color of a sample's surface, which is essential for many microscale\nresearch applications that require both 3D geometry and color information.\nPrevious methods have attempted to overcome this limitation by modifying WLI\nhardware and analysis software, but these solutions are often costly. In this\nwork, we address this challenge from a computer vision multi-modal\nreconstruction perspective for the first time. We introduce OpticFusion, a\nnovel approach that uses an additional digital optical microscope (OM) to\nachieve 3D reconstruction with natural color textures using multi-view WLI and\nOM images. Our method employs a two-step data association process to obtain the\nposes of WLI and OM data. By leveraging the neural implicit representation, we\nfuse multi-modal data and apply color decomposition technology to extract the\nsample's natural color. Tested on our multi-modal dataset of various microscale\nsamples, OpticFusion achieves detailed 3D reconstructions with color textures.\nOur method provides an effective tool for practical applications across\nnumerous microscale research fields. The source code and our real-world dataset\nare available at https://github.com/zju3dv/OpticFusion.\n","authors":["Shuo Chen","Yijin Li","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.09259v1.pdf","comment":"3DV 2025"},{"id":"http://arxiv.org/abs/2303.13397v6","updated":"2025-01-16T02:48:38Z","published":"2023-03-23T16:15:18Z","title":"DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery\n  from Videos","summary":"  Human mesh recovery (HMR) provides rich human body information for various\nreal-world applications. While image-based HMR methods have achieved impressive\nresults, they often struggle to recover humans in dynamic scenarios, leading to\ntemporal inconsistencies and non-smooth 3D motion predictions due to the\nabsence of human motion. In contrast, video-based approaches leverage temporal\ninformation to mitigate this issue. In this paper, we present DiffMesh, an\ninnovative motion-aware Diffusion-like framework for video-based HMR. DiffMesh\nestablishes a bridge between diffusion models and human motion, efficiently\ngenerating accurate and smooth output mesh sequences by incorporating human\nmotion within the forward process and reverse process in the diffusion model.\nExtensive experiments are conducted on the widely used datasets (Human3.6M\n\\cite{h36m_pami} and 3DPW \\cite{pw3d2018}), which demonstrate the effectiveness\nand efficiency of our DiffMesh. Visual comparisons in real-world scenarios\nfurther highlight DiffMesh's suitability for practical applications.\n","authors":["Ce Zheng","Xianpeng Liu","Qucheng Peng","Tianfu Wu","Pu Wang","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13397v6.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2501.05264v3","updated":"2025-01-16T02:39:20Z","published":"2025-01-09T14:19:33Z","title":"Towards Balanced Continual Multi-Modal Learning in Human Pose Estimation","summary":"  3D human pose estimation (3D HPE) has emerged as a prominent research topic,\nparticularly in the realm of RGB-based methods. However, RGB images are\nsusceptible to limitations such as sensitivity to lighting conditions and\npotential user discomfort. Consequently, multi-modal sensing, which leverages\nnon-intrusive sensors, is gaining increasing attention. Nevertheless,\nmulti-modal 3D HPE still faces challenges, including modality imbalance and the\nimperative for continual learning. In this work, we introduce a novel balanced\ncontinual multi-modal learning method for 3D HPE, which harnesses the power of\nRGB, LiDAR, mmWave, and WiFi. Specifically, we propose a Shapley value-based\ncontribution algorithm to quantify the contribution of each modality and\nidentify modality imbalance. To address this imbalance, we employ a re-learning\nstrategy. Furthermore, recognizing that raw data is prone to noise\ncontamination, we develop a novel denoising continual learning approach. This\napproach incorporates a noise identification and separation module to mitigate\nthe adverse effects of noise and collaborates with the balanced learning\nstrategy to enhance optimization. Additionally, an adaptive EWC mechanism is\nemployed to alleviate catastrophic forgetting. We conduct extensive experiments\non the widely-adopted multi-modal dataset, MM-Fi, which demonstrate the\nsuperiority of our approach in boosting 3D pose estimation and mitigating\ncatastrophic forgetting in complex scenarios. We will release our codes.\n","authors":["Jiaxuan Peng","Mengshi Qi","Dong Zhao","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2501.05264v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00689v4","updated":"2025-01-16T02:12:45Z","published":"2023-11-01T17:45:22Z","title":"Collaboration in Immersive Environments: Challenges and Solutions","summary":"  Virtual Reality (VR) and Augmented Reality (AR) tools have been applied in\nall engineering fields in order to avoid the use of physical prototypes, to\ntrain in high-risk situations, and to interpret real or simulated results. In\norder to complete a shared task or assign tasks to the agents in such immersive\nenvironments, collaboration or Shared Cooperative Activities are a necessity.\nCollaboration in immersive environments is an emerging field of research that\naims to study and enhance the ways in which people interact and work together\nin Virtual and Augmented Reality settings. Collaboration in immersive\nenvironments is a complex process that involves different factors such as\ncommunication, coordination, and social presence. This paper provides an\noverview of the current state of research on collaboration in immersive\nenvironments. It discusses the different types of immersive environments,\nincluding VR and AR, and the different forms of collaboration that can occur in\nthese environments. The paper also highlights the challenges and limitations of\ncollaboration in immersive environments, such as the lack of physical cues,\ncost and usability and the need for further research in this area. Overall,\ncollaboration in immersive environments is a promising field with a wide range\nof potential applications, from education to industry, and it can benefit both\nindividuals and groups by enhancing their ability to work together effectively.\n","authors":["Shahin Doroudian"],"pdf_url":"https://arxiv.org/pdf/2311.00689v4.pdf","comment":"Added new references in Networking section"},{"id":"http://arxiv.org/abs/2408.01167v3","updated":"2025-01-16T02:09:15Z","published":"2024-08-02T10:34:23Z","title":"Rethinking Pre-Trained Feature Extractor Selection in Multiple Instance\n  Learning for Whole Slide Image Classification","summary":"  Multiple instance learning (MIL) has become a preferred method for gigapixel\nwhole slide image (WSI) classification without requiring patch-level\nannotations. Current MIL research primarily relies on embedding-based\napproaches, which extract patch features using a pre-trained feature extractor\nand aggregate them for slide-level prediction. Despite the critical role of\nfeature extraction, there is limited guidance on selecting optimal feature\nextractors to maximize WSI performance. This study addresses this gap by\nsystematically evaluating MIL feature extractors across three dimensions:\npre-training dataset, backbone model, and pre-training method. Extensive\nexperiments were conducted on two public WSI datasets (TCGA-NSCLC and\nCamelyon16) using four state-of-the-art (SOTA) MIL models. Our findings reveal\nthat: 1) selecting a robust self-supervised learning (SSL) method has a greater\nimpact on performance than relying solely on an in-domain pre-training dataset;\n2) prioritizing Transformer-based backbones with deeper architectures over\nCNN-based models; and 3) using larger, more diverse pre-training datasets\nsignificantly enhances classification outcomes. We hope that these insights can\nprovide practical guidance for optimizing WSI classification and explain the\nreasons behind the performance advantages of the current SOTA pathology\nfoundation models. Furthermore, this work may inform the development of more\neffective pathology foundation models. Our code is publicly available at\nhttps://github.com/bryanwong17/MIL-Feature-Extractor-Selection\n","authors":["Bryan Wong","Mun Yong Yi"],"pdf_url":"https://arxiv.org/pdf/2408.01167v3.pdf","comment":"Accepted to IEEE International Symposium on Biomedical Imaging (ISBI)\n  2025"},{"id":"http://arxiv.org/abs/2408.01077v3","updated":"2025-01-16T02:08:47Z","published":"2024-08-02T07:52:28Z","title":"PhysMamba: State Space Duality Model for Remote Physiological\n  Measurement","summary":"  Remote Photoplethysmography (rPPG) enables non-contact physiological signal\nextraction from facial videos, offering applications in psychological state\nanalysis, medical assistance, and anti-face spoofing. However, challenges such\nas motion artifacts, lighting variations, and noise limit its real-world\napplicability. To address these issues, we propose PhysMamba, a novel\ndual-pathway time-frequency interaction model based on Synergistic State Space\nDuality (SSSD), which for the first time integrates state space models with\nattention mechanisms in a dual-branch framework. Combined with a Multi-Scale\nQuery (MQ) mechanism, PhysMamba achieves efficient information exchange and\nenhanced feature representation, ensuring robustness under noisy and dynamic\nconditions. Experiments on PURE, UBFC-rPPG, and MMPD datasets demonstrate that\nPhysMamba outperforms state-of-the-art methods, offering superior accuracy and\ngeneralization. This work lays a strong foundation for practical applications\nin non-contact health monitoring, including real-time remote patient care.\n","authors":["Zhixin Yan","Yan Zhong","Hongbin Xu","Wenjun Zhang","Shangru Yi","Lin Shu","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2408.01077v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10377v4","updated":"2025-01-16T01:30:35Z","published":"2024-07-15T01:11:30Z","title":"Enhanced Masked Image Modeling to Avoid Model Collapse on Multi-modal\n  MRI Datasets","summary":"  Multi-modal magnetic resonance imaging (MRI) provides information of lesions\nfor computer-aided diagnosis from different views. Deep learning algorithms are\nsuitable for identifying specific anatomical structures, segmenting lesions,\nand classifying diseases. Manual labels are limited due to the high expense,\nwhich hinders further improvement of accuracy. Self-supervised learning,\nparticularly masked image modeling (MIM), has shown promise in utilizing\nunlabeled data. However, we spot model collapse when applying MIM to\nmulti-modal MRI datasets. The performance of downstream tasks does not see any\nimprovement following the collapsed model. To solve model collapse, we analyze\nand address it in two types: complete collapse and dimensional collapse. We\nfind complete collapse occurs because the collapsed loss value in multi-modal\nMRI datasets falls below the normally converged loss value. Based on this, the\nhybrid mask pattern (HMP) masking strategy is introduced to elevate the\ncollapsed loss above the normally converged loss value and avoid complete\ncollapse. Additionally, we reveal that dimensional collapse stems from\ninsufficient feature uniformity in MIM. We mitigate dimensional collapse by\nintroducing the pyramid barlow twins (PBT) module as an explicit regularization\nmethod. Overall, we construct the enhanced MIM (E-MIM) with HMP and PBT module\nto avoid model collapse multi-modal MRI. Experiments are conducted on three\nmulti-modal MRI datasets to validate the effectiveness of our approach in\npreventing both types of model collapse. By preventing model collapse, the\ntraining of the model becomes more stable, resulting in a decent improvement in\nperformance for segmentation and classification tasks. The code is available at\nhttps://github.com/LinxuanHan/E-MIM.\n","authors":["Linxuan Han","Sa Xiao","Zimeng Li","Haidong Li","Xiuchao Zhao","Yeqing Han","Fumin Guo","Xin Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.10377v4.pdf","comment":"This work has been submitted to the lEEE for possible publication.\n  copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.05526v2","updated":"2025-01-16T00:54:04Z","published":"2024-08-10T11:48:14Z","title":"CryoBench: Diverse and challenging datasets for the heterogeneity\n  problem in cryo-EM","summary":"  Cryo-electron microscopy (cryo-EM) is a powerful technique for determining\nhigh-resolution 3D biomolecular structures from imaging data. Its unique\nability to capture structural variability has spurred the development of\nheterogeneous reconstruction algorithms that can infer distributions of 3D\nstructures from noisy, unlabeled imaging data. Despite the growing number of\nadvanced methods, progress in the field is hindered by the lack of standardized\nbenchmarks with ground truth information and reliable validation metrics. Here,\nwe introduce CryoBench, a suite of datasets, metrics, and benchmarks for\nheterogeneous reconstruction in cryo-EM. CryoBench includes five datasets\nrepresenting different sources of heterogeneity and degrees of difficulty.\nThese include conformational heterogeneity generated from designed motions of\nantibody complexes or sampled from a molecular dynamics simulation, as well as\ncompositional heterogeneity from mixtures of ribosome assembly states or 100\ncommon complexes present in cells. We then analyze state-of-the-art\nheterogeneous reconstruction tools, including neural and non-neural methods,\nassess their sensitivity to noise, and propose new metrics for quantitative\nevaluation. We hope that CryoBench will be a foundational resource for\naccelerating algorithmic development and evaluation in the cryo-EM and machine\nlearning communities. Project page: https://cryobench.cs.princeton.edu.\n","authors":["Minkyu Jeon","Rishwanth Raghu","Miro Astore","Geoffrey Woollard","Ryan Feathers","Alkin Kaz","Sonya M. Hanson","Pilar Cossio","Ellen D. Zhong"],"pdf_url":"https://arxiv.org/pdf/2408.05526v2.pdf","comment":"Accepted by NeurIPS 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2501.09221v1","updated":"2025-01-16T00:45:05Z","published":"2025-01-16T00:45:05Z","title":"Leveraging Scale-aware Representations for improved\n  Concept-Representation Alignment in ViTs","summary":"  Vision Transformers (ViTs) are increasingly being adopted in various\nsensitive vision applications - like medical diagnosis, facial recognition,\netc. To improve the interpretability of such models, many approaches attempt to\nforward-align them with carefully annotated abstract, human-understandable\nsemantic entities - concepts. Concepts provide global rationales to the model\npredictions and can be quickly understood/intervened on by domain experts. Most\ncurrent research focuses on designing model-agnostic, plug-and-play generic\nconcept-based explainability modules that do not incorporate the inner workings\nof foundation models (e.g., inductive biases, scale invariance, etc.) during\ntraining. To alleviate this issue for ViTs, in this paper, we propose a novel\nConcept Representation Alignment Module (CRAM) which learns both scale and\nposition-aware representations from multi-scale feature pyramids and patch\nrepresentations respectively. CRAM further aligns these representations with\nconcept annotations through an attention matrix. The proposed CRAM module\nimproves the predictive performance of ViT architectures and also provides\naccurate and robust concept explanations as demonstrated on five datasets -\nincluding three widely used benchmarks (CUB, Pascal APY, Concept-MNIST) and 2\nreal-world datasets (AWA2, KITS).\n","authors":["Sanchit Sinha","Guangzhi Xiong","Aidong Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.09221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09217v1","updated":"2025-01-16T00:33:01Z","published":"2025-01-16T00:33:01Z","title":"Adaptive Law-Based Transformation (ALT): A Lightweight Feature\n  Representation for Time Series Classification","summary":"  Time series classification (TSC) is fundamental in numerous domains,\nincluding finance, healthcare, and environmental monitoring. However,\ntraditional TSC methods often struggle with the inherent complexity and\nvariability of time series data. Building on our previous work with the linear\nlaw-based transformation (LLT) - which improved classification accuracy by\ntransforming the feature space based on key data patterns - we introduce\nadaptive law-based transformation (ALT). ALT enhances LLT by incorporating\nvariable-length shifted time windows, enabling it to capture distinguishing\npatterns of various lengths and thereby handle complex time series more\neffectively. By mapping features into a linearly separable space, ALT provides\na fast, robust, and transparent solution that achieves state-of-the-art\nperformance with only a few hyperparameters.\n","authors":["Marcell T. Kurbucz","Balázs Hajós","Balázs P. Halmos","Vince Á. Molnár","Antal Jakovác"],"pdf_url":"https://arxiv.org/pdf/2501.09217v1.pdf","comment":"8 pages, 1 figure, 5 tables"},{"id":"http://arxiv.org/abs/2405.03762v3","updated":"2025-01-16T00:22:53Z","published":"2024-05-06T18:01:13Z","title":"Swin transformers are robust to distribution and concept drift in\n  endoscopy-based longitudinal rectal cancer assessment","summary":"  Endoscopic images are used at various stages of rectal cancer treatment\nstarting from cancer screening, diagnosis, during treatment to assess response\nand toxicity from treatments such as colitis, and at follow up to detect new\ntumor or local regrowth (LR). However, subjective assessment is highly variable\nand can underestimate the degree of response in some patients, subjecting them\nto unnecessary surgery, or overestimate response that places patients at risk\nof disease spread. Advances in deep learning has shown the ability to produce\nconsistent and objective response assessment for endoscopic images. However,\nmethods for detecting cancers, regrowth, and monitoring response during the\nentire course of patient treatment and follow-up are lacking. This is because,\nautomated diagnosis and rectal cancer response assessment requires methods that\nare robust to inherent imaging illumination variations and confounding\nconditions (blood, scope, blurring) present in endoscopy images as well as\nchanges to the normal lumen and tumor during treatment. Hence, a hierarchical\nshifted window (Swin) transformer was trained to distinguish rectal cancer from\nnormal lumen using endoscopy images. Swin as well as two convolutional\n(ResNet-50, WideResNet-50), and vision transformer (ViT) models were trained\nand evaluated on follow-up longitudinal images to detect LR on private dataset\nas well as on out-of-distribution (OOD) public colonoscopy datasets to detect\npre/non-cancerous polyps. Color shifts were applied using optimal transport to\nsimulate distribution shifts. Swin and ResNet models were similarly accurate in\nthe in-distribution dataset. Swin was more accurate than other methods\n(follow-up: 0.84, OOD: 0.83) even when subject to color shifts (follow-up:\n0.83, OOD: 0.87), indicating capability to provide robust performance for\nlongitudinal cancer assessment.\n","authors":["Jorge Tapias Gomez","Aneesh Rangnekar","Hannah Williams","Hannah Thompson","Julio Garcia-Aguilar","Joshua Jesse Smith","Harini Veeraraghavan"],"pdf_url":"https://arxiv.org/pdf/2405.03762v3.pdf","comment":"The work has been accepted for publication in 2024 SPIE Medical\n  Imaging conference proceedings"},{"id":"http://arxiv.org/abs/2501.09209v1","updated":"2025-01-16T00:03:04Z","published":"2025-01-16T00:03:04Z","title":"Surgical Visual Understanding (SurgVU) Dataset","summary":"  Owing to recent advances in machine learning and the ability to harvest large\namounts of data during robotic-assisted surgeries, surgical data science is\nripe for foundational work. We present a large dataset of surgical videos and\ntheir accompanying labels for this purpose. We describe how the data was\ncollected and some of its unique attributes. Multiple example problems are\noutlined. Although the dataset was curated for a particular set of scientific\nchallenges (in an accompanying paper), it is general enough to be used for a\nbroad range machine learning questions. Our hope is that this dataset exposes\nthe larger machine learning community to the challenging problems within\nsurgical data science, and becomes a touchstone for future research. The videos\nare available at\nhttps://storage.googleapis.com/isi-surgvu/surgvu24_videos_only.zip, the labels\nat https://storage.googleapis.com/isi-surgvu/surgvu24_labels_updated_v2.zip,\nand a validation set for tool detection problem at\nhttps://storage.googleapis.com/isi-surgvu/cat1_test_set_public.zip.\n","authors":["Aneeq Zia","Max Berniker","Rogerio Nespolo","Conor Perreault","Ziheng Wang","Benjamin Mueller","Ryan Schmidt","Kiran Bhattacharyya","Xi Liu","Anthony Jarc"],"pdf_url":"https://arxiv.org/pdf/2501.09209v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.04269v2","updated":"2025-01-16T18:59:53Z","published":"2024-03-19T23:27:15Z","title":"Algorithmic Collective Action in Recommender Systems: Promoting Songs by\n  Reordering Playlists","summary":"  We investigate algorithmic collective action in transformer-based recommender\nsystems. Our use case is a music streaming platform where a collective of fans\naims to promote the visibility of an underrepresented artist by strategically\nplacing one of their songs in the existing playlists they control. We introduce\ntwo easily implementable strategies to select the position at which to insert\nthe song with the goal to boost recommendations at test time. The strategies\nexploit statistical properties of the learner by targeting discontinuities in\nthe recommendations, and leveraging the long-tail nature of song distributions.\nWe evaluate the efficacy of our strategies using a publicly available\nrecommender system model released by a major music streaming platform. Our\nfindings reveal that through strategic placement even small collectives\n(controlling less than 0.01\\% of the training data) can achieve up to\n$40\\times$ more test time recommendations than an average song with the same\nnumber of training set occurrences. Focusing on the externalities of the\nstrategy, we find that the recommendations of other songs are largely\npreserved, and the newly gained recommendations are distributed across various\nartists. Together, our findings demonstrate how carefully designed collective\naction strategies can be effective while not necessarily being adversarial.\n","authors":["Joachim Baumann","Celestine Mendler-Dünner"],"pdf_url":"https://arxiv.org/pdf/2404.04269v2.pdf","comment":"Published at NeurIPS 2024, camera-ready updates"},{"id":"http://arxiv.org/abs/2501.09749v1","updated":"2025-01-16T18:57:20Z","published":"2025-01-16T18:57:20Z","title":"Enhancing Lexicon-Based Text Embeddings with Large Language Models","summary":"  Recent large language models (LLMs) have demonstrated exceptional performance\non general-purpose text embedding tasks. While dense embeddings have dominated\nrelated research, we introduce the first Lexicon-based EmbeddiNgS (LENS)\nleveraging LLMs that achieve competitive performance on these tasks. Regarding\nthe inherent tokenization redundancy issue and unidirectional attention\nlimitations in traditional causal LLMs, LENS consolidates the vocabulary space\nthrough token embedding clustering, and investigates bidirectional attention\nand various pooling strategies. Specifically, LENS simplifies lexicon matching\nby assigning each dimension to a specific token cluster, where semantically\nsimilar tokens are grouped together, and unlocking the full potential of LLMs\nthrough bidirectional attention. Extensive experiments demonstrate that LENS\noutperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB),\ndelivering compact feature representations that match the sizes of dense\ncounterparts. Notably, combining LENSE with dense embeddings achieves\nstate-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).\n","authors":["Yibin Lei","Tao Shen","Yu Cao","Andrew Yates"],"pdf_url":"https://arxiv.org/pdf/2501.09749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09243v2","updated":"2025-01-16T16:38:42Z","published":"2024-12-12T12:53:30Z","title":"SPRec: Leveraging Self-Play to Debias Preference Alignment for Large\n  Language Model-based Recommendations","summary":"  Large language models (LLMs) have attracted significant attention in\nrecommendation systems. Current LLM-based recommender systems primarily rely on\nsupervised fine-tuning (SFT) to train the model for recommendation tasks.\nHowever, relying solely on positive samples limits the model's ability to align\nwith user satisfaction and expectations. To address this, researchers have\nintroduced Direct Preference Optimization (DPO), which explicitly aligns\nrecommendations with user preferences using offline preference ranking data.\nDespite its advantages, our theoretical analysis reveals that DPO inherently\nbiases the model towards a few items, exacerbating the filter bubble issue and\nultimately degrading user experience. In this paper, we propose SPRec, a novel\nself-play recommendation framework designed to mitigate over-recommendation and\nimprove fairness without requiring additional data or manual intervention. In\neach self-play iteration, the model undergoes an SFT step followed by a DPO\nstep, treating offline interaction data as positive samples and the predicted\noutputs from the previous iteration as negative samples. This effectively\nre-weights the DPO loss function using the model's logits, adaptively\nsuppressing biased items. Extensive experiments on multiple real-world datasets\ndemonstrate SPRec's effectiveness in enhancing recommendation accuracy and\naddressing fairness concerns. The implementation is available via\nhttps://github.com/RegionCh/SPRec\n","authors":["Chongming Gao","Ruijun Chen","Shuai Yuan","Kexin Huang","Yuanqing Yu","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2412.09243v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09608v1","updated":"2025-01-16T15:32:41Z","published":"2025-01-16T15:32:41Z","title":"Metric Learning with Progressive Self-Distillation for Audio-Visual\n  Embedding Learning","summary":"  Metric learning projects samples into an embedded space, where similarities\nand dissimilarities are quantified based on their learned representations.\nHowever, existing methods often rely on label-guided representation learning,\nwhere representations of different modalities, such as audio and visual data,\nare aligned based on annotated labels. This approach tends to underutilize\nlatent complex features and potential relationships inherent in the\ndistributions of audio and visual data that are not directly tied to the\nlabels, resulting in suboptimal performance in audio-visual embedding learning.\nTo address this issue, we propose a novel architecture that integrates\ncross-modal triplet loss with progressive self-distillation. Our method\nenhances representation learning by leveraging inherent distributions and\ndynamically refining soft audio-visual alignments -- probabilistic alignments\nbetween audio and visual data that capture the inherent relationships beyond\nexplicit labels. Specifically, the model distills audio-visual\ndistribution-based knowledge from annotated labels in a subset of each batch.\nThis self-distilled knowledge is used t\n","authors":["Donghuo Zeng","Kazushi Ikeda"],"pdf_url":"https://arxiv.org/pdf/2501.09608v1.pdf","comment":"5 pages, 3 figures, 2 tables. Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.09493v1","updated":"2025-01-16T12:06:56Z","published":"2025-01-16T12:06:56Z","title":"Evaluating Conversational Recommender Systems with Large Language\n  Models: A User-Centric Evaluation Framework","summary":"  Conversational recommender systems (CRS) involve both recommendation and\ndialogue tasks, which makes their evaluation a unique challenge. Although past\nresearch has analyzed various factors that may affect user satisfaction with\nCRS interactions from the perspective of user studies, few evaluation metrics\nfor CRS have been proposed. Recent studies have shown that LLMs can align with\nhuman preferences, and several LLM-based text quality evaluation measures have\nbeen introduced. However, the application of LLMs in CRS evaluation remains\nrelatively limited. To address this research gap and advance the development of\nuser-centric conversational recommender systems, this study proposes an\nautomated LLM-based CRS evaluation framework, building upon existing research\nin human-computer interaction and psychology. The framework evaluates CRS from\nfour dimensions: dialogue behavior, language expression, recommendation items,\nand response content. We use this framework to evaluate four different\nconversational recommender systems.\n","authors":["Nuo Chen","Quanyu Dai","Xiaoyu Dong","Xiao-Ming Wu","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2501.09493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04997v2","updated":"2025-01-16T09:58:54Z","published":"2024-01-10T08:28:56Z","title":"Tapping the Potential of Large Language Models as Recommender Systems: A\n  Comprehensive Framework and Empirical Analysis","summary":"  Recently, Large Language Models~(LLMs) such as ChatGPT have showcased\nremarkable abilities in solving general tasks, demonstrating the potential for\napplications in recommender systems. To assess how effectively LLMs can be used\nin recommendation tasks, our study primarily focuses on employing LLMs as\nrecommender systems through prompting engineering. We propose a general\nframework for utilizing LLMs in recommendation tasks, focusing on the\ncapabilities of LLMs as recommenders. To conduct our analysis, we formalize the\ninput of LLMs for recommendation into natural language prompts with two key\naspects, and explain how our framework can be generalized to various\nrecommendation scenarios. As for the use of LLMs as recommenders, we analyze\nthe impact of public availability, tuning strategies, model architecture,\nparameter scale, and context length on recommendation results based on the\nclassification of LLMs. As for prompt engineering, we further analyze the\nimpact of four important components of prompts, \\ie task descriptions, user\ninterest modeling, candidate items construction and prompting strategies. In\neach section, we first define and categorize concepts in line with the existing\nliterature. Then, we propose inspiring research questions followed by detailed\nexperiments on two public datasets, in order to systematically analyze the\nimpact of different factors on performance. Based on our empirical analysis, we\nfinally summarize promising directions to shed lights on future research.\n","authors":["Lanling Xu","Junjie Zhang","Bingqian Li","Jinpeng Wang","Sheng Chen","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2401.04997v2.pdf","comment":"52 pages, under review"},{"id":"http://arxiv.org/abs/2501.04635v2","updated":"2025-01-16T09:30:38Z","published":"2025-01-08T17:29:46Z","title":"Knowledge Retrieval Based on Generative AI","summary":"  This study develops a question-answering system based on Retrieval-Augmented\nGeneration (RAG) using Chinese Wikipedia and Lawbank as retrieval sources.\nUsing TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for\ndense vector retrieval to obtain highly relevant search results and\nBGE-reranker to reorder these results based on query relevance. The most\npertinent retrieval outcomes serve as reference knowledge for a Large Language\nModel (LLM), enhancing its ability to answer questions and establishing a\nknowledge retrieval system grounded in generative AI. The system's\neffectiveness is assessed through a two-stage evaluation: automatic and\nassisted performance evaluations. The automatic evaluation calculates accuracy\nby comparing the model's auto-generated labels with ground truth answers,\nmeasuring performance under standardized conditions without human intervention.\nThe assisted performance evaluation involves 20 finance-related multiple-choice\nquestions answered by 20 participants without financial backgrounds. Initially,\nparticipants answer independently. Later, they receive system-generated\nreference information to assist in answering, examining whether the system\nimproves accuracy when assistance is provided. The main contributions of this\nresearch are: (1) Enhanced LLM Capability: By integrating BGE-M3 and\nBGE-reranker, the system retrieves and reorders highly relevant results,\nreduces hallucinations, and dynamically accesses authorized or public knowledge\nsources. (2) Improved Data Privacy: A customized RAG architecture enables local\noperation of the LLM, eliminating the need to send private data to external\nservers. This approach enhances data security, reduces reliance on commercial\nservices, lowers operational costs, and mitigates privacy risks.\n","authors":["Te-Lun Yang","Jyi-Shane Liu","Yuen-Hsien Tseng","Jyh-Shing Roger Jang"],"pdf_url":"https://arxiv.org/pdf/2501.04635v2.pdf","comment":"8 pages, 13 figures, 1 table"},{"id":"http://arxiv.org/abs/2501.09384v1","updated":"2025-01-16T08:52:50Z","published":"2025-01-16T08:52:50Z","title":"Evaluating LLM Abilities to Understand Tabular Electronic Health\n  Records: A Comprehensive Study of Patient Data Extraction and Retrieval","summary":"  Electronic Health Record (EHR) tables pose unique challenges among which is\nthe presence of hidden contextual dependencies between medical features with a\nhigh level of data dimensionality and sparsity. This study presents the first\ninvestigation into the abilities of LLMs to comprehend EHRs for patient data\nextraction and retrieval. We conduct extensive experiments using the MIMICSQL\ndataset to explore the impact of the prompt structure, instruction, context,\nand demonstration, of two backbone LLMs, Llama2 and Meditron, based on task\nperformance. Through quantitative and qualitative analyses, our findings show\nthat optimal feature selection and serialization methods can enhance task\nperformance by up to 26.79% compared to naive approaches. Similarly, in-context\nlearning setups with relevant example selection improve data extraction\nperformance by 5.95%. Based on our study findings, we propose guidelines that\nwe believe would help the design of LLM-based models to support health search.\n","authors":["Jesus Lovon","Martin Mouysset","Jo Oleiwan","Jose G. Moreno","Christine Damase-Michel","Lynda Tamine"],"pdf_url":"https://arxiv.org/pdf/2501.09384v1.pdf","comment":"To be published as full paper in the Proceedings of the European\n  Conference on Information Retrieval (ECIR) 2025. Preprint"},{"id":"http://arxiv.org/abs/2405.13238v3","updated":"2025-01-16T08:49:06Z","published":"2024-05-21T22:53:00Z","title":"Enhancing User Interest based on Stream Clustering and Memory Networks\n  in Large-Scale Recommender Systems","summary":"  Recommender Systems (RSs) provide personalized recommendation service based\non user interest, which are widely used in various platforms. However, there\nare lots of users with sparse interest due to lacking consumption behaviors,\nwhich leads to poor recommendation results for them. This problem is widespread\nin large-scale RSs and is particularly difficult to address. To solve this\nproblem, we propose a novel solution named User Interest Enhancement (UIE)\nwhich enhances user interest including user profile and user history behavior\nsequences using the enhancement vectors and personalized enhancement vector\ngenerated based on stream clustering and memory networks from different\nperspectives. UIE not only remarkably improves model performance on the users\nwith sparse interest but also significantly enhance model performance on other\nusers. UIE is an end-to-end solution which is easy to be implemented based on\nranking model. Moreover, we expand our solution and apply similar methods to\nlong-tail items, which also achieves excellent improvement. Furthermore, we\nconduct extensive offline and online experiments in a large-scale industrial\nRS. The results demonstrate that our model outperforms other models remarkably,\nespecially for the users with sparse interest. Until now, UIE has been fully\ndeployed in multiple large-scale RSs and achieved remarkable improvements.\n","authors":["Peng Liu","Nian Wang","Cong Xu","Ming Zhao","Bin Wang","Yi Ren"],"pdf_url":"https://arxiv.org/pdf/2405.13238v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09359v1","updated":"2025-01-16T08:15:21Z","published":"2025-01-16T08:15:21Z","title":"A Multi-tiered Solution for Personalized Baggage Item Recommendations\n  using FastText and Association Rule Mining","summary":"  This paper introduces an intelligent baggage item recommendation system to\noptimize packing for air travelers by providing tailored suggestions based on\nspecific travel needs and destinations. Using FastText word embeddings and\nAssociation Rule Mining (ARM), the system ensures efficient luggage space\nutilization, compliance with weight limits, and an enhanced travel experience.\nThe methodology comprises four phases: (1) data collection and preprocessing\nwith pre-trained FastText embeddings for text representation and similarity\nscoring (2) a content-based recommendation system enriched by user search\nhistory (3) application of ARM to user interactions to uncover meaningful item\nassociations and (4) integration of FastText and ARM for accurate, personalized\nrecommendations. Performance is evaluated using metrics such as coverage,\nsupport, confidence, lift, leverage, and conviction. Results demonstrate the\nsystem's effectiveness in providing relevant suggestions, improving customer\nsatisfaction, and simplifying the packing process. These insights advance\npersonalized recommendations, targeted marketing, and product optimization in\nair travel and beyond.\n","authors":["Mudavath Ravi","Atul Negi"],"pdf_url":"https://arxiv.org/pdf/2501.09359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09354v1","updated":"2025-01-16T08:05:39Z","published":"2025-01-16T08:05:39Z","title":"Style4Rec: Enhancing Transformer-based E-commerce Recommendation Systems\n  with Style and Shopping Cart Information","summary":"  Understanding users' product preferences is essential to the efficacy of a\nrecommendation system. Precision marketing leverages users' historical data to\ndiscern these preferences and recommends products that align with them.\nHowever, recent browsing and purchase records might better reflect current\npurchasing inclinations. Transformer-based recommendation systems have made\nstrides in sequential recommendation tasks, but they often fall short in\nutilizing product image style information and shopping cart data effectively.\nIn light of this, we propose Style4Rec, a transformer-based e-commerce\nrecommendation system that harnesses style and shopping cart information to\nenhance existing transformer-based sequential product recommendation systems.\nStyle4Rec represents a significant step forward in personalized e-commerce\nrecommendations, outperforming benchmarks across various evaluation metrics.\nStyle4Rec resulted in notable improvements: HR@5 increased from 0.681 to 0.735,\nNDCG@5 increased from 0.594 to 0.674, and MRR@5 increased from 0.559 to 0.654.\nWe tested our model using an e-commerce dataset from our partnering company and\nfound that it exceeded established transformer-based sequential recommendation\nbenchmarks across various evaluation metrics. Thus, Style4Rec presents a\nsignificant step forward in personalized e-commerce recommendation systems.\n","authors":["Berke Ugurlu","Ming-Yi Hong","Che Lin"],"pdf_url":"https://arxiv.org/pdf/2501.09354v1.pdf","comment":"9 pages, 6 images, 4 tables"},{"id":"http://arxiv.org/abs/2409.01192v2","updated":"2025-01-16T08:04:43Z","published":"2024-09-02T11:58:56Z","title":"SSD4Rec: A Structured State Space Duality Model for Efficient Sequential\n  Recommendation","summary":"  Sequential recommendation methods are crucial in modern recommender systems\nfor their remarkable capability to understand a user's changing interests based\non past interactions. However, a significant challenge faced by current methods\n(e.g., RNN- or Transformer-based models) is to effectively and efficiently\ncapture users' preferences by modeling long behavior sequences, which impedes\ntheir various applications like short video platforms where user interactions\nare numerous. Recently, an emerging architecture named Mamba, built on state\nspace models (SSM) with efficient hardware-aware designs, has showcased the\ntremendous potential for sequence modeling, presenting a compelling avenue for\naddressing the challenge effectively. Inspired by this, we propose a novel\ngeneric and efficient sequential recommendation backbone, SSD4Rec, which\nexplores the seamless adaptation of Mamba for sequential recommendations.\nSpecifically, SSD4Rec marks the variable- and long-length item sequences with\nsequence registers and processes the item representations with bidirectional\nStructured State Space Duality (SSD) blocks. This not only allows for\nhardware-aware matrix multiplication but also empowers outstanding capabilities\nin variable-length and long-range sequence modeling. Extensive evaluations on\nfour benchmark datasets demonstrate that the proposed model achieves\nstate-of-the-art performance while maintaining near-linear scalability with\nuser sequence length. Our code is publicly available at\nhttps://github.com/ZhangYifeng1995/SSD4Rec.\n","authors":["Haohao Qu","Yifeng Zhang","Liangbo Ning","Wenqi Fan","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2409.01192v2.pdf","comment":"Significant revisions have been implemented in our paper,\n  particularly focusing on both the methodology and experimental sections"},{"id":"http://arxiv.org/abs/2501.09292v1","updated":"2025-01-16T04:56:33Z","published":"2025-01-16T04:56:33Z","title":"To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic\n  Retrieval Augmented Generation","summary":"  Retrieval-Augmented Generation equips large language models with the\ncapability to retrieve external knowledge, thereby mitigating hallucinations by\nincorporating information beyond the model's intrinsic abilities. However, most\nprior works have focused on invoking retrieval deterministically, which makes\nit unsuitable for tasks such as long-form question answering. Instead,\ndynamically performing retrieval by invoking it only when the underlying LLM\nlacks the required knowledge can be more efficient. In this context, we delve\ndeeper into the question, \"To Retrieve or Not to Retrieve?\" by exploring\nmultiple uncertainty detection methods. We evaluate these methods for the task\nof long-form question answering, employing dynamic retrieval, and present our\ncomparisons. Our findings suggest that uncertainty detection metrics, such as\nDegree Matrix Jaccard and Eccentricity, can reduce the number of retrieval\ncalls by almost half, with only a slight reduction in question-answering\naccuracy.\n","authors":["Kaustubh D. Dhole"],"pdf_url":"https://arxiv.org/pdf/2501.09292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17374v2","updated":"2025-01-16T04:40:18Z","published":"2024-12-23T08:15:34Z","title":"Scenario-Wise Rec: A Multi-Scenario Recommendation Benchmark","summary":"  Multi Scenario Recommendation (MSR) tasks, referring to building a unified\nmodel to enhance performance across all recommendation scenarios, have recently\ngained much attention. However, current research in MSR faces two significant\nchallenges that hinder the field's development: the absence of uniform\nprocedures for multi-scenario dataset processing, thus hindering fair\ncomparisons, and most models being closed-sourced, which complicates\ncomparisons with current SOTA models. Consequently, we introduce our benchmark,\n\\textbf{Scenario-Wise Rec}, which comprises 6 public datasets and 12 benchmark\nmodels, along with a training and evaluation pipeline. Additionally, we\nvalidated the benchmark using an industrial advertising dataset, reinforcing\nits reliability and applicability in real-world scenarios. We aim for this\nbenchmark to offer researchers valuable insights from prior work, enabling the\ndevelopment of novel models based on our benchmark and thereby fostering a\ncollaborative research ecosystem in MSR. Our source code is also publicly\navailable.\n","authors":["Xiaopeng Li","Jingtong Gao","Pengyue Jia","Xiangyu Zhao","Yichao Wang","Wanyu Wang","Yejing Wang","Yuhao Wang","Xiangyu Zhao","Huifeng Guo","Ruiming Tang"],"pdf_url":"https://arxiv.org/pdf/2412.17374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19692v4","updated":"2025-01-16T02:17:21Z","published":"2024-07-29T04:30:38Z","title":"Fusion Self-supervised Learning for Recommendation","summary":"  Recommender systems are widely deployed in various web environments, and\nself-supervised learning (SSL) has recently attracted significant attention in\nthis field. Contrastive learning (CL) stands out as a major SSL paradigm due to\nits robust ability to generate self-supervised signals. Mainstream graph\ncontrastive learning (GCL)-based methods typically implement CL by creating\ncontrastive views through various data augmentation techniques. Despite these\nmethods are effective, we argue that there still exist several challenges. i)\nData augmentation ($e.g.,$ discarding edges or adding noise) necessitates\nadditional graph convolution (GCN) or modeling operations, which are highly\ntime-consuming and potentially harm the embedding quality. ii) Existing\nCL-based methods use traditional CL objectives to capture self-supervised\nsignals. However, few studies have explored obtaining CL objectives from more\nperspectives and have attempted to fuse the varying signals from these CL\nobjectives to enhance recommendation performance.\n  To overcome these challenges, we propose a Fusion Self-supervised Learning\nframework for recommendation. Specifically, instead of facilitating data\naugmentations, we use high-order information from GCN process to create\ncontrastive views. Additionally, to integrate self-supervised signals from\nvarious CL objectives, we propose an advanced CL objective. By ensuring that\npositive pairs are distanced from negative samples derived from both\ncontrastive views, we effectively fuse self-supervised signals from distinct CL\nobjectives, thereby enhancing the mutual information between positive pairs.\nExperimental results on three public datasets demonstrate the superior\nrecommendation performance and efficiency of HFGCL compared to the\nstate-of-the-art baselines.\n","authors":["Yu Zhang","Lei Sang","Yi Zhang","Yiwen Zhang","Yun Yang"],"pdf_url":"https://arxiv.org/pdf/2407.19692v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09211v1","updated":"2025-01-16T00:06:33Z","published":"2025-01-16T00:06:33Z","title":"Fuzzy Integration of Data Lake Tables","summary":"  Data integration is an important step in any data science pipeline where the\nobjective is to unify the information available in different datasets for\ncomprehensive analysis. Full Disjunction, which is an associative extension of\nthe outer join operator, has been shown to be an effective operator for\nintegrating datasets. It fully preserves and combines the available\ninformation. Existing Full Disjunction algorithms only consider the equi-join\nscenario where only tuples having the same value on joining columns are\nintegrated. This, however, does not realistically represent an open data\nscenario, where datasets come from diverse sources with inconsistent values\n(e.g., synonyms, abbreviations, etc.) and with limited metadata. So, joining\njust on equal values severely limits the ability of Full Disjunction to fully\ncombine datasets. Thus, in this work, we propose an extension of Full\nDisjunction to also account for \"fuzzy\" matches among tuples. We present a\nnovel data-driven approach to enable the joining of approximate or fuzzy\nmatches within Full Disjunction. Experimentally, we show that fuzzy Full\nDisjunction does not add significant time overhead over a state-of-the-art Full\nDisjunction implementation and also that it enhances the integration\neffectiveness.\n","authors":["Aamod Khatiwada","Roee Shraga","Renée J. Miller"],"pdf_url":"https://arxiv.org/pdf/2501.09211v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2404.04269v2","updated":"2025-01-16T18:59:53Z","published":"2024-03-19T23:27:15Z","title":"Algorithmic Collective Action in Recommender Systems: Promoting Songs by\n  Reordering Playlists","summary":"  We investigate algorithmic collective action in transformer-based recommender\nsystems. Our use case is a music streaming platform where a collective of fans\naims to promote the visibility of an underrepresented artist by strategically\nplacing one of their songs in the existing playlists they control. We introduce\ntwo easily implementable strategies to select the position at which to insert\nthe song with the goal to boost recommendations at test time. The strategies\nexploit statistical properties of the learner by targeting discontinuities in\nthe recommendations, and leveraging the long-tail nature of song distributions.\nWe evaluate the efficacy of our strategies using a publicly available\nrecommender system model released by a major music streaming platform. Our\nfindings reveal that through strategic placement even small collectives\n(controlling less than 0.01\\% of the training data) can achieve up to\n$40\\times$ more test time recommendations than an average song with the same\nnumber of training set occurrences. Focusing on the externalities of the\nstrategy, we find that the recommendations of other songs are largely\npreserved, and the newly gained recommendations are distributed across various\nartists. Together, our findings demonstrate how carefully designed collective\naction strategies can be effective while not necessarily being adversarial.\n","authors":["Joachim Baumann","Celestine Mendler-Dünner"],"pdf_url":"https://arxiv.org/pdf/2404.04269v2.pdf","comment":"Published at NeurIPS 2024, camera-ready updates"},{"id":"http://arxiv.org/abs/2501.09753v1","updated":"2025-01-16T18:59:02Z","published":"2025-01-16T18:59:02Z","title":"SRE-Conv: Symmetric Rotation Equivariant Convolution for Biomedical\n  Image Classification","summary":"  Convolutional neural networks (CNNs) are essential tools for computer vision\ntasks, but they lack traditionally desired properties of extracted features\nthat could further improve model performance, e.g., rotational equivariance.\nSuch properties are ubiquitous in biomedical images, which often lack explicit\norientation. While current work largely relies on data augmentation or explicit\nmodules to capture orientation information, this comes at the expense of\nincreased training costs or ineffective approximations of the desired\nequivariance. To overcome these challenges, we propose a novel and efficient\nimplementation of the Symmetric Rotation-Equivariant (SRE) Convolution\n(SRE-Conv) kernel, designed to learn rotation-invariant features while\nsimultaneously compressing the model size. The SRE-Conv kernel can easily be\nincorporated into any CNN backbone. We validate the ability of a deep SRE-CNN\nto capture equivariance to rotation using the public MedMNISTv2 dataset (16\ntotal tasks). SRE-Conv-CNN demonstrated improved rotated image classification\nperformance accuracy on all 16 test datasets in both 2D and 3D images, all\nwhile increasing efficiency with fewer parameters and reduced memory footprint.\nThe code is available at https://github.com/XYPB/SRE-Conv.\n","authors":["Yuexi Du","Jiazhen Zhang","Tal Zeevi","Nicha C. Dvornek","John A. Onofrey"],"pdf_url":"https://arxiv.org/pdf/2501.09753v1.pdf","comment":"Accepted by IEEE ISBI 2025 4-page paper"},{"id":"http://arxiv.org/abs/2501.09747v1","updated":"2025-01-16T18:57:04Z","published":"2025-01-16T18:57:04Z","title":"FAST: Efficient Action Tokenization for Vision-Language-Action Models","summary":"  Autoregressive sequence models, such as Transformer-based vision-language\naction (VLA) policies, can be tremendously effective for capturing complex and\ngeneralizable robotic behaviors. However, such models require us to choose a\ntokenization of our continuous action signals, which determines how the\ndiscrete symbols predicted by the model map to continuous robot actions. We\nfind that current approaches for robot action tokenization, based on simple\nper-dimension, per-timestep binning schemes, typically perform poorly when\nlearning dexterous skills from high-frequency robot data. To address this\nchallenge, we propose a new compression-based tokenization scheme for robot\nactions, based on the discrete cosine transform. Our tokenization approach,\nFrequency-space Action Sequence Tokenization (FAST), enables us to train\nautoregressive VLAs for highly dexterous and high-frequency tasks where\nstandard discretization methods fail completely. Based on FAST, we release\nFAST+, a universal robot action tokenizer, trained on 1M real robot action\ntrajectories. It can be used as a black-box tokenizer for a wide range of robot\naction sequences, with diverse action spaces and control frequencies. Finally,\nwe show that, when combined with the pi0 VLA, our method can scale to training\non 10k hours of robot data and match the performance of diffusion VLAs, while\nreducing training time by up to 5x.\n","authors":["Karl Pertsch","Kyle Stachowicz","Brian Ichter","Danny Driess","Suraj Nair","Quan Vuong","Oier Mees","Chelsea Finn","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2501.09747v1.pdf","comment":"Website: https://www.pi.website/research/fast"},{"id":"http://arxiv.org/abs/2501.09745v1","updated":"2025-01-16T18:55:38Z","published":"2025-01-16T18:55:38Z","title":"Suggesting Code Edits in Interactive Machine Learning Notebooks Using\n  Large Language Models","summary":"  Machine learning developers frequently use interactive computational\nnotebooks, such as Jupyter notebooks, to host code for data processing and\nmodel training. Jupyter notebooks provide a convenient tool for writing machine\nlearning pipelines and interactively observing outputs, however, maintaining\nJupyter notebooks, e.g., to add new features or fix bugs, can be challenging\ndue to the length and complexity of the notebooks. Moreover, there is no\nexisting benchmark related to developer edits on Jupyter notebooks. To address\nthis, we present the first dataset of 48,398 Jupyter notebook edits derived\nfrom 20,095 revisions of 792 machine learning repositories on GitHub, and\nperform the first study of the using LLMs to predict code edits in Jupyter\nnotebooks. Our dataset captures granular details of cell-level and line-level\nmodifications, offering a foundation for understanding real-world maintenance\npatterns in machine learning workflows. We observed that the edits on Jupyter\nnotebooks are highly localized, with changes averaging only 166 lines of code\nin repositories. While larger models outperform smaller counterparts in code\nediting, all models have low accuracy on our dataset even after finetuning,\ndemonstrating the complexity of real-world machine learning maintenance tasks.\nOur findings emphasize the critical role of contextual information in improving\nmodel performance and point toward promising avenues for advancing large\nlanguage models' capabilities in engineering machine learning code.\n","authors":["Bihui Jin","Jiayue Wang","Pengyu Nie"],"pdf_url":"https://arxiv.org/pdf/2501.09745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04845v2","updated":"2025-01-16T18:48:36Z","published":"2024-12-06T08:30:01Z","title":"Using Machine Learning to Discover Parsimonious and\n  Physically-Interpretable Representations of Catchment-Scale Rainfall-Runoff\n  Dynamics","summary":"  Despite the excellent real-world predictive performance of modern machine\nlearning (ML) methods, many scientists remain hesitant to discard traditional\nphysical-conceptual (PC) approaches due mainly to their relative\ninterpretability, which contributes to credibility during decision-making. In\nthis context, a currently underexplored aspect of ML is how to develop\nminimally-optimal representations that can facilitate better insight regarding\nsystem functioning. Regardless of how this is achieved, it is arguably true\nthat parsimonious representations better support the advancement of scientific\nunderstanding. Our own view is that ML-based modeling of geoscientific systems\nshould be based in the use of computational units that are fundamentally\ninterpretable by design.\n  This paper continues our exploration of how the strengths of ML can be\nexploited in the service of better understanding via scientific investigation.\nHere, we use the Mass Conserving Perceptron (MCP) as the fundamental\ncomputational unit in a generic network architecture consisting of nodes\narranged in series and parallel to explore several generic and important issues\nrelated to the use of observational data for constructing input-state-output\nmodels of dynamical systems. In the context of lumped catchment modeling, we\nshow that physical interpretability and excellent predictive performance can\nboth be achieved using a relatively parsimonious distributed-state\nmultiple-flow-path network with context-dependent gating and information\nsharing across the nodes, suggesting that MCP-based modeling can play a\nsignificant role in application of ML to geoscientific investigation.\n","authors":["Yuan-Heng Wang","Hoshin V. Gupta"],"pdf_url":"https://arxiv.org/pdf/2412.04845v2.pdf","comment":"74 Pages, 4 Tables, 13 Figures, 11 Tables and 11 Figures in\n  Supplementary Materials"},{"id":"http://arxiv.org/abs/2501.09734v1","updated":"2025-01-16T18:37:59Z","published":"2025-01-16T18:37:59Z","title":"Random Subspace Cubic-Regularization Methods, with Applications to\n  Low-Rank Functions","summary":"  We propose and analyze random subspace variants of the second-order Adaptive\nRegularization using Cubics (ARC) algorithm. These methods iteratively restrict\nthe search space to some random subspace of the parameters, constructing and\nminimizing a local model only within this subspace. Thus, our variants only\nrequire access to (small-dimensional) projections of first- and second-order\nproblem derivatives and calculate a reduced step inexpensively. Under suitable\nassumptions, the ensuing methods maintain the optimal first-order, and\nsecond-order, global rates of convergence of (full-dimensional) cubic\nregularization, while showing improved scalability both theoretically and\nnumerically, particularly when applied to low-rank functions. When applied to\nthe latter, our adaptive variant naturally adapts the subspace size to the true\nrank of the function, without knowing it a priori.\n","authors":["Coralia Cartis","Zhen Shao","Edward Tansley"],"pdf_url":"https://arxiv.org/pdf/2501.09734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09731v1","updated":"2025-01-16T18:30:33Z","published":"2025-01-16T18:30:33Z","title":"Predictions as Surrogates: Revisiting Surrogate Outcomes in the Age of\n  AI","summary":"  We establish a formal connection between the decades-old surrogate outcome\nmodel in biostatistics and economics and the emerging field of\nprediction-powered inference (PPI). The connection treats predictions from\npre-trained models, prevalent in the age of AI, as cost-effective surrogates\nfor expensive outcomes. Building on the surrogate outcomes literature, we\ndevelop recalibrated prediction-powered inference, a more efficient approach to\nstatistical inference than existing PPI proposals. Our method departs from the\nexisting proposals by using flexible machine learning techniques to learn the\noptimal ``imputed loss'' through a step we call recalibration. Importantly, the\nmethod always improves upon the estimator that relies solely on the data with\navailable true outcomes, even when the optimal imputed loss is estimated\nimperfectly, and it achieves the smallest asymptotic variance among PPI\nestimators if the estimate is consistent. Computationally, our optimization\nobjective is convex whenever the loss function that defines the target\nparameter is convex. We further analyze the benefits of recalibration, both\ntheoretically and numerically, in several common scenarios where machine\nlearning predictions systematically deviate from the outcome of interest. We\ndemonstrate significant gains in effective sample size over existing PPI\nproposals via three applications leveraging state-of-the-art machine\nlearning/AI models.\n","authors":["Wenlong Ji","Lihua Lei","Tijana Zrnic"],"pdf_url":"https://arxiv.org/pdf/2501.09731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09729v1","updated":"2025-01-16T18:25:50Z","published":"2025-01-16T18:25:50Z","title":"Generating particle physics Lagrangians with transformers","summary":"  In physics, Lagrangians provide a systematic way to describe laws governing\nphysical systems. In the context of particle physics, they encode the\ninteractions and behavior of the fundamental building blocks of our universe.\nBy treating Lagrangians as complex, rule-based constructs similar to linguistic\nexpressions, we trained a transformer model -- proven to be effective in\nnatural language tasks -- to predict the Lagrangian corresponding to a given\nlist of particles. We report on the transformer's performance in constructing\nLagrangians respecting the Standard Model $\\mathrm{SU}(3)\\times\n\\mathrm{SU}(2)\\times \\mathrm{U}(1)$ gauge symmetries. The resulting model is\nshown to achieve high accuracies (over 90\\%) with Lagrangians up to six matter\nfields, with the capacity to generalize beyond the training distribution,\nalbeit within architectural constraints. We show through an analysis of input\nembeddings that the model has internalized concepts such as group\nrepresentations and conjugation operations as it learned to generate\nLagrangians. We make the model and training datasets available to the\ncommunity. An interactive demonstration can be found at:\n\\url{https://huggingface.co/spaces/JoseEliel/generate-lagrangians}.\n","authors":["Yong Sheng Koay","Rikard Enberg","Stefano Moretti","Eliel Camargo-Molina"],"pdf_url":"https://arxiv.org/pdf/2501.09729v1.pdf","comment":"32 pages, 11 figues, 18 tables"},{"id":"http://arxiv.org/abs/2501.09722v1","updated":"2025-01-16T18:10:37Z","published":"2025-01-16T18:10:37Z","title":"Attention based Bidirectional GRU hybrid model for inappropriate content\n  detection in Urdu language","summary":"  With the increased use of the internet and social networks for online\ndiscussions, the spread of toxic and inappropriate content on social networking\nsites has also increased. Several studies have been conducted in different\nlanguages. However, there is less work done for South Asian languages for\ninappropriate content identification using deep learning techniques. In Urdu\nlanguage, the spellings are not unique, and people write different common\nspellings for the same word, while mixing it other languages, like English in\nthe text makes it more challenging, and limited research work is available to\nprocess such language with the finest algorithms. The use of attention layer\nwith a deep learning model can help handling the long-term dependencies and\nincrease its efficiency . To explore the effects of the attention layer, this\nstudy proposes attention-based Bidirectional GRU hybrid model for identifying\ninappropriate content in Urdu Unicode text language. Four different baseline\ndeep learning models; LSTM, Bi-LSTM, GRU, and TCN, are used to compare the\nperformance of the proposed model. The results of these models were compared\nbased on evaluation metrics, dataset size, and impact of the word embedding\nlayer. The pre-trained Urdu word2Vec embeddings were utilized for our case. Our\nproposed model BiGRU-A outperformed all other baseline models by yielding 84\\%\naccuracy without using pre-trained word2Vec layer. From our experiments, we\nhave established that the attention layer improves the model's efficiency, and\npre-trained word2Vec embedding does not work well with an inappropriate content\ndataset.\n","authors":["Ezzah Shoukat","Rabia Irfan","Iqra Basharat","Muhammad Ali Tahir","Sameen Shaukat"],"pdf_url":"https://arxiv.org/pdf/2501.09722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09705v1","updated":"2025-01-16T17:57:53Z","published":"2025-01-16T17:57:53Z","title":"Practical Continual Forgetting for Pre-trained Vision Models","summary":"  For privacy and security concerns, the need to erase unwanted information\nfrom pre-trained vision models is becoming evident nowadays. In real-world\nscenarios, erasure requests originate at any time from both users and model\nowners, and these requests usually form a sequence. Therefore, under such a\nsetting, selective information is expected to be continuously removed from a\npre-trained model while maintaining the rest. We define this problem as\ncontinual forgetting and identify three key challenges. (i) For unwanted\nknowledge, efficient and effective deleting is crucial. (ii) For remaining\nknowledge, the impact brought by the forgetting procedure should be minimal.\n(iii) In real-world scenarios, the training samples may be scarce or partially\nmissing during the process of forgetting. To address them, we first propose\nGroup Sparse LoRA (GS-LoRA). Specifically, towards (i), we introduce LoRA\nmodules to fine-tune the FFN layers in Transformer blocks for each forgetting\ntask independently, and towards (ii), a simple group sparse regularization is\nadopted, enabling automatic selection of specific LoRA groups and zeroing out\nthe others. To further extend GS-LoRA to more practical scenarios, we\nincorporate prototype information as additional supervision and introduce a\nmore practical approach, GS-LoRA++. For each forgotten class, we move the\nlogits away from its original prototype. For the remaining classes, we pull the\nlogits closer to their respective prototypes. We conduct extensive experiments\non face recognition, object detection and image classification and demonstrate\nthat our method manages to forget specific classes with minimal impact on other\nclasses. Codes have been released on https://github.com/bjzhb666/GS-LoRA.\n","authors":["Hongbo Zhao","Fei Zhu","Bolin Ni","Feng Zhu","Gaofeng Meng","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.09705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09700v1","updated":"2025-01-16T17:54:56Z","published":"2025-01-16T17:54:56Z","title":"Cueless EEG imagined speech for subject identification: dataset and\n  benchmarks","summary":"  Electroencephalogram (EEG) signals have emerged as a promising modality for\nbiometric identification. While previous studies have explored the use of\nimagined speech with semantically meaningful words for subject identification,\nmost have relied on additional visual or auditory cues. In this study, we\nintroduce a cueless EEG-based imagined speech paradigm, where subjects imagine\nthe pronunciation of semantically meaningful words without any external cues.\nThis innovative approach addresses the limitations of prior methods by\nrequiring subjects to select and imagine words from a predefined list\nnaturally. The dataset comprises over 4,350 trials from 11 subjects across five\nsessions. We assess a variety of classification methods, including traditional\nmachine learning techniques such as Support Vector Machines (SVM) and XGBoost,\nas well as time-series foundation models and deep learning architectures\nspecifically designed for EEG classification, such as EEG Conformer and Shallow\nConvNet. A session-based hold-out validation strategy was employed to ensure\nreliable evaluation and prevent data leakage. Our results demonstrate\noutstanding classification accuracy, reaching 97.93%. These findings highlight\nthe potential of cueless EEG paradigms for secure and reliable subject\nidentification in real-world applications, such as brain-computer interfaces\n(BCIs).\n","authors":["Ali Derakhshesh","Zahra Dehghanian","Reza Ebrahimpour","Hamid R. Rabiee"],"pdf_url":"https://arxiv.org/pdf/2501.09700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09691v1","updated":"2025-01-16T17:44:18Z","published":"2025-01-16T17:44:18Z","title":"A Near-optimal Algorithm for Learning Margin Halfspaces with Massart\n  Noise","summary":"  We study the problem of PAC learning $\\gamma$-margin halfspaces in the\npresence of Massart noise. Without computational considerations, the sample\ncomplexity of this learning problem is known to be\n$\\widetilde{\\Theta}(1/(\\gamma^2 \\epsilon))$. Prior computationally efficient\nalgorithms for the problem incur sample complexity $\\tilde{O}(1/(\\gamma^4\n\\epsilon^3))$ and achieve 0-1 error of $\\eta+\\epsilon$, where $\\eta<1/2$ is the\nupper bound on the noise rate. Recent work gave evidence of an\ninformation-computation tradeoff, suggesting that a quadratic dependence on\n$1/\\epsilon$ is required for computationally efficient algorithms. Our main\nresult is a computationally efficient learner with sample complexity\n$\\widetilde{\\Theta}(1/(\\gamma^2 \\epsilon^2))$, nearly matching this lower\nbound. In addition, our algorithm is simple and practical, relying on online\nSGD on a carefully selected sequence of convex losses.\n","authors":["Ilias Diakonikolas","Nikos Zarifis"],"pdf_url":"https://arxiv.org/pdf/2501.09691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09687v1","updated":"2025-01-16T17:39:25Z","published":"2025-01-16T17:39:25Z","title":"U-Fair: Uncertainty-based Multimodal Multitask Learning for Fairer\n  Depression Detection","summary":"  Machine learning bias in mental health is becoming an increasingly pertinent\nchallenge. Despite promising efforts indicating that multitask approaches often\nwork better than unitask approaches, there is minimal work investigating the\nimpact of multitask learning on performance and fairness in depression\ndetection nor leveraged it to achieve fairer prediction outcomes. In this work,\nwe undertake a systematic investigation of using a multitask approach to\nimprove performance and fairness for depression detection. We propose a novel\ngender-based task-reweighting method using uncertainty grounded in how the\nPHQ-8 questionnaire is structured. Our results indicate that, although a\nmultitask approach improves performance and fairness compared to a unitask\napproach, the results are not always consistent and we see evidence of negative\ntransfer and a reduction in the Pareto frontier, which is concerning given the\nhigh-stake healthcare setting. Our proposed approach of gender-based\nreweighting with uncertainty improves performance and fairness and alleviates\nboth challenges to a certain extent. Our findings on each PHQ-8 subitem task\ndifficulty are also in agreement with the largest study conducted on the PHQ-8\nsubitem discrimination capacity, thus providing the very first tangible\nevidence linking ML findings with large-scale empirical population studies\nconducted on the PHQ-8.\n","authors":["Jiaee Cheong","Aditya Bangar","Sinan Kalkan","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2501.09687v1.pdf","comment":"To appear at the Proceedings of Machine Learning Research 259, 1-14,\n  2024 as part of the Machine Learning for Health (ML4H) Symposium 2024"},{"id":"http://arxiv.org/abs/2501.09685v1","updated":"2025-01-16T17:37:35Z","published":"2025-01-16T17:37:35Z","title":"Reward-Guided Controlled Generation for Inference-Time Alignment in\n  Diffusion Models: Tutorial and Review","summary":"  This tutorial provides an in-depth guide on inference-time guidance and\nalignment methods for optimizing downstream reward functions in diffusion\nmodels. While diffusion models are renowned for their generative modeling\ncapabilities, practical applications in fields such as biology often require\nsample generation that maximizes specific metrics (e.g., stability, affinity in\nproteins, closeness to target structures). In these scenarios, diffusion models\ncan be adapted not only to generate realistic samples but also to explicitly\nmaximize desired measures at inference time without fine-tuning. This tutorial\nexplores the foundational aspects of such inference-time algorithms. We review\nthese methods from a unified perspective, demonstrating that current techniques\n-- such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling,\nand classifier guidance -- aim to approximate soft optimal denoising processes\n(a.k.a. policies in RL) that combine pre-trained denoising processes with value\nfunctions serving as look-ahead functions that predict from intermediate states\nto terminal rewards. Within this framework, we present several novel algorithms\nnot yet covered in the literature. Furthermore, we discuss (1) fine-tuning\nmethods combined with inference-time techniques, (2) inference-time algorithms\nbased on search algorithms such as Monte Carlo tree search, which have received\nlimited attention in current research, and (3) connections between\ninference-time algorithms in language models and diffusion models. The code of\nthis tutorial on protein design is available at\nhttps://github.com/masa-ue/AlignInversePro\n","authors":["Masatoshi Uehara","Yulai Zhao","Chenyu Wang","Xiner Li","Aviv Regev","Sergey Levine","Tommaso Biancalani"],"pdf_url":"https://arxiv.org/pdf/2501.09685v1.pdf","comment":"We plan to add more content/codes. Please let us know if there are\n  any comments"},{"id":"http://arxiv.org/abs/2501.09683v1","updated":"2025-01-16T17:34:49Z","published":"2025-01-16T17:34:49Z","title":"Rough kernel hedging","summary":"  Building on the functional-analytic framework of operator-valued kernels and\nun-truncated signature kernels, we propose a scalable, provably convergent\nsignature-based algorithm for a broad class of high-dimensional, path-dependent\nhedging problems. We make minimal assumptions about market dynamics by\nmodelling them as general geometric rough paths, yielding a fully model-free\napproach. Furthermore, through a representer theorem, we provide theoretical\nguarantees on the existence and uniqueness of a global minimum for the\nresulting optimization problem and derive an analytic solution under highly\ngeneral loss functions. Similar to the popular deep hedging approach, but in a\nmore rigorous fashion, our method can also incorporate additional features via\nthe underlying operator-valued kernel, such as trading signals, news analytics,\nand past hedging decisions, closely aligning with true machine-learning\npractice.\n","authors":["Nicola Muca Cirone","Cristopher Salvi"],"pdf_url":"https://arxiv.org/pdf/2501.09683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04202v7","updated":"2025-01-16T17:28:26Z","published":"2024-03-07T04:12:24Z","title":"Dynamics of Moral Behavior in Heterogeneous Populations of Learning\n  Agents","summary":"  Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents: a promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents; however, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., focused on maximizing outcomes\nover time), norm-based (i.e., conforming to specific norms), or virtue-based\n(i.e., considering a combination of different virtues). The extent to which\nagents' co-development may be impacted by such moral heterogeneity in\npopulations is not well understood. In this paper, we present a study of the\nlearning dynamics of morally heterogeneous populations interacting in a social\ndilemma setting. Using an Iterated Prisoner's Dilemma environment with a\npartner selection mechanism, we investigate the extent to which the prevalence\nof diverse moral agents in populations affects individual agents' learning\nbehaviors and emergent population-level outcomes. We observe several types of\nnon-trivial interactions between pro-social and anti-social agents, and find\nthat certain types of moral agents are able to steer selfish agents towards\nmore cooperative behavior.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2403.04202v7.pdf","comment":"Presented at AIES 2024 (7th AAAI/ACM Conference on AI, Ethics, and\n  Society - San Jose, CA, USA) - see\n  https://ojs.aaai.org/index.php/AIES/article/view/31736"},{"id":"http://arxiv.org/abs/2501.09659v1","updated":"2025-01-16T16:54:40Z","published":"2025-01-16T16:54:40Z","title":"Fokker-Planck to Callan-Symanzik: evolution of weight matrices under\n  training","summary":"  The dynamical evolution of a neural network during training has been an\nincredibly fascinating subject of study. First principal derivation of generic\nevolution of variables in statistical physics systems has proved useful when\nused to describe training dynamics conceptually, which in practice means\nnumerically solving equations such as Fokker-Planck equation. Simulating entire\nnetworks inevitably runs into the curse of dimensionality. In this paper, we\nutilize Fokker-Planck to simulate the probability density evolution of\nindividual weight matrices in the bottleneck layers of a simple\n2-bottleneck-layered auto-encoder and compare the theoretical evolutions\nagainst the empirical ones by examining the output data distributions. We also\nderive physically relevant partial differential equations such as\nCallan-Symanzik and Kardar-Parisi-Zhang equations from the dynamical equation\nwe have.\n","authors":["Wei Bu","Uri Kol","Ziming Liu"],"pdf_url":"https://arxiv.org/pdf/2501.09659v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2501.09655v1","updated":"2025-01-16T16:51:59Z","published":"2025-01-16T16:51:59Z","title":"A Survey of Research in Large Language Models for Electronic Design\n  Automation","summary":"  Within the rapidly evolving domain of Electronic Design Automation (EDA),\nLarge Language Models (LLMs) have emerged as transformative technologies,\noffering unprecedented capabilities for optimizing and automating various\naspects of electronic design. This survey provides a comprehensive exploration\nof LLM applications in EDA, focusing on advancements in model architectures,\nthe implications of varying model sizes, and innovative customization\ntechniques that enable tailored analytical insights. By examining the\nintersection of LLM capabilities and EDA requirements, the paper highlights the\nsignificant impact these models have on extracting nuanced understandings from\ncomplex datasets. Furthermore, it addresses the challenges and opportunities in\nintegrating LLMs into EDA workflows, paving the way for future research and\napplication in this dynamic field. Through this detailed analysis, the survey\naims to offer valuable insights to professionals in the EDA industry, AI\nresearchers, and anyone interested in the convergence of advanced AI\ntechnologies and electronic design.\n","authors":["Jingyu Pan","Guanglei Zhou","Chen-Chia Chang","Isaac Jacobson","Jiang Hu","Yiran Chen"],"pdf_url":"https://arxiv.org/pdf/2501.09655v1.pdf","comment":"21 pages, 2 figures, 3 tables, accepted by TODAES"},{"id":"http://arxiv.org/abs/2405.17097v2","updated":"2025-01-16T16:27:33Z","published":"2024-05-27T12:12:26Z","title":"A Comparative Study on Multi-task Uncertainty Quantification in Semantic\n  Segmentation and Monocular Depth Estimation","summary":"  Deep neural networks excel in perception tasks such as semantic segmentation\nand monocular depth estimation, making them indispensable in safety-critical\napplications like autonomous driving and industrial inspection. However, they\noften suffer from overconfidence and poor explainability, especially for\nout-of-domain data. While uncertainty quantification has emerged as a promising\nsolution to these challenges, multi-task settings have yet to be explored. In\nan effort to shed light on this, we evaluate Monte Carlo Dropout, Deep\nSub-Ensembles, and Deep Ensembles for joint semantic segmentation and monocular\ndepth estimation. Thereby, we reveal that Deep Ensembles stand out as the\npreferred choice, particularly in out-of-domain scenarios, and show the\npotential benefit of multi-task learning with regard to the uncertainty quality\nin comparison to solving both tasks separately. Additionally, we highlight the\nimpact of employing different uncertainty thresholds to classify pixels as\ncertain or uncertain, with the median uncertainty emerging as a robust default.\n","authors":["Steven Landgraf","Markus Hillemann","Theodor Kapler","Markus Ulrich"],"pdf_url":"https://arxiv.org/pdf/2405.17097v2.pdf","comment":"This manuscript is an extended version of a previously published\n  conference paper and is currently in review for a journal"},{"id":"http://arxiv.org/abs/2501.09636v1","updated":"2025-01-16T16:25:30Z","published":"2025-01-16T16:25:30Z","title":"LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading","summary":"  Recent advances in deep learning and large language models (LLMs) have\nfacilitated the deployment of the mixture-of-experts (MoE) mechanism in the\nstock investment domain. While these models have demonstrated promising trading\nperformance, they are often unimodal, neglecting the wealth of information\navailable in other modalities, such as textual data. Moreover, the traditional\nneural network-based router selection mechanism fails to consider contextual\nand real-world nuances, resulting in suboptimal expert selection. To address\nthese limitations, we propose LLMoE, a novel framework that employs LLMs as the\nrouter within the MoE architecture. Specifically, we replace the conventional\nneural network-based router with LLMs, leveraging their extensive world\nknowledge and reasoning capabilities to select experts based on historical\nprice data and stock news. This approach provides a more effective and\ninterpretable selection mechanism. Our experiments on multimodal real-world\nstock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models\nand other deep neural network approaches. Additionally, the flexible\narchitecture of LLMoE allows for easy adaptation to various downstream tasks.\n","authors":["Kuan-Ming Liu","Ming-Chih Lo"],"pdf_url":"https://arxiv.org/pdf/2501.09636v1.pdf","comment":"Accepted by AAAI 2025 Workshop on AI for Social Impact - Bridging\n  Innovations in Finance, Social Media, and Crime Prevention"},{"id":"http://arxiv.org/abs/2501.09631v1","updated":"2025-01-16T16:19:53Z","published":"2025-01-16T16:19:53Z","title":"Empowering Large Language Models in Wireless Communication: A Novel\n  Dataset and Fine-Tuning Framework","summary":"  In this work, we develop a specialized dataset aimed at enhancing the\nevaluation and fine-tuning of large language models (LLMs) specifically for\nwireless communication applications. The dataset includes a diverse set of\nmulti-hop questions, including true/false and multiple-choice types, spanning\nvarying difficulty levels from easy to hard. By utilizing advanced language\nmodels for entity extraction and question generation, rigorous data curation\nprocesses are employed to maintain high quality and relevance. Additionally, we\nintroduce a Pointwise V-Information (PVI) based fine-tuning method, providing a\ndetailed theoretical analysis and justification for its use in quantifying the\ninformation content of training data with 2.24\\% and 1.31\\% performance boost\nfor different models compared to baselines, respectively. To demonstrate the\neffectiveness of the fine-tuned models with the proposed methodologies on\npractical tasks, we also consider different tasks, including summarizing\noptimization problems from technical papers and solving the mathematical\nproblems related to non-orthogonal multiple access (NOMA), which are generated\nby using the proposed multi-agent framework. Simulation results show\nsignificant performance gain in summarization tasks with 20.9\\% in the ROUGE-L\nmetrics. We also study the scaling laws of fine-tuning LLMs and the challenges\nLLMs face in the field of wireless communications, offering insights into their\nadaptation to wireless communication tasks. This dataset and fine-tuning\nmethodology aim to enhance the training and evaluation of LLMs, contributing to\nadvancements in LLMs for wireless communication research and applications.\n","authors":["Yushen Lin","Ruichen Zhang","Wenqi Huang","Kaidi Wang","Zhiguo Ding","Daniel K. C. So","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2501.09631v1.pdf","comment":"13 pages, 13 figure, journal"},{"id":"http://arxiv.org/abs/2501.06278v2","updated":"2025-01-16T16:19:24Z","published":"2025-01-10T13:07:11Z","title":"Aligning Brain Activity with Advanced Transformer Models: Exploring the\n  Role of Punctuation in Semantic Processing","summary":"  This research examines the congruence between neural activity and advanced\ntransformer models, emphasizing the semantic significance of punctuation in\ntext understanding. Utilizing an innovative approach originally proposed by\nToneva and Wehbe, we evaluate four advanced transformer models RoBERTa,\nDistiliBERT, ALBERT, and ELECTRA against neural activity data. Our findings\nindicate that RoBERTa exhibits the closest alignment with neural activity,\nsurpassing BERT in accuracy. Furthermore, we investigate the impact of\npunctuation removal on model performance and neural alignment, revealing that\nBERT's accuracy enhances in the absence of punctuation. This study contributes\nto the comprehension of how neural networks represent language and the\ninfluence of punctuation on semantic processing within the human brain.\n","authors":["Zenon Lamprou","Frank Polick","Yashar Moshfeghi"],"pdf_url":"https://arxiv.org/pdf/2501.06278v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03840v2","updated":"2025-01-16T16:12:29Z","published":"2024-11-06T11:24:02Z","title":"Flexible task abstractions emerge in linear networks with fast and\n  bounded units","summary":"  Animals survive in dynamic environments changing at arbitrary timescales, but\nsuch data distribution shifts are a challenge to neural networks. To adapt to\nchange, neural systems may change a large number of parameters, which is a slow\nprocess involving forgetting past information. In contrast, animals leverage\ndistribution changes to segment their stream of experience into tasks and\nassociate them with internal task abstracts. Animals can then respond flexibly\nby selecting the appropriate task abstraction. However, how such flexible task\nabstractions may arise in neural systems remains unknown. Here, we analyze a\nlinear gated network where the weights and gates are jointly optimized via\ngradient descent, but with neuron-like constraints on the gates including a\nfaster timescale, nonnegativity, and bounded activity. We observe that the\nweights self-organize into modules specialized for tasks or sub-tasks\nencountered, while the gates layer forms unique representations that switch the\nappropriate weight modules (task abstractions). We analytically reduce the\nlearning dynamics to an effective eigenspace, revealing a virtuous cycle: fast\nadapting gates drive weight specialization by protecting previous knowledge,\nwhile weight specialization in turn increases the update rate of the gating\nlayer. Task switching in the gating layer accelerates as a function of\ncurriculum block size and task training, mirroring key findings in cognitive\nneuroscience. We show that the discovered task abstractions support\ngeneralization through both task and subtask composition, and we extend our\nfindings to a non-linear network switching between two tasks. Overall, our work\noffers a theory of cognitive flexibility in animals as arising from joint\ngradient descent on synaptic and neural gating in a neural network\narchitecture.\n","authors":["Kai Sandbrink","Jan P. Bauer","Alexandra M. Proca","Andrew M. Saxe","Christopher Summerfield","Ali Hummos"],"pdf_url":"https://arxiv.org/pdf/2411.03840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10729v3","updated":"2025-01-16T16:04:07Z","published":"2024-06-15T20:04:06Z","title":"A Comprehensive Survey of Foundation Models in Medicine","summary":"  Foundation models (FMs) are large-scale deep learning models trained on\nmassive datasets, often using self-supervised learning techniques. These models\nserve as a versatile base for a wide range of downstream tasks, including those\nin medicine and healthcare. FMs have demonstrated remarkable success across\nmultiple healthcare domains. However, existing surveys in this field do not\ncomprehensively cover all areas where FMs have made significant strides. In\nthis survey, we present a comprehensive review of FMs in medicine, focusing on\ntheir evolution, learning strategies, flagship models, applications, and\nassociated challenges. We examine how prominent FMs, such as the BERT and GPT\nfamilies, are transforming various aspects of healthcare, including clinical\nlarge language models, medical image analysis, and omics research.\nAdditionally, we provide a detailed taxonomy of FM-enabled healthcare\napplications, spanning clinical natural language processing, medical computer\nvision, graph learning, and other biology- and omics- related tasks. Despite\nthe transformative potentials of FMs, they also pose unique challenges. This\nsurvey delves into these challenges and highlights open research questions and\nlessons learned to guide researchers and practitioners. Our goal is to provide\nvaluable insights into the capabilities of FMs in health, facilitating\nresponsible deployment and mitigating associated risks.\n","authors":["Wasif Khan","Seowung Leem","Kyle B. See","Joshua K. Wong","Shaoting Zhang","Ruogu Fang"],"pdf_url":"https://arxiv.org/pdf/2406.10729v3.pdf","comment":"Currently under review in IEEE REVIEWS IN BIOMEDICAL ENGINEERING"},{"id":"http://arxiv.org/abs/2501.09621v1","updated":"2025-01-16T16:00:52Z","published":"2025-01-16T16:00:52Z","title":"Weight for Robustness: A Comprehensive Approach towards Optimal\n  Fault-Tolerant Asynchronous ML","summary":"  We address the challenges of Byzantine-robust training in asynchronous\ndistributed machine learning systems, aiming to enhance efficiency amid massive\nparallelization and heterogeneous computing resources. Asynchronous systems,\nmarked by independently operating workers and intermittent updates, uniquely\nstruggle with maintaining integrity against Byzantine failures, which encompass\nmalicious or erroneous actions that disrupt learning. The inherent delays in\nsuch settings not only introduce additional bias to the system but also obscure\nthe disruptions caused by Byzantine faults. To tackle these issues, we adapt\nthe Byzantine framework to asynchronous dynamics by introducing a novel\nweighted robust aggregation framework. This allows for the extension of robust\naggregators and a recent meta-aggregator to their weighted versions, mitigating\nthe effects of delayed updates. By further incorporating a recent\nvariance-reduction technique, we achieve an optimal convergence rate for the\nfirst time in an asynchronous Byzantine environment. Our methodology is\nrigorously validated through empirical and theoretical analysis, demonstrating\nits effectiveness in enhancing fault tolerance and optimizing performance in\nasynchronous ML systems.\n","authors":["Tehila Dahan","Kfir Y. Levy"],"pdf_url":"https://arxiv.org/pdf/2501.09621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09620v1","updated":"2025-01-16T16:00:37Z","published":"2025-01-16T16:00:37Z","title":"Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment","summary":"  Recent advances in large language models (LLMs) have demonstrated significant\nprogress in performing complex tasks. While Reinforcement Learning from Human\nFeedback (RLHF) has been effective in aligning LLMs with human preferences, it\nis susceptible to spurious correlations in reward modeling. Consequently, it\noften introduces biases-such as length bias, sycophancy, conceptual bias, and\ndiscrimination that hinder the model's ability to capture true causal\nrelationships. To address this, we propose a novel causal reward modeling\napproach that integrates causal inference to mitigate these spurious\ncorrelations. Our method enforces counterfactual invariance, ensuring reward\npredictions remain consistent when irrelevant variables are altered. Through\nexperiments on both synthetic and real-world datasets, we show that our\napproach mitigates various types of spurious correlations effectively,\nresulting in more reliable and fair alignment of LLMs with human preferences.\nAs a drop-in enhancement to the existing RLHF workflow, our causal reward\nmodeling provides a practical way to improve the trustworthiness and fairness\nof LLM finetuning.\n","authors":["Chaoqi Wang","Zhuokai Zhao","Yibo Jiang","Zhaorun Chen","Chen Zhu","Yuxin Chen","Jiayi Liu","Lizhu Zhang","Xiangjun Fan","Hao Ma","Sinong Wang"],"pdf_url":"https://arxiv.org/pdf/2501.09620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01818v3","updated":"2025-01-16T15:58:24Z","published":"2023-12-04T11:46:34Z","title":"Hybrid Approaches for Moral Value Alignment in AI Agents: a Manifesto","summary":"  Increasing interest in ensuring the safety of next-generation Artificial\nIntelligence (AI) systems calls for novel approaches to embedding morality into\nautonomous agents. This goal differs qualitatively from traditional\ntask-specific AI methodologies. In this paper, we provide a systematization of\nexisting approaches to the problem of introducing morality in machines -\nmodelled as a continuum. Our analysis suggests that popular techniques lie at\nthe extremes of this continuum - either being fully hard-coded into top-down,\nexplicit rules, or entirely learned in a bottom-up, implicit fashion with no\ndirect statement of any moral principle (this includes learning from human\nfeedback, as applied to the training and finetuning of large language models,\nor LLMs). Given the relative strengths and weaknesses of each type of\nmethodology, we argue that more hybrid solutions are needed to create adaptable\nand robust, yet controllable and interpretable agentic systems. To that end,\nthis paper discusses both the ethical foundations (including deontology,\nconsequentialism and virtue ethics) and implementations of morally aligned AI\nsystems.\n  We present a series of case studies that rely on intrinsic rewards, moral\nconstraints or textual instructions, applied to either pure-Reinforcement\nLearning or LLM-based agents. By analysing these diverse implementations under\none framework, we compare their relative strengths and shortcomings in\ndeveloping morally aligned AI systems. We then discuss strategies for\nevaluating the effectiveness of moral learning agents. Finally, we present open\nresearch questions and implications for the future of AI safety and ethics\nwhich are emerging from this hybrid framework.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2312.01818v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09980v4","updated":"2025-01-16T15:56:56Z","published":"2022-07-20T15:39:30Z","title":"ReFactor GNNs: Revisiting Factorisation-based Models from a\n  Message-Passing Perspective","summary":"  Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring\nsuccess for Knowledge Graph Completion (KGC) tasks, often outperforming Graph\nNeural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node\nfeatures and generalise to unseen nodes in inductive settings. Our work bridges\nthe gap between FMs and GNNs by proposing ReFactor GNNs. This new architecture\ndraws upon both modelling paradigms, which previously were largely thought of\nas disjoint. Concretely, using a message-passing formalism, we show how FMs can\nbe cast as GNNs by reformulating the gradient descent procedure as\nmessage-passing operations, which forms the basis of our ReFactor GNNs. Across\na multitude of well-established KGC benchmarks, our ReFactor GNNs achieve\ncomparable transductive performance to FMs, and state-of-the-art inductive\nperformance while using an order of magnitude fewer parameters.\n","authors":["Yihong Chen","Pushkar Mishra","Luca Franceschi","Pasquale Minervini","Pontus Stenetorp","Sebastian Riedel"],"pdf_url":"https://arxiv.org/pdf/2207.09980v4.pdf","comment":"36th Conference on Neural Information Processing Systems (NeurIPS\n  2022)"},{"id":"http://arxiv.org/abs/2411.12878v2","updated":"2025-01-16T15:46:14Z","published":"2024-11-19T21:53:06Z","title":"Local Anti-Concentration Class: Logarithmic Regret for Greedy Linear\n  Contextual Bandit","summary":"  We study the performance guarantees of exploration-free greedy algorithms for\nthe linear contextual bandit problem. We introduce a novel condition, named the\n\\textit{Local Anti-Concentration} (LAC) condition, which enables a greedy\nbandit algorithm to achieve provable efficiency. We show that the LAC condition\nis satisfied by a broad class of distributions, including Gaussian,\nexponential, uniform, Cauchy, and Student's~$t$ distributions, along with other\nexponential family distributions and their truncated variants. This\nsignificantly expands the class of distributions under which greedy algorithms\ncan perform efficiently. Under our proposed LAC condition, we prove that the\ncumulative expected regret of the greedy algorithm for the linear contextual\nbandit is bounded by $O(\\operatorname{poly} \\log T)$. Our results establish the\nwidest range of distributions known to date that allow a sublinear regret bound\nfor greedy algorithms, further achieving a sharp poly-logarithmic regret.\n","authors":["Seok-Jin Kim","Min-hwan Oh"],"pdf_url":"https://arxiv.org/pdf/2411.12878v2.pdf","comment":"NeurIPS2024"},{"id":"http://arxiv.org/abs/2501.09616v1","updated":"2025-01-16T15:43:32Z","published":"2025-01-16T15:43:32Z","title":"ARMAX identification of low rank graphical models","summary":"  In large-scale systems, complex internal relationships are often present.\nSuch interconnected systems can be effectively described by low rank stochastic\nprocesses. When identifying a predictive model of low rank processes from\nsampling data, the rank-deficient property of spectral densities is often\nobscured by the inevitable measurement noise in practice. However, existing low\nrank identification approaches often did not take noise into explicit\nconsideration, leading to non-negligible inaccuracies even under weak noise. In\nthis paper, we address the identification issue of low rank processes under\nmeasurement noise. We find that the noisy measurement model admits a sparse\nplus low rank structure in latent-variable graphical models. Specifically, we\nfirst decompose the problem into a maximum entropy covariance extension\nproblem, and a low rank graphical estimation problem based on an autoregressive\nmoving-average with exogenous input (ARMAX) model. To identify the ARMAX low\nrank graphical models, we propose an estimation approach based on maximum\nlikelihood. The identifiability and consistency of this approach are proven\nunder certain conditions. Simulation results confirm the reliable performance\nof the entire algorithm in both the parameter estimation and noisy data\nfiltering.\n","authors":["Wenqi Cao","Aming Li"],"pdf_url":"https://arxiv.org/pdf/2501.09616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09611v1","updated":"2025-01-16T15:35:48Z","published":"2025-01-16T15:35:48Z","title":"EVaDE : Event-Based Variational Thompson Sampling for Model-Based\n  Reinforcement Learning","summary":"  Posterior Sampling for Reinforcement Learning (PSRL) is a well-known\nalgorithm that augments model-based reinforcement learning (MBRL) algorithms\nwith Thompson sampling. PSRL maintains posterior distributions of the\nenvironment transition dynamics and the reward function, which are intractable\nfor tasks with high-dimensional state and action spaces. Recent works show that\ndropout, used in conjunction with neural networks, induces variational\ndistributions that can approximate these posteriors. In this paper, we propose\nEvent-based Variational Distributions for Exploration (EVaDE), which are\nvariational distributions that are useful for MBRL, especially when the\nunderlying domain is object-based. We leverage the general domain knowledge of\nobject-based domains to design three types of event-based convolutional layers\nto direct exploration. These layers rely on Gaussian dropouts and are inserted\nbetween the layers of the deep neural network model to help facilitate\nvariational Thompson sampling. We empirically show the effectiveness of\nEVaDE-equipped Simulated Policy Learning (EVaDE-SimPLe) on the 100K Atari game\nsuite.\n","authors":["Siddharth Aravindan","Dixant Mittal","Wee Sun Lee"],"pdf_url":"https://arxiv.org/pdf/2501.09611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09609v1","updated":"2025-01-16T15:34:00Z","published":"2025-01-16T15:34:00Z","title":"Adversarial-Ensemble Kolmogorov Arnold Networks for Enhancing Indoor\n  Wi-Fi Positioning: A Defensive Approach Against Spoofing and Signal\n  Manipulation Attacks","summary":"  The research presents a study on enhancing the robustness of Wi-Fi-based\nindoor positioning systems against adversarial attacks. The goal is to improve\nthe positioning accuracy and resilience of these systems under two attack\nscenarios: Wi-Fi Spoofing and Signal Strength Manipulation. Three models are\ndeveloped and evaluated: a baseline model (M_Base), an adversarially trained\nrobust model (M_Rob), and an ensemble model (M_Ens). All models utilize a\nKolmogorov-Arnold Network (KAN) architecture. The robust model is trained with\nadversarially perturbed data, while the ensemble model combines predictions\nfrom both the base and robust models. Experimental results show that the robust\nmodel reduces positioning error by approximately 10% compared to the baseline,\nachieving 2.03 meters error under Wi-Fi spoofing and 2.00 meters under signal\nstrength manipulation. The ensemble model further outperforms with errors of\n2.01 meters and 1.975 meters for the respective attack types. This analysis\nhighlights the effectiveness of adversarial training techniques in mitigating\nattack impacts. The findings underscore the importance of considering\nadversarial scenarios in developing indoor positioning systems, as improved\nresilience can significantly enhance the accuracy and reliability of such\nsystems in mission-critical environments.\n","authors":["Mitul Goswami","Romit Chatterjee","Somnath Mahato","Prasant Kumar Pattnaik"],"pdf_url":"https://arxiv.org/pdf/2501.09609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08389v3","updated":"2025-01-16T15:32:33Z","published":"2024-09-12T20:37:14Z","title":"Higher-Order Topological Directionality and Directed Simplicial Neural\n  Networks","summary":"  Topological Deep Learning (TDL) has emerged as a paradigm to process and\nlearn from signals defined on higher-order combinatorial topological spaces,\nsuch as simplicial or cell complexes. Although many complex systems have an\nasymmetric relational structure, most TDL models forcibly symmetrize these\nrelationships. In this paper, we first introduce a novel notion of higher-order\ndirectionality and we then design Directed Simplicial Neural Networks\n(Dir-SNNs) based on it. Dir-SNNs are message-passing networks operating on\ndirected simplicial complexes able to leverage directed and possibly asymmetric\ninteractions among the simplices. To our knowledge, this is the first TDL model\nusing a notion of higher-order directionality. We theoretically and empirically\nprove that Dir-SNNs are more expressive than their directed graph counterpart\nin distinguishing isomorphic directed graphs. Experiments on a synthetic source\nlocalization task demonstrate that Dir-SNNs outperform undirected SNNs when the\nunderlying complex is directed, and perform comparably when the underlying\ncomplex is undirected.\n","authors":["Manuel Lecha","Andrea Cavallo","Francesca Dominici","Elvin Isufi","Claudio Battiloro"],"pdf_url":"https://arxiv.org/pdf/2409.08389v3.pdf","comment":"7 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2501.09597v1","updated":"2025-01-16T15:21:18Z","published":"2025-01-16T15:21:18Z","title":"Reducing the Sensitivity of Neural Physics Simulators to Mesh Topology\n  via Pretraining","summary":"  Meshes are used to represent complex objects in high fidelity physics\nsimulators across a variety of domains, such as radar sensing and aerodynamics.\nThere is growing interest in using neural networks to accelerate physics\nsimulations, and also a growing body of work on applying neural networks\ndirectly to irregular mesh data. Since multiple mesh topologies can represent\nthe same object, mesh augmentation is typically required to handle topological\nvariation when training neural networks. Due to the sensitivity of physics\nsimulators to small changes in mesh shape, it is challenging to use these\naugmentations when training neural network-based physics simulators. In this\nwork, we show that variations in mesh topology can significantly reduce the\nperformance of neural network simulators. We evaluate whether pretraining can\nbe used to address this issue, and find that employing an established\nautoencoder pretraining technique with graph embedding models reduces the\nsensitivity of neural network simulators to variations in mesh topology.\nFinally, we highlight future research directions that may further reduce neural\nsimulator sensitivity to mesh topology.\n","authors":["Nathan Vaska","Justin Goodwin","Robin Walters","Rajmonda S. Caceres"],"pdf_url":"https://arxiv.org/pdf/2501.09597v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2501.09595v1","updated":"2025-01-16T15:20:22Z","published":"2025-01-16T15:20:22Z","title":"IFRA: a machine learning-based Instrumented Fall Risk Assessment Scale\n  derived from Instrumented Timed Up and Go test in stroke patients","summary":"  Effective fall risk assessment is critical for post-stroke patients. The\npresent study proposes a novel, data-informed fall risk assessment method based\non the instrumented Timed Up and Go (ITUG) test data, bringing in many mobility\nmeasures that traditional clinical scales fail to capture. IFRA, which stands\nfor Instrumented Fall Risk Assessment, has been developed using a two-step\nprocess: first, features with the highest predictive power among those\ncollected in a ITUG test have been identified using machine learning\ntechniques; then, a strategy is proposed to stratify patients into low, medium,\nor high-risk strata. The dataset used in our analysis consists of 142\nparticipants, out of which 93 were used for training (15 synthetically\ngenerated), 17 for validation and 32 to test the resulting IFRA scale (22\nnon-fallers and 10 fallers). Features considered in the IFRA scale include gait\nspeed, vertical acceleration during sit-to-walk transition, and turning angular\nvelocity, which align well with established literature on the risk of fall in\nneurological patients. In a comparison with traditional clinical scales such as\nthe traditional Timed Up & Go and the Mini-BESTest, IFRA demonstrates\ncompetitive performance, being the only scale to correctly assign more than\nhalf of the fallers to the high-risk stratum (Fischer's Exact test p = 0.004).\nDespite the dataset's limited size, this is the first proof-of-concept study to\npave the way for future evidence regarding the use of IFRA tool for continuous\npatient monitoring and fall prevention both in clinical stroke rehabilitation\nand at home post-discharge.\n","authors":["Simone Macciò","Alessandro Carfì","Alessio Capitanelli","Peppino Tropea","Massimo Corbo","Fulvio Mastrogiovanni","Michela Picardi"],"pdf_url":"https://arxiv.org/pdf/2501.09595v1.pdf","comment":"26 pages, 2 figures, submitted for review dec 2024"},{"id":"http://arxiv.org/abs/2501.09591v1","updated":"2025-01-16T15:17:27Z","published":"2025-01-16T15:17:27Z","title":"Metrics for Inter-Dataset Similarity with Example Applications in\n  Synthetic Data and Feature Selection Evaluation -- Extended Version","summary":"  Measuring inter-dataset similarity is an important task in machine learning\nand data mining with various use cases and applications. Existing methods for\nmeasuring inter-dataset similarity are computationally expensive, limited, or\nsensitive to different entities and non-trivial choices for parameters. They\nalso lack a holistic perspective on the entire dataset. In this paper, we\npropose two novel metrics for measuring inter-dataset similarity. We discuss\nthe mathematical foundation and the theoretical basis of our proposed metrics.\nWe demonstrate the effectiveness of the proposed metrics by investigating two\napplications in the evaluation of synthetic data and in the evaluation of\nfeature selection methods. The theoretical and empirical studies conducted in\nthis paper illustrate the effectiveness of the proposed metrics.\n","authors":["Muhammad Rajabinasab","Anton D. Lautrup","Arthur Zimek"],"pdf_url":"https://arxiv.org/pdf/2501.09591v1.pdf","comment":"This is the extended version of a paper accepted at 2025 SIAM\n  International Conference on Data Mining (SDM)"},{"id":"http://arxiv.org/abs/2501.09588v1","updated":"2025-01-16T15:11:33Z","published":"2025-01-16T15:11:33Z","title":"Atleus: Accelerating Transformers on the Edge Enabled by 3D\n  Heterogeneous Manycore Architectures","summary":"  Transformer architectures have become the standard neural network model for\nvarious machine learning applications including natural language processing and\ncomputer vision. However, the compute and memory requirements introduced by\ntransformer models make them challenging to adopt for edge applications.\nFurthermore, fine-tuning pre-trained transformers (e.g., foundation models) is\na common task to enhance the model's predictive performance on specific\ntasks/applications. Existing transformer accelerators are oblivious to\ncomplexities introduced by fine-tuning. In this paper, we propose the design of\na three-dimensional (3D) heterogeneous architecture referred to as Atleus that\nincorporates heterogeneous computing resources specifically optimized to\naccelerate transformer models for the dual purposes of fine-tuning and\ninference. Specifically, Atleus utilizes non-volatile memory and systolic array\nfor accelerating transformer computational kernels using an integrated 3D\nplatform. Moreover, we design a suitable NoC to achieve high performance and\nenergy efficiency. Finally, Atleus adopts an effective quantization scheme to\nsupport model compression. Experimental results demonstrate that Atleus\noutperforms existing state-of-the-art by up to 56x and 64.5x in terms of\nperformance and energy efficiency respectively\n","authors":["Pratyush Dhingra","Janardhan Rao Doppa","Partha Pratim Pande"],"pdf_url":"https://arxiv.org/pdf/2501.09588v1.pdf","comment":"Accepted for Publication in IEEE Transactions on Computer-Aided\n  Design of Integrated Circuits and Systems (TCAD)"},{"id":"http://arxiv.org/abs/2307.02229v2","updated":"2025-01-16T15:00:38Z","published":"2023-07-05T12:13:56Z","title":"Hybrid additive modeling with partial dependence for supervised\n  regression and dynamical systems forecasting","summary":"  Learning processes by exploiting restricted domain knowledge is an important\ntask across a plethora of scientific areas, with more and more hybrid training\nmethods additively combining data-driven and model-based approaches. Although\nthe obtained models are more accurate than purely data-driven models, the\noptimization process usually comes with sensitive regularization constraints.\nFurthermore, while such hybrid methods have been tested in various scientific\napplications, they have been mostly tested on dynamical systems, with only\nlimited study about the influence of each model component on global performance\nand parameter identification. In this work, we introduce a new hybrid training\napproach based on partial dependence, which removes the need for intricate\nregularization. Moreover, we assess the performance of hybrid modeling against\ntraditional machine learning methods on standard regression problems. We\ncompare, on both synthetic and real regression problems, several approaches for\ntraining such hybrid models. We focus on hybrid methods that additively combine\na parametric term with a machine learning term and investigate model-agnostic\ntraining procedures. Therefore, experiments are carried out with different\ntypes of machine learning models, including tree-based models and artificial\nneural networks. We also extend our partial dependence optimization process for\ndynamical systems forecasting and compare it to existing schemes.\n","authors":["Yann Claes","Vân Anh Huynh-Thu","Pierre Geurts"],"pdf_url":"https://arxiv.org/pdf/2307.02229v2.pdf","comment":"Extended version of the paper entitled \"Knowledge-Guided Additive\n  Modeling for Supervised Regression\"\n  (https://link.springer.com/chapter/10.1007/978-3-031-45275-8_5), accepted for\n  publication in the Machine Learning journal. The extension includes new\n  experiments in the static setting, along with a dedicated section on the\n  application of our method to the problem of dynamical systems forecasting"},{"id":"http://arxiv.org/abs/2501.09579v1","updated":"2025-01-16T14:56:41Z","published":"2025-01-16T14:56:41Z","title":"Sequential PatchCore: Anomaly Detection for Surface Inspection using\n  Synthetic Impurities","summary":"  The appearance of surface impurities (e.g., water stains, fingerprints,\nstickers) is an often-mentioned issue that causes degradation of automated\nvisual inspection systems. At the same time, synthetic data generation\ntechniques for visual surface inspection have focused primarily on generating\nperfect examples and defects, disregarding impurities. This study highlights\nthe importance of considering impurities when generating synthetic data. We\nintroduce a procedural method to include photorealistic water stains in\nsynthetic data. The synthetic datasets are generated to correspond to real\ndatasets and are further used to train an anomaly detection model and\ninvestigate the influence of water stains. The high-resolution images used for\nsurface inspection lead to memory bottlenecks during anomaly detection\ntraining. To address this, we introduce Sequential PatchCore - a method to\nbuild coresets sequentially and make training on large images using\nconsumer-grade hardware tractable. This allows us to perform transfer learning\nusing coresets pre-trained on different dataset versions. Our results show the\nbenefits of using synthetic data for pre-training an explicit coreset anomaly\nmodel and the extended performance benefits of finetuning the coreset using\nreal data. We observed how the impurities and labelling ambiguity lower the\nmodel performance and have additionally reported the defect-wise recall to\nprovide an industrially relevant perspective on model performance.\n","authors":["Runzhou Mao","Juraj Fulir","Christoph Garth","Petra Gospodnetić"],"pdf_url":"https://arxiv.org/pdf/2501.09579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09572v1","updated":"2025-01-16T14:45:53Z","published":"2025-01-16T14:45:53Z","title":"Towards Spectral Convergence of Locally Linear Embedding on Manifolds\n  with Boundary","summary":"  We study the eigenvalues and eigenfunctions of a differential operator that\ngoverns the asymptotic behavior of the unsupervised learning algorithm known as\nLocally Linear Embedding when a large data set is sampled from an interval or\ndisc. In particular, the differential operator is of second order, mixed-type,\nand degenerates near the boundary. We show that a natural regularity condition\non the eigenfunctions imposes a consistent boundary condition and use the\nFrobenius method to estimate pointwise behavior. We then determine the limiting\nsequence of eigenvalues analytically and compare them to numerical predictions.\nFinally, we propose a variational framework for determining eigenvalues on\nother compact manifolds.\n","authors":["Andrew Lyons"],"pdf_url":"https://arxiv.org/pdf/2501.09572v1.pdf","comment":"26 pages, 7 figures; the author welcomes all comments"},{"id":"http://arxiv.org/abs/2405.17061v3","updated":"2025-01-16T14:45:52Z","published":"2024-05-27T11:31:54Z","title":"Provably Efficient Reinforcement Learning with Multinomial Logit\n  Function Approximation","summary":"  We study a new class of MDPs that employs multinomial logit (MNL) function\napproximation to ensure valid probability distributions over the state space.\nDespite its significant benefits, incorporating the non-linear function raises\nsubstantial challenges in both statistical and computational efficiency. The\nbest-known result of Hwang and Oh [2023] has achieved an\n$\\widetilde{\\mathcal{O}}(\\kappa^{-1}dH^2\\sqrt{K})$ regret upper bound, where\n$\\kappa$ is a problem-dependent quantity, $d$ is the feature dimension, $H$ is\nthe episode length, and $K$ is the number of episodes. However, we observe that\n$\\kappa^{-1}$ exhibits polynomial dependence on the number of reachable states,\nwhich can be as large as the state space size in the worst case and thus\nundermines the motivation for function approximation. Additionally, their\nmethod requires storing all historical data and the time complexity scales\nlinearly with the episode count, which is computationally expensive. In this\nwork, we propose a statistically efficient algorithm that achieves a regret of\n$\\widetilde{\\mathcal{O}}(dH^2\\sqrt{K} + \\kappa^{-1}d^2H^2)$, eliminating the\ndependence on $\\kappa^{-1}$ in the dominant term for the first time. We then\naddress the computational challenges by introducing an enhanced algorithm that\nachieves the same regret guarantee but with only constant cost. Finally, we\nestablish the first lower bound for this problem, justifying the optimality of\nour results in $d$ and $K$.\n","authors":["Long-Fei Li","Yu-Jie Zhang","Peng Zhao","Zhi-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2405.17061v3.pdf","comment":"NeurIPS 2024; v3 substantially improves the presentation and further\n  illustrates the role of $\\kappa$ in function approximation"},{"id":"http://arxiv.org/abs/2407.20891v4","updated":"2025-01-16T14:45:36Z","published":"2024-07-30T15:07:13Z","title":"Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian\n  Neural Networks","summary":"  Computational complexity of Bayesian learning is impeding its adoption in\npractical, large-scale tasks. Despite demonstrations of significant merits such\nas improved robustness and resilience to unseen or out-of-distribution inputs\nover their non- Bayesian counterparts, their practical use has faded to near\ninsignificance. In this study, we introduce an innovative framework to mitigate\nthe computational burden of Bayesian neural networks (BNNs). Our approach\nfollows the principle of Bayesian techniques based on deep ensembles, but\nsignificantly reduces their cost via multiple low-rank perturbations of\nparameters arising from a pre-trained neural network. Both vanilla version of\nensembles as well as more sophisticated schemes such as Bayesian learning with\nStein Variational Gradient Descent (SVGD), previously deemed impractical for\nlarge models, can be seamlessly implemented within the proposed framework,\ncalled Bayesian Low-Rank LeArning (Bella). In a nutshell, i) Bella achieves a\ndramatic reduction in the number of trainable parameters required to\napproximate a Bayesian posterior; and ii) it not only maintains, but in some\ninstances, surpasses the performance of conventional Bayesian learning methods\nand non-Bayesian baselines. Our results with large-scale tasks such as\nImageNet, CAMELYON17, DomainNet, VQA with CLIP, LLaVA demonstrate the\neffectiveness and versatility of Bella in building highly scalable and\npractical Bayesian deep models for real-world applications.\n","authors":["Bao Gia Doan","Afshar Shamsi","Xiao-Yu Guo","Arash Mohammadi","Hamid Alinejad-Rokny","Dino Sejdinovic","Damien Teney","Damith C. Ranasinghe","Ehsan Abbasnejad"],"pdf_url":"https://arxiv.org/pdf/2407.20891v4.pdf","comment":"This paper is accepted in AAAI'2025"},{"id":"http://arxiv.org/abs/2501.09571v1","updated":"2025-01-16T14:45:12Z","published":"2025-01-16T14:45:12Z","title":"MatrixNet: Learning over symmetry groups using learned group\n  representations","summary":"  Group theory has been used in machine learning to provide a theoretically\ngrounded approach for incorporating known symmetry transformations in tasks\nfrom robotics to protein modeling. In these applications, equivariant neural\nnetworks use known symmetry groups with predefined representations to learn\nover geometric input data. We propose MatrixNet, a neural network architecture\nthat learns matrix representations of group element inputs instead of using\npredefined representations. MatrixNet achieves higher sample efficiency and\ngeneralization over several standard baselines in prediction tasks over the\nseveral finite groups and the Artin braid group. We also show that MatrixNet\nrespects group relations allowing generalization to group elements of greater\nword length than in the training set.\n","authors":["Lucas Laird","Circe Hsu","Asilata Bapat","Robin Walters"],"pdf_url":"https://arxiv.org/pdf/2501.09571v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.04755v2","updated":"2025-01-16T14:44:39Z","published":"2024-12-06T03:40:21Z","title":"Latent Space Characterization of Autoencoder Variants","summary":"  Understanding the latent spaces learned by deep learning models is crucial in\nexploring how they represent and generate complex data. Autoencoders (AEs) have\nplayed a key role in the area of representation learning, with numerous\nregularization techniques and training principles developed not only to enhance\ntheir ability to learn compact and robust representations, but also to reveal\nhow different architectures influence the structure and smoothness of the\nlower-dimensional non-linear manifold. We strive to characterize the structure\nof the latent spaces learned by different autoencoders including convolutional\nautoencoders (CAEs), denoising autoencoders (DAEs), and variational\nautoencoders (VAEs) and how they change with the perturbations in the input. By\ncharacterizing the matrix manifolds corresponding to the latent spaces, we\nprovide an explanation for the well-known observation that the latent spaces of\nCAE and DAE form non-smooth manifolds, while that of VAE forms a smooth\nmanifold. We also map the points of the matrix manifold to a Hilbert space\nusing distance preserving transforms and provide an alternate view in terms of\nthe subspaces generated in the Hilbert space as a function of the distortion in\nthe input. The results show that the latent manifolds of CAE and DAE are\nstratified with each stratum being a smooth product manifold, while the\nmanifold of VAE is a smooth product manifold of two symmetric positive definite\nmatrices and a symmetric positive semi-definite matrix.\n","authors":["Anika Shrivastava","Renu Rameshan","Samar Agnihotri"],"pdf_url":"https://arxiv.org/pdf/2412.04755v2.pdf","comment":"9 pages, 6 figures, and 1 table"},{"id":"http://arxiv.org/abs/2408.14234v3","updated":"2025-01-16T14:29:47Z","published":"2024-08-26T12:49:41Z","title":"FSDEM: Feature Selection Dynamic Evaluation Metric","summary":"  Expressive evaluation metrics are indispensable for informative experiments\nin all areas, and while several metrics are established in some areas, in\nothers, such as feature selection, only indirect or otherwise limited\nevaluation metrics are found. In this paper, we propose a novel evaluation\nmetric to address several problems of its predecessors and allow for flexible\nand reliable evaluation of feature selection algorithms. The proposed metric is\na dynamic metric with two properties that can be used to evaluate both the\nperformance and the stability of a feature selection algorithm. We conduct\nseveral empirical experiments to illustrate the use of the proposed metric in\nthe successful evaluation of feature selection algorithms. We also provide a\ncomparison and analysis to show the different aspects involved in the\nevaluation of the feature selection algorithms. The results indicate that the\nproposed metric is successful in carrying out the evaluation task for feature\nselection algorithms.\n  This paper is an extended version of a paper published at SISAP 2024.\n","authors":["Muhammad Rajabinasab","Anton D. Lautrup","Tobias Hyrup","Arthur Zimek"],"pdf_url":"https://arxiv.org/pdf/2408.14234v3.pdf","comment":"Short version of this paper is published at 17th International\n  Conference on Similarity Search and Applications, SISAP 2024"},{"id":"http://arxiv.org/abs/2501.09556v1","updated":"2025-01-16T14:18:10Z","published":"2025-01-16T14:18:10Z","title":"Overshoot: Taking advantage of future gradients in momentum-based\n  stochastic optimization","summary":"  Overshoot is a novel, momentum-based stochastic gradient descent optimization\nmethod designed to enhance performance beyond standard and Nesterov's momentum.\nIn conventional momentum methods, gradients from previous steps are aggregated\nwith the gradient at current model weights before taking a step and updating\nthe model. Rather than calculating gradient at the current model weights,\nOvershoot calculates the gradient at model weights shifted in the direction of\nthe current momentum. This sacrifices the immediate benefit of using the\ngradient w.r.t. the exact model weights now, in favor of evaluating at a point,\nwhich will likely be more relevant for future updates. We show that\nincorporating this principle into momentum-based optimizers (SGD with momentum\nand Adam) results in faster convergence (saving on average at least 15% of\nsteps). Overshoot consistently outperforms both standard and Nesterov's\nmomentum across a wide range of tasks and integrates into popular\nmomentum-based optimizers with zero memory and small computational overhead.\n","authors":["Jakub Kopal","Michal Gregor","Santiago de Leon-Martinez","Jakub Simko"],"pdf_url":"https://arxiv.org/pdf/2501.09556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09551v1","updated":"2025-01-16T14:12:03Z","published":"2025-01-16T14:12:03Z","title":"Intra-day Solar and Power Forecast for Optimization of Intraday Market\n  Participation","summary":"  The prediction of solar irradiance enhances reliability in photovoltaic (PV)\nsolar plant generation and grid integration. In Colombia, PV plants face\npenalties if energy production deviates beyond governmental thresholds from\nintraday market offers. This research employs Long Short-Term Memory (LSTM) and\nBidirectional-LSTM (Bi-LSTM) models, utilizing meteorological data from a PV\nplant in El Paso, Cesar, Colombia, to predict solar irradiance with a 6-hour\nhorizon and 10-minute resolution. While Bi-LSTM showed superior performance,\nthe LSTM model achieved comparable results with significantly reduced training\ntime (6 hours versus 18 hours), making it computationally advantageous. The\nLSTM predictions were averaged to create an hourly resolution model, evaluated\nusing Mean Absolute Error, Root-Mean-Square Error, Normalized Root-Mean-Square\nError, and Mean Absolute Percentage Error metrics. Comparison with the Global\nForecast System (GFS) revealed similar performance, with both models\neffectively capturing daily solar irradiance patterns. The forecast model\nintegrates with an Object-Oriented power production model, enabling accurate\nenergy offers in the intraday market while minimizing penalty costs.\n","authors":["Nelson Salazar-Peña","Adolfo Palma-Vergara","Mateo Montes","María Alejandra Vargas-Torres","Adriana Salinas","Andrés Velasco","Alejandra Tabares","Andrés González-Mancera"],"pdf_url":"https://arxiv.org/pdf/2501.09551v1.pdf","comment":"20 pages, 37 figures, 9 tables"},{"id":"http://arxiv.org/abs/2404.14388v3","updated":"2025-01-16T14:02:26Z","published":"2024-04-22T17:46:29Z","title":"STROOBnet Optimization via GPU-Accelerated Proximal Recurrence\n  Strategies","summary":"  Spatiotemporal networks' observational capabilities are crucial for accurate\ndata gathering and informed decisions across multiple sectors. This study\nfocuses on the Spatiotemporal Ranged Observer-Observable Bipartite Network\n(STROOBnet), linking observational nodes (e.g., surveillance cameras) to events\nwithin defined geographical regions, enabling efficient monitoring. Using data\nfrom Real-Time Crime Camera (RTCC) systems and Calls for Service (CFS) in New\nOrleans, where RTCC combats rising crime amidst reduced police presence, we\naddress the network's initial observational imbalances. Aiming for uniform\nobservational efficacy, we propose the Proximal Recurrence approach. It\noutperformed traditional clustering methods like k-means and DBSCAN by offering\nholistic event frequency and spatial consideration, enhancing observational\ncoverage.\n","authors":["Ted Edward Holmberg","Mahdi Abdelguerfi","Elias Ioup"],"pdf_url":"https://arxiv.org/pdf/2404.14388v3.pdf","comment":"10 pages, 17 figures, 2023 IEEE International Conference on Big Data\n  (BigData)"},{"id":"http://arxiv.org/abs/2412.07223v3","updated":"2025-01-16T13:53:47Z","published":"2024-12-10T06:19:08Z","title":"A Consolidated Volatility Prediction with Back Propagation Neural\n  Network and Genetic Algorithm","summary":"  This paper provides a unique approach with AI algorithms to predict emerging\nstock markets volatility. Traditionally, stock volatility is derived from\nhistorical volatility,Monte Carlo simulation and implied volatility as well. In\nthis paper, the writer designs a consolidated model with back-propagation\nneural network and genetic algorithm to predict future volatility of emerging\nstock markets and found that the results are quite accurate with low errors.\n","authors":["Zong Ke","Jingyu Xu","Zizhou Zhang","Yu Cheng","Wenjun Wu"],"pdf_url":"https://arxiv.org/pdf/2412.07223v3.pdf","comment":"6 pages, 7 figures, 1 table, The paper will be published by IEEE on\n  conference: 2024 3rd International Conference on Image Processing, Computer\n  Vision and Machine Learning (ICICML 2024) (V2)"},{"id":"http://arxiv.org/abs/2203.07260v3","updated":"2025-01-16T13:40:01Z","published":"2022-03-14T16:40:57Z","title":"SimHawNet: A Modified Hawkes Process for Temporal Network Simulation","summary":"  Temporal networks allow representing connections between objects while\nincorporating the temporal dimension. While static network models can capture\nunchanging topological regularities, they often fail to model the effects\nassociated with the causal generative process of the network that occurs in\ntime. Hence, exploiting the temporal aspect of networks has been the focus of\nmany recent studies. In this context, we propose a new framework for generative\nmodels of continuous-time temporal networks. We assume that the activation of\nthe edges in a temporal network is driven by a specified temporal point\nprocess. This approach allows to directly model the waiting time between events\nwhile incorporating time-varying history-based features as covariates in the\npredictions. Coupled with a thinning algorithm designed for the simulation of\npoint processes, SimHawNet enables simulation of the evolution of temporal\nnetworks in continuous time. Finally, we introduce a comprehensive evaluation\nframework to assess the performance of such an approach, in which we\ndemonstrate that SimHawNet successfully simulates the evolution of networks\nwith very different generative processes and achieves performance comparable to\nthe state of the art, while being significantly faster.\n","authors":["Mathilde Perez","Raphaël Romero","Bo Kang","Tijl De Bie","Jefrey Lijffijt","Charlotte Laclau"],"pdf_url":"https://arxiv.org/pdf/2203.07260v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09531v1","updated":"2025-01-16T13:30:20Z","published":"2025-01-16T13:30:20Z","title":"MOGNET: A Mux-residual quantized Network leveraging Online-Generated\n  weights","summary":"  This paper presents a compact model architecture called MOGNET, compatible\nwith a resource-limited hardware. MOGNET uses a streamlined Convolutional\nfactorization block based on a combination of 2 point-wise (1x1) convolutions\nwith a group-wise convolution in-between. To further limit the overall model\nsize and reduce the on-chip required memory, the second point-wise\nconvolution's parameters are on-line generated by a Cellular Automaton\nstructure. In addition, MOGNET enables the use of low-precision weights and\nactivations, by taking advantage of a Multiplexer mechanism with a proper\nBitshift rescaling for integrating residual paths without increasing the\nhardware-related complexity. To efficiently train this model we also introduce\na novel weight ternarization method favoring the balance between quantized\nlevels. Experimental results show that given tiny memory budget (sub-2Mb),\nMOGNET can achieve higher accuracy with a clear gap up to 1% at a similar or\neven lower model size compared to recent state-of-the-art methods.\n","authors":["Van Thien Nguyen","William Guicquero","Gilles Sicard"],"pdf_url":"https://arxiv.org/pdf/2501.09531v1.pdf","comment":"Published at IEEE AICAS 2022"},{"id":"http://arxiv.org/abs/2501.09527v1","updated":"2025-01-16T13:23:07Z","published":"2025-01-16T13:23:07Z","title":"Confidence Estimation for Error Detection in Text-to-SQL Systems","summary":"  Text-to-SQL enables users to interact with databases through natural\nlanguage, simplifying the retrieval and synthesis of information. Despite the\nsuccess of large language models (LLMs) in converting natural language\nquestions into SQL queries, their broader adoption is limited by two main\nchallenges: achieving robust generalization across diverse queries and ensuring\ninterpretative confidence in their predictions. To tackle these issues, our\nresearch investigates the integration of selective classifiers into Text-to-SQL\nsystems. We analyse the trade-off between coverage and risk using entropy based\nconfidence estimation with selective classifiers and assess its impact on the\noverall performance of Text-to-SQL models. Additionally, we explore the models'\ninitial calibration and improve it with calibration techniques for better model\nalignment between confidence and accuracy. Our experimental results show that\nencoder-decoder T5 is better calibrated than in-context-learning GPT 4 and\ndecoder-only Llama 3, thus the designated external entropy-based selective\nclassifier has better performance. The study also reveal that, in terms of\nerror detection, selective classifier with a higher probability detects errors\nassociated with irrelevant questions rather than incorrect query generations.\n","authors":["Oleg Somov","Elena Tutubalina"],"pdf_url":"https://arxiv.org/pdf/2501.09527v1.pdf","comment":"15 pages, 11 figures, to be published in AAAI 2025 Proceedings"},{"id":"http://arxiv.org/abs/2501.09525v1","updated":"2025-01-16T13:20:29Z","published":"2025-01-16T13:20:29Z","title":"Class Incremental Fault Diagnosis under Limited Fault Data via\n  Supervised Contrastive Knowledge Distillation","summary":"  Class-incremental fault diagnosis requires a model to adapt to new fault\nclasses while retaining previous knowledge. However, limited research exists\nfor imbalanced and long-tailed data. Extracting discriminative features from\nfew-shot fault data is challenging, and adding new fault classes often demands\ncostly model retraining. Moreover, incremental training of existing methods\nrisks catastrophic forgetting, and severe class imbalance can bias the model's\ndecisions toward normal classes. To tackle these issues, we introduce a\nSupervised Contrastive knowledge distiLlation for class Incremental Fault\nDiagnosis (SCLIFD) framework proposing supervised contrastive knowledge\ndistillation for improved representation learning capability and less\nforgetting, a novel prioritized exemplar selection method for sample replay to\nalleviate catastrophic forgetting, and the Random Forest Classifier to address\nthe class imbalance. Extensive experimentation on simulated and real-world\nindustrial datasets across various imbalance ratios demonstrates the\nsuperiority of SCLIFD over existing approaches. Our code can be found at\nhttps://github.com/Zhang-Henry/SCLIFD_TII.\n","authors":["Hanrong Zhang","Yifei Yao","Zixuan Wang","Jiayuan Su","Mengxuan Li","Peng Peng","Hongwei Wang"],"pdf_url":"https://arxiv.org/pdf/2501.09525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09522v1","updated":"2025-01-16T13:17:24Z","published":"2025-01-16T13:17:24Z","title":"Merging Models on the Fly Without Retraining: A Sequential Approach to\n  Scalable Continual Model Merging","summary":"  Deep model merging represents an emerging research direction that combines\nmultiple fine-tuned models to harness their specialized capabilities across\ndifferent tasks and domains. Current model merging techniques focus on merging\nall available models simultaneously, with weight interpolation-based methods\nbeing the predominant approaches. However, these conventional approaches are\nnot well-suited for scenarios where models become available sequentially, and\nthey often suffer from high memory requirements and potential interference\nbetween tasks. In this study, we propose a training-free projection-based\ncontinual merging method that processes models sequentially through orthogonal\nprojections of weight matrices and adaptive scaling mechanisms. Our method\noperates by projecting new parameter updates onto subspaces orthogonal to\nexisting merged parameter updates while using an adaptive scaling mechanism to\nmaintain stable parameter distances, enabling efficient sequential integration\nof task-specific knowledge. Our approach maintains constant memory complexity\nto the number of models, minimizes interference between tasks through\northogonal projections, and retains the performance of previously merged models\nthrough adaptive task vector scaling. Extensive experiments on CLIP-ViT models\ndemonstrate that our method achieves a 5-8% average accuracy improvement while\nmaintaining robust performance in different task orderings.\n","authors":["Anke Tang","Enneng Yang","Li Shen","Yong Luo","Han Hu","Bo Du","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2501.09522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09519v1","updated":"2025-01-16T13:09:37Z","published":"2025-01-16T13:09:37Z","title":"Multi-task deep-learning for sleep event detection and stage\n  classification","summary":"  Polysomnographic sleep analysis is the standard clinical method to accurately\ndiagnose and treat sleep disorders. It is an intricate process which involves\nthe manual identification, classification, and location of multiple sleep event\npatterns. This is complex, for which identification of different types of\nevents involves focusing on different subsets of signals, resulting on an\niterative time-consuming process entailing several visual analysis passes. In\nthis paper we propose a multi-task deep-learning approach for the simultaneous\ndetection of sleep events and hypnogram construction in one single pass. Taking\nas reference state-of-the-art methodology for object-detection in the field of\nComputer Vision, we reformulate the problem for the analysis of multi-variate\ntime sequences, and more specifically for pattern detection in the sleep\nanalysis scenario. We investigate the performance of the resulting method in\nidentifying different assembly combinations of EEG arousals, respiratory events\n(apneas and hypopneas) and sleep stages, also considering different input\nsignal montage configurations. Furthermore, we evaluate our approach using two\nindependent datasets, assessing true-generalization effects involving local and\nexternal validation scenarios. Based on our results, we analyze and discuss our\nmethod's capabilities and its potential wide-range applicability across\ndifferent settings and datasets.\n","authors":["Adriana Anido-Alonso","Diego Alvarez-Estevez"],"pdf_url":"https://arxiv.org/pdf/2501.09519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09512v1","updated":"2025-01-16T12:57:33Z","published":"2025-01-16T12:57:33Z","title":"PIER: A Novel Metric for Evaluating What Matters in Code-Switching","summary":"  Code-switching, the alternation of languages within a single discourse,\npresents a significant challenge for Automatic Speech Recognition. Despite the\nunique nature of the task, performance is commonly measured with established\nmetrics such as Word-Error-Rate (WER). However, in this paper, we question\nwhether these general metrics accurately assess performance on code-switching.\nSpecifically, using both Connectionist-Temporal-Classification and\nEncoder-Decoder models, we show fine-tuning on non-code-switched data from both\nmatrix and embedded language improves classical metrics on code-switching test\nsets, although actual code-switched words worsen (as expected). Therefore, we\npropose Point-of-Interest Error Rate (PIER), a variant of WER that focuses only\non specific words of interest. We instantiate PIER on code-switched utterances\nand show that this more accurately describes the code-switching performance,\nshowing huge room for improvement in future work. This focused evaluation\nallows for a more precise assessment of model performance, particularly in\nchallenging aspects such as inter-word and intra-word code-switching.\n","authors":["Enes Yavuz Ugan","Ngoc-Quan Pham","Leonard Bärmann","Alex Waibel"],"pdf_url":"https://arxiv.org/pdf/2501.09512v1.pdf","comment":"Accepted at ICASSP 2025"},{"id":"http://arxiv.org/abs/2408.10274v2","updated":"2025-01-16T12:45:17Z","published":"2024-08-17T10:53:06Z","title":"Benchmarking quantum machine learning kernel training for classification\n  tasks","summary":"  Quantum-enhanced machine learning is a rapidly evolving field that aims to\nleverage the unique properties of quantum mechanics to enhance classical\nmachine learning. However, the practical applicability of these methods remains\nan open question, particularly beyond the context of specifically-crafted toy\nproblems, and given the current limitations of quantum hardware. This study\nfocuses on quantum kernel methods in the context of classification tasks. In\nparticular, it examines the performance of Quantum Kernel Estimation (QKE) and\nQuantum Kernel Training (QKT) in connection with two quantum feature mappings,\nnamely ZZFeatureMap and CovariantFeatureMap. Remarkably, these feature maps\nhave been proposed in the literature under the conjecture of possible near-term\nquantum advantage and have shown promising performance in ad-hoc datasets. In\nthis study, we aim to evaluate their versatility and generalization\ncapabilities in a more general benchmark, encompassing both artificial and\nestablished reference datasets. Classical machine learning methods,\nspecifically Support Vector Machines (SVMs) and logistic regression, are also\nincorporated as baseline comparisons. Experimental results indicate that\nquantum methods exhibit varying performance across different datasets. Despite\noutperforming classical methods in ad-hoc datasets, mixed results are obtained\nfor the general case among standard classical benchmarks. Our experiments call\ninto question a general added value of applying QKT optimization, for which the\nadditional computational cost does not necessarily translate into improved\nclassification performance. Instead, it is suggested that a careful choice of\nthe quantum feature map in connection with proper hyperparameterization may\nprove more effective.\n","authors":["Diego Alvarez-Estevez"],"pdf_url":"https://arxiv.org/pdf/2408.10274v2.pdf","comment":"19 pages, 4 figures; extended experiments and datasets, fixed typos;\n  in consideration for publication in IEEE TQE"},{"id":"http://arxiv.org/abs/2501.09506v1","updated":"2025-01-16T12:38:49Z","published":"2025-01-16T12:38:49Z","title":"Multimodal Marvels of Deep Learning in Medical Diagnosis: A\n  Comprehensive Review of COVID-19 Detection","summary":"  This study presents a comprehensive review of the potential of multimodal\ndeep learning (DL) in medical diagnosis, using COVID-19 as a case example.\nMotivated by the success of artificial intelligence applications during the\nCOVID-19 pandemic, this research aims to uncover the capabilities of DL in\ndisease screening, prediction, and classification, and to derive insights that\nenhance the resilience, sustainability, and inclusiveness of science,\ntechnology, and innovation systems. Adopting a systematic approach, we\ninvestigate the fundamental methodologies, data sources, preprocessing steps,\nand challenges encountered in various studies and implementations. We explore\nthe architecture of deep learning models, emphasising their data-specific\nstructures and underlying algorithms. Subsequently, we compare different deep\nlearning strategies utilised in COVID-19 analysis, evaluating them based on\nmethodology, data, performance, and prerequisites for future research. By\nexamining diverse data types and diagnostic modalities, this research\ncontributes to scientific understanding and knowledge of the multimodal\napplication of DL and its effectiveness in diagnosis. We have implemented and\nanalysed 11 deep learning models using COVID-19 image, text, and speech (ie,\ncough) data. Our analysis revealed that the MobileNet model achieved the\nhighest accuracy of 99.97% for COVID-19 image data and 93.73% for speech data\n(i.e., cough). However, the BiGRU model demonstrated superior performance in\nCOVID-19 text classification with an accuracy of 99.89%. The broader\nimplications of this research suggest potential benefits for other domains and\ndisciplines that could leverage deep learning techniques for image, text, and\nspeech analysis.\n","authors":["Md Shofiqul Islama","Khondokar Fida Hasanc","Hasibul Hossain Shajeebd","Humayan Kabir Ranae","Md Saifur Rahmand","Md Munirul Hasanb","AKM Azadf","Ibrahim Abdullahg","Mohammad Ali Moni"],"pdf_url":"https://arxiv.org/pdf/2501.09506v1.pdf","comment":"43 pages"},{"id":"http://arxiv.org/abs/2404.06808v2","updated":"2025-01-16T12:38:39Z","published":"2024-04-10T07:55:10Z","title":"Formation-Controlled Dimensionality Reduction","summary":"  Dimensionality reduction represents the process of generating a low\ndimensional representation of high dimensional data. Motivated by the formation\ncontrol of mobile agents, we propose a nonlinear dynamical system for\ndimensionality reduction. The system consists of two parts; the control of\nneighbor points, addressing local structures, and the control of remote points,\naccounting for global structures.We also include a brief mathematical analysis\nof the model and its numerical procedure. Numerical experiments are performed\non both synthetic and real datasets and comparisons with existing models\ndemonstrate the soundness and effectiveness of the proposed model.\n","authors":["Taeuk Jeong","Yoon Mo Jung","Euntack Lee"],"pdf_url":"https://arxiv.org/pdf/2404.06808v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08201v3","updated":"2025-01-16T12:33:37Z","published":"2023-09-15T07:05:33Z","title":"Sparsity-Aware Distributed Learning for Gaussian Processes with Linear\n  Multiple Kernel","summary":"  Gaussian processes (GPs) stand as crucial tools in machine learning and\nsignal processing, with their effectiveness hinging on kernel design and\nhyper-parameter optimization. This paper presents a novel GP linear multiple\nkernel (LMK) and a generic sparsity-aware distributed learning framework to\noptimize the hyper-parameters. The newly proposed grid spectral mixture product\n(GSMP) kernel is tailored for multi-dimensional data, effectively reducing the\nnumber of hyper-parameters while maintaining good approximation capability. We\nfurther demonstrate that the associated hyper-parameter optimization of this\nkernel yields sparse solutions. To exploit the inherent sparsity of the\nsolutions, we introduce the Sparse LInear Multiple Kernel Learning (SLIM-KL)\nframework. The framework incorporates a quantized alternating direction method\nof multipliers (ADMM) scheme for collaborative learning among multiple agents,\nwhere the local optimization problem is solved using a distributed successive\nconvex approximation (DSCA) algorithm. SLIM-KL effectively manages large-scale\nhyper-parameter optimization for the proposed kernel, simultaneously ensuring\ndata privacy and minimizing communication costs. Theoretical analysis\nestablishes convergence guarantees for the learning framework, while\nexperiments on diverse datasets demonstrate the superior prediction performance\nand efficiency of our proposed methods.\n","authors":["Richard Cornelius Suwandi","Zhidi Lin","Feng Yin","Zhiguo Wang","Sergios Theodoridis"],"pdf_url":"https://arxiv.org/pdf/2309.08201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10142v2","updated":"2025-01-16T12:29:28Z","published":"2024-09-16T10:13:09Z","title":"AALF: Almost Always Linear Forecasting","summary":"  Recent works for time-series forecasting more and more leverage the high\npredictive power of Deep Learning models. With this increase in model\ncomplexity, however, comes a lack in understanding of the underlying model\ndecision process, which is problematic for high-stakes application scenarios.\nAt the same time, simple, interpretable forecasting methods such as ARIMA still\nperform very well, sometimes on-par, with Deep Learning approaches. We argue\nthat simple models are good enough most of the time, and that forecasting\nperformance could be improved by choosing a Deep Learning method only for few,\nimportant predictions, increasing the overall interpretability of the\nforecasting process. In this context, we propose a novel online model selection\nframework which learns to identify these predictions. An extensive empirical\nstudy on various real-world datasets shows that our selection methodology\nperforms comparable to state-of-the-art online model selections methods in most\ncases while being significantly more interpretable. We find that almost always\nchoosing a simple autoregressive linear model for forecasting results in\ncompetitive performance, suggesting that the need for opaque black-box models\nin time-series forecasting might be smaller than recent works would suggest.\n","authors":["Matthias Jakobs","Thomas Liebig"],"pdf_url":"https://arxiv.org/pdf/2409.10142v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.09377v3","updated":"2025-01-16T12:12:24Z","published":"2023-06-15T08:18:29Z","title":"Evaluating alignment between humans and neural network representations\n  in image-based learning tasks","summary":"  Humans represent scenes and objects in rich feature spaces, carrying\ninformation that allows us to generalise about category memberships and\nabstract functions with few examples. What determines whether a neural network\nmodel generalises like a human? We tested how well the representations of $86$\npretrained neural network models mapped to human learning trajectories across\ntwo tasks where humans had to learn continuous relationships and categories of\nnatural images. In these tasks, both human participants and neural networks\nsuccessfully identified the relevant stimulus features within a few trials,\ndemonstrating effective generalisation. We found that while training dataset\nsize was a core determinant of alignment with human choices, contrastive\ntraining with multi-modal data (text and imagery) was a common feature of\ncurrently publicly available models that predicted human generalisation.\nIntrinsic dimensionality of representations had different effects on alignment\nfor different model types. Lastly, we tested three sets of human-aligned\nrepresentations and found no consistent improvements in predictive accuracy\ncompared to the baselines. In conclusion, pretrained neural networks can serve\nto extract representations for cognitive models, as they appear to capture some\nfundamental aspects of cognition that are transferable across tasks. Both our\nparadigms and modelling approach offer a novel way to quantify alignment\nbetween neural networks and humans and extend cognitive science into more\nnaturalistic domains.\n","authors":["Can Demircan","Tankred Saanum","Leonardo Pettini","Marcel Binz","Blazej M Baczkowski","Christian F Doeller","Mona M Garvert","Eric Schulz"],"pdf_url":"https://arxiv.org/pdf/2306.09377v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08443v2","updated":"2025-01-16T12:06:35Z","published":"2024-12-26T05:41:31Z","title":"Instruction-Guided Fusion of Multi-Layer Visual Features in Large\n  Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) have achieved significant success in\nmultimodal tasks by combining pre-trained vision encoders and large language\nmodels. However, current LVLMs mainly rely on features from the final layers of\nthe vision encoder, neglecting complementary information in shallower layers.\nWhile recent methods have explored multi-layer features, they are often\ntask-agnostic. We investigate the contributions of visual features from\ndifferent encoder layers across 18 benchmarks and 6 task categories. Our\nresults show that multi-layer features provide complementary strengths with\nvarying task dependencies, and uniform fusion performs suboptimally. Based on\nthese findings, we propose an instruction-guided vision aggregator that\ndynamically integrates multi-layer features based on textual instructions,\nwithout increasing the number of visual tokens. Extensive evaluations show\nsuperior performance, and analysis reveals the dominance of mid-to-high-level\nfeatures in semantic tasks and the critical role of low-level features in\nfine-grained perception. This work provides valuable insights into the adaptive\nuse of hierarchical visual features in LVLMs, advancing more flexible\nmultimodal systems.\n","authors":["Xu Li","Yi Zheng","Haotian Chen","Xiaolei Chen","Yuxuan Liang","Chenghang Lai","Bin Li","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2501.08443v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04613v3","updated":"2025-01-16T12:05:26Z","published":"2024-02-07T06:30:39Z","title":"Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in\n  Reproducing Kernel Hilbert Spaces","summary":"  Commonly used $f$-divergences of measures, e.g., the Kullback-Leibler\ndivergence, are subject to limitations regarding the support of the involved\nmeasures. A remedy is regularizing the $f$-divergence by a squared maximum mean\ndiscrepancy (MMD) associated with a characteristic kernel $K$. We use the\nkernel mean embedding to show that this regularization can be rewritten as the\nMoreau envelope of some function on the associated reproducing kernel Hilbert\nspace. Then, we exploit well-known results on Moreau envelopes in Hilbert\nspaces to analyze the MMD-regularized $f$-divergences, particularly their\ngradients. Subsequently, we use our findings to analyze Wasserstein gradient\nflows of MMD-regularized $f$-divergences. We provide proof-of-the-concept\nnumerical examples for flows starting from empirical measures. Here, we cover\n$f$-divergences with infinite and finite recession constants. Lastly, we extend\nour results to the tight variational formulation of $f$-divergences and\nnumerically compare the resulting flows.\n","authors":["Viktor Stein","Sebastian Neumayer","Nicolaj Rux","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2402.04613v3.pdf","comment":"56 pages, 14 figures, 3 tables. Comments welcome! NEW: Incorporated\n  Reviewers' suggestions, added FISTA and tight formulation"},{"id":"http://arxiv.org/abs/2407.16485v3","updated":"2025-01-16T11:59:02Z","published":"2024-07-23T14:00:18Z","title":"Learning Constraint Network from Demonstrations via Positive-Unlabeled\n  Learning with Memory Replay","summary":"  Planning for a wide range of real-world tasks necessitates to know and write\nall constraints. However, instances exist where these constraints are either\nunknown or challenging to specify accurately. A possible solution is to infer\nthe unknown constraints from expert demonstration. The majority of prior works\nlimit themselves to learning simple linear constraints, or require strong\nknowledge of the true constraint parameterization or environmental model. To\nmitigate these problems, this paper presents a positive-unlabeled (PU) learning\napproach to infer a continuous, arbitrary and possibly nonlinear, constraint\nfrom demonstration. From a PU learning view, We treat all data in\ndemonstrations as positive (feasible) data, and learn a (sub)-optimal policy to\ngenerate high-reward-winning but potentially infeasible trajectories, which\nserve as unlabeled data containing both feasible and infeasible states. Under\nan assumption on data distribution, a feasible-infeasible classifier (i.e.,\nconstraint model) is learned from the two datasets through a postprocessing PU\nlearning technique. The entire method employs an iterative framework\nalternating between updating the policy, which generates and selects\nhigher-reward policies, and updating the constraint model. Additionally, a\nmemory buffer is introduced to record and reuse samples from previous\niterations to prevent forgetting. The effectiveness of the proposed method is\nvalidated in two Mujoco environments, successfully inferring continuous\nnonlinear constraints and outperforming a baseline method in terms of\nconstraint accuracy and policy safety.\n","authors":["Baiyu Peng","Aude Billard"],"pdf_url":"https://arxiv.org/pdf/2407.16485v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09481v1","updated":"2025-01-16T11:35:22Z","published":"2025-01-16T11:35:22Z","title":"MonoSOWA: Scalable monocular 3D Object detector Without human\n  Annotations","summary":"  Detecting the three-dimensional position and orientation of objects using a\nsingle RGB camera is a foundational task in computer vision with many important\napplications. Traditionally, 3D object detection methods are trained in a\nfully-supervised setup, requiring vast amounts of human annotations, which are\nlaborious, costly, and do not scale well with the ever-increasing amounts of\ndata being captured.\n  In this paper, we present the first method to train 3D object detectors for\nmonocular RGB cameras without domain-specific human annotations, thus making\norders of magnitude more data available for training. Thanks to newly proposed\nCanonical Object Space, the method can not only exploit data across a variety\nof datasets and camera setups to train a single 3D detector, but unlike\nprevious work it also works out of the box in previously unseen camera setups.\nAll this is crucial for practical applications, where the data and cameras are\nextremely heterogeneous.\n  The method is evaluated on two standard autonomous driving datasets, where it\noutperforms previous works, which, unlike our method, still rely on 2D human\nannotations.\n","authors":["Jan Skvrna","Lukas Neumann"],"pdf_url":"https://arxiv.org/pdf/2501.09481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09480v1","updated":"2025-01-16T11:32:03Z","published":"2025-01-16T11:32:03Z","title":"Utilizing AI Language Models to Identify Prognostic Factors for Coronary\n  Artery Disease: A Study in Mashhad Residents","summary":"  Abstract: Background: Understanding cardiovascular artery disease risk\nfactors, the leading global cause of mortality, is crucial for influencing its\netiology, prevalence, and treatment. This study aims to evaluate prognostic\nmarkers for coronary artery disease in Mashhad using Naive Bayes, REP Tree,\nJ48, CART, and CHAID algorithms. Methods:\n  Using data from the 2009 MASHAD STUDY, prognostic factors for coronary artery\ndisease were determined with Naive Bayes, REP Tree, J48, CART, CHAID, and\nRandom Forest algorithms using R 3.5.3 and WEKA 3.9.4. Model efficiency was\ncompared by sensitivity, specificity, and accuracy. Cases were patients with\ncoronary artery disease; each had three controls (totally 940). Results:\nPrognostic factors for coronary artery disease in Mashhad residents varied by\nalgorithm. CHAID identified age, myocardial infarction history, and\nhypertension. CART included depression score and physical activity. REP added\neducation level and anxiety score. NB included diabetes and family history. J48\nhighlighted father's heart disease and weight loss. CHAID had the highest\naccuracy (0.80).\n  Conclusion:\n  Key prognostic factors for coronary artery disease in CART and CHAID models\ninclude age, myocardial infarction history, hypertension, depression score,\nphysical activity, and BMI. NB, REP Tree, and J48 identified numerous factors.\nCHAID had the highest accuracy, sensitivity, and specificity. CART offers\nsimpler interpretation, aiding physician and paramedic model selection based on\nspecific. Keywords: RF, Na\\\"ive Bayes, REP, J48 algorithms, Coronary Artery\nDisease (CAD).\n","authors":["Bami Zahra","Behnampour Nasser","Doosti Hassan","Ghayour Mobarhan Majid"],"pdf_url":"https://arxiv.org/pdf/2501.09480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22944v2","updated":"2025-01-16T11:26:02Z","published":"2024-10-30T12:01:48Z","title":"Focus On This, Not That! Steering LLMs With Adaptive Feature\n  Specification","summary":"  Despite the success of Instruction Tuning (IT) in training large language\nmodels (LLMs) to perform arbitrary user-specified tasks, these models often\nstill leverage spurious or biased features learned from their training data,\nleading to undesired behaviours when deploying them in new contexts. In this\nwork, we introduce Focus Instruction Tuning (FIT), which trains LLMs to\ncondition their responses by focusing on specific features whilst ignoring\nothers, leading to different behaviours based on what features are specified.\nAcross several experimental settings, we show that focus-tuned models can be\nadaptively steered by focusing on different features at inference-time: for\ninstance, robustness can be improved by focusing on task-causal features and\nignoring spurious features, and social bias can be mitigated by ignoring\ndemographic categories. Furthermore, FIT can steer behaviour in new contexts,\ngeneralising under distribution shift and to new unseen features at inference\ntime, and thereby facilitating more robust, fair, and controllable LLM\napplications in real-world environments.\n","authors":["Tom A. Lamb","Adam Davies","Alasdair Paren","Philip H. S. Torr","Francesco Pinto"],"pdf_url":"https://arxiv.org/pdf/2410.22944v2.pdf","comment":"28pages, 14 figures"},{"id":"http://arxiv.org/abs/2209.04747v6","updated":"2025-01-16T11:17:04Z","published":"2022-09-10T22:00:30Z","title":"Diffusion Models in Vision: A Survey","summary":"  Denoising diffusion models represent a recent emerging topic in computer\nvision, demonstrating remarkable results in the area of generative modeling. A\ndiffusion model is a deep generative model that is based on two stages, a\nforward diffusion stage and a reverse diffusion stage. In the forward diffusion\nstage, the input data is gradually perturbed over several steps by adding\nGaussian noise. In the reverse stage, a model is tasked at recovering the\noriginal input data by learning to gradually reverse the diffusion process,\nstep by step. Diffusion models are widely appreciated for the quality and\ndiversity of the generated samples, despite their known computational burdens,\ni.e. low speeds due to the high number of steps involved during sampling. In\nthis survey, we provide a comprehensive review of articles on denoising\ndiffusion models applied in vision, comprising both theoretical and practical\ncontributions in the field. First, we identify and present three generic\ndiffusion modeling frameworks, which are based on denoising diffusion\nprobabilistic models, noise conditioned score networks, and stochastic\ndifferential equations. We further discuss the relations between diffusion\nmodels and other deep generative models, including variational auto-encoders,\ngenerative adversarial networks, energy-based models, autoregressive models and\nnormalizing flows. Then, we introduce a multi-perspective categorization of\ndiffusion models applied in computer vision. Finally, we illustrate the current\nlimitations of diffusion models and envision some interesting directions for\nfuture research.\n","authors":["Florinel-Alin Croitoru","Vlad Hondru","Radu Tudor Ionescu","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2209.04747v6.pdf","comment":"Accepted in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence. 25 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.19320v4","updated":"2025-01-16T11:11:30Z","published":"2024-07-27T18:33:10Z","title":"WindsorML: High-Fidelity Computational Fluid Dynamics Dataset For\n  Automotive Aerodynamics","summary":"  This paper presents a new open-source high-fidelity dataset for Machine\nLearning (ML) containing 355 geometric variants of the Windsor body, to help\nthe development and testing of ML surrogate models for external automotive\naerodynamics. Each Computational Fluid Dynamics (CFD) simulation was run with a\nGPU-native high-fidelity Wall-Modeled Large-Eddy Simulations (WMLES) using a\nCartesian immersed-boundary method using more than 280M cells to ensure the\ngreatest possible accuracy. The dataset contains geometry variants that\nexhibits a wide range of flow characteristics that are representative of those\nobserved on road-cars. The dataset itself contains the 3D time-averaged volume\n& boundary data as well as the geometry and force & moment coefficients. This\npaper discusses the validation of the underlying CFD methods as well as\ncontents and structure of the dataset. To the authors knowledge, this\nrepresents the first, large-scale high-fidelity CFD dataset for the Windsor\nbody with a permissive open-source license (CC-BY-SA).\n","authors":["Neil Ashton","Jordan B. Angel","Aditya S. Ghate","Gaetan K. W. Kenway","Man Long Wong","Cetin Kiris","Astrid Walle","Danielle C. Maddix","Gary Page"],"pdf_url":"https://arxiv.org/pdf/2407.19320v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09469v1","updated":"2025-01-16T11:10:38Z","published":"2025-01-16T11:10:38Z","title":"Predicting Air Temperature from Volumetric Urban Morphology with Machine\n  Learning","summary":"  In this study, we firstly introduce a method that converts CityGML data into\nvoxels which works efficiently and fast in high resolution for large scale\ndatasets such as cities but by sacrificing some building details to overcome\nthe limitations of previous voxelization methodologies that have been\ncomputationally intensive and inefficient at transforming large-scale urban\nareas into voxel representations for high resolution. Those voxelized 3D city\ndata from multiple cities and corresponding air temperature data are used to\ndevelop a machine learning model. Before the model training, Gaussian blurring\nis implemented on input data to consider spatial relationships, as a result the\ncorrelation rate between air temperature and volumetric building morphology is\nalso increased after the Gaussian blurring. After the model training, the\nprediction results are not just evaluated with Mean Square Error (MSE) but some\nimage similarity metrics such as Structural Similarity Index Measure (SSIM) and\nLearned Perceptual Image Patch Similarity (LPIPS) that are able to detect and\nconsider spatial relations during the evaluation process. This trained model is\ncapable of predicting the spatial distribution of air temperature by using\nbuilding volume information of corresponding pixel as input. By doing so, this\nresearch aims to assist urban planners in incorporating environmental\nparameters into their planning strategies, thereby facilitating more\nsustainable and inhabitable urban environments.\n","authors":["Berk Kıvılcım","Patrick Erik Bradley"],"pdf_url":"https://arxiv.org/pdf/2501.09469v1.pdf","comment":"30 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2501.09464v1","updated":"2025-01-16T10:55:05Z","published":"2025-01-16T10:55:05Z","title":"Pruning for Sparse Diffusion Models based on Gradient Flow","summary":"  Diffusion Models (DMs) have impressive capabilities among generation models,\nbut are limited to slower inference speeds and higher computational costs.\nPrevious works utilize one-shot structure pruning to derive lightweight DMs\nfrom pre-trained ones, but this approach often leads to a significant drop in\ngeneration quality and may result in the removal of crucial weights. Thus we\npropose a iterative pruning method based on gradient flow, including the\ngradient flow pruning process and the gradient flow pruning criterion. We\nemploy a progressive soft pruning strategy to maintain the continuity of the\nmask matrix and guide it along the gradient flow of the energy function based\non the pruning criterion in sparse space, thereby avoiding the sudden\ninformation loss typically caused by one-shot pruning. Gradient-flow based\ncriterion prune parameters whose removal increases the gradient norm of loss\nfunction and can enable fast convergence for a pruned model in iterative\npruning stage. Our extensive experiments on widely used datasets demonstrate\nthat our method achieves superior performance in efficiency and consistency\nwith pre-trained models.\n","authors":["Ben Wan","Tianyi Zheng","Zhaoyu Chen","Yuxiao Wang","Jia Wang"],"pdf_url":"https://arxiv.org/pdf/2501.09464v1.pdf","comment":"5 pages, 1 figure, accepted by ICASSP2025"},{"id":"http://arxiv.org/abs/2406.19185v2","updated":"2025-01-16T10:54:59Z","published":"2024-06-27T14:03:49Z","title":"Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a\n  supervised-friendly fashion","summary":"  Reinforcement Learning (RL) has been used to finetune Large Language Models\n(LLMs) using a reward model trained from preference data, to better align with\nhuman judgment. The recently introduced direct alignment methods, which are\noften simpler, more stable, and computationally lighter, can more directly\nachieve this. However, these approaches cannot optimize arbitrary rewards, and\nthe preference-based ones are not the only rewards of interest for LLMs (eg.,\nunit tests for code generation or textual entailment for summarization, among\nothers). RL-finetuning is usually done with a variation of policy gradient,\nwhich calls for on-policy or near-on-policy samples, requiring costly\ngenerations. We introduce Contrastive Policy Gradient, or CoPG, a simple and\nmathematically principled new RL algorithm that can estimate the optimal policy\neven from off-policy data. It can be seen as an off-policy policy gradient\napproach that does not rely on important sampling techniques and highlights the\nimportance of using (the right) state baseline. We show this approach to\ngeneralize the direct alignment method IPO (identity preference optimization)\nand classic policy gradient. We experiment with the proposed CoPG on a toy\nbandit problem to illustrate its properties, as well as for finetuning LLMs on\na summarization task, using a learned reward function considered as ground\ntruth for the purpose of the experiments.\n","authors":["Yannis Flet-Berliac","Nathan Grinsztajn","Florian Strub","Bill Wu","Eugene Choi","Chris Cremer","Arash Ahmadian","Yash Chandak","Mohammad Gheshlaghi Azar","Olivier Pietquin","Matthieu Geist"],"pdf_url":"https://arxiv.org/pdf/2406.19185v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.07708v3","updated":"2025-01-16T10:46:57Z","published":"2024-09-12T02:25:04Z","title":"Dataset-Free Weight-Initialization on Restricted Boltzmann Machine","summary":"  In feed-forward neural networks, dataset-free weight-initialization methods\nsuch as LeCun, Xavier (or Glorot), and He initializations have been developed.\nThese methods randomly determine the initial values of weight parameters based\non specific distributions (e.g., Gaussian or uniform distributions) without\nusing training datasets. To the best of the authors' knowledge, such a\ndataset-free weight-initialization method is yet to be developed for restricted\nBoltzmann machines (RBMs), which are probabilistic neural networks consisting\nof two layers. In this study, we derive a dataset-free weight-initialization\nmethod for Bernoulli--Bernoulli RBMs based on statistical mechanical analysis.\nIn the proposed weight-initialization method, the weight parameters are drawn\nfrom a Gaussian distribution with zero mean. The standard deviation of the\nGaussian distribution is optimized based on our hypothesis that a standard\ndeviation providing a larger layer correlation (LC) between the two layers\nimproves the learning efficiency. The expression of the LC is derived based on\na statistical mechanical analysis. The optimal value of the standard deviation\ncorresponds to the maximum point of the LC. The proposed weight-initialization\nmethod is identical to Xavier initialization in a specific case (i.e., when the\nsizes of the two layers are the same, the random variables of the layers are\n$\\{-1,1\\}$-binary, and all bias parameters are zero). The validity of the\nproposed weight-initialization method is demonstrated in numerical experiments\nusing a toy and real-world datasets.\n","authors":["Muneki Yasuda","Ryosuke Maeno","Chako Takahashi"],"pdf_url":"https://arxiv.org/pdf/2409.07708v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22801v2","updated":"2025-01-16T10:42:39Z","published":"2024-10-30T08:30:46Z","title":"Machine Learning Nonadiabatic Dynamics: Eliminating Phase Freedom of\n  Nonadiabatic Couplings with the State-Intraction State-Averaged\n  Spin-Restricted Ensemble-Referenced Kohn-Sham Approach","summary":"  Excited-state molecular dynamics (ESMD) simulations near conical\nintersections (CIs) pose significant challenges when using machine learning\npotentials (MLPs). Although MLPs have gained recognition for their integration\ninto mixed quantum-classical (MQC) methods, such as trajectory surface hopping\n(TSH), and their capacity to model correlated electron-nuclear dynamics\nefficiently, difficulties persist in managing nonadiabatic dynamics.\nSpecifically, singularities at CIs and double-valued coupling elements result\nin discontinuities that disrupt the smoothness of predictive functions. Partial\nsolutions have been provided by learning diabatic Hamiltonians with phaseless\nloss functions to these challenges. However, a definitive method for addressing\nthe discontinuities caused by CIs and double-valued coupling elements has yet\nto be developed. Here, we introduce the phaseless coupling term, $\\Delta^2$,\nderived from the square of the off-diagonal elements of the diabatic\nHamiltonian in the state-interaction state-averaged spin-restricted\nensemble-referenced Kohn-Sham (SI-SA-REKS, briefly SSR)(2,2) formalism. This\napproach improves the stability and accuracy of the MLP model by addressing the\nissues arising from CI singularities and double-valued coupling functions. We\napply this method to the penta-2,4-dieniminium cation (PSB3), demonstrating its\neffectiveness in improving MLP training for ML-based nonadiabatic dynamics. Our\nresults show that the $\\Delta^2$ based ML-ESMD method can reproduce ab initio\nESMD simulations, underscoring its potential and efficiency for broader\napplications, particularly in large-scale and long-timescale ESMD simulations.\n","authors":["Sung Wook Moon","Soohaeng Yoo Willow","Tae Hyeon Park","Seung Kyu Min","Chang Woo Myung"],"pdf_url":"https://arxiv.org/pdf/2410.22801v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09459v1","updated":"2025-01-16T10:37:07Z","published":"2025-01-16T10:37:07Z","title":"Teaching Wav2Vec2 the Language of the Brain","summary":"  The decoding of continuously spoken speech from neuronal activity has the\npotential to become an important clinical solution for paralyzed patients. Deep\nLearning Brain Computer Interfaces (BCIs) have recently successfully mapped\nneuronal activity to text contents in subjects who attempted to formulate\nspeech. However, only small BCI datasets are available. In contrast, labeled\ndata and pre-trained models for the closely related task of speech recognition\nfrom audio are widely available. One such model is Wav2Vec2 which has been\ntrained in a self-supervised fashion to create meaningful representations of\nspeech audio data. In this study, we show that patterns learned by Wav2Vec2 are\ntransferable to brain data. Specifically, we replace its audio feature\nextractor with an untrained Brain Feature Extractor (BFE) model. We then\nexecute full fine-tuning with pre-trained weights for Wav2Vec2, training ''from\nscratch'' without pre-trained weights as well as freezing a pre-trained\nWav2Vec2 and training only the BFE each for 45 different BFE architectures.\nAcross these experiments, the best run is from full fine-tuning with\npre-trained weights, achieving a Character Error Rate (CER) of 18.54\\%,\noutperforming the best training from scratch run by 20.46\\% and that of frozen\nWav2Vec2 training by 15.92\\% percentage points. These results indicate that\nknowledge transfer from audio speech recognition to brain decoding is possible\nand significantly improves brain decoding performance for the same\narchitectures. Related source code is available at\nhttps://github.com/tfiedlerdev/Wav2Vec2ForBrain.\n","authors":["Tobias Fiedler","Leon Hermann","Florian Müller","Sarel Cohen","Peter Chin","Tobias Friedrich","Eilon Vaadia"],"pdf_url":"https://arxiv.org/pdf/2501.09459v1.pdf","comment":"Paper was submitted to ICASSP 2025 but marginally rejected"},{"id":"http://arxiv.org/abs/2408.01622v2","updated":"2025-01-16T10:30:40Z","published":"2024-08-03T01:09:48Z","title":"Positive-Unlabeled Constraint Learning for Inferring Nonlinear\n  Continuous Constraints Functions from Expert Demonstrations","summary":"  Planning for diverse real-world robotic tasks necessitates to know and write\nall constraints. However, instances exist where these constraints are either\nunknown or challenging to specify accurately. A possible solution is to infer\nthe unknown constraints from expert demonstration. This paper presents a novel\ntwo-step Positive-Unlabeled Constraint Learning (PUCL) algorithm to infer a\ncontinuous constraint function from demonstrations, without requiring prior\nknowledge of the true constraint parameterization or environmental model as\nexisting works. We treat all data in demonstrations as positive (feasible)\ndata, and learn a control policy to generate potentially infeasible\ntrajectories, which serve as unlabeled data. The proposed two-step learning\nframework first identifies reliable infeasible data using a distance metric,\nand secondly learns a binary feasibility classifier (i.e., constraint function)\nfrom the feasible demonstrations and reliable infeasible data. The proposed\nmethod is flexible to learn complex-shaped constraint boundary and will not\nmistakenly classify demonstrations as infeasible as previous methods. The\neffectiveness of the proposed method is verified in four constrained\nenvironments, using a networked policy or a dynamical system policy. It\nsuccessfully infers the continuous nonlinear constraints and outperforms other\nbaseline methods in terms of constraint accuracy and policy safety. This work\nhas been published in IEEE Robotics and Automation Letters (RA-L). Please refer\nto the final version at https://doi.org/10.1109/LRA.2024.3522756\n","authors":["Baiyu Peng","Aude Billard"],"pdf_url":"https://arxiv.org/pdf/2408.01622v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19106v2","updated":"2025-01-16T10:29:53Z","published":"2024-12-26T07:48:47Z","title":"ERGNN: Spectral Graph Neural Network With Explicitly-Optimized Rational\n  Graph Filters","summary":"  Approximation-based spectral graph neural networks, which construct graph\nfilters with function approximation, have shown substantial performance in\ngraph learning tasks. Despite their great success, existing works primarily\nemploy polynomial approximation to construct the filters, whereas another\nsuperior option, namely ration approximation, remains underexplored. Although a\nhandful of prior works have attempted to deploy the rational approximation,\ntheir implementations often involve intensive computational demands or still\nresort to polynomial approximations, hindering full potential of the rational\ngraph filters. To address the issues, this paper introduces ERGNN, a novel\nspectral GNN with explicitly-optimized rational filter. ERGNN adopts a unique\ntwo-step framework that sequentially applies the numerator filter and the\ndenominator filter to the input signals, thus streamlining the model paradigm\nwhile enabling explicit optimization of both numerator and denominator of the\nrational filter. Extensive experiments validate the superiority of ERGNN over\nstate-of-the-art methods, establishing it as a practical solution for deploying\nrational-based GNNs.\n","authors":["Guoming Li","Jian Yang","Shangsong Liang"],"pdf_url":"https://arxiv.org/pdf/2412.19106v2.pdf","comment":"Accepted in 2025 IEEE International Conference on Acoustics, Speech,\n  and Signal Processing, ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.09444v1","updated":"2025-01-16T10:17:58Z","published":"2025-01-16T10:17:58Z","title":"Solving the unsolvable: Translating case law in Hong Kong","summary":"  This paper addresses the challenges translating case law under Hong Kong's\nbilingual legal system. It highlights the initial success of translating all\nwritten statutes into Chinese before the 1997 handover, a task mandated by the\nBasic Law. The effort involved significant collaboration among legal,\nlinguistic, and translation experts, resulting in a comprehensive and\nculturally appropriate bilingual legal system. However, translating case law\nremains a significant challenge due to the sheer volume and continuous growth\nof judicial decisions. The paper critiques the governments and judiciarys\nsporadic and uncoordinated efforts to translate case law, contrasting it with\nthe thorough approach previously taken for statute translation. Although the\ngovernment acknowledges the importance of legal bilingualism, it lacks a\nsustainable strategy for translating case law. The Judiciarys position that\ntranslating all judgments is unnecessary, unrealistic, and not cost-effectiveis\nanalyzed and critiqued for its impact on legal transparency and public trust. A\nproposed solution involves leveraging machine translation technology through a\nhuman-machine interactive translation platform, which undergoes two major\ntransitions. Initially based on a neural model, the platform transitions to\nusing a large language model for improved translation accuracy. Furthermore, it\nevolves from a single-agent system to a multi-agent system, incorporating\nTranslator, Annotator, and Proofreader agents. This multi-agent approach,\nsupported by a grant, aims to facilitate efficient, high-quality translation of\njudicial judgments by integrating advanced artificial intelligence and\ncontinuous feedback mechanisms, thus better meeting the needs of a bilingual\nlegal system.\n","authors":["King-kui Sin","Xi Xuan","Chunyu Kit","Clara Ho-yan Chan","Honic Ho-kin Ip"],"pdf_url":"https://arxiv.org/pdf/2501.09444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07700v2","updated":"2025-01-16T10:02:59Z","published":"2025-01-13T21:24:15Z","title":"An Adaptive Collocation Point Strategy For Physics Informed Neural\n  Networks via the QR Discrete Empirical Interpolation Method","summary":"  Physics-informed neural networks (PINNs) have gained significant attention\nfor solving forward and inverse problems related to partial differential\nequations (PDEs). While advancements in loss functions and network\narchitectures have improved PINN accuracy, the impact of collocation point\nsampling on their performance remains underexplored. Fixed sampling methods,\nsuch as uniform random sampling and equispaced grids, can fail to capture\ncritical regions with high solution gradients, limiting their effectiveness for\ncomplex PDEs. Adaptive methods, inspired by adaptive mesh refinement from\ntraditional numerical methods, address this by dynamically updating collocation\npoints during training but may overlook residual dynamics between updates,\npotentially losing valuable information. To overcome this limitation, we\npropose an adaptive collocation point selection strategy utilizing the QR\nDiscrete Empirical Interpolation Method (QR-DEIM), a reduced-order modeling\ntechnique for efficiently approximating nonlinear functions. Our results on\nbenchmark PDEs, including the wave, Allen-Cahn, and Burgers' equations,\ndemonstrate that our QR-DEIM-based approach improves PINN accuracy compared to\nexisting methods, offering a promising direction for adaptive collocation point\nstrategies.\n","authors":["Adrian Celaya","David Fuentes","Beatrice Riviere"],"pdf_url":"https://arxiv.org/pdf/2501.07700v2.pdf","comment":"Submitted to ICML 2025. Under review"},{"id":"http://arxiv.org/abs/2501.09429v1","updated":"2025-01-16T09:58:24Z","published":"2025-01-16T09:58:24Z","title":"ADAGE: A generic two-layer framework for adaptive agent based modelling","summary":"  Agent-based models (ABMs) are valuable for modelling complex, potentially\nout-of-equilibria scenarios. However, ABMs have long suffered from the Lucas\ncritique, stating that agent behaviour should adapt to environmental changes.\nFurthermore, the environment itself often adapts to these behavioural changes,\ncreating a complex bi-level adaptation problem. Recent progress integrating\nmulti-agent reinforcement learning into ABMs introduces adaptive agent\nbehaviour, beginning to address the first part of this critique, however, the\napproaches are still relatively ad hoc, lacking a general formulation, and\nfurthermore, do not tackle the second aspect of simultaneously adapting\nenvironmental level characteristics in addition to the agent behaviours. In\nthis work, we develop a generic two-layer framework for ADaptive AGEnt based\nmodelling (ADAGE) for addressing these problems. This framework formalises the\nbi-level problem as a Stackelberg game with conditional behavioural policies,\nproviding a consolidated framework for adaptive agent-based modelling based on\nsolving a coupled set of non-linear equations. We demonstrate how this generic\napproach encapsulates several common (previously viewed as distinct) ABM tasks,\nsuch as policy design, calibration, scenario generation, and robust behavioural\nlearning under one unified framework. We provide example simulations on\nmultiple complex economic and financial environments, showing the strength of\nthe novel framework under these canonical settings, addressing long-standing\ncritiques of traditional ABMs.\n","authors":["Benjamin Patrick Evans","Sihan Zeng","Sumitra Ganesh","Leo Ardon"],"pdf_url":"https://arxiv.org/pdf/2501.09429v1.pdf","comment":"Accepted at the 2025 International Conference on Autonomous Agents\n  and Multiagent Systems (AAMAS)"},{"id":"http://arxiv.org/abs/2501.09420v1","updated":"2025-01-16T09:47:18Z","published":"2025-01-16T09:47:18Z","title":"Dynamic Neural Style Transfer for Artistic Image Generation using VGG19","summary":"  Throughout history, humans have created remarkable works of art, but\nartificial intelligence has only recently started to make strides in generating\nvisually compelling art. Breakthroughs in the past few years have focused on\nusing convolutional neural networks (CNNs) to separate and manipulate the\ncontent and style of images, applying texture synthesis techniques.\nNevertheless, a number of current techniques continue to encounter obstacles,\nincluding lengthy processing times, restricted choices of style images, and the\ninability to modify the weight ratio of styles. We proposed a neural style\ntransfer system that can add various artistic styles to a desired image to\naddress these constraints allowing flexible adjustments to style weight ratios\nand reducing processing time. The system uses the VGG19 model for feature\nextraction, ensuring high-quality, flexible stylization without compromising\ncontent integrity.\n","authors":["Kapil Kashyap","Mehak Garg","Sean Fargose","Sindhu Nair"],"pdf_url":"https://arxiv.org/pdf/2501.09420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09412v1","updated":"2025-01-16T09:38:39Z","published":"2025-01-16T09:38:39Z","title":"FASP: Fast and Accurate Structured Pruning of Large Language Models","summary":"  The rapid increase in the size of large language models (LLMs) has\nsignificantly escalated their computational and memory demands, posing\nchallenges for efficient deployment, especially on resource-constrained\ndevices. Structured pruning has emerged as an effective model compression\nmethod that can reduce these demands while preserving performance. In this\npaper, we introduce FASP (Fast and Accurate Structured Pruning), a novel\nstructured pruning framework for LLMs that emphasizes both speed and accuracy.\nFASP employs a distinctive pruning structure that interlinks sequential layers,\nallowing for the removal of columns in one layer while simultaneously\neliminating corresponding rows in the preceding layer without incurring\nadditional performance loss. The pruning metric, inspired by Wanda, is\ncomputationally efficient and effectively selects components to prune.\nAdditionally, we propose a restoration mechanism that enhances model fidelity\nby adjusting the remaining weights post-pruning. We evaluate FASP on the OPT\nand LLaMA model families, demonstrating superior performance in terms of\nperplexity and accuracy on downstream tasks compared to state-of-the-art\nmethods. Our approach achieves significant speed-ups, pruning models such as\nOPT-125M in 17 seconds and LLaMA-30B in 15 minutes on a single NVIDIA RTX 4090\nGPU, making it a highly practical solution for optimizing LLMs.\n","authors":["Hanyu Hu","Pengxiang Zhao","Ping Li","Yi Zheng","Zhefeng Wang","Xiaoming Yuan"],"pdf_url":"https://arxiv.org/pdf/2501.09412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09410v1","updated":"2025-01-16T09:36:32Z","published":"2025-01-16T09:36:32Z","title":"MoE$^2$: Optimizing Collaborative Inference for Edge Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of natural language processing tasks. Exploiting the heterogeneous\ncapabilities of edge LLMs is crucial for diverse emerging applications, as it\nenables greater cost-effectiveness and reduced latency. In this work, we\nintroduce \\textit{Mixture-of-Edge-Experts (MoE$^2$)}, a novel collaborative\ninference framework for edge LLMs. We formulate the joint gating and expert\nselection problem to optimize inference performance under energy and latency\nconstraints. Unlike conventional MoE problems, LLM expert selection is\nsignificantly more challenging due to the combinatorial nature and the\nheterogeneity of edge LLMs across various attributes. To this end, we propose a\ntwo-level expert selection mechanism through which we uncover an\noptimality-preserving property of gating parameters across expert selections.\nThis property enables the decomposition of the training and selection\nprocesses, significantly reducing complexity. Furthermore, we leverage the\nobjective's monotonicity and design a discrete monotonic optimization algorithm\nfor optimal expert selection. We implement edge servers with NVIDIA Jetson AGX\nOrins and NVIDIA RTX 4090 GPUs, and perform extensive experiments. Our results\nvalidate that performance improvements of various LLM models and show that our\nMoE$^2$ method can achieve optimal trade-offs among different delay and energy\nbudgets, and outperforms baselines under various system resource constraints.\n","authors":["Lyudong Jin","Yanning Zhang","Yanhan Li","Shurong Wang","Howard H. Yang","Jian Wu","Meng Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.09410v1.pdf","comment":"Submitted to IEEE/ACM Transactions on Networking"},{"id":"http://arxiv.org/abs/2501.09403v1","updated":"2025-01-16T09:18:59Z","published":"2025-01-16T09:18:59Z","title":"PISCO: Self-Supervised k-Space Regularization for Improved Neural\n  Implicit k-Space Representations of Dynamic MRI","summary":"  Neural implicit k-space representations (NIK) have shown promising results\nfor dynamic magnetic resonance imaging (MRI) at high temporal resolutions. Yet,\nreducing acquisition time, and thereby available training data, results in\nsevere performance drops due to overfitting. To address this, we introduce a\nnovel self-supervised k-space loss function $\\mathcal{L}_\\mathrm{PISCO}$,\napplicable for regularization of NIK-based reconstructions. The proposed loss\nfunction is based on the concept of parallel imaging-inspired self-consistency\n(PISCO), enforcing a consistent global k-space neighborhood relationship\nwithout requiring additional data. Quantitative and qualitative evaluations on\nstatic and dynamic MR reconstructions show that integrating PISCO significantly\nimproves NIK representations. Particularly for high acceleration factors\n(R$\\geq$54), NIK with PISCO achieves superior spatio-temporal reconstruction\nquality compared to state-of-the-art methods. Furthermore, an extensive\nanalysis of the loss assumptions and stability shows PISCO's potential as\nversatile self-supervised k-space loss function for further applications and\narchitectures. Code is available at:\nhttps://github.com/compai-lab/2025-pisco-spieker\n","authors":["Veronika Spieker","Hannah Eichhorn","Wenqi Huang","Jonathan K. Stelter","Tabita Catalan","Rickmer F. Braren","Daniel Rueckert","Francisco Sahli Costabal","Kerstin Hammernik","Dimitrios C. Karampinos","Claudia Prieto","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2501.09403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09399v1","updated":"2025-01-16T09:11:48Z","published":"2025-01-16T09:11:48Z","title":"Fast Searching of Extreme Operating Conditions for Relay Protection\n  Setting Calculation Based on Graph Neural Network and Reinforcement Learning","summary":"  Searching for the Extreme Operating Conditions (EOCs) is one of the core\nproblems of power system relay protection setting calculation. The current\nmethods based on brute-force search, heuristic algorithms, and mathematical\nprogramming can hardly meet the requirements of today's power systems in terms\nof computation speed due to the drastic changes in operating conditions induced\nby renewables and power electronics. This paper proposes an EOC fast search\nmethod, named Graph Dueling Double Deep Q Network (Graph D3QN), which combines\ngraph neural network and deep reinforcement learning to address this challenge.\nFirst, the EOC search problem is modeled as a Markov decision process, where\nthe information of the underlying power system is extracted using graph neural\nnetworks, so that the EOC of the system can be found via deep reinforcement\nlearning. Then, a two-stage Guided Learning and Free Exploration (GLFE)\ntraining framework is constructed to accelerate the convergence speed of\nreinforcement learning. Finally, the proposed Graph D3QN method is validated\nthrough case studies of searching maximum fault current for relay protection\nsetting calculation on the IEEE 39-bus and 118-bus systems. The experimental\nresults demonstrate that Graph D3QN can reduce the computation time by 10 to\n1000 times while guaranteeing the accuracy of the selected EOCs.\n","authors":["Yan Li","Jingyu Wang","Jiankang Zhang","Huaiqiang Li","Longfei Ren","Yinhong Li","Dongyuan Shi","Xianzhong Duan"],"pdf_url":"https://arxiv.org/pdf/2501.09399v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.04088v3","updated":"2025-01-16T09:07:51Z","published":"2024-06-06T13:58:41Z","title":"Deterministic Uncertainty Propagation for Improved Model-Based Offline\n  Reinforcement Learning","summary":"  Current approaches to model-based offline reinforcement learning often\nincorporate uncertainty-based reward penalization to address the distributional\nshift problem. These approaches, commonly known as pessimistic value iteration,\nuse Monte Carlo sampling to estimate the Bellman target to perform temporal\ndifference-based policy evaluation. We find out that the randomness caused by\nthis sampling step significantly delays convergence. We present a theoretical\nresult demonstrating the strong dependency of suboptimality on the number of\nMonte Carlo samples taken per Bellman target calculation. Our main contribution\nis a deterministic approximation to the Bellman target that uses progressive\nmoment matching, a method developed originally for deterministic variational\ninference. The resulting algorithm, which we call Moment Matching Offline\nModel-Based Policy Optimization (MOMBO), propagates the uncertainty of the next\nstate through a nonlinear Q-network in a deterministic fashion by approximating\nthe distributions of hidden layer activations by a normal distribution. We show\nthat it is possible to provide tighter guarantees for the suboptimality of\nMOMBO than the existing Monte Carlo sampling approaches. We also observe MOMBO\nto converge faster than these approaches in a large set of benchmark tasks.\n","authors":["Abdullah Akgül","Manuel Haußmann","Melih Kandemir"],"pdf_url":"https://arxiv.org/pdf/2406.04088v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08710v2","updated":"2025-01-16T09:07:00Z","published":"2025-01-15T10:50:54Z","title":"Disentangled Interleaving Variational Encoding","summary":"  Conflicting objectives present a considerable challenge in interleaving\nmulti-task learning, necessitating the need for meticulous design and balance\nto ensure effective learning of a representative latent data space across all\ntasks without mutual negative impact. Drawing inspiration from the concept of\nmarginal and conditional probability distributions in probability theory, we\ndesign a principled and well-founded approach to disentangle the original input\ninto marginal and conditional probability distributions in the latent space of\na variational autoencoder. Our proposed model, Deep Disentangled Interleaving\nVariational Encoding (DeepDIVE) learns disentangled features from the original\ninput to form clusters in the embedding space and unifies these features via\nthe cross-attention mechanism in the fusion stage. We theoretically prove that\ncombining the objectives for reconstruction and forecasting fully captures the\nlower bound and mathematically derive a loss function for disentanglement using\nNa\\\"ive Bayes. Under the assumption that the prior is a mixture of log-concave\ndistributions, we also establish that the Kullback-Leibler divergence between\nthe prior and the posterior is upper bounded by a function minimized by the\nminimizer of the cross entropy loss, informing our adoption of radial basis\nfunctions (RBF) and cross entropy with interleaving training for DeepDIVE to\nprovide a justified basis for convergence. Experiments on two public datasets\nshow that DeepDIVE disentangles the original input and yields forecast\naccuracies better than the original VAE and comparable to existing\nstate-of-the-art baselines.\n","authors":["Noelle Y. L. Wong","Eng Yeow Cheu","Zhonglin Chiam","Dipti Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2501.08710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09395v1","updated":"2025-01-16T09:06:43Z","published":"2025-01-16T09:06:43Z","title":"ELM-DeepONets: Backpropagation-Free Training of Deep Operator Networks\n  via Extreme Learning Machines","summary":"  Deep Operator Networks (DeepONets) are among the most prominent frameworks\nfor operator learning, grounded in the universal approximation theorem for\noperators. However, training DeepONets typically requires significant\ncomputational resources. To address this limitation, we propose ELM-DeepONets,\nan Extreme Learning Machine (ELM) framework for DeepONets that leverages the\nbackpropagation-free nature of ELM. By reformulating DeepONet training as a\nleast-squares problem for newly introduced parameters, the ELM-DeepONet\napproach significantly reduces training complexity. Validation on benchmark\nproblems, including nonlinear ODEs and PDEs, demonstrates that the proposed\nmethod not only achieves superior accuracy but also drastically reduces\ncomputational costs. This work offers a scalable and efficient alternative for\noperator learning in scientific computing.\n","authors":["Hwijae Son"],"pdf_url":"https://arxiv.org/pdf/2501.09395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09394v1","updated":"2025-01-16T09:06:10Z","published":"2025-01-16T09:06:10Z","title":"Quantum-Enhanced Transformers for Robust Acoustic Scene Classification\n  in IoT Environments","summary":"  The proliferation of Internet of Things (IoT) devices equipped with acoustic\nsensors necessitates robust acoustic scene classification (ASC) capabilities,\neven in noisy and data-limited environments. Traditional machine learning\nmethods often struggle to generalize effectively under such conditions. To\naddress this, we introduce Q-ASC, a novel Quantum-Inspired Acoustic Scene\nClassifier that leverages the power of quantum-inspired transformers. By\nintegrating quantum concepts like superposition and entanglement, Q-ASC\nachieves superior feature learning and enhanced noise resilience compared to\nclassical models. Furthermore, we introduce a Quantum Variational Autoencoder\n(QVAE) based data augmentation technique to mitigate the challenge of limited\nlabeled data in IoT deployments. Extensive evaluations on the Tampere\nUniversity of Technology (TUT) Acoustic Scenes 2016 benchmark dataset\ndemonstrate that Q-ASC achieves remarkable accuracy between 68.3% and 88.5%\nunder challenging conditions, outperforming state-of-the-art methods by over 5%\nin the best case. This research paves the way for deploying intelligent\nacoustic sensing in IoT networks, with potential applications in smart homes,\nindustrial monitoring, and environmental surveillance, even in adverse acoustic\nenvironments.\n","authors":["Minh K. Quan","Mayuri Wijayasundara","Sujeeva Setunge","Pubudu N. Pathirana"],"pdf_url":"https://arxiv.org/pdf/2501.09394v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2306.05515v4","updated":"2025-01-16T08:53:23Z","published":"2023-06-08T19:12:42Z","title":"PeFLL: Personalized Federated Learning by Learning to Learn","summary":"  We present PeFLL, a new personalized federated learning algorithm that\nimproves over the state-of-the-art in three aspects: 1) it produces more\naccurate models, especially in the low-data regime, and not only for clients\npresent during its training phase, but also for any that may emerge in the\nfuture; 2) it reduces the amount of on-client computation and client-server\ncommunication by providing future clients with ready-to-use personalized models\nthat require no additional finetuning or optimization; 3) it comes with\ntheoretical guarantees that establish generalization from the observed clients\nto future ones. At the core of PeFLL lies a learning-to-learn approach that\njointly trains an embedding network and a hypernetwork. The embedding network\nis used to represent clients in a latent descriptor space in a way that\nreflects their similarity to each other. The hypernetwork takes as input such\ndescriptors and outputs the parameters of fully personalized client models. In\ncombination, both networks constitute a learning algorithm that achieves\nstate-of-the-art performance in several personalized federated learning\nbenchmarks.\n","authors":["Jonathan Scott","Hossein Zakerinia","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2306.05515v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07124v2","updated":"2025-01-16T08:49:10Z","published":"2025-01-13T08:26:43Z","title":"LLM360 K2: Building a 65B 360-Open-Source Large Language Model from\n  Scratch","summary":"  We detail the training of the LLM360 K2-65B model, scaling up our 360-degree\nOPEN SOURCE approach to the largest and most powerful models under project\nLLM360. While open-source LLMs continue to advance, the answer to \"How are the\nlargest LLMs trained?\" remains unclear within the community. The implementation\ndetails for such high-capacity models are often protected due to business\nconsiderations associated with their high cost. This lack of transparency\nprevents LLM researchers from leveraging valuable insights from prior\nexperience, e.g., \"What are the best practices for addressing loss spikes?\" The\nLLM360 K2 project addresses this gap by providing full transparency and access\nto resources accumulated during the training of LLMs at the largest scale. This\nreport highlights key elements of the K2 project, including our first model, K2\nDIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals\nLLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the\nimplementation steps and present a longitudinal analysis of K2 DIAMOND's\ncapabilities throughout its training process. We also outline ongoing projects\nsuch as TXT360, setting the stage for future models in the series. By offering\npreviously unavailable resources, the K2 project also resonates with the\n360-degree OPEN SOURCE principles of transparency, reproducibility, and\naccessibility, which we believe are vital in the era of resource-intensive AI\nresearch.\n","authors":["Zhengzhong Liu","Bowen Tan","Hongyi Wang","Willie Neiswanger","Tianhua Tao","Haonan Li","Fajri Koto","Yuqi Wang","Suqi Sun","Omkar Pangarkar","Richard Fan","Yi Gu","Victor Miller","Liqun Ma","Liping Tang","Nikhil Ranjan","Yonghao Zhuang","Guowei He","Renxi Wang","Mingkai Deng","Robin Algayres","Yuanzhi Li","Zhiqiang Shen","Preslav Nakov","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2501.07124v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13238v3","updated":"2025-01-16T08:49:06Z","published":"2024-05-21T22:53:00Z","title":"Enhancing User Interest based on Stream Clustering and Memory Networks\n  in Large-Scale Recommender Systems","summary":"  Recommender Systems (RSs) provide personalized recommendation service based\non user interest, which are widely used in various platforms. However, there\nare lots of users with sparse interest due to lacking consumption behaviors,\nwhich leads to poor recommendation results for them. This problem is widespread\nin large-scale RSs and is particularly difficult to address. To solve this\nproblem, we propose a novel solution named User Interest Enhancement (UIE)\nwhich enhances user interest including user profile and user history behavior\nsequences using the enhancement vectors and personalized enhancement vector\ngenerated based on stream clustering and memory networks from different\nperspectives. UIE not only remarkably improves model performance on the users\nwith sparse interest but also significantly enhance model performance on other\nusers. UIE is an end-to-end solution which is easy to be implemented based on\nranking model. Moreover, we expand our solution and apply similar methods to\nlong-tail items, which also achieves excellent improvement. Furthermore, we\nconduct extensive offline and online experiments in a large-scale industrial\nRS. The results demonstrate that our model outperforms other models remarkably,\nespecially for the users with sparse interest. Until now, UIE has been fully\ndeployed in multiple large-scale RSs and achieved remarkable improvements.\n","authors":["Peng Liu","Nian Wang","Cong Xu","Ming Zhao","Bin Wang","Yi Ren"],"pdf_url":"https://arxiv.org/pdf/2405.13238v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04329v4","updated":"2025-01-16T08:46:16Z","published":"2024-06-06T17:59:10Z","title":"Simplified and Generalized Masked Diffusion for Discrete Data","summary":"  Masked (or absorbing) diffusion is actively explored as an alternative to\nautoregressive models for generative modeling of discrete data. However,\nexisting work in this area has been hindered by unnecessarily complex model\nformulations and unclear relationships between different perspectives, leading\nto suboptimal parameterization, training objectives, and ad hoc adjustments to\ncounteract these issues. In this work, we aim to provide a simple and general\nframework that unlocks the full potential of masked diffusion models. We show\nthat the continuous-time variational objective of masked diffusion models is a\nsimple weighted integral of cross-entropy losses. Our framework also enables\ntraining generalized masked diffusion models with state-dependent masking\nschedules. When evaluated by perplexity, our models trained on OpenWebText\nsurpass prior diffusion language models at GPT-2 scale and demonstrate superior\nperformance on 4 out of 5 zero-shot language modeling tasks. Furthermore, our\nmodels vastly outperform previous discrete diffusion models on pixel-level\nimage modeling, achieving 2.75 (CIFAR-10) and 3.40 (ImageNet 64x64) bits per\ndimension that are better than autoregressive models of similar sizes. Our code\nis available at https://github.com/google-deepmind/md4.\n","authors":["Jiaxin Shi","Kehang Han","Zhe Wang","Arnaud Doucet","Michalis K. Titsias"],"pdf_url":"https://arxiv.org/pdf/2406.04329v4.pdf","comment":"NeurIPS 2024. Code is available at:\n  https://github.com/google-deepmind/md4"},{"id":"http://arxiv.org/abs/2408.12112v3","updated":"2025-01-16T08:44:22Z","published":"2024-08-22T03:54:08Z","title":"Balancing Act: Prioritization Strategies for LLM-Designed Restless\n  Bandit Rewards","summary":"  LLMs are increasingly used to design reward functions based on human\npreferences in Reinforcement Learning (RL). We focus on LLM-designed rewards\nfor Restless Multi-Armed Bandits, a framework for allocating limited resources\namong agents. In applications such as public health, this approach empowers\ngrassroots health workers to tailor automated allocation decisions to community\nneeds. In the presence of multiple agents, altering the reward function based\non human preferences can impact subpopulations very differently, leading to\ncomplex tradeoffs and a multi-objective resource allocation problem. We are the\nfirst to present a principled method termed Social Choice Language Model for\ndealing with these tradeoffs for LLM-designed rewards for multiagent planners\nin general and restless bandits in particular. The novel part of our model is a\ntransparent and configurable selection component, called an adjudicator,\nexternal to the LLM that controls complex tradeoffs via a user-selected social\nwelfare function. Our experiments demonstrate that our model reliably selects\nmore effective, aligned, and balanced reward functions compared to purely\nLLM-based approaches.\n","authors":["Shresth Verma","Niclas Boehmer","Lingkai Kong","Milind Tambe"],"pdf_url":"https://arxiv.org/pdf/2408.12112v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02623v3","updated":"2025-01-16T08:18:01Z","published":"2024-11-04T21:31:04Z","title":"Learning to Assist Humans without Inferring Rewards","summary":"  Assistive agents should make humans' lives easier. Classically, such\nassistance is studied through the lens of inverse reinforcement learning, where\nan assistive agent (e.g., a chatbot, a robot) infers a human's intention and\nthen selects actions to help the human reach that goal. This approach requires\ninferring intentions, which can be difficult in high-dimensional settings. We\nbuild upon prior work that studies assistance through the lens of empowerment:\nan assistive agent aims to maximize the influence of the human's actions such\nthat they exert a greater control over the environmental outcomes and can solve\ntasks in fewer steps. We lift the major limitation of prior work in this\narea--scalability to high-dimensional settings--with contrastive successor\nrepresentations. We formally prove that these representations estimate a\nsimilar notion of empowerment to that studied by prior work and provide a\nready-made mechanism for optimizing it. Empirically, our proposed method\noutperforms prior methods on synthetic benchmarks, and scales to Overcooked, a\ncooperative game setting. Theoretically, our work connects ideas from\ninformation theory, neuroscience, and reinforcement learning, and charts a path\nfor representations to play a critical role in solving assistive problems.\n","authors":["Vivek Myers","Evan Ellis","Sergey Levine","Benjamin Eysenbach","Anca Dragan"],"pdf_url":"https://arxiv.org/pdf/2411.02623v3.pdf","comment":"Conference on Neural Information Processing Systems (NeurIPS), 2024"},{"id":"http://arxiv.org/abs/2501.09352v1","updated":"2025-01-16T08:04:04Z","published":"2025-01-16T08:04:04Z","title":"PAL: Prompting Analytic Learning with Missing Modality for Multi-Modal\n  Class-Incremental Learning","summary":"  Multi-modal class-incremental learning (MMCIL) seeks to leverage multi-modal\ndata, such as audio-visual and image-text pairs, thereby enabling models to\nlearn continuously across a sequence of tasks while mitigating forgetting.\nWhile existing studies primarily focus on the integration and utilization of\nmulti-modal information for MMCIL, a critical challenge remains: the issue of\nmissing modalities during incremental learning phases. This oversight can\nexacerbate severe forgetting and significantly impair model performance. To\nbridge this gap, we propose PAL, a novel exemplar-free framework tailored to\nMMCIL under missing-modality scenarios. Concretely, we devise modality-specific\nprompts to compensate for missing information, facilitating the model to\nmaintain a holistic representation of the data. On this foundation, we\nreformulate the MMCIL problem into a Recursive Least-Squares task, delivering\nan analytical linear solution. Building upon these, PAL not only alleviates the\ninherent under-fitting limitation in analytic learning but also preserves the\nholistic representation of missing-modality data, achieving superior\nperformance with less forgetting across various multi-modal incremental\nscenarios. Extensive experiments demonstrate that PAL significantly outperforms\ncompetitive methods across various datasets, including UPMC-Food101 and\nN24News, showcasing its robustness towards modality absence and its\nanti-forgetting ability to maintain high incremental accuracy.\n","authors":["Xianghu Yue","Yiming Chen","Xueyi Zhang","Xiaoxue Gao","Mengling Feng","Mingrui Lao","Huiping Zhuang","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2501.09352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09345v1","updated":"2025-01-16T07:58:33Z","published":"2025-01-16T07:58:33Z","title":"Rational Tuning of LLM Cascades via Probabilistic Modeling","summary":"  Understanding the reliability of large language models (LLMs) has recently\ngarnered significant attention. Given LLMs' propensity to hallucinate, as well\nas their high sensitivity to prompt design, it is already challenging to\npredict the performance of an individual LLM. However, the problem becomes more\ncomplex for compound LLM systems such as cascades, where in addition to each\nmodel's standalone performance, we must understand how the error rates of\ndifferent models interact. In this paper, we present a probabilistic model for\nthe joint performance distribution of a sequence of LLMs, which enables a\nframework for rationally tuning the confidence thresholds of a LLM cascade\nusing continuous optimization. Compared to selecting confidence thresholds\nusing grid search, our parametric Markov-copula model significantly improves\nruntime scaling with respect to the length of the cascade and the desired\nresolution of the cost-error curve, turning them from intractable into\nlow-order polynomial. In addition, the optimal thresholds computed using our\ncontinuous optimization-based algorithm increasingly outperform those found via\ngrid search as cascade length grows, improving the area under the cost-error\ncurve by 1.9% on average for cascades consisting of at least three models.\nOverall, our Markov-copula model provides a rational basis for tuning LLM\ncascade performance and points to the potential of probabilistic methods in\nanalyzing LLM systems.\n","authors":["Michael J. Zellinger","Matt Thomson"],"pdf_url":"https://arxiv.org/pdf/2501.09345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03584v2","updated":"2025-01-16T07:56:42Z","published":"2025-01-07T07:17:04Z","title":"Discriminative Representation learning via Attention-Enhanced\n  Contrastive Learning for Short Text Clustering","summary":"  Contrastive learning has gained significant attention in short text\nclustering, yet it has an inherent drawback of mistakenly identifying samples\nfrom the same category as negatives and then separating them in the feature\nspace (false negative separation), which hinders the generation of superior\nrepresentations. To generate more discriminative representations for efficient\nclustering, we propose a novel short text clustering method, called\nDiscriminative Representation learning via \\textbf{A}ttention-\\textbf{E}nhanced\n\\textbf{C}ontrastive \\textbf{L}earning for Short Text Clustering\n(\\textbf{AECL}). The \\textbf{AECL} consists of two modules which are the\npseudo-label generation module and the contrastive learning module. Both\nmodules build a sample-level attention mechanism to capture similarity\nrelationships between samples and aggregate cross-sample features to generate\nconsistent representations. Then, the former module uses the more\ndiscriminative consistent representation to produce reliable supervision\ninformation for assist clustering, while the latter module explores similarity\nrelationships and consistent representations optimize the construction of\npositive samples to perform similarity-guided contrastive learning, effectively\naddressing the false negative separation issue. Experimental results\ndemonstrate that the proposed \\textbf{AECL} outperforms state-of-the-art\nmethods. If the paper is accepted, we will open-source the code.\n","authors":["Zhihao Yao"],"pdf_url":"https://arxiv.org/pdf/2501.03584v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03337v4","updated":"2025-01-16T07:40:27Z","published":"2024-07-22T07:19:12Z","title":"PsyDI: Towards a Personalized and Progressively In-depth Chatbot for\n  Psychological Measurements","summary":"  In the field of psychology, traditional assessment methods, such as\nstandardized scales, are frequently critiqued for their static nature, lack of\npersonalization, and reduced participant engagement, while comprehensive\ncounseling evaluations are often inaccessible. The complexity of quantifying\npsychological traits further limits these methods. Despite advances with large\nlanguage models (LLMs), many still depend on single-round Question-and-Answer\ninteractions. To bridge this gap, we introduce PsyDI, a personalized and\nprogressively in-depth chatbot designed for psychological measurements,\nexemplified by its application in the Myers-Briggs Type Indicator (MBTI)\nframework. PsyDI leverages user-related multi-modal information and engages in\ncustomized, multi-turn interactions to provide personalized, easily accessible\nmeasurements, while ensuring precise MBTI type determination. To address the\nchallenge of unquantifiable psychological traits, we introduce a novel training\nparadigm that involves learning the ranking of proxy variables associated with\nthese traits, culminating in a robust score model for MBTI measurements. The\nscore model enables PsyDI to conduct comprehensive and precise measurements\nthrough multi-turn interactions within a unified estimation context. Through\nvarious experiments, we validate the efficacy of both the score model and the\nPsyDI pipeline, demonstrating its potential to serve as a general framework for\npsychological measurements. Furthermore, the online deployment of PsyDI has\ngarnered substantial user engagement, with over 3,000 visits, resulting in the\ncollection of numerous multi-turn dialogues annotated with MBTI types, which\nfacilitates further research. The source code for the training and web service\ncomponents is publicly available as a part of OpenDILab at:\nhttps://github.com/opendilab/PsyDI\n","authors":["Xueyan Li","Xinyan Chen","Yazhe Niu","Shuai Hu","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03337v4.pdf","comment":"29 pages, 15 figures"},{"id":"http://arxiv.org/abs/2211.15081v8","updated":"2025-01-16T07:34:31Z","published":"2022-11-28T05:54:24Z","title":"Mitigating Overfitting in Graph Neural Networks via Feature and\n  Hyperplane Perturbation","summary":"  Graph neural networks (GNNs) are commonly used in semi-supervised settings.\nPrevious research has primarily focused on finding appropriate graph filters\n(e.g. aggregation methods) to perform well on both homophilic and heterophilic\ngraphs. While these methods are effective, they can still suffer from the\nsparsity of node features, where the initial data contain few non-zero\nelements. This can lead to overfitting in certain dimensions in the first\nprojection matrix, as training samples may not cover the entire range of graph\nfilters (hyperplanes). To address this, we propose a novel data augmentation\nstrategy. Specifically, by flipping both the initial features and hyperplane,\nwe create additional space for training, which leads to more precise updates of\nthe learnable parameters and improved robustness for unseen features during\ninference. To the best of our knowledge, this is the first attempt to mitigate\nthe overfitting caused by the initial features. Extensive experiments on\nreal-world datasets show that our proposed technique increases node\nclassification accuracy by up to 46.5% relatively.\n","authors":["Yoonhyuk Choi","Jiho Choi","Taewook Ko","Chong-Kwon Kim"],"pdf_url":"https://arxiv.org/pdf/2211.15081v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09336v1","updated":"2025-01-16T07:23:26Z","published":"2025-01-16T07:23:26Z","title":"Estimating shared subspace with AJIVE: the power and limitation of\n  multiple data matrices","summary":"  Integrative data analysis often requires disentangling joint and individual\nvariations across multiple datasets, a challenge commonly addressed by the\nJoint and Individual Variation Explained (JIVE) model. While numerous methods\nhave been developed to estimate the shared subspace under JIVE, the theoretical\nunderstanding of their performance remains limited, particularly in the context\nof multiple matrices and varying levels of subspace misalignment. This paper\nbridges this gap by providing a systematic analysis of shared subspace\nestimation in multi-matrix settings.\n  We focus on the Angle-based Joint and Individual Variation Explained (AJIVE)\nmethod, a two-stage spectral approach, and establish new performance guarantees\nthat uncover its strengths and limitations. Specifically, we show that in high\nsignal-to-noise ratio (SNR) regimes, AJIVE's estimation error decreases with\nthe number of matrices, demonstrating the power of multi-matrix integration.\nConversely, in low-SNR settings, AJIVE exhibits a non-diminishing error,\nhighlighting fundamental limitations. To complement these results, we derive\nminimax lower bounds, showing that AJIVE achieves optimal rates in high-SNR\nregimes. Furthermore, we analyze an oracle-aided spectral estimator to\ndemonstrate that the non-diminishing error in low-SNR scenarios is a\nfundamental barrier. Extensive numerical experiments corroborate our\ntheoretical findings, providing insights into the interplay between SNR, matrix\ncount, and subspace misalignment.\n","authors":["Yuepeng Yang","Cong Ma"],"pdf_url":"https://arxiv.org/pdf/2501.09336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09331v1","updated":"2025-01-16T07:02:05Z","published":"2025-01-16T07:02:05Z","title":"Identifying Information from Observations with Uncertainty and Novelty","summary":"  A machine learning tasks from observations must encounter and process\nuncertainty and novelty, especially when it is expected to maintain performance\nwhen observing new information and to choose the best fitting hypothesis to the\ncurrently observed information. In this context, some key questions arise: what\nis information, how much information did the observations provide, how much\ninformation is required to identify the data-generating process, how many\nobservations remain to get that information, and how does a predictor determine\nthat it has observed novel information? This paper strengthens existing answers\nto these questions by formalizing the notion of \"identifiable information\" that\narises from the language used to express the relationship between distinct\nstates. Model identifiability and sample complexity are defined via computation\nof an indicator function over a set of hypotheses. Their properties and\nasymptotic statistics are described for data-generating processes ranging from\ndeterministic processes to ergodic stationary stochastic processes. This\nconnects the notion of identifying information in finite steps with asymptotic\nstatistics and PAC-learning. The indicator function's computation naturally\nformalizes novel information and its identification from observations with\nrespect to a hypothesis set. We also proved that computable PAC-Bayes learners'\nsample complexity distribution is determined by its moments in terms of the the\nprior probability distribution over a fixed finite hypothesis set.\n","authors":["Derek S. Prijatelj","Timothy J. Ireland","Walter J. Scheirer"],"pdf_url":"https://arxiv.org/pdf/2501.09331v1.pdf","comment":"43 pages, 1 figure, 1 table, and 2 inline algorithms. Submitted to\n  JMLR Jan. 6, 2025"},{"id":"http://arxiv.org/abs/2501.09327v1","updated":"2025-01-16T06:52:58Z","published":"2025-01-16T06:52:58Z","title":"On Learning Informative Trajectory Embeddings for Imitation,\n  Classification and Regression","summary":"  In real-world sequential decision making tasks like autonomous driving,\nrobotics, and healthcare, learning from observed state-action trajectories is\ncritical for tasks like imitation, classification, and clustering. For example,\nself-driving cars must replicate human driving behaviors, while robots and\nhealthcare systems benefit from modeling decision sequences, whether or not\nthey come from expert data. Existing trajectory encoding methods often focus on\nspecific tasks or rely on reward signals, limiting their ability to generalize\nacross domains and tasks. Inspired by the success of embedding models like CLIP\nand BERT in static domains, we propose a novel method for embedding\nstate-action trajectories into a latent space that captures the skills and\ncompetencies in the dynamic underlying decision-making processes. This method\noperates without the need for reward labels, enabling better generalization\nacross diverse domains and tasks. Our contributions are threefold: (1) We\nintroduce a trajectory embedding approach that captures multiple abilities from\nstate-action data. (2) The learned embeddings exhibit strong representational\npower across downstream tasks, including imitation, classification, clustering,\nand regression. (3) The embeddings demonstrate unique properties, such as\ncontrolling agent behaviors in IQ-Learn and an additive structure in the latent\nspace. Experimental results confirm that our method outperforms traditional\napproaches, offering more flexible and powerful trajectory representations for\nvarious applications. Our code is available at\nhttps://github.com/Erasmo1015/vte.\n","authors":["Zichang Ge","Changyu Chen","Arunesh Sinha","Pradeep Varakantham"],"pdf_url":"https://arxiv.org/pdf/2501.09327v1.pdf","comment":"AAMAS 2025"},{"id":"http://arxiv.org/abs/2501.08037v2","updated":"2025-01-16T06:44:29Z","published":"2025-01-14T11:42:51Z","title":"Enhanced SPS Velocity-adaptive Scheme: Access Fairness in 5G NR V2I\n  Networks","summary":"  Vehicle-to-Infrastructure (V2I) technology enables information exchange\nbetween vehicles and road infrastructure. Specifically, when a vehicle\napproaches a roadside unit (RSU), it can exchange information with the RSU to\nobtain accurate data that assists in driving. With the release of the 3rd\nGeneration Partnership Project (3GPP) Release 16, which includes the 5G New\nRadio (NR) Vehicle-to-Everything (V2X) standards, vehicles typically adopt\nmode-2 communication using sensing-based semi-persistent scheduling (SPS) for\nresource allocation. In this approach, vehicles identify candidate resources\nwithin a selection window and exclude ineligible resources based on information\nfrom a sensing window. However, vehicles often drive at different speeds,\nresulting in varying amounts of data transmission with RSUs as they pass by,\nwhich leads to unfair access. Therefore, it is essential to design an access\nscheme that accounts for different vehicle speeds to achieve fair access across\nthe network. This paper formulates an optimization problem for vehicular\nnetworks and proposes a multi-objective optimization scheme to address it by\nadjusting the selection window in the SPS mechanism of 5G NR V2I mode-2.\nSimulation results demonstrate the effectiveness of the proposed scheme\n","authors":["Xiao Xu","Qiong Wu","Pingyi Fan","Kezhi Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08037v2.pdf","comment":"This paper has been submitted to IEEE Journal. The source code has\n  been released at:\n  https://github.com/qiongwu86/Enhanced-SPS-Velocity-adaptiveScheme-Access-Fariness-in-5G-NR-V2I-Networks"},{"id":"http://arxiv.org/abs/2501.09320v1","updated":"2025-01-16T06:22:35Z","published":"2025-01-16T06:22:35Z","title":"Cooperative Decentralized Backdoor Attacks on Vertical Federated\n  Learning","summary":"  Federated learning (FL) is vulnerable to backdoor attacks, where adversaries\nalter model behavior on target classification labels by embedding triggers into\ndata samples. While these attacks have received considerable attention in\nhorizontal FL, they are less understood for vertical FL (VFL), where devices\nhold different features of the samples, and only the server holds the labels.\nIn this work, we propose a novel backdoor attack on VFL which (i) does not rely\non gradient information from the server and (ii) considers potential collusion\namong multiple adversaries for sample selection and trigger embedding. Our\nlabel inference model augments variational autoencoders with metric learning,\nwhich adversaries can train locally. A consensus process over the adversary\ngraph topology determines which datapoints to poison. We further propose\nmethods for trigger splitting across the adversaries, with an intensity-based\nimplantation scheme skewing the server towards the trigger. Our convergence\nanalysis reveals the impact of backdoor perturbations on VFL indicated by a\nstationarity gap for the trained model, which we verify empirically as well. We\nconduct experiments comparing our attack with recent backdoor VFL approaches,\nfinding that ours obtains significantly higher success rates for the same main\ntask performance despite not using server information. Additionally, our\nresults verify the impact of collusion on attack performance.\n","authors":["Seohyun Lee","Wenzhi Fang","Anindya Bijoy Das","Seyyedali Hosseinalipour","David J. Love","Christopher G. Brinton"],"pdf_url":"https://arxiv.org/pdf/2501.09320v1.pdf","comment":"This paper is currently under review in the IEEE/ACM Transactions on\n  Networking Special Issue on AI and Networking"},{"id":"http://arxiv.org/abs/2410.01186v3","updated":"2025-01-16T05:50:54Z","published":"2024-10-02T02:38:33Z","title":"Efficient PAC Learning of Halfspaces with Constant Malicious Noise Rate","summary":"  Understanding noise tolerance of machine learning algorithms is a central\nquest in learning theory. In this work, we study the problem of computationally\nefficient PAC learning of halfspaces in the presence of malicious noise, where\nan adversary can corrupt both instances and labels of training samples. The\nbest-known noise tolerance either depends on a target error rate under\ndistributional assumptions or on a margin parameter under large-margin\nconditions. In this work, we show that when both types of conditions are\nsatisfied, it is possible to achieve constant noise tolerance by minimizing a\nreweighted hinge loss. Our key ingredients include: 1) an efficient algorithm\nthat finds weights to control the gradient deterioration from corrupted\nsamples, and 2) a new analysis on the robustness of the hinge loss equipped\nwith such weights.\n","authors":["Jie Shen"],"pdf_url":"https://arxiv.org/pdf/2410.01186v3.pdf","comment":"ALT 2025"},{"id":"http://arxiv.org/abs/2408.01432v3","updated":"2025-01-16T05:42:28Z","published":"2024-07-18T19:44:44Z","title":"VLG-CBM: Training Concept Bottleneck Models with Vision-Language\n  Guidance","summary":"  Concept Bottleneck Models (CBMs) provide interpretable prediction by\nintroducing an intermediate Concept Bottleneck Layer (CBL), which encodes\nhuman-understandable concepts to explain models' decision. Recent works\nproposed to utilize Large Language Models and pre-trained Vision-Language\nModels to automate the training of CBMs, making it more scalable and automated.\nHowever, existing approaches still fall short in two aspects: First, the\nconcepts predicted by CBL often mismatch the input image, raising doubts about\nthe faithfulness of interpretation. Second, it has been shown that concept\nvalues encode unintended information: even a set of random concepts could\nachieve comparable test accuracy to state-of-the-art CBMs. To address these\ncritical limitations, in this work, we propose a novel framework called\nVision-Language-Guided Concept Bottleneck Model (VLG-CBM) to enable faithful\ninterpretability with the benefits of boosted performance. Our method leverages\noff-the-shelf open-domain grounded object detectors to provide visually\ngrounded concept annotation, which largely enhances the faithfulness of concept\nprediction while further improving the model performance. In addition, we\npropose a new metric called Number of Effective Concepts (NEC) to control the\ninformation leakage and provide better interpretability. Extensive evaluations\nacross five standard benchmarks show that our method, VLG-CBM, outperforms\nexisting methods by at least 4.27% and up to 51.09% on Accuracy at NEC=5\n(denoted as ANEC-5), and by at least 0.45% and up to 29.78% on average accuracy\n(denoted as ANEC-avg), while preserving both faithfulness and interpretability\nof the learned concepts as demonstrated in extensive experiments.\n","authors":["Divyansh Srivastava","Ge Yan","Tsui-Wei Weng"],"pdf_url":"https://arxiv.org/pdf/2408.01432v3.pdf","comment":"Appeared at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.15368v2","updated":"2025-01-16T05:40:08Z","published":"2024-11-22T22:29:37Z","title":"The Power of Types: Exploring the Impact of Type Checking on Neural Bug\n  Detection in Dynamically Typed Languages","summary":"  Motivation: Automated bug detection in dynamically typed languages such as\nPython is essential for maintaining code quality. The lack of mandatory type\nannotations in such languages can lead to errors that are challenging to\nidentify early with traditional static analysis tools. Recent progress in deep\nneural networks has led to increased use of neural bug detectors. In statically\ntyped languages, a type checker is integrated into the compiler and thus taken\ninto consideration when the neural bug detector is designed for these\nlanguages.\n  Problem: However, prior studies overlook this aspect during the training and\ntesting of neural bug detectors for dynamically typed languages. When an\noptional type checker is used, assessing existing neural bug detectors on bugs\neasily detectable by type checkers may impact their performance estimation.\nMoreover, including these bugs in the training set of neural bug detectors can\nshift their detection focus toward the wrong type of bugs.\n  Contribution: We explore the impact of type checking on various neural bug\ndetectors for variable misuse bugs, a common type targeted by neural bug\ndetectors. Existing synthetic and real-world datasets are type-checked to\nevaluate the prevalence of type-related bugs. Then, we investigate how\ntype-related bugs influence the training and testing of the neural bug\ndetectors.\n  Findings: Our findings indicate that existing bug detection datasets contain\na significant proportion of type-related bugs. Building on this insight, we\ndiscover integrating the neural bug detector with a type checker can be\nbeneficial, especially when the code is annotated with types. Further\ninvestigation reveals neural bug detectors perform better on type-related bugs\nthan other bugs. Moreover, removing type-related bugs from the training data\nhelps improve neural bug detectors' ability to identify bugs beyond the scope\nof type checkers.\n","authors":["Boqi Chen","José Antonio Hernández López","Gunter Mussbacher","Dániel Varró"],"pdf_url":"https://arxiv.org/pdf/2411.15368v2.pdf","comment":"Accepted by ICSE'25 Research Track"},{"id":"http://arxiv.org/abs/2501.09304v1","updated":"2025-01-16T05:39:28Z","published":"2025-01-16T05:39:28Z","title":"Finding the Trigger: Causal Abductive Reasoning on Video Events","summary":"  This paper introduces a new problem, Causal Abductive Reasoning on Video\nEvents (CARVE), which involves identifying causal relationships between events\nin a video and generating hypotheses about causal chains that account for the\noccurrence of a target event. To facilitate research in this direction, we\ncreate two new benchmark datasets with both synthetic and realistic videos,\naccompanied by trigger-target labels generated through a novel counterfactual\nsynthesis approach. To explore the challenge of solving CARVE, we present a\nCausal Event Relation Network (CERN) that examines the relationships between\nvideo events in temporal and semantic spaces to efficiently determine the\nroot-cause trigger events. Through extensive experiments, we demonstrate the\ncritical roles of event relational representation learning and interaction\nmodeling in solving video causal reasoning challenges. The introduction of the\nCARVE task, along with the accompanying datasets and the CERN framework, will\nadvance future research on video causal reasoning and significantly facilitate\nvarious applications, including video surveillance, root-cause analysis and\nmovie content management.\n","authors":["Thao Minh Le","Vuong Le","Kien Do","Sunil Gupta","Svetha Venkatesh","Truyen Tran"],"pdf_url":"https://arxiv.org/pdf/2501.09304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20062v3","updated":"2025-01-16T05:21:05Z","published":"2024-06-28T17:20:13Z","title":"Cost-aware Bayesian Optimization via the Pandora's Box Gittins Index","summary":"  Bayesian optimization is a technique for efficiently optimizing unknown\nfunctions in a black-box manner. To handle practical settings where gathering\ndata requires use of finite resources, it is desirable to explicitly\nincorporate function evaluation costs into Bayesian optimization policies. To\nunderstand how to do so, we develop a previously-unexplored connection between\ncost-aware Bayesian optimization and the Pandora's Box problem, a decision\nproblem from economics. The Pandora's Box problem admits a Bayesian-optimal\nsolution based on an expression called the Gittins index, which can be\nreinterpreted as an acquisition function. We study the use of this acquisition\nfunction for cost-aware Bayesian optimization, and demonstrate empirically that\nit performs well, particularly in medium-high dimensions. We further show that\nthis performance carries over to classical Bayesian optimization without\nexplicit evaluation costs. Our work constitutes a first step towards\nintegrating techniques from Gittins index theory into Bayesian optimization.\n","authors":["Qian Xie","Raul Astudillo","Peter I. Frazier","Ziv Scully","Alexander Terenin"],"pdf_url":"https://arxiv.org/pdf/2406.20062v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14641v3","updated":"2025-01-16T05:07:46Z","published":"2023-05-24T02:22:00Z","title":"Graph Analysis Using a GPU-based Parallel Algorithm: Quantum Clustering","summary":"  The article introduces a new method for applying Quantum Clustering to graph\nstructures. Quantum Clustering (QC) is a novel density-based unsupervised\nlearning method that determines cluster centers by constructing a potential\nfunction. In this method, we use the Graph Gradient Descent algorithm to find\nthe centers of clusters. GPU parallelization is utilized for computing\npotential values. We also conducted experiments on five widely used datasets\nand evaluated using four indicators. The results show superior performance of\nthe method. Finally, we discuss the influence of $\\sigma$ on the experimental\nresults.\n","authors":["Zhe Wang","ZhiJie He","Ding Liu"],"pdf_url":"https://arxiv.org/pdf/2305.14641v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09298v1","updated":"2025-01-16T05:07:05Z","published":"2025-01-16T05:07:05Z","title":"Physics-informed deep learning for infectious disease forecasting","summary":"  Accurate forecasting of contagious illnesses has become increasingly\nimportant to public health policymaking, and better prediction could prevent\nthe loss of millions of lives. To better prepare for future pandemics, it is\nessential to improve forecasting methods and capabilities. In this work, we\npropose a new infectious disease forecasting model based on physics-informed\nneural networks (PINNs), an emerging area of scientific machine learning. The\nproposed PINN model incorporates dynamical systems representations of disease\ntransmission into the loss function, thereby assimilating epidemiological\ntheory and data using neural networks (NNs). Our approach is designed to\nprevent model overfitting, which often occurs when training deep learning\nmodels with observation data alone. In addition, we employ an additional\nsub-network to account for mobility, vaccination, and other covariates that\ninfluence the transmission rate, a key parameter in the compartment model. To\ndemonstrate the capability of the proposed model, we examine the performance of\nthe model using state-level COVID-19 data in California. Our simulation results\nshow that predictions of PINN model on the number of cases, deaths, and\nhospitalizations are consistent with existing benchmarks. In particular, the\nPINN model outperforms the basic NN model and naive baseline forecast. We also\nshow that the performance of the PINN model is comparable to a sophisticated\nGaussian infection state space with time dependence (GISST) forecasting model\nthat integrates the compartment model with a data observation model and a\nregression model for inferring parameters in the compartment model.\nNonetheless, the PINN model offers a simpler structure and is easier to\nimplement. Our results show that the proposed forecaster could potentially\nserve as a new computational tool to enhance the current capacity of infectious\ndisease forecasting.\n","authors":["Ying Qian","Éric Marty","Avranil Basu","Eamon B. O'Dea","Xianqiao Wang","Spencer Fox","Pejman Rohani","John M. Drake","He Li"],"pdf_url":"https://arxiv.org/pdf/2501.09298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09283v1","updated":"2025-01-16T04:12:05Z","published":"2025-01-16T04:12:05Z","title":"Free-Knots Kolmogorov-Arnold Network: On the Analysis of Spline Knots\n  and Advancing Stability","summary":"  Kolmogorov-Arnold Neural Networks (KANs) have gained significant attention in\nthe machine learning community. However, their implementation often suffers\nfrom poor training stability and heavy trainable parameter. Furthermore, there\nis limited understanding of the behavior of the learned activation functions\nderived from B-splines. In this work, we analyze the behavior of KANs through\nthe lens of spline knots and derive the lower and upper bound for the number of\nknots in B-spline-based KANs. To address existing limitations, we propose a\nnovel Free Knots KAN that enhances the performance of the original KAN while\nreducing the number of trainable parameters to match the trainable parameter\nscale of standard Multi-Layer Perceptrons (MLPs). Additionally, we introduce\nnew a training strategy to ensure $C^2$ continuity of the learnable spline,\nresulting in smoother activation compared to the original KAN and improve the\ntraining stability by range expansion. The proposed method is comprehensively\nevaluated on 8 datasets spanning various domains, including image, text, time\nseries, multimodal, and function approximation tasks. The promising results\ndemonstrates the feasibility of KAN-based network and the effectiveness of\nproposed method.\n","authors":["Liangwewi Nathan Zheng","Wei Emma Zhang","Lin Yue","Miao Xu","Olaf Maennel","Weitong Chen"],"pdf_url":"https://arxiv.org/pdf/2501.09283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17638v3","updated":"2025-01-16T04:11:29Z","published":"2024-05-27T20:18:20Z","title":"The surprising efficiency of temporal difference learning for rare event\n  prediction","summary":"  We quantify the efficiency of temporal difference (TD) learning over the\ndirect, or Monte Carlo (MC), estimator for policy evaluation in reinforcement\nlearning, with an emphasis on estimation of quantities related to rare events.\nPolicy evaluation is complicated in the rare event setting by the long\ntimescale of the event and by the need for \\emph{relative accuracy} in\nestimates of very small values. Specifically, we focus on least-squares TD\n(LSTD) prediction for finite state Markov chains, and show that LSTD can\nachieve relative accuracy far more efficiently than MC. We prove a central\nlimit theorem for the LSTD estimator and upper bound the \\emph{relative\nasymptotic variance} by simple quantities characterizing the connectivity of\nstates relative to the transition probabilities between them. Using this bound,\nwe show that, even when both the timescale of the rare event and the relative\naccuracy of the MC estimator are exponentially large in the number of states,\nLSTD maintains a fixed level of relative accuracy with a total number of\nobserved transitions of the Markov chain that is only \\emph{polynomially} large\nin the number of states.\n","authors":["Xiaoou Cheng","Jonathan Weare"],"pdf_url":"https://arxiv.org/pdf/2405.17638v3.pdf","comment":"Final camera-ready version published at NeurIPS 2024. Correct an\n  assumption statement and typos, and change/add a few sentences from the last\n  version"},{"id":"http://arxiv.org/abs/2501.09274v1","updated":"2025-01-16T03:44:16Z","published":"2025-01-16T03:44:16Z","title":"Large Language Model is Secretly a Protein Sequence Optimizer","summary":"  We consider the protein sequence engineering problem, which aims to find\nprotein sequences with high fitness levels, starting from a given wild-type\nsequence. Directed evolution has been a dominating paradigm in this field which\nhas an iterative process to generate variants and select via experimental\nfeedback. We demonstrate large language models (LLMs), despite being trained on\nmassive texts, are secretly protein sequence optimizers. With a directed\nevolutionary method, LLM can perform protein engineering through Pareto and\nexperiment-budget constrained optimization, demonstrating success on both\nsynthetic and experimental fitness landscapes.\n","authors":["Yinkai Wang","Jiaxing He","Yuanqi Du","Xiaohui Chen","Jianan Canal Li","Li-Ping Liu","Xiaolin Xu","Soha Hassoun"],"pdf_url":"https://arxiv.org/pdf/2501.09274v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2408.04498v3","updated":"2025-01-16T03:35:07Z","published":"2024-08-08T14:46:01Z","title":"Model-Based Transfer Learning for Contextual Reinforcement Learning","summary":"  Deep reinforcement learning (RL) is a powerful approach to complex decision\nmaking. However, one issue that limits its practical application is its\nbrittleness, sometimes failing to train in the presence of small changes in the\nenvironment. Motivated by the success of zero-shot transfer-where pre-trained\nmodels perform well on related tasks-we consider the problem of selecting a\ngood set of training tasks to maximize generalization performance across a\nrange of tasks. Given the high cost of training, it is critical to select\ntraining tasks strategically, but not well understood how to do so. We hence\nintroduce Model-Based Transfer Learning (MBTL), which layers on top of existing\nRL methods to effectively solve contextual RL problems. MBTL models the\ngeneralization performance in two parts: 1) the performance set point, modeled\nusing Gaussian processes, and 2) performance loss (generalization gap), modeled\nas a linear function of contextual similarity. MBTL combines these two pieces\nof information within a Bayesian optimization (BO) framework to strategically\nselect training tasks. We show theoretically that the method exhibits sublinear\nregret in the number of training tasks and discuss conditions to further\ntighten regret bounds. We experimentally validate our methods using urban\ntraffic and standard continuous control benchmarks. The experimental results\nsuggest that MBTL can achieve up to 43x improved sample efficiency compared\nwith canonical independent training and multi-task training. Further\nexperiments demonstrate the efficacy of BO and the insensitivity to the\nunderlying RL algorithm and hyperparameters. This work lays the foundations for\ninvestigating explicit modeling of generalization, thereby enabling principled\nyet effective methods for contextual RL.\n","authors":["Jung-Hoon Cho","Vindula Jayawardana","Sirui Li","Cathy Wu"],"pdf_url":"https://arxiv.org/pdf/2408.04498v3.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)"},{"id":"http://arxiv.org/abs/2403.05811v4","updated":"2025-01-16T03:31:46Z","published":"2024-03-09T06:19:53Z","title":"Statistical Efficiency of Distributional Temporal Difference Learning\n  and Freedman's Inequality in Hilbert Spaces","summary":"  Distributional reinforcement learning (DRL) has achieved empirical success in\nvarious domains. One core task in DRL is distributional policy evaluation,\nwhich involves estimating the return distribution $\\eta^\\pi$ for a given policy\n$\\pi$. Distributional temporal difference learning has been accordingly\nproposed, which extends the classic temporal difference learning (TD) in RL. In\nthis paper, we focus on the non-asymptotic statistical rates of distributional\nTD. To facilitate theoretical analysis, we propose non-parametric\ndistributional TD (NTD). For a $\\gamma$-discounted infinite-horizon tabular\nMarkov decision process, we show that for NTD with a generative model, we need\n$\\tilde{O}(\\varepsilon^{-2}\\mu_{\\min}^{-1}(1-\\gamma)^{-3})$ interactions with\nthe environment to achieve an $\\varepsilon$-optimal estimator with high\nprobability, when the estimation error is measured by the $1$-Wasserstein. This\nsample complexity bound is minimax optimal up to logarithmic factors. In\naddition, we revisit categorical distributional TD (CTD), showing that the same\nnon-asymptotic convergence bounds hold for CTD in the case of the\n$1$-Wasserstein distance. We also extend our analysis to the more general\nsetting where the data generating process is Markovian. In the Markovian\nsetting, we propose variance-reduced variants of NTD and CTD, and show that\nboth can achieve a $\\tilde{O}(\\varepsilon^{-2}\n\\mu_{\\pi,\\min}^{-1}(1-\\gamma)^{-3}+t_{mix}\\mu_{\\pi,\\min}^{-1}(1-\\gamma)^{-1})$\nsample complexity bounds in the case of the $1$-Wasserstein distance, which\nmatches the state-of-the-art statistical results for classic policy evaluation.\nTo achieve the sharp statistical rates, we establish a novel Freedman's\ninequality in Hilbert spaces. This new Freedman's inequality would be of\nindependent interest for statistical analysis of various infinite-dimensional\nonline learning problems.\n","authors":["Yang Peng","Liangyu Zhang","Zhihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05811v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06848v3","updated":"2025-01-16T03:18:14Z","published":"2025-01-12T15:34:24Z","title":"A General Framework for Inference-time Scaling and Steering of Diffusion\n  Models","summary":"  Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we propose Feynman Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models, even\nwith off-the-shelf rewards, can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering .\n","authors":["Raghav Singhal","Zachary Horvitz","Ryan Teehan","Mengye Ren","Zhou Yu","Kathleen McKeown","Rajesh Ranganath"],"pdf_url":"https://arxiv.org/pdf/2501.06848v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09262v1","updated":"2025-01-16T03:11:50Z","published":"2025-01-16T03:11:50Z","title":"On the convergence of noisy Bayesian Optimization with Expected\n  Improvement","summary":"  Expected improvement (EI) is one of the most widely-used acquisition\nfunctions in Bayesian optimization (BO). Despite its proven success in\napplications for decades, important open questions remain on the theoretical\nconvergence behaviors and rates for EI. In this paper, we contribute to the\nconvergence theories of EI in three novel and critical area. First, we consider\nobjective functions that are under the Gaussian process (GP) prior assumption,\nwhereas existing works mostly focus on functions in the reproducing kernel\nHilbert space (RKHS). Second, we establish the first asymptotic error bound and\nits corresponding rate for GP-EI with noisy observations under the GP prior\nassumption. Third, by investigating the exploration and exploitation of the\nnon-convex EI function, we prove improved error bounds for both the noise-free\nand noisy cases. The improved noiseless bound is extended to the RKHS\nassumption as well.\n","authors":["Jingyi Wang","Haowei Wang","Cosmin G. Petra","Nai-Yuan Chiang"],"pdf_url":"https://arxiv.org/pdf/2501.09262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11654v3","updated":"2025-01-16T03:04:10Z","published":"2024-12-16T10:56:58Z","title":"Smoothness Really Matters: A Simple Yet Effective Approach for\n  Unsupervised Graph Domain Adaptation","summary":"  Unsupervised Graph Domain Adaptation (UGDA) seeks to bridge distribution\nshifts between domains by transferring knowledge from labeled source graphs to\ngiven unlabeled target graphs. Existing UGDA methods primarily focus on\naligning features in the latent space learned by graph neural networks (GNNs)\nacross domains, often overlooking structural shifts, resulting in limited\neffectiveness when addressing structurally complex transfer scenarios. Given\nthe sensitivity of GNNs to local structural features, even slight discrepancies\nbetween source and target graphs could lead to significant shifts in node\nembeddings, thereby reducing the effectiveness of knowledge transfer. To\naddress this issue, we introduce a novel approach for UGDA called Target-Domain\nStructural Smoothing (TDSS). TDSS is a simple and effective method designed to\nperform structural smoothing directly on the target graph, thereby mitigating\nstructural distribution shifts and ensuring the consistency of node\nrepresentations. Specifically, by integrating smoothing techniques with\nneighborhood sampling, TDSS maintains the structural coherence of the target\ngraph while mitigating the risk of over-smoothing. Our theoretical analysis\nshows that TDSS effectively reduces target risk by improving model smoothness.\nEmpirical results on three real-world datasets demonstrate that TDSS\noutperforms recent state-of-the-art baselines, achieving significant\nimprovements across six transfer scenarios. The code is available in\nhttps://github.com/cwei01/TDSS.\n","authors":["Wei Chen","Guo Ye","Yakun Wang","Zhao Zhang","Libang Zhang","Daixin Wang","Zhiqiang Zhang","Fuzhen Zhuang"],"pdf_url":"https://arxiv.org/pdf/2412.11654v3.pdf","comment":"11 pages, Accpected by AAAI2025"},{"id":"http://arxiv.org/abs/2404.13885v2","updated":"2025-01-16T02:45:07Z","published":"2024-04-22T05:12:52Z","title":"Surveying Attitudinal Alignment Between Large Language Models Vs. Humans\n  Towards 17 Sustainable Development Goals","summary":"  Large Language Models (LLMs) have emerged as potent tools for advancing the\nUnited Nations' Sustainable Development Goals (SDGs). However, the attitudinal\ndisparities between LLMs and humans towards these goals can pose significant\nchallenges. This study conducts a comprehensive review and analysis of the\nexisting literature on the attitudes of LLMs towards the 17 SDGs, emphasizing\nthe comparison between their attitudes and support for each goal and those of\nhumans. We examine the potential disparities, primarily focusing on aspects\nsuch as understanding and emotions, cultural and regional differences, task\nobjective variations, and factors considered in the decision-making process.\nThese disparities arise from the underrepresentation and imbalance in LLM\ntraining data, historical biases, quality issues, lack of contextual\nunderstanding, and skewed ethical values reflected. The study also investigates\nthe risks and harms that may arise from neglecting the attitudes of LLMs\ntowards the SDGs, including the exacerbation of social inequalities, racial\ndiscrimination, environmental destruction, and resource wastage. To address\nthese challenges, we propose strategies and recommendations to guide and\nregulate the application of LLMs, ensuring their alignment with the principles\nand goals of the SDGs, and therefore creating a more just, inclusive, and\nsustainable future.\n","authors":["Qingyang Wu","Ying Xu","Tingsong Xiao","Yunze Xiao","Yitong Li","Tianyang Wang","Yichi Zhang","Shanghai Zhong","Yuwei Zhang","Wei Lu","Yifan Yang"],"pdf_url":"https://arxiv.org/pdf/2404.13885v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09254v1","updated":"2025-01-16T02:43:44Z","published":"2025-01-16T02:43:44Z","title":"Clone-Robust AI Alignment","summary":"  A key challenge in training Large Language Models (LLMs) is properly aligning\nthem with human preferences. Reinforcement Learning with Human Feedback (RLHF)\nuses pairwise comparisons from human annotators to train reward functions and\nhas emerged as a popular alignment method. However, input datasets in RLHF are\nnot necessarily balanced in the types of questions and answers that are\nincluded. Therefore, we want RLHF algorithms to perform well even when the set\nof alternatives is not uniformly distributed. Drawing on insights from social\nchoice theory, we introduce robustness to approximate clones, a desirable\nproperty of RLHF algorithms which requires that adding near-duplicate\nalternatives does not significantly change the learned reward function. We\nfirst demonstrate that the standard RLHF algorithm based on regularized maximum\nlikelihood estimation (MLE) fails to satisfy this property. We then propose the\nweighted MLE, a new RLHF algorithm that modifies the standard regularized MLE\nby weighting alternatives based on their similarity to other alternatives. This\nnew algorithm guarantees robustness to approximate clones while preserving\ndesirable theoretical properties.\n","authors":["Ariel D. Procaccia","Benjamin Schiffer","Shirley Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.09254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13345v2","updated":"2025-01-16T02:37:08Z","published":"2024-05-22T05:04:44Z","title":"Autonomous Algorithm for Training Autonomous Vehicles with Minimal Human\n  Intervention","summary":"  Recent reinforcement learning (RL) algorithms have demonstrated impressive\nresults in simulated driving environments. However, autonomous vehicles trained\nin simulation often struggle to work well in the real world due to the fidelity\ngap between simulated and real-world environments. While directly training\nreal-world autonomous vehicles with RL algorithms is a promising approach to\nbypass the fidelity gap problem, it presents several challenges. One critical\nyet often overlooked challenge is the need to reset a driving environment\nbetween every episode. This reset process demands significant human\nintervention, leading to poor training efficiency in the real world. In this\npaper, we introduce a novel autonomous algorithm that enables off-the-shelf RL\nalgorithms to train autonomous vehicles with minimal human intervention. Our\nalgorithm reduces unnecessary human intervention by aborting episodes to\nprevent unsafe states and identifying informative initial states for subsequent\nepisodes. The key idea behind identifying informative initial states is to\nestimate the expected amount of information that can be obtained from\nunder-explored but reachable states. Our algorithm also revisits rule-based\nautonomous driving algorithms and highlights their benefits in safely returning\nan autonomous vehicle to initial states. To evaluate how much human\nintervention is required during training, we implement challenging urban\ndriving tasks that require an autonomous vehicle to reset to initial states on\nits own. The experimental results show that our autonomous algorithm is\ntask-agnostic and achieves competitive driving performance with much less human\nintervention than baselines.\n","authors":["Sang-Hyun Lee","Daehyeok Kwon","Seung-Woo Seo"],"pdf_url":"https://arxiv.org/pdf/2405.13345v2.pdf","comment":"8 pages, 6 figures, 2 tables, conference"},{"id":"http://arxiv.org/abs/2501.00230v2","updated":"2025-01-16T02:28:47Z","published":"2024-12-31T02:46:29Z","title":"Federated Deep Subspace Clustering","summary":"  This paper introduces FDSC, a private-protected subspace clustering (SC)\napproach with federated learning (FC) schema. In each client, there is a deep\nsubspace clustering network accounting for grouping the isolated data, composed\nof a encode network, a self-expressive layer, and a decode network. FDSC is\nachieved by uploading the encode network to communicate with other clients in\nthe server. Besides, FDSC is also enhanced by preserving the local neighborhood\nrelationship in each client. With the effects of federated learning and\nlocality preservation, the learned data features from the encoder are boosted\nso as to enhance the self-expressiveness learning and result in better\nclustering performance. Experiments test FDSC on public datasets and compare\nwith other clustering methods, demonstrating the effectiveness of FDSC.\n","authors":["Yupei Zhang","Ruojia Feng","Yifei Wang","Xuequn Shang"],"pdf_url":"https://arxiv.org/pdf/2501.00230v2.pdf","comment":"8pages,4 figures, 4 Tables"},{"id":"http://arxiv.org/abs/2501.09240v1","updated":"2025-01-16T01:54:23Z","published":"2025-01-16T01:54:23Z","title":"Task Vectors in In-Context Learning: Emergence, Formation, and Benefit","summary":"  In-context learning is a remarkable capability of transformers, referring to\ntheir ability to adapt to specific tasks based on a short history or context.\nPrevious research has found that task-specific information is locally encoded\nwithin models, though their emergence and functionality remain unclear due to\nopaque pre-training processes. In this work, we investigate the formation of\ntask vectors in a controlled setting, using models trained from scratch on\nsynthetic datasets. Our findings confirm that task vectors naturally emerge\nunder certain conditions, but the tasks may be relatively weakly and/or\nnon-locally encoded within the model. To promote strong task vectors encoded at\na prescribed location within the model, we propose an auxiliary training\nmechanism based on a task vector prompting loss (TVP-loss). This method\neliminates the need to search for task-correlated encodings within the trained\nmodel and demonstrably improves robustness and generalization.\n","authors":["Liu Yang","Ziqian Lin","Kangwook Lee","Dimitris Papailiopoulos","Robert Nowak"],"pdf_url":"https://arxiv.org/pdf/2501.09240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09238v1","updated":"2025-01-16T01:50:34Z","published":"2025-01-16T01:50:34Z","title":"Mono-Forward: Backpropagation-Free Algorithm for Efficient Neural\n  Network Training Harnessing Local Errors","summary":"  Backpropagation is the standard method for achieving state-of-the-art\naccuracy in neural network training, but it often imposes high memory costs and\nlacks biological plausibility. In this paper, we introduce the Mono-Forward\nalgorithm, a purely local layerwise learning method inspired by Hinton's\nForward-Forward framework. Unlike backpropagation, Mono-Forward optimizes each\nlayer solely with locally available information, eliminating the reliance on\nglobal error signals. We evaluated Mono-Forward on multi-layer perceptrons and\nconvolutional neural networks across multiple benchmarks, including MNIST,\nFashion-MNIST, CIFAR-10, and CIFAR-100. The test results show that Mono-Forward\nconsistently matches or surpasses the accuracy of backpropagation across all\ntasks, with significantly reduced and more even memory usage, better\nparallelizability, and a comparable convergence rate.\n","authors":["James Gong","Bruce Li","Waleed Abdulla"],"pdf_url":"https://arxiv.org/pdf/2501.09238v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2405.00846v4","updated":"2025-01-16T01:49:35Z","published":"2024-05-01T20:21:44Z","title":"Gameplay Filters: Robust Zero-Shot Safety through Adversarial\n  Imagination","summary":"  Despite the impressive recent advances in learning-based robot control,\nensuring robustness to out-of-distribution conditions remains an open\nchallenge. Safety filters can, in principle, keep arbitrary control policies\nfrom incurring catastrophic failures by overriding unsafe actions, but existing\nsolutions for complex (e.g., legged) robot dynamics do not span the full motion\nenvelope and instead rely on local, reduced-order models. These filters tend to\noverly restrict agility and can still fail when perturbed away from nominal\nconditions. This paper presents the gameplay filter, a new class of predictive\nsafety filter that continually plays out hypothetical matches between its\nsimulation-trained safety strategy and a virtual adversary co-trained to invoke\nworst-case events and sim-to-real error, and precludes actions that would cause\nfailures down the line. We demonstrate the scalability and robustness of the\napproach with a first-of-its-kind full-order safety filter for (36-D)\nquadrupedal dynamics. Physical experiments on two different quadruped platforms\ndemonstrate the superior zero-shot effectiveness of the gameplay filter under\nlarge perturbations such as tugging and unmodeled terrain. Experiment videos\nand open-source software are available online:\nhttps://saferobotics.org/research/gameplay-filter\n","authors":["Duy P. Nguyen","Kai-Chieh Hsu","Wenhao Yu","Jie Tan","Jaime F. Fisac"],"pdf_url":"https://arxiv.org/pdf/2405.00846v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09229v1","updated":"2025-01-16T01:28:45Z","published":"2025-01-16T01:28:45Z","title":"Tessellated Linear Model for Age Prediction from Voice","summary":"  Voice biometric tasks, such as age estimation require modeling the often\ncomplex relationship between voice features and the biometric variable. While\ndeep learning models can handle such complexity, they typically require large\namounts of accurately labeled data to perform well. Such data are often scarce\nfor biometric tasks such as voice-based age prediction. On the other hand,\nsimpler models like linear regression can work with smaller datasets but often\nfail to generalize to the underlying non-linear patterns present in the data.\nIn this paper we propose the Tessellated Linear Model (TLM), a piecewise linear\napproach that combines the simplicity of linear models with the capacity of\nnon-linear functions. TLM tessellates the feature space into convex regions and\nfits a linear model within each region. We optimize the tessellation and the\nlinear models using a hierarchical greedy partitioning. We evaluated TLM on the\nTIMIT dataset on the task of age prediction from voice, where it outperformed\nstate-of-the-art deep learning models.\n","authors":["Dareen Alharthi","Mahsa Zamani","Bhiksha Raj","Rita Singh"],"pdf_url":"https://arxiv.org/pdf/2501.09229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04061v3","updated":"2025-01-16T01:18:40Z","published":"2024-10-05T07:05:21Z","title":"Enhancing Graph Self-Supervised Learning with Graph Interplay","summary":"  Graph self-supervised learning (GSSL) has emerged as a compelling framework\nfor extracting informative representations from graph-structured data without\nextensive reliance on labeled inputs. In this study, we introduce Graph\nInterplay (GIP), an innovative and versatile approach that significantly\nenhances the performance equipped with various existing GSSL methods. To this\nend, GIP advocates direct graph-level communications by introducing random\ninter-graph edges within standard batches. Against GIP's simplicity, we further\ntheoretically show that \\textsc{GIP} essentially performs a principled manifold\nseparation via combining inter-graph message passing and GSSL, bringing about\nmore structured embedding manifolds and thus benefits a series of downstream\ntasks. Our empirical study demonstrates that GIP surpasses the performance of\nprevailing GSSL methods across multiple benchmarks by significant margins,\nhighlighting its potential as a breakthrough approach. Besides, GIP can be\nreadily integrated into a series of GSSL methods and consistently offers\nadditional performance gain. This advancement not only amplifies the capability\nof GSSL but also potentially sets the stage for a novel graph learning paradigm\nin a broader sense.\n","authors":["Xinjian Zhao","Wei Pang","Xiangru Jian","Yaoyao Xu","Chaolong Ying","Tianshu Yu"],"pdf_url":"https://arxiv.org/pdf/2410.04061v3.pdf","comment":"Due to potential implicit data leakage in our experimental setup,\n  where the pretraining dataset was ordered by default labels, we withdraw this\n  manuscript for further self-examination and rigorous validation"},{"id":"http://arxiv.org/abs/2501.09223v1","updated":"2025-01-16T01:03:56Z","published":"2025-01-16T01:03:56Z","title":"Foundations of Large Language Models","summary":"  This is a book about large language models. As indicated by the title, it\nprimarily focuses on foundational concepts rather than comprehensive coverage\nof all cutting-edge technologies. The book is structured into four main\nchapters, each exploring a key area: pre-training, generative models, prompting\ntechniques, and alignment methods. It is intended for college students,\nprofessionals, and practitioners in natural language processing and related\nfields, and can serve as a reference for anyone interested in large language\nmodels.\n","authors":["Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2501.09223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05526v2","updated":"2025-01-16T00:54:04Z","published":"2024-08-10T11:48:14Z","title":"CryoBench: Diverse and challenging datasets for the heterogeneity\n  problem in cryo-EM","summary":"  Cryo-electron microscopy (cryo-EM) is a powerful technique for determining\nhigh-resolution 3D biomolecular structures from imaging data. Its unique\nability to capture structural variability has spurred the development of\nheterogeneous reconstruction algorithms that can infer distributions of 3D\nstructures from noisy, unlabeled imaging data. Despite the growing number of\nadvanced methods, progress in the field is hindered by the lack of standardized\nbenchmarks with ground truth information and reliable validation metrics. Here,\nwe introduce CryoBench, a suite of datasets, metrics, and benchmarks for\nheterogeneous reconstruction in cryo-EM. CryoBench includes five datasets\nrepresenting different sources of heterogeneity and degrees of difficulty.\nThese include conformational heterogeneity generated from designed motions of\nantibody complexes or sampled from a molecular dynamics simulation, as well as\ncompositional heterogeneity from mixtures of ribosome assembly states or 100\ncommon complexes present in cells. We then analyze state-of-the-art\nheterogeneous reconstruction tools, including neural and non-neural methods,\nassess their sensitivity to noise, and propose new metrics for quantitative\nevaluation. We hope that CryoBench will be a foundational resource for\naccelerating algorithmic development and evaluation in the cryo-EM and machine\nlearning communities. Project page: https://cryobench.cs.princeton.edu.\n","authors":["Minkyu Jeon","Rishwanth Raghu","Miro Astore","Geoffrey Woollard","Ryan Feathers","Alkin Kaz","Sonya M. Hanson","Pilar Cossio","Ellen D. Zhong"],"pdf_url":"https://arxiv.org/pdf/2408.05526v2.pdf","comment":"Accepted by NeurIPS 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2311.12530v4","updated":"2025-01-16T00:53:15Z","published":"2023-11-21T11:21:53Z","title":"An efficient likelihood-free Bayesian inference method based on\n  sequential neural posterior estimation","summary":"  Sequential neural posterior estimation (SNPE) techniques have been recently\nproposed for dealing with simulation-based models with intractable likelihoods.\nUnlike approximate Bayesian computation, SNPE techniques learn the posterior\nfrom sequential simulation using neural network-based conditional density\nestimators by minimizing a specific loss function. The SNPE method proposed by\nLueckmann et al. (2017) used a calibration kernel to boost the sample weights\naround the observed data, resulting in a concentrated loss function. However,\nthe use of calibration kernels may increase the variances of both the empirical\nloss and its gradient, making the training inefficient. To improve the\nstability of SNPE, this paper proposes to use an adaptive calibration kernel\nand several variance reduction techniques. The proposed method greatly speeds\nup the process of training and provides a better approximation of the posterior\nthan the original SNPE method and some existing competitors as confirmed by\nnumerical experiments. We also managed to demonstrate the superiority of the\nproposed method for a high-dimensional model with a real-world dataset.\n","authors":["Yifei Xiong","Xiliang Yang","Sanguo Zhang","Zhijian He"],"pdf_url":"https://arxiv.org/pdf/2311.12530v4.pdf","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2501.09221v1","updated":"2025-01-16T00:45:05Z","published":"2025-01-16T00:45:05Z","title":"Leveraging Scale-aware Representations for improved\n  Concept-Representation Alignment in ViTs","summary":"  Vision Transformers (ViTs) are increasingly being adopted in various\nsensitive vision applications - like medical diagnosis, facial recognition,\netc. To improve the interpretability of such models, many approaches attempt to\nforward-align them with carefully annotated abstract, human-understandable\nsemantic entities - concepts. Concepts provide global rationales to the model\npredictions and can be quickly understood/intervened on by domain experts. Most\ncurrent research focuses on designing model-agnostic, plug-and-play generic\nconcept-based explainability modules that do not incorporate the inner workings\nof foundation models (e.g., inductive biases, scale invariance, etc.) during\ntraining. To alleviate this issue for ViTs, in this paper, we propose a novel\nConcept Representation Alignment Module (CRAM) which learns both scale and\nposition-aware representations from multi-scale feature pyramids and patch\nrepresentations respectively. CRAM further aligns these representations with\nconcept annotations through an attention matrix. The proposed CRAM module\nimproves the predictive performance of ViT architectures and also provides\naccurate and robust concept explanations as demonstrated on five datasets -\nincluding three widely used benchmarks (CUB, Pascal APY, Concept-MNIST) and 2\nreal-world datasets (AWA2, KITS).\n","authors":["Sanchit Sinha","Guangzhi Xiong","Aidong Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.09221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09217v1","updated":"2025-01-16T00:33:01Z","published":"2025-01-16T00:33:01Z","title":"Adaptive Law-Based Transformation (ALT): A Lightweight Feature\n  Representation for Time Series Classification","summary":"  Time series classification (TSC) is fundamental in numerous domains,\nincluding finance, healthcare, and environmental monitoring. However,\ntraditional TSC methods often struggle with the inherent complexity and\nvariability of time series data. Building on our previous work with the linear\nlaw-based transformation (LLT) - which improved classification accuracy by\ntransforming the feature space based on key data patterns - we introduce\nadaptive law-based transformation (ALT). ALT enhances LLT by incorporating\nvariable-length shifted time windows, enabling it to capture distinguishing\npatterns of various lengths and thereby handle complex time series more\neffectively. By mapping features into a linearly separable space, ALT provides\na fast, robust, and transparent solution that achieves state-of-the-art\nperformance with only a few hyperparameters.\n","authors":["Marcell T. Kurbucz","Balázs Hajós","Balázs P. Halmos","Vince Á. Molnár","Antal Jakovác"],"pdf_url":"https://arxiv.org/pdf/2501.09217v1.pdf","comment":"8 pages, 1 figure, 5 tables"}],"Multimedia":[{"id":"http://arxiv.org/abs/2412.07948v2","updated":"2025-01-16T17:56:53Z","published":"2024-12-10T22:22:19Z","title":"Frechet Music Distance: A Metric For Generative Symbolic Music\n  Evaluation","summary":"  In this paper we introduce the Frechet Music Distance (FMD), a novel\nevaluation metric for generative symbolic music models, inspired by the Frechet\nInception Distance (FID) in computer vision and Frechet Audio Distance (FAD) in\ngenerative audio. FMD calculates the distance between distributions of\nreference and generated symbolic music embeddings, capturing abstract musical\nfeatures. We validate FMD across several datasets and models. Results indicate\nthat FMD effectively differentiates model quality, providing a domain-specific\nmetric for evaluating symbolic music generation, and establishing a\nreproducible standard for future research in symbolic music modeling.\n","authors":["Jan Retkowski","Jakub Stępniak","Mateusz Modrzejewski"],"pdf_url":"https://arxiv.org/pdf/2412.07948v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09608v1","updated":"2025-01-16T15:32:41Z","published":"2025-01-16T15:32:41Z","title":"Metric Learning with Progressive Self-Distillation for Audio-Visual\n  Embedding Learning","summary":"  Metric learning projects samples into an embedded space, where similarities\nand dissimilarities are quantified based on their learned representations.\nHowever, existing methods often rely on label-guided representation learning,\nwhere representations of different modalities, such as audio and visual data,\nare aligned based on annotated labels. This approach tends to underutilize\nlatent complex features and potential relationships inherent in the\ndistributions of audio and visual data that are not directly tied to the\nlabels, resulting in suboptimal performance in audio-visual embedding learning.\nTo address this issue, we propose a novel architecture that integrates\ncross-modal triplet loss with progressive self-distillation. Our method\nenhances representation learning by leveraging inherent distributions and\ndynamically refining soft audio-visual alignments -- probabilistic alignments\nbetween audio and visual data that capture the inherent relationships beyond\nexplicit labels. Specifically, the model distills audio-visual\ndistribution-based knowledge from annotated labels in a subset of each batch.\nThis self-distilled knowledge is used t\n","authors":["Donghuo Zeng","Kazushi Ikeda"],"pdf_url":"https://arxiv.org/pdf/2501.09608v1.pdf","comment":"5 pages, 3 figures, 2 tables. Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.09352v1","updated":"2025-01-16T08:04:04Z","published":"2025-01-16T08:04:04Z","title":"PAL: Prompting Analytic Learning with Missing Modality for Multi-Modal\n  Class-Incremental Learning","summary":"  Multi-modal class-incremental learning (MMCIL) seeks to leverage multi-modal\ndata, such as audio-visual and image-text pairs, thereby enabling models to\nlearn continuously across a sequence of tasks while mitigating forgetting.\nWhile existing studies primarily focus on the integration and utilization of\nmulti-modal information for MMCIL, a critical challenge remains: the issue of\nmissing modalities during incremental learning phases. This oversight can\nexacerbate severe forgetting and significantly impair model performance. To\nbridge this gap, we propose PAL, a novel exemplar-free framework tailored to\nMMCIL under missing-modality scenarios. Concretely, we devise modality-specific\nprompts to compensate for missing information, facilitating the model to\nmaintain a holistic representation of the data. On this foundation, we\nreformulate the MMCIL problem into a Recursive Least-Squares task, delivering\nan analytical linear solution. Building upon these, PAL not only alleviates the\ninherent under-fitting limitation in analytic learning but also preserves the\nholistic representation of missing-modality data, achieving superior\nperformance with less forgetting across various multi-modal incremental\nscenarios. Extensive experiments demonstrate that PAL significantly outperforms\ncompetitive methods across various datasets, including UPMC-Food101 and\nN24News, showcasing its robustness towards modality absence and its\nanti-forgetting ability to maintain high incremental accuracy.\n","authors":["Xianghu Yue","Yiming Chen","Xueyi Zhang","Xiaoxue Gao","Mengling Feng","Mingrui Lao","Huiping Zhuang","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2501.09352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09291v1","updated":"2025-01-16T04:53:29Z","published":"2025-01-16T04:53:29Z","title":"LAVCap: LLM-based Audio-Visual Captioning using Optimal Transport","summary":"  Automated audio captioning is a task that generates textual descriptions for\naudio content, and recent studies have explored using visual information to\nenhance captioning quality. However, current methods often fail to effectively\nfuse audio and visual data, missing important semantic cues from each modality.\nTo address this, we introduce LAVCap, a large language model (LLM)-based\naudio-visual captioning framework that effectively integrates visual\ninformation with audio to improve audio captioning performance. LAVCap employs\nan optimal transport-based alignment loss to bridge the modality gap between\naudio and visual features, enabling more effective semantic extraction.\nAdditionally, we propose an optimal transport attention module that enhances\naudio-visual fusion using an optimal transport assignment map. Combined with\nthe optimal training strategy, experimental results demonstrate that each\ncomponent of our framework is effective. LAVCap outperforms existing\nstate-of-the-art methods on the AudioCaps dataset, without relying on large\ndatasets or post-processing. Code is available at\nhttps://github.com/NAVER-INTEL-Co-Lab/gaudi-lavcap.\n","authors":["Kyeongha Rho","Hyeongkeun Lee","Valentio Iverson","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2501.09291v1.pdf","comment":"5 pages, 2 figures; Accepted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.09275v1","updated":"2025-01-16T03:44:28Z","published":"2025-01-16T03:44:28Z","title":"MagnetDB: A Longitudinal Torrent Discovery Dataset with IMDb-Matched\n  Movies and TV Shows","summary":"  BitTorrent remains a prominent channel for illicit distribution of\ncopyrighted material, yet the supply side of such content remains understudied.\nWe introduce MagnetDB, a longitudinal dataset of torrents discovered through\nthe BitTorrent DHT between 2018 and 2024, containing more than 28.6 million\ntorrents and metadata of more than 950 million files. While our primary focus\nis on enabling research based on the supply of pirated movies and TV shows, the\ndataset also encompasses other legitimate and illegitimate torrents. By\napplying IMDb-matching and annotation to movie and TV show torrents, MagnetDB\nfacilitates detailed analyses of pirated content evolution in the BitTorrent\nnetwork. Researchers can leverage MagnetDB to examine distribution trends,\nsubcultural practices, and the gift economy within piracy ecosystems. Through\nits scale and temporal scope, MagnetDB presents a unique opportunity for\ninvestigating the broader dynamics of BitTorrent and advancing empirical\nknowledge on digital piracy.\n","authors":["Scott Seidenberger","Noah Pursell","Anindya Maiti"],"pdf_url":"https://arxiv.org/pdf/2501.09275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13397v6","updated":"2025-01-16T02:48:38Z","published":"2023-03-23T16:15:18Z","title":"DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery\n  from Videos","summary":"  Human mesh recovery (HMR) provides rich human body information for various\nreal-world applications. While image-based HMR methods have achieved impressive\nresults, they often struggle to recover humans in dynamic scenarios, leading to\ntemporal inconsistencies and non-smooth 3D motion predictions due to the\nabsence of human motion. In contrast, video-based approaches leverage temporal\ninformation to mitigate this issue. In this paper, we present DiffMesh, an\ninnovative motion-aware Diffusion-like framework for video-based HMR. DiffMesh\nestablishes a bridge between diffusion models and human motion, efficiently\ngenerating accurate and smooth output mesh sequences by incorporating human\nmotion within the forward process and reverse process in the diffusion model.\nExtensive experiments are conducted on the widely used datasets (Human3.6M\n\\cite{h36m_pami} and 3DPW \\cite{pw3d2018}), which demonstrate the effectiveness\nand efficiency of our DiffMesh. Visual comparisons in real-world scenarios\nfurther highlight DiffMesh's suitability for practical applications.\n","authors":["Ce Zheng","Xianpeng Liu","Qucheng Peng","Tianfu Wu","Pu Wang","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13397v6.pdf","comment":"WACV 2025"}]}}